---
- :id: ruby-dispatch-pairing-with-gpt-4
  :date: '2023-03-23'
  :category: ruby-dispatch
  :title: Pairing with GPT-4
  :author: brad
  :thumbnail: gpt-4-thumbnail.webp
  :alt:
  :link: ruby-dispatch/pairing-with-gpt-4
  :path: ruby-dispatch/2023-03-23
  :body: "\n\n**GPT-4 can be helpful for beginner and senior Ruby developers, but
    it does have limitations. It won't write all of your software for you, but it
    will point you in a useful direction, especially if you prefer learning by doing.**\n\nLet's
    look at how well GPT-4 pairing works by picking an easy, but less well known project
    with some edge cases: downloading a RubyGem, parsing the docs via YARN, and dumping
    them into a Sqlite database.\n\n## The initial prompt\n\n> Create a Ruby script
    for me that downloads a RubyGem, parses the docs via YARD, and dumps them into
    a Sqlite database that's named `#{gem-name}-#{gem-version}.db`. The database should
    also include information from the Gem spec.\n\nGPT-4 usually starts its responses
    with an overview of what it's about to do, then shows how it solves the problem,
    followed by recapping and summing up the solution.\n\n### GPT-4 response\n\nTo
    create a Ruby script that downloads a RubyGem, parses the documentation using
    YARD, and stores the data in a SQLite database named with the gem name and version,
    you will need to follow these steps:\n\n1. Install the required gems:\n    - `gem
    install yard`\n    - `gem install sqlite3`\n    - `gem install bundler`\n1. Create
    a new file named `gem_doc_parser.rb` and paste the following code:\n\n```ruby\nrequire
    'yard'\nrequire 'sqlite3'\nrequire 'bundler'\n\ndef download_and_parse_gem(gem_name,
    gem_version)\n  gem_path = \"#{gem_name}-#{gem_version}\"\n\n  # Download the
    gem\n  Bundler::CLI.start([\"gem\", gem_name, \"--version\", gem_version])\n\n
    \ # Parse the gem documentation using YARD\n  YARD::Registry.clear\n  YARD::Parser::SourceParser.parse(gem_path)\nend\n\ndef
    create_database(gem_name, gem_version)\n  db = SQLite3::Database.new(\"#{gem_name}-#{gem_version}.db\")\n\n
    \ # Create tables for storing gem spec and documentation\n  db.execute <<-SQL\n
    \   CREATE TABLE IF NOT EXISTS gem_spec (\n      id INTEGER PRIMARY KEY,\n      name
    TEXT,\n      version TEXT\n    );\n  SQL\n\n  db.execute <<-SQL\n    CREATE TABLE
    IF NOT EXISTS gem_docs (\n      id INTEGER PRIMARY KEY,\n      path TEXT,\n      type
    TEXT,\n      doc TEXT\n    );\n  SQL\n\n  db\nend\n\ndef store_gem_spec(db, gem_name,
    gem_version)\n  db.execute(\"INSERT INTO gem_spec (name, version) VALUES (?, ?)\",
    [gem_name, gem_version])\nend\n\ndef store_docs(db)\n  YARD::Registry.all.each
    do |object|\n    path = object.path\n    type = object.type\n    doc = object.docstring.to_raw\n\n
    \   db.execute(\"INSERT INTO gem_docs (path, type, doc) VALUES (?, ?, ?)\", [path,
    type, doc])\n  end\nend\n\ndef main\n  puts \"Enter the gem name:\"\n  gem_name
    = gets.chomp\n\n  puts \"Enter the gem version:\"\n  gem_version = gets.chomp\n\n
    \ download_and_parse_gem(gem_name, gem_version)\n\n  db = create_database(gem_name,
    gem_version)\n  store_gem_spec(db, gem_name, gem_version)\n  store_docs(db)\n\n
    \ puts \"Documentation has been stored in #{gem_name}-#{gem_version}.db\"\nend\n\nmain\n```\n\nThis
    script will perform the following tasks:\n\n1. Download and parse the gem documentation
    using YARD\n1. Create a SQLite database named with the gem name and version\n1.
    Create tables for storing gem spec and documentation\n1. Store the gem spec (name
    and version) in the database\n1. Store the documentation in the database\n\nTo
    run the script, simply execute `ruby gem_doc_parser.rb` and follow the prompts
    to enter the gem name and version. The documentation will be stored in the specified
    SQLite database.\n\n## Let's see if it works!\n\nWe have GPT-4's solution. Now
    its time to copy the code above into `gem_doc_parser.rb` and run it.\n\n```cmd\nruby
    gem_doc_parser.rb\n```\n```output\nIgnoring rbs-2.8.2 because its extensions are
    not built. Try: gem pristine rbs --version 2.8.2\nEnter the gem name:\nrake\nEnter
    the gem version:\n13.0.6\ngem_doc_parser.rb:9:in `download_and_parse_gem': uninitialized
    constant Bundler::CLI (NameError)\n\n  Bundler::CLI.start([\"gem\", gem_name,
    \"--version\", gem_version])\n         ^^^^^\n  from gem_doc_parser.rb:61:in `main'\n
    \ from gem_doc_parser.rb:70:in `<main>'\n\n```\n\nUh oh! We already run into our
    first error. Let's paste that output into GPT-4 and see what it tells us.\n\n>
    That didn't work! I got this error…\n\n### GPT-4 response\n\nI apologize for the
    confusion. I misunderstood your initial request. You want to download and parse
    a specific RubyGem, not create a new gem.\n\nPlease follow these steps to update
    the script:\n\n1. Replace the `download_and_parse_gem` method with the following:\n\n```ruby\ndef
    download_and_parse_gem(gem_name, gem_version)\n  gem_path = Bundler::CLI::Common.select_spec(gem_name,
    gem_version).full_gem_path\n\n  # Parse the gem documentation using YARD\n  YARD::Registry.clear\n
    \ YARD::Parser::SourceParser.parse(gem_path)\nend\n```\n\n1. Install the `bundler`
    gem by running:\n\n```\ngem install bundler\n```\n\n1. Update your Gemfile to
    include the gem you want to download and parse:\n\n```\nsource 'https://rubygems.org'\n\ngem
    'rake', '13.0.6'\n```\n\n1. Install the specified gem with Bundler:\n\n```\nbundle
    install\n```\n\nAfter making these updates, you should be able to run the script
    with `ruby gem_doc_parser.rb`. The script will parse the documentation for the
    gem specified in the Gemfile and store it in a SQLite database.\n\n## Will GPT-4's
    fix work?\n\nOk, let's do this one more time just for fun to see if it runs.\n\n```ruby\nIgnoring
    rbs-2.8.2 because its extensions are not built. Try: gem pristine rbs --version
    2.8.2\nEnter the gem name:\nrake\nEnter the gem version:\n13.0.6\ngem_doc_parser.rb:6:in
    `download_and_parse_gem': uninitialized constant Bundler::CLI (NameError)\n\n
    \ gem_path = Bundler::CLI::Common.select_spec(gem_name, gem_version).full_gem_path\n
    \                        ^^^^^^^^\n  from gem_doc_parser.rb:58:in `main'\n  from
    gem_doc_parser.rb:67:in `<main>'\n```\n\nNope!\n\nThis is where GPT-4 can be problematic
    for people who might be new to software development that are trying to solve specific
    problems right away. If you're using it this way, you'll find that GPT-4 will
    give you a plausible answer, but it usually won't work on the first try.\n\nThat's
    OK though, let's get more specific about how we want this to run.\n\n## More experience
    helps GPT-4 give better results\n\nIf you've built applications before, you can
    give GPT-4 a more specific prompt and you'll get back an answer that's more inline
    with what you want.\n\n> That doesn't work either. Let's try this instead: instead
    of using Bundler to install the gem, just download the .gem file directly from
    RubyGems, save it to a temporary path, then have Yard document that one gem and
    dump it into a sqlite file.\n>\n>  Put all of this into one class that I can call
    from Ruby code. Then let's call that class from the CLI so that looks something
    like this:\n>\n> $ ruby gem-to-sqlite rake 13.0.6\n\nThis type of prompt is something
    that feels more like an experienced developer talking to a developer who might
    just be starting out. It gives us back some more reasonable code.\n\n```ruby\nrequire
    'yard'\nrequire 'sqlite3'\nrequire 'net/http'\nrequire 'tmpdir'\nrequire 'rubygems/package'\n\nclass
    GemToSqlite\n  def initialize(gem_name, gem_version)\n    @gem_name = gem_name\n
    \   @gem_version = gem_version\n  end\n\n  def download_gem\n    gem_url = \"https://rubygems.org/downloads/#{@gem_name}-#{@gem_version}.gem\"\n
    \   uri = URI(gem_url)\n    file_path = File.join(Dir.tmpdir, \"#{@gem_name}-#{@gem_version}.gem\")\n\n
    \   File.write(file_path, Net::HTTP.get(uri))\n    file_path\n  end\n\n  def extract_gem(file_path)\n
    \   spec = nil\n    Gem::Package.new(file_path).spec.files.each do |file|\n      YARD::Registry.clear\n
    \     YARD::Parser::SourceParser.parse_string(file)\n      spec = Gem::Package.new(file_path).spec\n
    \   end\n    spec\n  end\n\n  def create_database\n    db = SQLite3::Database.new(\"#{@gem_name}-#{@gem_version}.db\")\n\n
    \   db.execute <<-SQL\n      CREATE TABLE IF NOT EXISTS gem_spec (\n        id
    INTEGER PRIMARY KEY,\n        name TEXT,\n        version TEXT\n      );\n    SQL\n\n
    \   db.execute <<-SQL\n      CREATE TABLE IF NOT EXISTS gem_docs (\n        id
    INTEGER PRIMARY KEY,\n        path TEXT,\n        type TEXT,\n        doc TEXT\n
    \     );\n    SQL\n\n    db\n  end\n\n  def store_gem_spec(db, spec)\n    db.execute(\"INSERT
    INTO gem_spec (name, version) VALUES (?, ?)\", [@gem_name, @gem_version])\n  end\n\n
    \ def store_docs(db)\n    YARD::Registry.all.each do |object|\n      path = object.path\n
    \     type = object.type\n      doc = object.docstring.to_raw\n\n      db.execute(\"INSERT
    INTO gem_docs (path, type, doc) VALUES (?, ?, ?)\", [path, type, doc])\n    end\n
    \ end\n\n  def run\n    gem_path = download_gem\n    spec = extract_gem(gem_path)\n\n
    \   db = create_database\n    store_gem_spec(db, spec)\n    store_docs(db)\n\n
    \   puts \"Documentation has been stored in #{@gem_name}-#{@gem_version}.db\"\n
    \ end\nend\n```\n\nWhen I run it I get another error:\n\n```\nfly-apps/gpt4 →
    ruby gem_to_sqlite.rb rake 13.0.6\nIgnoring rbs-2.8.2 because its extensions are
    not built. Try: gem pristine rbs --version 2.8.2\n[warn]: Syntax error in `(stdin)`:(1,8):
    no .<digit> floating literal anymore; put 0 before dot\n[warn]: ParserSyntaxError:
    syntax error in `(stdin)`:(1,8): no .<digit> floating literal anymore; put 0 before
    dot\n[warn]: Stack trace:\n  /Users/bradgessler/.rbenv/versions/3.2.0/lib/ruby/gems/3.2.0/gems/yard-0.9.28/lib/yard/parser/ruby/ruby_parser.rb:601:in
    `on_parse_error'\n  /Users/bradgessler/.rbenv/versions/3.2.0/lib/ruby/gems/3.2.0/gems/yard-0.9.28/lib/yard/parser/ruby/ruby_parser.rb:56:in
    `parse'\n  /Users/bradgessler/.rbenv/versions/3.2.0/lib/ruby/gems/3.2.0/gems/yard-0.9.28/lib/yard/parser/ruby/ruby_parser.rb:56:in
    `parse'\n  /Users/bradgessler/.rbenv/versions/3.2.0/lib/ruby/gems/3.2.0/gems/yard-0.9.28/lib/yard/parser/ruby/ruby_parser.rb:17:in
    `parse'\n  /Users/bradgessler/.rbenv/versions/3.2.0/lib/ruby/gems/3.2.0/gems/yard-0.9.28/lib/yard/parser/source_parser.rb:442:in
    `parse'\n  /Users/bradgessler/.rbenv/versions/3.2.0/lib/ruby/gems/3.2.0/gems/yard-0.9.28/lib/yard/parser/source_parser.rb:124:in
    `parse_string'\n```\n\nIt gives me back this fix:\n\n### GPT-4 response\n\nApologies
    for the issues you've encountered. It appears the error is related to the YARD
    parser when parsing the gem. I've updated the code to address this issue.\n\nUpdate
    the `extract_gem` method in the `gem_to_sqlite.rb` file with the following code:\n\n```ruby\ndef
    extract_gem(file_path)\n  spec = nil\n  gem_dir = File.join(Dir.tmpdir, \"#{@gem_name}-#{@gem_version}\")\n\n
    \ # Extract the gem to a temporary directory\n  Gem::Package.new(file_path).extract_files(gem_dir)\n
    \ spec = Gem::Package.new(file_path).spec\n\n  # Parse the gem documentation using
    YARD\n  YARD::Registry.clear\n  Dir.glob(File.join(gem_dir, '**', '*.rb')).each
    do |file|\n    YARD::Parser::SourceParser.parse(file)\n  end\n\n  spec\nend\n```\n\n##
    An aside: inline the Gem bundle\n\nI also remembered that I want to share this
    code for this post, so I asked GPT-4 to move the Gems into an inline bundler.\n\n>
    Could you move the require statements up top into an inline bundler block?\n\n###
    GPT-4 response\n\nSure! You can use an inline Bundler block to manage gem dependencies.
    Update the `gem_to_sqlite.rb` file with the following code:\n\n```\nrequire 'bundler/inline'\n\ngemfile
    do\n  source 'https://rubygems.org'\n  gem 'yard', '~> 0.9.28'\n  gem 'sqlite3',
    '~> 1.4'\nend\n\nrequire 'yard'\nrequire 'sqlite3'\nrequire 'net/http'\nrequire
    'tmpdir'\nrequire 'rubygems/package'\n\n## ... (rest of the GemToSqlite class
    definition)\n```\n\nIt's impressive that GPT-4 is able to give me a reasonable
    response for an \"aside\". Let's see if that distracted it because I ran into
    another bug running this.\n\n## Another bug\n\n> I ran into another bug! Check
    it out:\n\n```bash\nfly-apps/gpt4 → ruby gem_to_sqlite-2.rb\n[warn]: in YARD::Handlers::Ruby::VisibilityHandler:
    Undocumentable statement, cannot determine method name\n  in file '/var/folders/_n/58_fmw2j26sc63pr7lcvcyfh0000gn/T/rake-13.0.6/lib/rake/dsl_definition.rb':23:\n\n
    \ 23: private(*FileUtils.instance_methods(false))\n\n[warn]: in YARD::Handlers::Ruby::VisibilityHandler:
    Undocumentable statement, cannot determine method name\n  in file '/var/folders/_n/58_fmw2j26sc63pr7lcvcyfh0000gn/T/rake-13.0.6/lib/rake/dsl_definition.rb':24:\n\n
    \ 24: private(*FileUtilsExt.instance_methods(false))\n\n/Users/bradgessler/.rbenv/versions/3.2.0/lib/ruby/gems/3.2.0/gems/sqlite3-1.6.1-arm64-darwin/lib/sqlite3/statement.rb:41:in
    `bind_param': can't prepare Symbol (RuntimeError)\n  from /Users/bradgessler/.rbenv/versions/3.2.0/lib/ruby/gems/3.2.0/gems/sqlite3-1.6.1-arm64-darwin/lib/sqlite3/statement.rb:41:in
    `block in bind_params'\n```\n\n### GPT-4 response\n\nI apologize for the oversight.
    It seems that the error is related to the insertion of data into the SQLite database.
    The issue comes from the YARD object type being a Symbol, which SQLite doesn't
    support. To fix this, we need to convert the object type to a String before inserting
    it into the database.\n\nUpdate the `store_docs` method in the `gem_to_sqlite.rb`
    file with the following code:\n\n```\ndef store_docs(db)\n  YARD::Registry.all.each
    do |object|\n    path = object.path\n    type = object.type.to_s\n    doc = object.docstring.to_raw\n\n
    \   db.execute(\"INSERT INTO gem_docs (path, type, doc) VALUES (?, ?, ?)\", [path,
    type, doc])\n  end\nend\n```\n\nThe only change made is converting the object
    type to a string using `to_s`:\n\n```\ntype = object.type.to_s\n```\n\nWith this
    change, you should be able to run the script without any issues:\n\n```\n./gem-to-sqlite
    rake 13.0.6\n```\n\nLet's try it again!\n\n```bash\nfly-apps/gpt4 → ruby gem_to_sqlite-2.rb\n[warn]:
    in YARD::Handlers::Ruby::VisibilityHandler: Undocumentable statement, cannot determine
    method name\n  in file '/var/folders/_n/58_fmw2j26sc63pr7lcvcyfh0000gn/T/rake-13.0.6/lib/rake/dsl_definition.rb':23:\n\n
    \ 23: private(*FileUtils.instance_methods(false))\n\n[warn]: in YARD::Handlers::Ruby::VisibilityHandler:
    Undocumentable statement, cannot determine method name\n  in file '/var/folders/_n/58_fmw2j26sc63pr7lcvcyfh0000gn/T/rake-13.0.6/lib/rake/dsl_definition.rb':24:\n\n
    \ 24: private(*FileUtilsExt.instance_methods(false))\n\nDocumentation has been
    stored in rake-13.0.6.db\n```\n\nSuccess! Let's open up the sqlite database and
    make sure it has what we expect.\n\n```\nsqlite> select * from gem_spec;\n1|rake|13.0.6\nsqlite>
    select * from gem_docs;\n1|RDoc|module|\n2|RDoc::Page|module|\n3|RDoc::Page::FONTS|constant|\n4|RDoc::Page::STYLE|constant|\n5|RDoc::Page::XHTML_PREAMBLE|constant|\n6|RDoc::Page::HEADER|constant|\n7|RDoc::Page::FILE_PAGE|constant|\n8|RDoc::Page::CLASS_PAGE|constant|\n9|RDoc::Page::METHOD_LIST|constant|\n10|RDoc::Page::FOOTER|constant|\n11|RDoc::Page::BODY|constant|\n12|RDoc::Page::SRC_PAGE|constant|Source
    code ##########################\n13|RDoc::Page::FR_INDEX_BODY|constant|Index ################################\n14|RDoc::Page::FILE_INDEX|constant|\n15|RDoc::Page::CLASS_INDEX|constant|\n16|RDoc::Page::METHOD_INDEX|constant|\n17|RDoc::Page::INDEX|constant|\n18|Rake|module|--\nCopyright
    2003-2010 by Jim Weirich (jim.weirich@gmail.com)\n\nPermission is hereby granted,
    free of charge, to any person obtaining a copy\nof this software and associated
    documentation files (the \"Software\"), to\ndeal in the Software without restriction,
    including without limitation the\nrights to use, copy, modify, merge, publish,
    distribute, sublicense, and/or\nsell copies of the Software, and to permit persons
    to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe
    above copyright notice and this permission notice shall be included in\nall copies
    or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\",
    WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO
    THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
    IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES
    OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\nFROM,
    OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS\nIN THE
    SOFTWARE.\n++\n19|Rake::CommandLineOptionError|constant|\n```\n\nNot bad! We can
    see our gem name and version in the `gem_spec` table and all of the docs in `gem_docs`.\n\n##
    Let's have GPT-4 clean this up for a demo\n\nSince I need to bundle all of this
    up in a way that I can share it, I'm going to ask GPT-4 to do the work.\n\n> It
    works! I need to get all of this into one file so that I can share it via a Github
    Gist. Could you consolidate into one file, remove any unnecessary code, and include
    in code comments up top how to run the script? Make sure its a `.rb` file and
    let's tell people to run it via `ruby gem-to-sqlite.rb rake 13.0.6` (for example)\n\n###
    GPT-4 response\n\nSure! Here's the consolidated version of the script as a single
    file named `gem-to-sqlite.rb`. I've also added comments at the beginning of the
    file with instructions on how to run it.\n\n```ruby\n# gem-to-sqlite.rb\n#\n#
    Usage: ruby gem-to-sqlite.rb <gem_name> <gem_version>\n# Example: ruby gem-to-sqlite.rb
    rake 13.0.6\n\nrequire 'bundler/inline'\n\ngemfile do\n  source 'https://rubygems.org'\n
    \ gem 'yard', '~> 0.9.28'\n  gem 'sqlite3', '~> 1.4'\nend\n\nrequire 'yard'\nrequire
    'sqlite3'\nrequire 'net/http'\nrequire 'tmpdir'\nrequire 'rubygems/package'\n\nclass
    GemToSqlite\n  def initialize(gem_name, gem_version)\n    @gem_name = gem_name\n
    \   @gem_version = gem_version\n  end\n\n  def download_gem\n    gem_url = \"https://rubygems.org/downloads/#{@gem_name}-#{@gem_version}.gem\"\n
    \   uri = URI(gem_url)\n    file_path = File.join(Dir.tmpdir, \"#{@gem_name}-#{@gem_version}.gem\")\n\n
    \   File.write(file_path, Net::HTTP.get(uri))\n    file_path\n  end\n\n  def extract_gem(file_path)\n
    \   spec = nil\n    gem_dir = File.join(Dir.tmpdir, \"#{@gem_name}-#{@gem_version}\")\n\n
    \   # Extract the gem to a temporary directory\n    Gem::Package.new(file_path).extract_files(gem_dir)\n
    \   spec = Gem::Package.new(file_path).spec\n\n    # Parse the gem documentation
    using YARD\n    YARD::Registry.clear\n    Dir.glob(File.join(gem_dir, '**', '*.rb')).each
    do |file|\n      YARD::Parser::SourceParser.parse(file)\n    end\n\n    spec\n
    \ end\n\n  def create_database\n    db = SQLite3::Database.new(\"#{@gem_name}-#{@gem_version}.db\")\n\n
    \   db.execute <<-SQL\n      CREATE TABLE IF NOT EXISTS gem_spec (\n        id
    INTEGER PRIMARY KEY,\n        name TEXT,\n        version TEXT\n      );\n    SQL\n\n
    \   db.execute <<-SQL\n      CREATE TABLE IF NOT EXISTS gem_docs (\n        id
    INTEGER PRIMARY KEY,\n        path TEXT,\n        type TEXT,\n        doc TEXT\n
    \     );\n    SQL\n\n    db\n  end\n\n  def store_gem_spec(db, spec)\n    db.execute(\"INSERT
    INTO gem_spec (name, version) VALUES (?, ?)\", [@gem_name, @gem_version])\n  end\n\n
    \ def store_docs(db)\n    YARD::Registry.all.each do |object|\n      path = object.path\n
    \     type = object.type.to_s\n      doc = object.docstring.to_raw\n\n      db.execute(\"INSERT
    INTO gem_docs (path, type, doc) VALUES (?, ?, ?)\", [path, type, doc])\n    end\n
    \ end\n\n  def run\n    gem_path = download_gem\n    spec = extract_gem(gem_path)\n\n
    \   db = create_database\n    store_gem_spec(db, spec)\n    store_docs(db)\n\n
    \   puts \"Documentation has been stored in #{@gem_name}-#{@gem_version}.db\"\n
    \ end\nend\n\nif ARGV.length != 2\n  puts \"Usage: ruby gem-to-sqlite.rb <gem_name>
    <gem_version>\"\n  exit 1\nend\n\ngem_name = ARGV[0]\ngem_version = ARGV[1]\n\ngem_to_sqlite
    = GemToSqlite.new(gem_name, gem_version)\ngem_to_sqlite.run\n```\n\nThat's pretty
    darn good, and the [gist is available at Github](https://gist.github.com/bradgessler/02b201c1cc94c24836195db762ada02d).\n\n##
    GPT-4 likes to fake it until it makes it (or you lose your patience)\n\nNow I'm
    going to ask it a question that will probably break it using questionably imprecise
    language:\n\n> How could I use an instance of `YARD::Registry` instead of the
    `.all` singleton?\n\n### GPT-4 response\n\nYou can create a new instance of `YARD::Registry`
    and use it to parse and store the documentation. First, create an instance variable
    `@registry` in the `initialize` method:\n\n```ruby\ndef initialize(gem_name, gem_version)\n
    \ @gem_name = gem_name\n  @gem_version = gem_version\n  @registry = YARD::Registry.new\nend\n```\n\n---\n\nBzzt
    wrong! `YARD::Registry` is a module. GPT-4 doesn't necessarily know what the actual
    Ruby code is behind what it's telling you—it seems to be matching patterns on
    something that seems like it should be a class, but it's not.\n\n## Let's make
    a version for squirrels \U0001F43F️\n\nTime to get silly! We made this script
    for squirrels, so let's ask GPT to rename a few things to make it more undqerstable
    to squirrels.\n\n> Let's rename everything to something that would make a squirrel
    really happy\n\n```ruby\n# squirrel_gem_nut_collector.rb\n#\n# Usage: ruby squirrel_gem_nut_collector.rb
    <gem_name> <gem_version>\n# Example: ruby squirrel_gem_nut_collector.rb rake 13.0.6\n\nrequire
    'bundler/inline'\n\ngemfile do\n  source 'https://rubygems.org'\n  gem 'yard',
    '~> 0.9.28'\n  gem 'sqlite3', '~> 1.4'\nend\n\nrequire 'yard'\nrequire 'sqlite3'\nrequire
    'net/http'\nrequire 'tmpdir'\nrequire 'rubygems/package'\n\nclass SquirrelGemNutCollector\n
    \ def initialize(nutty_gem_name, crunchy_gem_version)\n    @nutty_gem_name = nutty_gem_name\n
    \   @crunchy_gem_version = crunchy_gem_version\n    @nut_registry = YARD::Registry.new\n
    \ end\n\n  def nut_fetcher\n    nut_url = \"https://rubygems.org/downloads/#{@nutty_gem_name}-#{@crunchy_gem_version}.gem\"\n
    \   uri = URI(nut_url)\n    temp_nut_path = File.join(Dir.tmpdir, \"#{@nutty_gem_name}-#{@crunchy_gem_version}.gem\")\n\n
    \   File.write(temp_nut_path, Net::HTTP.get(uri))\n    temp_nut_path\n  end\n\n
    \ def crack_nut(file_path)\n    nut_spec = nil\n    cracked_nut_dir = File.join(Dir.tmpdir,
    \"#{@nutty_gem_name}-#{@crunchy_gem_version}\")\n\n    # Crack open the gem nut
    and release the code\n    Gem::Package.new(file_path).extract_files(cracked_nut_dir)\n
    \   nut_spec = Gem::Package.new(file_path).spec\n\n    # Parse the code with the
    power of YARD\n    @nut_registry.clear\n    Dir.glob(File.join(cracked_nut_dir,
    '**', '*.rb')).each do |file|\n      YARD::Parser::SourceParser.parse(file, @nut_registry)\n
    \   end\n\n    nut_spec\n  end\n\n  def nut_storage\n    nut_db = SQLite3::Database.new(\"#{@nutty_gem_name}-#{@crunchy_gem_version}.db\")\n\n
    \   nut_db.execute <<-SQL\n      CREATE TABLE IF NOT EXISTS nut_specs (\n        id
    INTEGER PRIMARY KEY,\n        name TEXT,\n        version TEXT\n      );\n    SQL\n\n
    \   nut_db.execute <<-SQL\n      CREATE TABLE IF NOT EXISTS nut_docs (\n        id
    INTEGER PRIMARY KEY,\n        path TEXT,\n        type TEXT,\n        doc TEXT\n
    \     );\n    SQL\n\n    nut_db\n  end\n\n  def store_nut_specs(nut_db, spec)\n
    \   nut_db.execute(\"INSERT INTO nut_specs (name, version) VALUES (?, ?)\", [@nutty_gem_name,
    @crunchy_gem_version])\n  end\n\n  def fill_nut_storage_with_magic(nut_db)\n    @nut_registry.all.each
    do |object|\n      nutty_path = object.path\n      crunchy_type = object.type.to_s\n
    \     docs_of_wisdom = object.docstring.to_raw\n\n      nut_db.execute(\"INSERT
    INTO nut_docs (path, type, doc) VALUES (?, ?, ?)\", [nutty_path, crunchy_type,
    docs_of_wisdom])\n    end\n  end\n\n  def collect_all_the_nuts\n    path_to_nut
    = nut_fetcher\n    nut_spec = crack_nut(path_to_nut)\n\n    nut_db = nut_storage\n
    \   store_nut_specs(nut_db, nut_spec)\n    fill_nut_storage_with_magic(nut_db)\n
    \ end\nend\n\nif ARGV.size\n```\n\n\U0001F606\n\nI'll spare you these prompts,
    but I can assure you they were hilarious.\n\n- How about for a old timey prospector
    from the gold rush?\n- Could you give me a cool hip hop version?\n- Do one for
    using words a 4 year old would understand\n\n## Overall GPT-4 is a useful tool\n\nTo
    sum it up, here's what you should keep in mind when you add GPT-4 to your set
    of tools:\n\n### You still need to know what you're doing\n\nIf you're a senior
    developer, giving GPT-4 more specific prompts results in better responses. It
    can reason with you through architectural problems or write you code samples;
    however, many times it doesn't know the right answer but confidently tells you
    something anyway.\n\nIf you're a beginner, GPT-4 will give you responses that
    you might be able to fix and get running, but it's not the most concise way of
    doing it. Often times though the code won't work and you might not know how to
    ask it to fix it.\n\n### It's great at jumping contexts\n\nGPT-4 shines when spanning
    different contexts. If comfortable in one programming language or framework and
    need to learn a new one, GPT-4 can help explain it to you and relate it back to
    concepts that you're accustomed to. In a lot of ways, GPT-4 is like looking into
    a mirror.\n\n### It makes up seemingly plausible answers that could be wrong.\n\nWhen
    I asked GPT-4 how to create an instance of a `YARD::Registry`, instead of using
    the singleton, it told me `YARD::Registry.new` . That seems right when thinking
    about the structure of idiomatic Ruby code, but YARD is not, so GPT-4 has a harder
    time answering the question.\n\n### It get's stuck when the \"contexts\" get too
    large, like if you're asking for code from 5 different files\n\nIf you give GPT-5
    too broad of a problem to solve, it can get lost and forget things. If you ask
    it to solve a problem for you like \"how do I create server-side anchor tags using
    Turbo and Hotwire\", it will give you an initial answer that seems right across
    5+ files. When you start fixing errors in those files, GPT-4 will loose track
    of some of the files and start giving you suggestions for fixing completely different
    code."
- :id: laravel-bytes-time-insight-video-livewire
  :date: '2023-03-22'
  :category: laravel-bytes
  :title: Collecting Time Video Metrics with Livewire
  :author: kathryn
  :thumbnail: paused-video-thumbnail.webp
  :alt: A monitor stands on top of a table. The monitor contains a video player that
    is paused, displaying the play button in the middle. Next to the monitor is a
    hanging clock.
  :link: laravel-bytes/time-insight-video-livewire
  :path: laravel-bytes/2023-03-22
  :body: "\n<p class=\"lead\">Today we'll use Livewire to communicate video event
    data to our server. Deploy now on Fly.io, and get your [Laravel app running](/docs/laravel/)
    in a jiffy!</p>\n\nAlright drumrolls. Because today, we'll intercept three user-video
    interaction events, and use [Livewire](https://laravel-livewire.com/docs/2.x/quickstart)
    to easily send event data to our server!\n\n## The Problem\n\nThere are cases
    when we'd want to get insight on how users interact with the video files we display
    in our website pages. \n\nFor example. Let's imagine we have a Laravel website
    where we share online courses. Each course would have video-lessons users can
    watch and complete in order to progress through a course.\n \nWouldn't it be neat
    to know which specific time users usually pause lessons in? Or how long does each
    pause take before the user plays the video again? How about points in time that
    are frequently visited by the users? \n\n## Solution\n\nThere are several events
    a `video` tag emits which we can hook onto to gain insight on how users interact
    with various video-lessons. In this article, we'll demonstrate monitoring user-video
    interaction data through three events:\n\n1. [pause event](https://developer.mozilla.org/en-US/docs/Web/API/HTMLMediaElement/pause_event)
    - to determine the point in time users usually pauses a lesson\n2. [play event](https://developer.mozilla.org/en-US/docs/Web/API/HTMLMediaElement/play_event)
    - to determine how long pauses endure before lessons are played again\n3. [seeked
    event](https://developer.mozilla.org/en-US/docs/Web/API/HTMLMediaElement/seeked_event)
    - to determine the  point in time that is usually jumped into for a lesson\n\nToday,
    we'll hook onto these events to get relevant data, and use [Livewire](https://laravel-livewire.com/docs/2.x/quickstart)
    to easily share data to our server \U0001F31F\n\n## Setting Up The Video Route\n\nWe'll
    be accessing our video file through an endpoint:\n\n```php\nRoute::get('/videos/{id}',\n
    \   [\\App\\Http\\Controllers\\VideoController::class,'stream']\n)->name('videos.stream');\n```\n\nOur
    VideoController class will have a simple stream method:\n```php\npublic function
    stream(Request $request, $id){\n    // getFileDetails() is a custom method to
    get our video file!\n    $details = $this->getFileDetails( $id );\n    $header
    = [\n        'Cache-Control','no-cache, must-revalidate'\n    ];\n    return response()->file(storage_path($details['path']),$header);\n}\n```\nNow
    that we have a route to our video file, let's proceed with creating a Livewire
    component we can play this video file in.\n\n\n## Setting Up the Livewire Component\nLet's
    create our Livewire component with `php artisan make:livewire video-player`. This
    should create two files: A component, and a view.\nWe can embed this component
    to any blade view, and even pass parameters to it, likeso:\n```php\n@livewire('video-player',
    ['videoId'=>1])\n```\n\nTo use `videoId`, we have to declare a matching public
    attribute in our Component. We'll use this later below to create the route to
    access our video file. First, \nlet's revise our view.\n\nWe'll add a video tag
    with no source yet; Instead of immediately getting the source, we'll let the page
    render the video tag first. Then, we'll use Livewire's `wire:init` [directive](https://laravel-livewire.com/docs/2.x/defer-loading)
    to trigger loading the source after rendering:\n\n\n```php\n<!-- view -->\n<div
    wire:init=\"initVideo\">\n  <video wire:ignore\n    id=\"videoPlayer\" controls
    width=\"500px\" height=\"900px\"\n  />\n</div>\n```\nOnce the page completes loading,
    the `wire:init` directive will call the method `initVideo()`. We can do some initial
    processing in this method. An example would be to generate a temporary `$url`
    to access our video file.\n  \nThen we can use Livewire's `dispatchBrowserEvent()`
    [method](https://laravel-livewire.com/docs/2.x/events#browser) to notify the view's
    JavaScript to receive this url by emitting a custom event called `init-complete`:\n\n```php\n/*
    Component */\npublic $videoId;\n\npublic function initVideo()\n\n  // As mentioned
    above, generate url with $videoId\n  $url =  URL::temporarySignedRoute(\n    'videos.stream',
    now()->addMinutes(30), ['id' =>  $this->videoId]\n  );\n  // Notify client to
    set source\n  $this->dispatchBrowserEvent('init-complete', ['url'=>$url]);\n}\n```
    \nWe can listen to this event in our JavaScript, and finally set the source of
    our video tag:\n\n```php\n<!-- view -->\n<script>\n  var video = document.getElementById(\"videoPlayer\");\n
    \ window.addEventListener('init-complete', event => {\n    video.src = event.detail.url;\n
    \ });\n```\n\nNow that we have our video tag loading our source( neatly after
    page load ), it's time to monitor our user interaction with the video!\n\n<%=
    partial \"shared/posts/cta\", locals: {\n  title: \"Fly.io ❤️ Laravel\",\n  text:
    \"Fly your servers close to your users&mdash;and marvel at the speed of close
    proximity. Deploy globally on Fly in minutes!\",\n  link_url: \"https://fly.io/docs/laravel\",\n
    \ link_text: \"Deploy your Laravel app!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>\",\n}
    %>\n\n## Only Time Can Tell\nOne of the main data we can use to get insight on
    how our user interacts with our video-lessons is through time data. In this article,
    we'll use two main sources for insight on time:\n\n1. A video tag's <b>[currentTime](https://developer.mozilla.org/en-US/docs/Web/API/HTMLMediaElement/currentTime)</b>
    that points to the time in seconds the video is currently at.\n2. A video event's
    <b>[timestamp](https://developer.mozilla.org/en-US/docs/Web/API/Event/timeStamp)</b>
    that points to the time in milliseconds the event occured in.\n\nWith these two
    time variables, we can build insight on our user's time-based interaction with
    their video lessons:\n\n### When do users often pause the Video-Lesson?\nLet's
    say we want to listen in on which part of our video-lessons users mostly pause
    on. We can do so by listening to the `pause` event triggered whenever the user
    pauses the video, and get the video tag's `currentTime` to determine at what video
    moment the user paused the video in:  \n\nThen we can use Livewire to discreetly
    send this information to be processed in the server through a custom method, `@this.sendPauseData()`.
    \n```php\n<!-- view -->\n<script>\n// ... previous listener here\n\nvideo.addEventListener('pause',function(e){\n
    \   // readyState = 4 ensures true pause interaction\n    if( video.readyState
    == 4 )\n      @this.sendPauseData( video.currentTime );\n});\n```\n`@this.sendPauseData()`
    will trigger a request to call a method in our Livewire component named `sendPauseData()`.
    We'll use this method to share the retrieved `currentTime` in our server, and
    ultimately record the \"pause\" event details.\n\n Doing so let's us keep a history
    of pauses for our lesson, paving way for determining \"possible\" engagement-insights
    on the lesson: like its average paused video moment. \n\n```php\n/* Component
    */\npublic function sendPauseData( $videoCurrentTime ){\n    // Do some processing
    on $videoCurrentTime\n}\n```\n\n### How long do users pause at this Video-Lesson
    moment?\n\nSince we're recording different points in time a video gets paused
    in, why not also add in how long the user paused the lesson? Duration would be:
    (the time played after pause) - (the time paused).\n\nTo get the pause time, we
    simply use `timeStamp` from the event variable passed in the pause listener event.
    Then to get our played time, simply get the `timeStamp` from the play event listener!\n\nLet's
    revise our pause event to include the `e.timeStamp` value. We'll also create a
    global variable to tell us the video has been paused&mdash;adding this variable
    will help us later in listening for \"true\" play events.\n```php\n<!-- view -->\n<script>\n//
    ... previous listener here\n\n+   var isPaused = true;// Our video is initially
    on pause\nvideo.addEventListener('pause',function(e){\n    // readyState=4 ensures
    true pause event interaction\n+    if( video.readyState == 4 ){\n+       isPaused
    = true;\n+       @this.sendPauseData( video.currentTime, e.timeStamp );\n+    }\n```\nThen
    in our method from the Livewire component, we'll have to track this timestamp
    as a public property `$pausedAt` so it doesn't get lost:\n```php\n/* Component
    */\n+ public $pausedAt;\n\npublic function sendPauseData( $videoCurrentTime, $pausedAtTimeStamp
    ){\n+    $this->pausedAt = $pausedAtTimeStamp;\n     // Do some processing on
    $videoCurrentTime and $pausedAtTimeStamp \n}\n```\nNext, let's set up our play
    event listener. When the user finally plays the video in our view, we can simply
    send the current `timeStamp` to our Livewire component:\n```php\n<!-- view -->\n<script>\n//
    ... previous listeners here\nvar isPaused = true;\nvideo.addEventListener('play',function(e){\n
    \   // Make sure video is prev paused\n    if( isPaused ){\n      isPaused = false;\n
    \     @this.sendPlayData( e.timeStamp );\n    }\n});\n```\nAnd use this value
    to get the time difference from the `$pausedAt` attribute previously set:\n```php\n/*
    Component */\npublic function sendPlayData( $playedAtTimestamp ){ \n    if( $this->pausedAt
    ){\n        $diff = $playedAtTimestamp - $this->pausedAt;\n        // Do some
    processing on this new found duration, $diff\n    }\n}\n```\n\n### Which point
    in time Users usually \"seek\" to?\nThere are certain parts of a video that users
    find really helpful to go back to, we can gain insight on this as well!\n\nAgain,
    we can simply use the `currentTime` to tell us at which lesson moment the user
    moved or \"seeked\" the video time to:\n```php\n<!-- view -->\n<script>\n// ...
    previous listeners here\n\nvideo.addEventListener('seeked',function(e){\n    console.log('User
    went to:', video.currentTime);\n    @this.sendSeekedData( video.currentTime );\n});\n```\nAnd
    in our Livewire component, make our due processing with a matching method:\n```php\n/*
    Component */\npublic function sendSeekedData( $videoCurrentTime ){\n  // Record
    this vide time please, with a \"seekedTo\" type\n  // to gather historical data\n
    \ // that can be basis for determining popular video timestamps!\n}\n```\n\n##
    Catching Time\n\nTime does fly! And often times it's a bit hard to catch up to
    it&mdash;for example, we just can't stop time adding up to the years in our lives.
    Luckily the same isn't true for time in video analytics and Livewire.\n\nIn this
    article, we've learned about three video events: `pause`, `play`, and `seek`.
    Listening on these events helped us to get data on three \"time-centric\" ways
    users interact with our video-lessons through: the video tag's `currentTime` and
    a video-event's `timeStamp` value. Finally, we showcased how easily data from
    video tags in client browsers can be shared to our server with the help of Livewire.\n\nOf
    course there's so much more video events we can listen on and test out, if that's
    an interest, reading through the [HTMLMediaElement](https://developer.mozilla.org/en-US/docs/Web/API/HTMLMediaElement/seeked_event)
    documentation would help for other video analytics implementations.\n\nTime does
    fly, and we can't often use it wisely. But, using Livewire to share video events
    from client browser to our server&mdash;now that's one <i>time-wise</i> way to
    get time analytics sent to our Laravel applications. \U0001F31F\n\n\n\n\n\n\n"
- :id: phoenix-files-building-a-chat-app-with-liveview-streams
  :date: '2023-03-22'
  :category: phoenix-files
  :title: Building a Chat App with LiveView Streams
  :author: sophie
  :thumbnail: chat-bird-thumbnail.webp
  :alt: A Fly bird using a chat application on their computer screen.
  :link: phoenix-files/building-a-chat-app-with-liveview-streams
  :path: phoenix-files/2023-03-22
  :body: |2


    <p class="lead">Streams are an exciting new feature in Phoenix. Sophie DeBenedetto walks us through creating our own Slack-like chat interface which features infinite scroll back, editing past messages, deleting messages, and appending new messages to the bottom all using Streams. It's a slick and efficient solution that avoids storing all that message data in the LiveView. Fly.io happens to be a great place to run Phoenix applications. Check out how to [get started](/docs/elixir/)!</p>

    In this post, we'll build out a LiveView chatroom app with the help of LiveView's new streams feature. You can follow along in the open source codebase or skip ahead to play around with the finished product. We'll see how streams seamlessly integrate into your existing live views to power interactive and efficient UIs. Along the way, we'll look at how streams work under the hood. When we're done, you'll have exercised the full functionality of streams and you'll understand how they work at a deep level.

    ## What are LiveView Streams?

    [LiveView 0.18.16](https://github.com/phoenixframework/phoenix_live_view/blob/v0.18.16/CHANGELOG.md) ships with the new streams functionality for managing large collections of data client-side, without having to store anything in the LiveView socket. Chris McCord tells us more about this feature and the problem it's designed to solve in [this excellent post](https://fly.io/phoenix-files/phoenix-dev-blog-streams/).

    For the past few years, a question I would often hear from developers interested in LiveView was: "What about large datasets?" Users who needed to display and manage long lists of data had to store that data on the server, or else work with the `phx-update="append"` feature. Storing large collections server-side can impact performance, while the `phx-update="append"` feature had its own drawbacks. But, as is so often the case with LiveView over the course of its development, the framework has come to provide a better solution for this commonly expressed concern. Now, you can use streams to efficiently manage large datasets in your live views by detaching that data from the socket and letting the client store it instead of the server.

    LiveView exposes an elegant and users-friendly API for storing data in a client-side stream and allowing your app's users to interact with that data by adding, updating, and deleting items in the stream. We'll explore this behavior as we build a real-time chat feature into an existing chatroom-style LiveView application. Our chat will even use streams to support an infinite scroll back feature that allows users to view their chat history. Let's get started.

    ## The StreamChat App

    For this project, we have a basic LiveView application set up with the following domain:

    - A `Room` has many messages.
    - A `Message` belongs to a room and a sender. A sender is a user.
    - A `User` has many messages.

    <aside class="right-sidenote">
      **Hot Tip!**
      Make sure to grab the project from [Github](https://github.com/SophieDeBenedetto/stream_chat) and play with it yourself!
    </aside>
    We also have a `Chat` context that exposes the CRUD functionality for rooms and messages. All of this backs the main LiveView of the application, `StreamChatWeb.ChatLive.Root`. This LiveView is mapped to the `/rooms` and `/rooms/:id` live routes and this is where we'll be building our stream-backed chatting feature. You can find the starting code for this blog post [here](https://github.com/SophieDeBenedetto/stream_chat/tree/start), including a seed file that will get you started with some chat rooms, users, and messages. If you'd like to follow along step-by-step with this post, clone down the repo at the `start` branch. Or, you can check out the completed project on the `main` branch [here](https://github.com/SophieDeBenedetto/stream_chat).

    The starting state for our code-along leaves us with a UI that looks like this:

    ![Screenshot of chat interface with two channels on the left and an empty input with a send button.](./01-initial-ui-opt.png?card&centered)

    A user can navigate to `/rooms/:id` and see the sidebar that lists the available chatrooms, with the current chatroom highlighted. But we're not displaying the messages for that room yet. And, while we have the form for a new message, the page doesn't yet update to reflect that new message in real-time. We'll use streams to implement both of these features, along with the "edit message" and "delete" message functionality. Let's get started.

    ## List Messages with Streams

    First up, we want to render a list of messages in each chat room. Here's the UI we're going for:

    ![Screenshot of chat interface with multiple messages listed from different people.](./02-target-result-opt.png?card&centered)

    We'll use a stream to store the most recent ten messages for the room and we'll render the contents of that stream in a HEEx template. Let's start by teaching the `ChatLive.Root` LiveView to query for the messages and put them in a stream when the `/rooms/:id` route is requested.

    ### Initialize the Stream

    In the `router.ex` file we have the following route definitions:

    ```elixir
    live "/rooms", ChatLive.Root, :index
    live "/rooms/:id", ChatLive.Root, :show
    ```

    Note that both the `/rooms` and `/rooms/:id` routes map to the same LiveView, `ChatLive.Root`. The `/rooms/:id` route is defined with a live action of `:show` in the socket assigns. The `ChatLive.Root` LiveView already implements a `handle_params/3` callback that queries for the room with the room ID from params and stores the active room in socket assigns. We'll add some additional code to this callback to fetch the list of messages for the current room and store them in the stream, like this:

    ```elixir
    def handle_params(%{"id" => id}, _uri, %{assigns: %{live_action: :show}} = socket) do
      {:noreply,
        socket
        |> assign_active_room(id)
        |> assign_active_room_messages()}
    end

    # single-purpose reducer functions

    def assign_active_room(socket, id) do
      assign(socket, :room, Chat.get_room!(id))
    end

    def assign_active_room_messages(%{assigns: %{room: room}} = socket) do
      stream(socket, :messages, Chat.last_ten_messages_for(room.id))
    end
    ```

    First, we use a single-purpose reducer function to assign the room with the given ID to the socket. Then, we pass that updated socket to another reducer function, `assign_active_room_messages/1`. That reducer pulls the room out of socket assigns and uses it to fetch the last ten messages. Finally, we create a stream for `:messages` with a value of this list of messages.

    Let's take a closer look at what happens when we call `stream(socket, :messages, Chat.last_ten_messages_for_room(room.id))`. Go ahead and pipe the updated socket into an `IO.inspect` like this:

    ```elixir
    def assign_active_room_messages(%{assigns: %{room: room}} = socket) do
      stream(socket, :messages, Chat.last_ten_messages_for(room.id))
      |> IO.inspect
    end
    ```

    Let the LiveView reload and you should see the socket inspected into the terminal. Looking closely at the `assigns` key, you'll see something like this:

    ```elixir
    streams: %{
    __changed__: MapSet.new([:messages]),
    messages: %Phoenix.LiveView.LiveStream{
    name: :messages,
    dom_id: #Function<3.113057034/1 in Phoenix.LiveView.LiveStream.new/3>,
    inserts: [
    {"messages-5", -1,
      %StreamChat.Chat.Message{
        __meta__: #Ecto.Schema.Metadata<:loaded, "messages">,
        id: 5,
        content: "Iste cum provident tenetur.",
        room_id: 1,
        room: #Ecto.Association.NotLoaded<association :room is not loaded>,
        sender_id: 8,
        sender: #StreamChat.Accounts.User<
          __meta__: #Ecto.Schema.Metadata<:loaded, "users">,
          id: 8,
          email: "keon@streamchat.io",
          confirmed_at: nil,
          inserted_at: ~N[2023-03-02 01:27:09],
          updated_at: ~N[2023-03-02 01:27:09],
          ...
        >,
        inserted_at: ~N[2023-03-02 01:27:10],
        updated_at: ~N[2023-03-02 01:27:10]
      }},
      # ...
      deletes: []
      }
    },
    # ...
    ```

    The call to `streams/4` adds a `:streams` key to socket assigns, which in turn points to a map with a `:messages` key. The `streams.messages` assignment contains a `Phoenix.LiveView.LiveStream` struct that holds all of the info the LiveView client-side code needs to display your stream data on the page.

    <aside class="callout">
    Notice that the struct has an `:inserts` key that contains the list of messages we're inserting into the initial stream. It also contains a `:deletes` key that is currently empty. All of this data is made available in our template as the `@streams.messages` assignment.
    </aside>

    After the initial render, the list of messages will no longer be present in the socket under `streams.messages.inserts`. It will be available only to the LiveView client-side code via the HTML on the page. Let's do that rendering now.

    ### Render Stream Data

    We'll use a function component, `Room.show/1`, to render the messages list from the `root.html.heex` template _if_ the `@live_action` assignment is set to `:show`. We'll pass in the messages from the stream when we do so, like this:

    ```elixir
    # lib/stream_chat_web/live/chat_live/root.html.heex
    <Room.show
      :if={@live_action == :show}
      messages={@streams.messages}
      current_user_id={@current_user.id}
      room={@room} />
    ```

    The `Room.show/1` function component will render both the list of messages _and_ a form for a new message. Let's add in that messages list rendering like this:

    ```elixir
    defmodule StreamChatWeb.ChatLive.Room do
      use Phoenix.Component
      alias StreamChatWeb.ChatLive.Messages

      def show(assigns) do
        ~H"""
        <div id={"room-#{@room.id}"}>
          <Messages.list_messages messages={@messages} />
          <!-- ... form for a new message -->
        </div>
        """
      end
    end
    ```

    This function component calls another function component, `Messages.list/1`. This nice, layered UI allows us to wrap up the different concepts on our page into appropriately named functions. Each of these functions can be relatively single-purpose, keeping our code short and sweet and ensuring we have a nice clean location to place our stream rendering code. Let's take a look at the stream rendering code in `Messages.list/1` now.

    ```elixir
    defmodule StreamChatWeb.ChatLive.Messages do
      use Phoenix.Component

      def list_messages(assigns) do
        ~H"""
        <div id="messages" phx-update="stream">
          <div :for={{dom_id, message} <- @messages} id={dom_id}>
            <.message_meta message={message} />
            <.message_content message={message} />
          </div>
        </div>
        """
      end
    end
    ```

    This is where the magic happens. We create a container `div` with a unique id of `"messages"` and a `phx-update="stream"` attribute. Both of these attributes are required in order for LiveView streams to be rendered and managed correctly. Then, we iterate over the `@messages` assignment, which we passed in all the way from the `root.html.heex` template's call to `@streams.messages`. At this point, `@messages` is set equal to the `Phoenix.LiveView.LiveStream` struct. This struct is enumerable such that when we iterate over it, it will yield tuples describing each item in the `:inserts` key. The first element of the tuple is the item's DOM id and the second element is the message struct itself. LiveView uses each item's DOM id to manage stream items on the page. More on that in a bit.

    [**Deep Dive: How LiveStream Implements Iteration**]

    _Keep reading if you want a closer look at how LiveStream implements enumeration. Or, skip this section to continue building the chat feature and return here later._

    The LiveStream struct implements the `Enumerable` protocol [here](https://github.com/phoenixframework/phoenix_live_view/blob/v0.18.16/lib/phoenix_live_view/live_stream.ex#L55) which let's us iterate over it and yield the tuples described above. Here's a look at one of protocol's `reduce` functions:

    ```elixir
    def reduce(%LiveStream{inserts: inserts}, acc, fun) do
      do_reduce(inserts, acc, fun)
    end
    ```

    You can see that when `reduce` is called, it pattern matches the _inserts_ out of the function head and passes that list into `do_reduce/3`. The `:inserts` key of the stream struct looks something like this:

    ```elixir
    [
      {"messages-5", -1,
        %StreamChat.Chat.Message{
          __meta__: #Ecto.Schema.Metadata<:loaded, "messages">,
          id: 5,
          content: "Iste cum provident tenetur.",
          room_id: 1,
          room: #Ecto.Association.NotLoaded<association :room is not loaded>,
          sender_id: 8,
          sender: #StreamChat.Accounts.User<
            __meta__: #Ecto.Schema.Metadata<:loaded, "users">,
            id: 8,
            email: "keon@streamchat.io",
            confirmed_at: nil,
            inserted_at: ~N[2023-03-02 01:27:09],
            updated_at: ~N[2023-03-02 01:27:09],
            ...
          >,
          inserted_at: ~N[2023-03-02 01:27:10],
          updated_at: ~N[2023-03-02 01:27:10]
        }},
      # ...
    ]
    ```

    It is a list of three-tuples, where the first element is the DOM id, the second element is an instruction to the LiveView client regarding where to position the item in the list (we don't care about that right now), and the third element is the item itself.

    Here's a simplified look at the version of the `do_reduce/3` function that does the heavy lifting:

    ```elixir
    defp do_reduce([{dom_id, _at, item} | tail], {:cont, acc}, fun) do
      do_reduce(tail, fun.({dom_id, item}, acc), fun)
    end
    ```

    The function ignores the `_at` element in the tuple, and collects new tuples composed of `{dom_id, item}`. So, when we iterate a LiveStream struct with a `for` comprehension, it yields these tuples.

    [**/Deep Dive**]

    Let's inspect this iteration more closely. Go ahead and add this code to the `list_messages/1` function and then hop on over to your terminal:

    ```elixir
    def list_messages(assigns) do
      for {dom_id, message} <- assigns.messages do
        IO.inspect {dom_id, message}
      end
      ~H"""
      # ...
      """
    end
    ```

    You should see something like this:

    ```elixir
    {"messages-5",
     %StreamChat.Chat.Message{
       __meta__: #Ecto.Schema.Metadata<:loaded, "messages">,
       id: 5,
       content: "Iste cum provident tenetur.",
       room_id: 1,
       room: #Ecto.Association.NotLoaded<association :room is not loaded>,
       sender_id: 8,
       sender: #StreamChat.Accounts.User<
         __meta__: #Ecto.Schema.Metadata<:loaded, "users">,
         id: 8,
         email: "keon@streamchat.io",
         confirmed_at: nil,
         inserted_at: ~N[2023-03-02 01:27:09],
         updated_at: ~N[2023-03-02 01:27:09],
         ...
       >,
       inserted_at: ~N[2023-03-02 01:27:10],
       updated_at: ~N[2023-03-02 01:27:10]
     }
    }
    # ...
    ```

    You can see that each tuple has a first element of the DOM id and a second element of the message itself. The DOM id of each element is computed by interpolating the name of the stream, in our case `"messages"`, along with the ID of the item. So, we get a DOM id of `"messages-5"` and so on.

    **[Deep Dive] How LiveView computes the stream item DOM id:**

    _Keep reading to take a deep dive into how LiveView computes the DOM id. Or, skip this section to continue building our feature and return to it later_.

    When you call `stream(socket, :messages, message_list)`, LiveView initializes a new LiveStream struct with the `Phoenix.LiveView.LiveStream.new/3` function. That function assigns the struct's `:dom_id` attribute to either a function you optionally provide to `stream/4`, or to the default DOM id function. Here's a peak at the source code:

    ```elixir
     def new(name, items, opts) when is_list(opts) do
      dom_prefix = to_string(name)
      dom_id = Keyword.get_lazy(opts, :dom_id, fn -> &default_id(dom_prefix, &1) end)

      unless is_function(dom_id, 1) do
        raise ArgumentError,
              "stream :dom_id must return a function which accepts each item, got: #{inspect(dom_id)}"
      end

      items_list = for item <- items, do: {dom_id.(item), -1, item}

      %LiveStream{
        name: name,
        dom_id: dom_id,
        inserts: items_list,
        deletes: [],
      }
    end
    ```

    It creates a variable, `dom_prefix` by stringifying the name of the stream--in our case `:messages`. Then, it sets `dom_id` either to a function you pass into `stream/4` like this: `stream(:messages, messages, &myFunc)`, or to an anonymous function that wraps the `default_id/2` function. Let's peek at the `default_id/2` function now:

    ```elixir
    defp default_id(dom_prefix, %{id: id} = _struct_or_map), do: dom_prefix <> "-#{to_string(id)}"
    ```

    The function is pretty straightforward, it returns a string that prepends the `dom_prefix` to the stringified item id.

    As you can see above, the `dom_id` function is then called when `LiveStream.new` iterates over the list of items to be inserted:

    ```elixir
    items_list = for item <- items, do: {dom_id.(item), -1, item}
    ```

    For each item in the list, this iteration creates a three-tuple where the first element is the result of invoking the `dom_id` function for the given item. So, we end up with tuples in which the first element is something like `"messages-52"`, and so on.

    _Takeway? Digging into the streams code we find it isn't mystical or scary. We shouldn't be afraid to peek inside the libraries we depend on._

    [**/Deep Dive**]

    LiveView uses the DOM id of each stream item to track that item and allow us to edit and delete the item. LiveView needs this DOM id to be attached to the HTML element that contains the stream item because stream data is not stored in socket assigns after the initial render. So, LiveView must be able to derive all the information it needs about the item and its position in the stream from the rendered HTML itself.

    We attach the DOM id to each div produced by the iteration in our `:for` directive. Here's another look at that code:

    ```xml
    <div :for={{dom_id, message} <- @messages} id={dom_id}>
      <.message_meta message={message} />
      <.message_content message={message} />
    </div>
    ```

    That's all we need to do to render the list of messages from the stream. We stored the initial stream in socket assigns, iterated over it, and rendered it using the required HTML structure and attributes. Now, the page will render with this list of messages from the stream, and the `ChatLive.Root` LiveView will no longer hold this list of messages in the `streams.messages` socket assigns. After the initial render, `socket.assigns.streams.messages` looks like this:

    ```elixir
    streams: %{
      __changed__: MapSet.new([:messages]),
      messages: %Phoenix.LiveView.LiveStream{
      name: :messages,
      dom_id: #Function<3.113057034/1 in Phoenix.LiveView.LiveStream.new/3>,
      inserts: [],
      deletes: []
    }
    ```

    We'll see LiveView's stream updating capabilities in action in the next section. Next up, we'll build the infinite scroll back feature that loads the previous chat history as the user scrolls the chat window up. Each time the user scrolls up and hits the top of the chat window, we'll prepend an older batch of messages to the stream. You'll see that LiveView handles the work of how and where to prepend those messages on the page. All we have to do is tell LiveView that an item should be prepended to the stream, and the framework takes care of the rest. Let's do that now.

    ## Prepend Stream Messages for Infinite Scroll Back

    Our app uses a JS hook to send the `"load_more"` event to the server when the user scrolls up to the top of the chat window. You can check out the hook implementation [here](https://github.com/SophieDeBenedetto/stream_chat/blob/start/assets/js/infiniteScroll.js). We won't get into the details of this JavaScript now though. Just note [this](https://github.com/SophieDeBenedetto/stream_chat/blob/start/assets/js/infiniteScroll.js#L5) line that pushes the `"load_more"` event. Now all you need to do is add a new div with the hook attached to the messages display, like this:

    ```elixir
    # lib/stream_chat_web/live/chat_live/messages.ex
    def list_messages(assigns) do
      ~H"""
      <div id="messages" phx-update="stream">
        <div id="infinite-scroll-marker" phx-hook="InfiniteScroll"></div> <!-- add me! -->
        <div :for={{dom_id, message} <- @messages} id={dom_id}>
          <.message_meta message={message} />
          <.message_content message={message} />
        </div>
      </div>
      """
    end
    ```

    Now we're ready to handle the `"load_more"` event in our LiveView by prepending items to the stream.

    ### Prepend Stream Items

    In the `ChatLive.Root` LiveView, we need an event handler to match the `"load_more"` event. Go ahead and implement the function definition like this:

    ```elixir
    # lib/stream_chat_web/live/chat_live/root.ex
    def handle_event("load_more", _params, socket) do
      # coming soon!
    end
    ```

    Our event handler needs to fetch the previous batch of messages from the database and prepend each of those messages to the stream. We do have a context function available to us to query for `n` messages older than a given ID: `Chat.get_previous_n_messages/2`, but we have one problem. Since LiveView does not store stream data in the socket, we have no way of knowing what the ID of the currently loaded oldest message is. So, we can't query for messages _older_ than that one. We need to store awareness of this "oldest message" ID in the socket. Let's fix that now and then we'll return to our event handler.

    When do we have access to the oldest message in the stream? When we query for the messages to add to the initial stream in our `handle_params/3` callback. At that time, we should grab the oldest message and store its ID in socket assigns. Here's our updated `handle_params` function:

    ```elixir
    # lib/stream_chat_web/live/chat_live/root.ex
    def handle_params(%{"id" => id}, _uri, %{assigns: %{live_action: :show}} = socket) do
      messages = Chat.last_ten_messages_for(socket.assigns.room.id)
      {:noreply,
        socket
        |> assign_active_room(id)
        |> assign_active_room_messages(messages)
        |> assign_oldest_message_id(List.first(messages))}
    end

    # ...

    def assign_active_room_messages(socket, messages) do
      stream(:messages, messages)
    end

    def assign_oldest_message_id(socket, message) do
      assign(socket, :oldest_message_id, message.id)
    end
    ```

    Now we can use the oldest message ID in socket assigns to query for the previous batch of messages. Let's do that in our event handler now.

    ```elixir
    # lib/stream_chat_web/live/chat_live/root.ex
    def handle_event("load_more", _params, %{assigns: %{oldest_message_id: id}} = socket) do
      messages = Chat.get_previous_n_messages(id, 5)

      {:noreply,
        socket
        |> stream_batch_insert(:messages, messages, at: 0)
        |> assign_oldest_message_id(List.last(messages))}
    end
    ```

    We query for the previous five messages that are older than the current oldest message. Then, we insert this batch of five messages into the stream. Finally, we assign a new oldest message ID.

    Let's take a closer look at the `stream_batch_insert` function now. This is a hand-rolled function since the streams API doesn't currently support a "batch insert" feature. You'll find it in the `live_view` behaviour implement in our app's `StreamChatWeb module`. I've placed it here because I feel that this function should be highly reuseable within the application, and I'd even like to see LiveView streams offer some such functionality in future release.

    Let's take a look at the `stream_batch_insert/4` function now:

    ```elixir
    # lib/stream_chat_web.ex
    def stream_batch_insert(socket, key, items, opts \\ %{}) do
      items
      |> Enum.reduce(socket, fn item, socket ->
        stream_insert(socket, key, item, opts)
      end)
    end
    ```

    Here, we iterate over the items with `Enum.reduce` using the socket as an accumulator. For each item, we insert it into the stream. In our event handler, we call `stream_batch_insert` with `opts` of `at: 0`. This option is passed to `stream_insert/4` for each item. As a result, we end up with a socket assigns with the following insertion instructions for LiveView:

    ```elixir
    streams: %{
      __changed__: MapSet.new([:messages]),
      messages: %Phoenix.LiveView.LiveStream{
        name: :messages,
        dom_id: #Function<3.113057034/1 in Phoenix.LiveView.LiveStream.new/3>,
        inserts: [
          {"messages-111", 0,
            %StreamChat.Chat.Message{
              id: 111,
              content: "10",
              #...
            }},
          {"messages-110", 0,
            %StreamChat.Chat.Message{
              id: 110,
              content: "9",
              # ...
          }},
        ],
        deletes: []
      }
    }
    # ...
    ```

    Notice that the second element of each tuple in the `:inserts` collection is `0`. This tells LiveView to insert these items at the _beginning_ of the stream on the page. When the page re-renders, it will display these five older messages in the correct order, at the top of the chat messages display. Here's what our feature looks like in action:

    <%= video_tag "./03-infinite-scrollback.mp4?centered&card" %>

    Now that we've built out our infinite scroll back feature and seen how streams work to prepend new data, we'll take a look at the form for a new message, and use streams to append new messages to the _end_ of the messages list.

    ## Append a New Message with `stream_insert`

    We're already rendering the [form for a new message](https://github.com/SophieDeBenedetto/stream_chat/blob/start/lib/stream_chat_web/live/chat_live/message/form.ex) in the `Room.show/1` function component like this:

    ```elixir
    # lib/stream_chat_web/live/chat_live/room.ex
    def show(assigns) do
      ~H"""
      <div id={"room-#{@room.id}"}>
        <Messages.list_messages messages={@messages} />
        <.live_component
          module={Message.Form}
          room_id={@room.id}
          sender_id={@current_user_id}
          id={"room-#{@room.id}-message-form"}
        />
      </div>
      """
    end
    ```

    When that form is submitted, it triggers an event handler implemented in the form live component that calls `Chat.create_message/1`. So, when the user submits the form, a new chat message is created. But that new message isn't added to the page in real-time. The user would have to refresh the page to see the latest message.

    We're ready to teach our LiveView to insert the new message once it's created. This is the responsibility of the `ChatLive.Root` LiveView, since that is the LiveView that has awareness of the `@streams.messages` assignment. Luckily for us, our chat feature is already backed by PubSub for real-time capabilities. The `Chat.create_message/1` function broadcasts an event when a new message is created, like [this](https://github.com/SophieDeBenedetto/stream_chat/blob/start/lib/stream_chat/chat.ex#L151-L154):

    ```elixir
    Endpoint.broadcast(
      "room:#{message.room_id}",
      "new_message",
      %{message: message})
    ```

    We just need to tell our `ChatLive.Root` LiveView to subscribe to the PubSub topic for the active room. We'll do that in `handle_params/3`:

    ```elixir
    def handle_params(%{"id" => id}, _uri, %{assigns: %{live_action: :show}} = socket) do
      if connected?(socket), do: Endpoint.subscribe("room:#{id}")

      # ...
    end
    ```

    Now, when a new message is created, any `ChatLive.Root` LiveView processes for that message's room will receive a PubSub event. The `handle_info/3` for this event is where we'll insert the new chat message into the stream. Let's build that now.

    ### Append a Stream Item

    Add the following `handle_info/3` function to `ChatLive.Root`:

    ```elixir
    # lib/stream_chat_web/chat_live/root.ex
    def handle_info(%{event: "new_message", payload: %{message: message}}, socket) do
      {:noreply, insert_new_message(socket, message)}
    end

    def insert_new_message(socket, message) do
      socket
      |> stream_insert(:messages, Chat.preload_message_sender(message))
    end
    ```

    This time around, we call `stream_insert/4` with no additional options. In this case, the resulting LiveStream struct in socket assigns will look something like this:

    ```elixir
    streams: %{
      __changed__: MapSet.new([:messages]),
      messages: %Phoenix.LiveView.LiveStream{
        name: :messages,
        dom_id: #Function<3.113057034/1 in Phoenix.LiveView.LiveStream.new/3>,
        inserts: [
          {"messages-111", -1,
            %StreamChat.Chat.Message{
              id: 111,
              content: "10",
              #...
            }
          },
        ],
        deletes: []
      }
    }
    # ...
    ```

    Once again, we have a LiveStream struct with the `:inserts` key populated with the list of inserts. Now we have just one item in the list. The tuple representing that item has a second element of `-1`. This tells LiveView to append the new item to the end of the stream. As a result, the new message will be rendered at the end of the list of messages on the page.

    That's it for our new message feature. Once again, streams did the heavy lifting for us. All we had to do was tell LiveView that a new item needed to appended. Now we're ready to build the edit message feature and take a look at how to update items in a stream. Then, we'll wrap up with our delete message feature.

    ## Update an Existing Message with `stream_insert`

    The edit message form lives in the `ChatLive.Message.EditForm live component`, which is contained in a modal that we show or hide based on user interactions. This form behaves similarly to the form for a new message. It's `"save"` event handler calls the `Chat.update_message` context function, which emits an `"updated_message"` event over PubSub.

    We'll implement a `handle_info/3` for this event in the `ChatLive.Root` LiveView, since that LiveView is responsible for managing the `@streams.messages` assigns. Let's do that now.

    ```elixir
    # lib/stream_chat_web/chat_live/root.ex
    def handle_info(%{event: "updated_message", payload: %{message: message}}, socket) do
      {:noreply,
        socket
        |> insert_updated_message(message)}
    end

    def insert_updated_message(socket, message) do
      socket
      |> stream_insert(:messages, Chat.preload_message_sender(message), at: -1)
    end
    ```

    Here, we call `stream_insert` yet again, this time with the updated message and the `at: -1` option. Since we're passing a message that the stream is already tracking on the page, LiveView will know to update the existing message item in the stream. The `at: -1` option tells LiveView to update the item at its current stream location, rather than appending it to the end of the list. Now, the page will re-render and display the updated in message in place, like this:

    <%= video_tag "./04-edit-message.mp4?centered&card" %>

    Before we wrap up, we need to build out the message delete feature. Let's do that now.

    ## Delete a Message with `stream_delete`

    We render a delete icon for each message when the message is hovered over, like this:

    ![Screenshot shows the hover delete button link for deleting a message.](./05-delete-message-opt.png?card&centered)

    When the user clicks that button, we send a `"delete_message"` event to the LiveView. Let's handle that event now by deleting the message from the stream.

    ```elixir
    # lib/stream_chat_web/chat_live/root.ex
    def handle_event("delete_message", %{"item_id" => message_id}, socket) do
      {:noreply, delete_message(socket, message_id)}
    end

    def delete_message(socket, message_id) do
      message = Chat.get_message!(message_id)
      Chat.delete_message(message)
      stream_delete(socket, :messages, message)
    end
    ```

    We query for the message to be deleted, execute a call to delete that message from the database, and then tell the stream to delete the message from its list. The call to `steam_delete` returns a socket with an assigns that looks something like this:

    ```elixir
    streams: %{
      __changed__: MapSet.new([:messages]),
      messages: %Phoenix.LiveView.LiveStream{
        name: :messages,
        dom_id: #Function<3.113057034/1 in Phoenix.LiveView.LiveStream.new/3>,
        inserts: [],
        deletes: ["messages-20"]
      }
    }
    ```

    Notice that `:inserts` is empty, but `:deletes` contains a list with the DOM ID of the item to be deleted. This instructs LiveView to remove the item with that DOM ID from the rendered list of `@streams.messages`. If you pass a struct to `stream_delete`, LiveView will compute the DOM ID to be deleted. Alternatively, if you don't have access to that struct or don't want to query for it, you can give `stream_delete` a third argument of the DOM ID directly, either by re-computing it yourself or invoking the stream's `dom_id/2` function stored in `@streams.messages.dom_id`.

    That's all we need to do to support our delete message functionality. Once we tell LiveView that there is a stream item to delete, the framework once again takes care of the rest. It re-renders the page, triggering LiveView JS framework code that removes the specified item from the rendered list of `@streams.messages`.

    Okay, we've covered a lot of ground. Let's wrap up.

    ## Wrap Up

    LiveView's new streams feature packs a powerful punch! It allows us to build and manage large datasets client-side, while writing very little custom code. True to the declarative nature of LiveView, the streams API asks you to provide LiveView with some basic instructions regarding _what_ data to manage in the stream and what to do with that data based on certain user interactions. You don't have to tell LiveView _how_ to render stream data on the page or how to prepend, append, update, or delete items from that data collection.

    Our interactive, real-time chatting feature successfully uses streams to manage chat messages fully on the client, and we had to write only a few lines of streams-specific code to make it happen. Client-side data management with streams opens up a whole new set of possibilities for LiveView developers to efficiently manage large data collections, and I'm excited to see what you build with it next.

    <%= partial "shared/posts/cta", locals: {
      title: "Fly.io ❤️ Elixir",
      text: "Fly.io is a great way to run your Phoenix LiveView app close to your users. It's really easy to get started. You can be running in minutes.",
      link_url: "https://fly.io/docs/elixir/",
      link_text: "Deploy a Phoenix app today!  <span class='opacity:50'>→</span>"
    } %>
- :id: phoenix-files-liveview_accessible_focus
  :date: '2023-03-21'
  :category: phoenix-files
  :title: Using LiveView's new primitives for accessibility
  :author: berenice
  :thumbnail: access-thumbnail.webp
  :alt:
  :link: phoenix-files/liveview_accessible_focus
  :path: phoenix-files/2023-03-21
  :body: |2


    <p class="lead">In this post, we'll take a look at the latest LiveView 0.18 features that improve accessibility by enhancing focus. We'll explore these features through practical examples, so you can see how they work in real-world scenarios. Fly.io is a great place to run your Phoenix LiveView applications! Check out how to [get started](https:///docs/elixir/)!</p>

    [In](https://fly.io/blog/intro-to-accessibility/)  [previous](https://fly.io/blog/accessibility-clearing-the-fog/)  posts, Nolan showed us some ways to improve accessibility in existing web applications using the Phoenix real-time social music app [LiveBeats](https://fly.io/blog/livebeats/) as an example.

    But what if we could integrate accessibility practices into our app development from the beginning, easily? Well, LiveView 0.18 recognizes the importance of accessibility and introduces a new range of built-in primitives that help us manage focus for more accessible LiveView apps, including [Phoenix.Component.focus_wrap/1](https://hexdocs.pm/phoenix_live_view/Phoenix.Component.html#focus_wrap/1), [JS.focus](https://hexdocs.pm/phoenix_live_view/Phoenix.LiveView.JS.html#focus/0), [JS.focus_first](https://hexdocs.pm/phoenix_live_view/Phoenix.LiveView.JS.html#focus_first/0), [JS.push_focus](https://hexdocs.pm/phoenix_live_view/Phoenix.LiveView.JS.html#push_focus/0), and [JS.pop_focus](https://hexdocs.pm/phoenix_live_view/Phoenix.LiveView.JS.html#pop_focus/0)

    Today, we&#39;ll explore these primitives with practical examples.

    ## Defining a navigation bar

    You are designing a navigation bar and have included default focusable tags, allowing users to navigate between its elements using the tab key. Additionally, it has incorporated a dropdown menu with submenus that can also be accessed using the keyboard:

    <%= video_tag "accessibility_01.mp4?card", title: "A navigation bar that can be navigated using the tab key is displayed. One of the navigation elements has a dropdown menu, but when the dropdown is opened, the focus is lost. If the user continues to tab, the focus returns to the navigation bar even if the dropdown is still open." %>


    While our nav bar appears to be functional, there are still a few details that require attention:

    1. The dropdown should [focus on the first available option](#hejyw-focusing-the-first-element-inside-a-container) when opened.
    1. The navbar element that was in focus prior to displaying the dropdown [should regain focus when the dropdown is closed.](#h7uls-focus-an-specific-element)
    1. After navigating through the dropdown options, focus currently shifts outside of the dropdown body and onto other elements in the navigation bar. To improve usability, only the list items in the dropdown should be [focusable when it is opened](#hykzs-manipulating-focus-programmatically).

    To address these issues, let&#39;s take a look at the dropdown code:

    ```elixir
    attr :id, :any, required: true

    slot :header

    def dropdown(assigns) do
      ~H"""
      <!-- Dropdown header -->
      <button id={@id}>
        <%%= render_slot(@header) %>
        ...
      </button>

      <!-- Dropdown body -->
      <div  id={"#{@id}-body"}>
        <ul id={"#{@id}-options"}>
          <li :for={option <- @option}>
            <.link>
              <%%= render_slot(option) %>
            </.link>
          </li>
        </ul>
       </div>
      """
    end

    ```

    The dropdown component has two main sections: the header, which is a button that displays the dropdown options, and the body, which contains the dropdown options themselves.

    Note that the component&#39;s `@id` is the same as the header button&#39;s id, which is also used to define the dropdown body and options container ids.

    With this in mind, let&#39;s address each of the issues!

    ## Focusing the first element inside a container

    Let&#39;s focus on the button that displays the dropdown options.

    We specify the function we want to invoke when the button is clicked, using the `phx-click`binding:

    ```elixir
    def dropdown(assigns) do
      ~H"""
      <!-- Dropdown header -->
      <button id={@id} phx-click={open_dropdown(@id)}>
        <%%= render_slot(@header) %>
        ...
      </button>

      <!-- Dropdown body -->
        ...
      """
    end

    ```

    Then we define the function `open_dropdown/2`:

    ```elixir
    def open_dropdown(js \\ %JS{}, id) when is_binary(id) do
      js
      |> JS.show(
          to: "##{id}-body",
          transition:
            {"transition-all transform ease-out duration-300",
             "opacity-0 translate-y-4 sm:translate-y-0 sm:scale-95",
             "opacity-100 translate-y-0 sm:scale-100"}
          )
      |> JS.focus_first(to: "##{id}-options")
    end

    ```

    To begin, we&#39;ll use the [JS.show/1](https://hexdocs.pm/phoenix_live_view/Phoenix.LiveView.JS.html#show/1) command to display the dropdown options container and then use the new [JS.focus_first/1](https://hexdocs.pm/phoenix_live_view/Phoenix.LiveView.JS.html#focus_first/1) command to set focus on the first element within the `<ul>` tag.

    The `JS.focus_first` command sets focus on the first focusable element of the specified selector. The element&#39;s selector can be specified using the `:to` option, or if left unspecified, focus will be set on the first child of the current element by default:

    <%= video_tag "accessibility_02.mp4?card", title: "The user is navigating through a navigation bar and when they open a dropdown, the focus is set to the first element within the dropdown." %>

    Tada! The first element is now automatically focused when the dropdown is opened. However, there is still an issue to address when closing the dropdown. Let&#39;s tackle that next!

    ## Focus a specific element

    Now let&#39;s address the second issue, which is to set focus on the last element that was focused before the dropdown was opened.

    To do this, let&#39;s focus on the last element that was focused before the dropdown was closed, the [link](https://hexdocs.pm/phoenix_live_view/Phoenix.Component.html#link/1) elements within the dropdown body:

    ```elixir
    def dropdown(assigns) do
      ~H"""
      <!-- Dropdown header -->
       ...

      <!-- Dropdown body -->
      <div  id={"#{@id}-body"}>
        <ul id={"#{@id}-options"}>
          <li :for={option <- @option}>
            <.link phx-keydown={close_dropdown(@id)} phx-key="escape">
              <%%= render_slot(option) %>
            </.link>
          </li>
        </ul>
       </div>
      """
    end

    ```

    We use `:phx-keydown` and `:phx-key`, to specify that the `close_dropdown/2` function is called when the user presses the escape key.

    Take a look at the code for the `close_dropdown/2` function below:

    ```elixir
    def close_dropdown(js \\ %JS{}, id) do
      js
      |> JS.hide(
        to: "##{id}-body",
        time: 200,
        transition:
          {"transition-all transform ease-in duration-200",
           "opacity-100 translate-y-0 sm:scale-100",
           "opacity-0 translate-y-4 sm:translate-y-0 sm:scale-95"}
      )
      |> JS.focus(to: "##{id}")
    end

    ```

    We use the [JS.hide/1](https://hexdocs.pm/phoenix_live_view/Phoenix.LiveView.JS.html#hide/1) command to hide the dropdown body, followed by the [JS.focus/1](https://hexdocs.pm/phoenix_live_view/Phoenix.LiveView.JS.html#focus/1) command with the `id` of the component&#39;s header to focus the dropdown button:

    <%= video_tag "accessibility_03.mp4?card", title: "The user opens the dropdown options and hits the escape key, the focus returns to the last navbar item that was focused before opening the dropdown."%>

    Excellent! With the opening and closing of the dropdown now functioning correctly, the next step is to ensure smooth navigation when the dropdown is open.

    ## Wrap the focused tab inside a container

    When the dropdown menu is open and we finish navigating its options, the focus shifts to the navigation bar instead of remaining within the dropdown. To prevent this from happening, we need to ensure that the focus remains inside the dropdown while it is open.

    The solution is simple. In LiveView 0.18, a new function component called `focus_wrap/1` was introduced, which allows us to wrap the focus tab within a single container.

    We just need to make a small change. Instead of using a `<div>` to define the body of the dropdown, we can use the [focus_wrap/1](https://hexdocs.pm/phoenix_live_view/Phoenix.Component.html#focus_wrap/1) function component to wrap the dropdown contents and ensure that the focus stays within the dropdown:

    ```elixir
    def dropdown(assigns) do
      ~H"""
      <!-- Dropdown header -->
       ...

      <!-- Dropdown body -->
      <.focus_wrap id={"#{@id}-body"}>
        <ul id={"#{@id}-options"}>
          <li :for={option <- @option}>
            <.link phx-keydown={close_dropdown(@id)} phx-key="escape">
              <%%= render_slot(option) %>
            </.link>
          </li>
        </ul>
       </.focus_wrap>
      """
    end

    ```

    Implementing this solution is simple. Now, when we open our dropdown options, we can simply wrap the options&#39; focus tab inside our component:

    <%= video_tag "accessibility_04.mp4?card", title: "After the user opens the dropdown in the navigation bar, they navigate through the dropdown options using the tab key. The focus remains within the dropdown body.
    "%>


    We&#39;ve made significant progress solving our issues, but this is not all LiveView can offer. In fact, we still have two more commands to cover.

    ## Changing focus programmatically

    In addition to the previous commands, there are a couple more commands that we can use to move and activate the focus at appropriate times: [JS.push_focus/2](https://hexdocs.pm/phoenix_live_view/Phoenix.LiveView.JS.html#push_focus/2) and [JS.pop_focus/0](https://hexdocs.pm/phoenix_live_view/Phoenix.LiveView.JS.html#pop_focus/0).

    To better understand these commands, let&#39;s consider an example scenario. Suppose you have a button that opens a modal to delete an item from a table. The modal presents two options - either delete the element or cancel the deletion process by pressing the Cancel button:



    <%= video_tag "accessibility_05.mp4?card", title: "The application displays a table with a list of users. Each row in the table has a button to delete the associated user. When the user selects a delete button with the Enter key, a modal dialog appears, asking the user to confirm whether they want to delete the selected user. " %>



    If the user decides to cancel the delete operation, we want to ensure that the focus returns to the button that opened the modal, even if the modal is not aware of which element triggered its display.

    To achieve this, we can use the `JS.push_focus/1` command to set the focus on the current button when the modal opens. Then, when the user clicks the Cancel button to exit the modal, we can activate the focus on the previously focused element using the `JS.pop_focus/0` command.

    Let&#39;s look at this code. We have a button that renders a small trash icon using [Heroicons](https://hexdocs.pm/heroicons_liveview/readme.html):

    ```elixir
    <.link
      id={"delete-user-#{user.id}"}
      phx-click={show_modal("delete-modal-#{user.id}") |> JS.push_focus()}

    >
      <Heroicons.trash fill="red" stroke="white" />
    </.link>

    ```

    When the user clicks on this button, it not only displays the modal but it also push the focus to itself using the `JS.push_focus/0` command.

    Next, when the user clicks the Cancel button within the modal, we can close the modal using the appropriate commands and use the `JS.pop_focus/0` command to activate the focus on the previously focused element:

    ```elixir
    <.button phx-click={hide_modal(@on_cancel, @id) |> JS.pop_focus}>
      Cancel
    </.button>

    ```

    By using these two commands, we are able to move the focus and activate it in two separate steps.

    Let&#39;s take a look at the final result:

    <%= video_tag "accessibility_06.mp4?card", title: "After selecting the delete button, a modal dialog appears, but the user decides to cancel the delete process and clicks the cancel button. The focus returns to the initial delete button after closing the modal.
    " %>

    Now that  looks good! The focus  movements  feel natural and obvious.

    <%= partial "shared/posts/cta", locals: {
      title: "Fly.io ❤️ Elixir",
      text: "Fly.io is a great way to run your Phoenix LiveView app close to your users. It's really easy to get started. You can be running in minutes.",
      link_url: "https://fly.io/docs/elixir/",
      link_text: "Deploy a Phoenix app today!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>"
    } %>

    ## Discussion

    LiveView&#39;s focus navigation commands provide a powerful tool to improve the accessibility and user experience of web applications. By using these commands, we can ensure that the focus is correctly managed and activated, allowing users to navigate through our app with ease. Whether it&#39;s navigating through dropdown menus or managing modal dialog boxes, LiveView&#39;s focus navigation commands provide an intuitive and reliable way to keep our users happy. So why not give them a try and see how they can improve your app&#39;s accessibility and user experience?
- :id: ruby-dispatch-mrsk-vs-flyio
  :date: '2023-03-16'
  :category: ruby-dispatch
  :title: MRSK vs Fly.io
  :author: rubys
  :thumbnail: mrsk-vs-flyio-thumbnail.webp
  :alt: MSRK logo vs Fly's balloon with boxing gloves
  :link: ruby-dispatch/mrsk-vs-flyio
  :path: ruby-dispatch/2023-03-16
  :body: |2


    [MRSK](https://world.hey.com/dhh/introducing-mrsk-9330a267) was introduced last month and it truly is a game changer.  From the announcement:

    > It sits on top of basic Docker, and harvests all the benefits you get from isolated containers with a sliver of the complexity associated with most other solutions. Instead of sending the deployment pipeline off to servers in the cloud, it runs entirely on your own machine. Just like Capistrano did.

    And it appears that the admiration is mutual:

    [![DHH Praise Tweet](dhh-praise-tweet.webp)](https://twitter.com/dhh/status/1632044101418745864)

    Aw, shucks.  Makes me want to blush.  I suspect that much of the admiration is due to the fact that [fly.io](https://fly.io/) shares more in common with your local data-center than with most cloud providers.  [We transmogrify Docker containers into lightweight micro-VMs and run them on our own hardware in racks around the world, so your apps can run close to your users](https://fly.io/blog/the-serverless-server/).  It is also worth noting that the back-end for our GraphQL API is a [Rails app](https://fly.io/docs/hiring/stack/).

    ---

    While there are plenty of differences between MRSK and fly.io, the starting and end points are pretty much the same.

    You start with an application and a Dockerfile.  Rails 7.1 will [provide a starter Dockerfile](https://fly.io/ruby-dispatch/rails-on-docker/).  `fly launch` will use this Dockerfile if it exists, otherwise it will provide Dockerfiles for all current versions of Rails as well as a number of other frameworks including Elixir, Laravel, Django, a number of Node frameworks, and even Rack apps.  We even make [dockerfile-rails](https://github.com/rubys/dockerfile-rails) available to all - you are welcome to use it to build Dockerfiles for your existing Rails applications and deploy to any datacenter or cloud using MRSK.

    The desired result is to have multiple instances of your application deployed, each connected to a common database, load balanced, protected by a firewall, with a SSL certificate, and monitored with [health checks](https://fly.io/ruby-dispatch/health-checks/).

    ---

    DHH followed up with a video demonstrating the workflow using MRSK:

    <div align="left" style="margin-bottom: 1em">
      <a href="https://www.youtube.com/watch?v=LL1cV2FXZ5I">
        <img src="https://img.youtube.com/vi/LL1cV2FXZ5I/0.jpg" style="width:100%;">
      </a>
    </div>

    Following are key points in the video:

    | Time | Description |
    | ---- | ----------- |
    | [2:15](https://www.youtube.com/watch?v=LL1cV2FXZ5I&t=134) | MRSK requires a bit of knowledge of Linux and Docker |
    | [2:55](https://www.youtube.com/watch?v=LL1cV2FXZ5I&t=174) | `rails new ship --main --css tailwind` |
    | [3:02](https://www.youtube.com/watch?v=LL1cV2FXZ5I&t=181) | `cd ship` |
    |      | `rails g scaffold post title:string body:text` |
    |      | `rails db:migrate` |
    | [3:39](https://www.youtube.com/watch?v=LL1cV2FXZ5I&t=218) | Hetzner - select region
    |      | Create server
    | [4:12](https://www.youtube.com/watch?v=LL1cV2FXZ5I&t=251) | create `config/deploy.yml`
    |      | specify the service name, image name
    | [4:20](https://www.youtube.com/watch?v=LL1cV2FXZ5I&t=259) | grab the ip address, paste into deploy.yml
    | [4:28](https://www.youtube.com/watch?v=LL1cV2FXZ5I&t=267) | set up an environment template: mrsk envify
    | [4:44](https://www.youtube.com/watch?v=LL1cV2FXZ5I&t=283) | `mrsk deploy`
    | [6:31](https://www.youtube.com/watch?v=LL1cV2FXZ5I&t=390) | note that docker may need to be restarted;
    |      | recommend remote docker buildx setup
    | [7:15](https://www.youtube.com/watch?v=LL1cV2FXZ5I&t=434) | create another vm for db and app servers
    | [7:32](https://www.youtube.com/watch?v=LL1cV2FXZ5I&t=451) | add two firewalls - ports 80 and 22 for the app servers, 3306 for db
    | [8:28](https://www.youtube.com/watch?v=LL1cV2FXZ5I&t=507) | add load balancer
    | [8:55](https://www.youtube.com/watch?v=LL1cV2FXZ5I&t=534) | change rails db config, install gem, change `config/deploy.yml`
    | [9:40](https://www.youtube.com/watch?v=LL1cV2FXZ5I&t=579) | create database
    | [10:09](https://www.youtube.com/watch?v=LL1cV2FXZ5I&t=608) | update `config/database.yml`
    | [10:23](https://www.youtube.com/watch?v=LL1cV2FXZ5I&t=622) | update env template with mysql root password
    | [10:44](https://www.youtube.com/watch?v=LL1cV2FXZ5I&t=643) | `mrsk setup`
    | [11:20](https://www.youtube.com/watch?v=LL1cV2FXZ5I&t=679) | `mrsk redploy`
    | [11:38](https://www.youtube.com/watch?v=LL1cV2FXZ5I&t=697) | `mrsk details`
    | [12:39](https://www.youtube.com/watch?v=LL1cV2FXZ5I&t=758) | `mrsk rollback`
    | [13:11](https://www.youtube.com/watch?v=LL1cV2FXZ5I&t=790) | setup domain name using Cloudflare
    | [13:48](https://www.youtube.com/watch?v=LL1cV2FXZ5I&t=827) | http3 and compression
    | [14:22](https://www.youtube.com/watch?v=LL1cV2FXZ5I&t=861) | failure - look at logs with nice grep feature!
    | [14:44](https://www.youtube.com/watch?v=LL1cV2FXZ5I&t=883) | origin mismatch
    | [15:06](https://www.youtube.com/watch?v=LL1cV2FXZ5I&t=905) | uncomment two lines in `config/environments/production.rb`
    | [15:28](https://www.youtube.com/watch?v=LL1cV2FXZ5I&t=927) | `mrsk redeploy`

    Inspired by DHH's video, I made one of my own, developing and deploying the same application.  I substituted fly.io for Hetzner.  Rails 7.0.4.3 for Rails 7.1.0.alpha. And Postgresql for MySQL.

    <div align="left" style="margin-bottom: 1em">
      <a href="https://www.youtube.com/watch?v=eHWPx04J1OY">
        <img src="https://img.youtube.com/vi/eHWPx04J1OY/0.jpg" style="width:100%;">
      </a>
    </div>

    Here are the key points in the video:

    | Time | Description |
    | ---- | ----------- |
    | [1:21](https://www.youtube.com/watch?v=eHWPx04J1OY&t=80) | `rails new ship --css tailwind`
    |      | `cd ship`
    | [1:50](https://www.youtube.com/watch?v=eHWPx04J1OY&t=109) | `rails g scaffold post title:string body:text`
    | [1:54](https://www.youtube.com/watch?v=eHWPx04J1OY&t=113) | `flyctl launch --force-machines --region ord`
    | [3:08](https://www.youtube.com/watch?v=eHWPx04J1OY&t=187) | `fly deploy`
    | [4:30](https://www.youtube.com/watch?v=eHWPx04J1OY&t=269) | `fly open`
    | [5:02](https://www.youtube.com/watch?v=eHWPx04J1OY&t=301) | `fly machine clone --region cdg`
    | [5:48](https://www.youtube.com/watch?v=eHWPx04J1OY&t=347) | `fly dashboard`
    | [7:25](https://www.youtube.com/watch?v=eHWPx04J1OY&t=444) | `vi app/views/posts/index.html.erb`
    | [7:42](https://www.youtube.com/watch?v=eHWPx04J1OY&t=461) | `fly deploy`

    <%= partial "shared/posts/cta", locals: {
      title: "You can play with this right now.",
      text: "I used the [Fly.io terminal](https://fly.io/terminal) for this demo - try it yourself if you have a GitHub account or an email address.  You can also run this on your own machine, we have a [Hands-on](https://fly.io/docs/hands-on/) guide that will walk you through the steps.",
      link_url: "https://fly.io/docs/rails/",
      link_text: "Try Fly for free&nbsp;&nbsp;<span class='opacity-50'>&rarr;</span>"
    } %>

    Like DHH, I started with a cold cache - in particular with no Docker layers already having previously been built.
    Unlike DHH, I chose **NOT** to fast forward over the boring parts, as I want
    to realistically depict how long it takes to deploy an application.

    ---

    Both Fly.io and MRSK are young, and undoubtedly will learn much from each other.  I am particularly envious of rollback, love the log grep feature, and while Cloudflare has moved on to HTTP/3, Fly.io is currently only at HTTP/2.

    As both MRSK and fly.io share the goal of providing to the developer only a <em>sliver of the complexity associated with most other solutions</em>,
    the true winners of these types of competition are developers everywhere.

    By standardizing on a common piece of infrastructure, in this case Dockerfiles,
    people are free to pick the solution that best suits their needs, confident in
    the fact that they can switch back and forth or even use both simultaneously as
    there is no lock in here.

    We welcome you to compare our
    [prices](https://fly.io/docs/about/pricing/) against your favorite data center
    or cloud, and to draw your own conclusions as to matters like developer
    ergonomics and ease of use.
- :id: laravel-bytes-livewire-autocomplete
  :date: '2023-03-16'
  :category: laravel-bytes
  :title: Autocomplete with Livewire
  :author: fideloper
  :thumbnail: fly-books-thumb.png
  :alt: Studying up for the Fly.io exam."
  :link: laravel-bytes/livewire-autocomplete
  :path: laravel-bytes/2023-03-16
  :body: "\n\n<p class=\"lead\">We're gonna do some autocompletion with Livewire.
    Livewire works best when your app is close to your user. With Fly.io, you can
    get your [Laravel app running](/docs/laravel/) globally in minutes!</p>\n\nThe
    deal with React is that I don't want it, but I'm jealous of the quality of React
    components.\n\nAuto complete fields are an example. There's a lot of hidden complexity
    and (even still) browser compatibility issues. \n\nHere's the rub: There are so
    many people using React that the quality of components is often very high. Porting
    that quality over to other frontends (such as Livewire) is hard!\n\nWord on the
    street is that Livewire is actually working on an auto-complete field of its own.
    But I want something *now*. Luckily, there just so happens to be [pretty good
    browser support for this idea](https://caniuse.com/datalist).\n\nIt's workable-but-kinda-ugly
    enough for me to love it.\n\n## Native Autocomplete\n\nIt turns out that modern
    browsers support the idea of auto-complete via [datalist](https://caniuse.com/datalist).\n\nWhen
    combined with a text input, we get a list of stuff that a person can select. This
    comes comes with some bells and whistles such as keyboard shortcuts to navigate
    the list. \nHere's an interactive look at [how to do that](https://play.tailwindcss.com/3CmyVgLdlo).
    \nThis will perform some basic auto-complete based on the options in the `datalist`.\nIf
    you select an option from the list of possible values, it will update the text
    input's `value`. Nice.\n\n![html input with datalist providing native autocomplete](tailwind-play-autocomplete-field-cover.webp)\n\nHere's
    where it gets a bit wonky, however. The only value we can get from the selected
    data is the *display* value. \n\nOften you want some machine-readable value but
    with a human-readable label. Maybe we can add some JS and a `data` attribute?\n\n```html\n<datalist
    id=\"some-data\">\n    <option data-value=\"1\" value=\"foo\" />\n    <option
    data-value=\"2\" value=\"bar\" />\n    <option data-value=\"3\" value=\"baz\"
    />\n</datalist>\n```\n\nThat looks good, we should be able to get the value from
    the datalist for a given selected label. But...we can't. \n\n## Getting Values\n\nWe
    want to figure out a way to get the machine-readable value held in the `data-value`
    attributes when a user selects a label.\n\nUnfortunately we can't actually listen
    for `change` events directly on the `datalist` element. Instead, we need to use
    a `change` event on the `<input />` itself.\n\nThis gives us an avenue to get
    the values we want, it's just a teansy bit hacky. But this is Javascript &mdash;
    When in Rome!\n\nHere's what we'll do. We listen for `change` events on the text
    input. When its value is changed, we find a matching value in the `datalist` and
    grab it's `data-value` attribute. \nThis means we're matching text input value
    with the datalist value, using the human-readable label. I don't really love this,
    but it works for most use cases.\n\n```js\n// When we change the value of the
    text input\nlet onChange = (e) => {\n    // Get the text input value\n    // It
    will be the human-readable label from the\n    // datalist's value=\"foo\" attribute\n
    \   let value = e.target.value\n\n    // Get the data-value attribute by selecting
    the datalist element\n    // with a matching value ('foo', 'bar', 'baz' in our
    case)\n    // This might create an invalid css selector, \n    // but you could
    also find the datalist element\n    // and do a foreach on its child options\n
    \   let selected = document.body.querySelector(\"datalist [value=\\\"\"+value+\"\\\"]\")\n\n
    \   // If we find the selected option, grab the\n    // machine-readable ID from
    the data-value attribute\n    if (selected) {\n    \tlet id = selected.dataset.value\n
    \       console.log('selected value is:', id)\n    }\n}\n```\n\nThen we add that
    function as the listener for that input:\n\n```html\n<input \n    type=\"text\"
    \n    name=\"autocomplete\" \n    class=\"rounded\"\n    list=\"some-data\" \n
    \   placeholder=\"choose a thing\"\n    onchange=\"onChange(event)\" />\n```\n\nHere's
    a [JS Fiddle](https://jsfiddle.net/g59f1cev/) you can use to play with that.\n\nNow
    we can get the `data-value` attribute to get a numerical ID (1, 2, 3) that relates
    to the human-readable label (foo, bar, baz) used when selecting a possible value
    from that dropdown list.\n\n<%= partial \"shared/posts/cta\", locals: {\n  title:
    \"Fly.io ❤️ Laravel\",\n  text: \"Speed up Livewire and Fly your servers close
    to your users. Deploy globally on Fly in minutes!\",\n  link_url: \"https://fly.io/docs/laravel\",\n
    \ link_text: \"Deploy your Laravel app!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>\",\n}
    %>\n\n## Livewire\n\nWe can translate this to Livewire (and a hint of AlpineJS)
    pretty easily! \nThe one thing I've added is populating the `dataset` options
    list dynamically based on user input. This was useful for a project where I auto-completed
    user addresses.\n\nAssuming you have Livewire installed, we can create a new component
    and use that to help autocomplete based on user input.\n\n```bash\nphp artisan
    make:livewire AddressAutocomplete\n```\n\nThis generates files:\n\n1. `app/Http/Livewire/AddressAutocomplete.php`\n2.
    `resources/views/livewire/address-autocomplete.blade.php`\n\nThe template file
    can contain (among other things for fancier presentation), the following:\n\n```php\n<form\n
    \   class=\"space-y-8 divide-y divide-gray-200\"\n    x-data='{\n        addressSelected(e)
    {\n            let value = e.target.value\n            let id = document.body.querySelector(\"datalist
    [value=\\\"\"+value+\"\\\"]\").dataset.value\n            \n            // todo:
    Do something interesting with this\n            console.log(id);\n        }\n
    \   }'\n>\n    <input\n        type=\"text\"\n        list=\"streetAddressOptions\"\n
    \       wire:model=\"streetAddress\"\n        class=\"fancy-tailwind-things\"\n
    \       x-on:change.debounce=\"addressSelected($event)\"\n    >\n\n    <datalist
    id=\"streetAddressOptions\">\n        @foreach($searchResults as $result)\n            <option\n
    \               wire:key=\"{{ $result->uniqueKey }}\"\n                data-value=\"{{
    $result->uniqueKey }}\"\n                value=\"{{ $result->fullAddress }}\"\n
    \           ></option>\n        @endforeach\n    </datalist>\n</form>\n```\n\nWe
    use AlpineJS's `x-data` on the `<form>` to define function `addressSelected`.
    This is the `change` event handler when the value of our input is updated. \nJust
    like we saw up above, this matches a given address to an option in the `datalist`
    and grabs the `data-value` attribute, so we get a numerical ID (or whatever we
    need for the machine-readable data).\n\nOn the `input`, we use Alpine again to
    listen for `change` events (with debounce used, so we don't send data over the
    wire on every keystroke).\n\nThe dynamic part here is the `@foreach` loop that
    updates the `options` in the `datalist`. This gets updated dynamically by Livewire.\n\nThe
    `$seachResults` variable gets updated based on the value that's in the text input.
    We've used Livewire to wire up that text input to PHP variable `$streetAddress`.\n\nTo
    decide what populates the `datalist` options, we need to look at our Livewire
    component controller, file `app/Http/Livewire/AddressAutocomplete.php` (where
    variable `$streetAddress` is defined).\n\n```php\n<?php\n\nnamespace App\\Http\\Livewire;\n\nuse
    Livewire\\Component;\nuse Facades\\App\\Services\\Smarty\\Smarty;\n\nclass AddressAutocomplete
    extends Component\n{\n    public string $streetAddress = '';\n    public string
    $city = '';\n    public string $state = '';\n    public string $zip = '';\n    public
    string $country = '';\n    public array $searchResults = [];\n\n    // Magic method
    that is fired when `streetAddress` is updated\n    public function updatedStreetAddress()\n
    \   {\n        if($this->streetAddress != '') {\n            // An array of SearchResults\n
    \           $this->searchResults = \n                Smarty::searchAddress($this->streetAddress);\n
    \       } else {\n            $this->searchResults = [];\n        }\n    }\n\n
    \   public function render()\n    {\n        return view('livewire.address-autocomplete');\n
    \   }\n}\n```\n\nI used the [Smarty](https://www.smarty.com/) address API (not
    really shown, it's hiding behind the `Smarty` facade) to take the user input and
    return a list of possible matching addresses.\n\nThe `updatedStreetAddress` function
    is a bit of [magic that Livewire provides](https://laravel-livewire.com/docs/2.x/lifecycle-hooks).
    When variable `streetAddress` is updated, that method is called if it's present.
    \nThat property is updated when there is user input, and so we can use that to
    have Smarty return a set of addresses to us.\n\n![autocompleting addresses](final-autocomplete.png)\n\nThe
    search results are set in the `$searchResults` variable, which is sent back to
    the frontend and populates the `datalist`. And boom, we have an autocomplete field!\n"
- :id: phoenix-files-phoenix-dev-blog-sounds-like-a-bug
  :date: '2023-03-15'
  :category: phoenix-files
  :title: Phoenix Dev Blog - Sounds Like a Bug
  :author: chris
  :thumbnail: phoenix-dev-blog-thumbnail.webp
  :alt: Phoenix dev blog cover illustration
  :link: phoenix-files/phoenix-dev-blog-sounds-like-a-bug
  :path: phoenix-files/2023-03-15
  :body: |2


    <p class="lead">This dev blog digs deeper into fixing an 8-year old bug. We learn more about the internals of Phoenix and LiveView in the process. Fly.io is a great place to run a Phoenix application! Check out how to [get started](/docs/elixir/)!</p>


    I've had my fair share of bug reports over the ~10 year life of maintaining Phoenix. Most are mundane Elixir tweaks, or wrangling some JavaScript issues. The core of Phoenix has been baked for years now, so color me surprised when I found myself spelunking through code about as old as Phoenix itself trying to make sense of what I was seeing.

    ## The bug report

    What's interesting about this report is that it almost went unaddressed. It could have been another 10 years before I poked at it.  I caught a passing comment on the Phoenix slack about an obscure problem. The user hadn't yet filed a GitHub issue, so if I had missed this comment, it may have been lost in the ether.

    ![Possible race condition in a liveview? I have a liveview that calls push_navigate to the same liveview. This causes mount to be called, which calls push_event to a JS hook, and the hook responds back with pushEventTo targeting the liveview. Roughly 25% of the time, that pushEventTo is not triggering the handle_event that should be called. Is there something about the lifecycle that is causing this?](./bug-report-1.png?center&card)

    This description makes it sound like a somewhat "obscure" issue, in that there are a couple features being used together, JavaScript Hooks and Live Navigation, at a specific point in the LiveView lifecycle. So when I read this, I thought it was a potential bug, but sure to be infrequently hit in the wild.

    ![Chris McCord: sounds like a bug](./bug-report-2.png?center&card)

    > Narrator: Yes Chris, it was indeed a bug.

    The user followed up with more info:

    ![@chrismccord following up on your comment "sounds like a bug". I am encountering the issue when using long-polling, but not websockets. Is there something in my code that I would need to change to account for this?](./bug-report-3.png?center&card)

    Great. So it happens only with long polling, and only when using JavaScript Hooks with Live navigation, and only during the LiveView mount. Nothing to get too worked up about, right?!

    Wrong.

    In about five minutes I had recreated the issue where the application would break, but to my dismay it was much worse than reported. What sounded like an obscure race of JavaScript Hooks pushing and receiving events to a LiveView doing live navigation sent me down a four hour rabbit hole.

    To try to isolate the issue, I commented out my JavaScript hook, and simply let the app do a `push_navigate` in the LiveView when a button was clicked.

    The client completely broke, about 25% of the time. This is bad.

    ## Live Navigation explained

    To understand why this is bad, let me explain a bit about what "Live Navigation" is and what the call to `push_navigate` is. Phoenix LiveView works by rendering the UI on the server and diffing changes back to the browser. We have an abstraction for navigating from page to page without requiring a full browser reload. We do this by issuing a page navigation over the existing transport, such as a WebSocket frame, or a Long-poll message. This has the benefit of avoiding extra HTTP handshakes for WebSockets, and avoiding full page loading and parsing in the case of long-polling.

    So rather than doing a `redirect(socket, to: "/path")` a user may `push_navigate(socket, to: "/path")`, and the browser updates and loads the page, but with a nice performance boost. This is a fundamental features of LiveView and one you should use whenever you can.

    So how was this fundamental feature breaking so easily, and how was this not yet reported? Also, _how_  _in the world_ was this a bug. We hadn't touched long-poll code in probably 8 years on the client or server.

    ## Lay of the land

    To understand the bug, we need to understand a little bit of how LiveView and Phoenix channels works. Phoenix LiveView is built on top of Phoenix channels, which is an abstraction for bidirectional client/server messaging. We get the same interface whether you're running over WebSockets (the default), or long-polling, or any other custom channel transport that speaks the channel protocol.  The interface looks like this:

    ```javascript
    let mainChatChannel = socket.channel("room:1")
    mainChatChannel.join()
      .receive("ok", ({welcome}) => alert(`joined! ${welcome}`))
      .receive("error", ({reason}) => alert(`error: ${reason}`))

    mainChatChannel.on("new_msg", ({body}) => alert(`room:1: ${body}`)

    let privateChatChannel = socket.channel("private:123")
    privateChatChannel.join()
      .receive("ok", ({welcome}) => alert(`joined! ${welcome}`))
      .receive("error", ({reason}) => alert(`error: ${reason}`))

    privateChatChannel.on("new_msg", ({body}) => alert(`private: ${body}`)
    ```

    One neat thing about channels is they are multiplexed. This means we can open any number of channels on a single physical connection. Channels are identified by a _topic_, which is a unique string, such as `"room:1"` or `"private:123"` above which allows these isolated "channels" of communication across a single wire.

    For LiveView, when you connect to the server, we open a channel for each LiveView in the UI. The LiveViews representing each part of the UI each get their own channel. This is nice because each channel process on the server runs concurrently. Blocking work in one channel won't block work in another, even for the same browser user.

    So we have multiplexed channels and each channel joins a topic. LiveViews are channels underneath, and so they also use a topic. LiveView topics are randomly generated.

    The LiveView topic is signed into a token when first rendering the page over the regular HTTP request. We affectionately call this the "dead render" or "dead mount".  On the dead render, we place the token into the page. Then the LiveView connects over channels for the live render, lifts the token from the HTML document, and joins the channel with the topic we signed in the token.

    We use the token to verify that the channel topic the client is trying to join is indeed the one we leased to them. Within this token is the LiveView module name, router, and other metadata of the code paths we'll invoke when the user performs a live mount.

    When you perform live navigation like `push_navigate` the client will _reuse the channel topic_ for the new LiveView.

    Hang with me. We're almost there where we'll be able to WTF together about the bug.

    ## Reusing Channel Topics

    So the LiveViews have topics they join, and they are randomly generated and verified on the server. So why are they re-used for `push_navigate`? Why not generate a new one? Remember, the token we signed from the _dead_ mount contains the topic. Our whole goal is to avoid making new HTTP requests to fetch a whole new page. We want to take advantage of the existing channel connection. This means re-using the signed token, and thus re-using the channel topic for the "old" LiveView we are navigating away from. The LiveView client conceptually does the following on live navigation:

    ```javascript
    this.mainLVChannel.leave()
    this.mainLVChannel = socket.channel(this.mainLVChannel.topic, {token, url: newURL}))
    this.mainLVChannel.join()
      .receive("ok", (diff) => {
        Browser.pushState(newURL)
        render(diff))
      })
    ```

    To perform live navigation, the client leaves the current LiveView channel and joins a new channel with the same topic. It passes its token from the original HTML document, and the new URL that it wishes to navigate to. On the server, LiveView verifies that the route for the `newURL` is allowed to be accessed from the old LiveView/URL, and it starts the new channel process. The browser gets the diff on the wire, and updates the URL in the address bar via push state.

    ## Duplicate Topics

    At this point, the final piece to understand about channels is the way topics are handled in the transport. Since channels are multiplexed, a topic must be unique on the physical connection. If you try run the following client code:

    ```javascript
    let chan1 = socket.channel("room:123")
    chan1.join()

    let chan2 = socket.channel("room:123")
    chan2.join()
    ```

    The server will close `chan1`, and start a new process for `chan2`. Each channel instance also carries a unique `join_ref` , which is essentially a unique session ID . This is important because it allows the server and client to avoid sending latent messages for a given topic to a newer instance on the client, if for example we raced a `phx_close` event from the server or vice versa.

    So if the Phoenix transport layer handles duplicate topics already, why are we having a bug with `push_navigate` re-using topics? And why only with the long-poll transport?

    ## Diagnosing the bug

    The bug is experienced in the application as the UI becoming unresponsive. The socket connection remains up, but when a user clicks a navigation link, everything just… stops.

    Here's the flow of events that I pieced together, when viewed from the client:

    ```
    1. User visits page LiveView page A
    2. User clicks a link that does push_navigate to LiveView page B
    3. client longpoll: send phx_leave page A
    4. client longpoll: send phx_join page B
    5. client longpoll: receive ack for # 3 phx_leave
    6. client longpoll: receive ack for # 4 phx_join
    7. client longpoll: receive phx_close page B (normal close)
    ```

    Everything in the logs makes sense until #7. When the user clicks a navigation link, the client sends the correct order of messages. A leave event for the current channel, a join to the new channel, and acknowledgments for each. The new LiveView channel is joined properly, but then _it immediately closes with a normal closure from the server._

    WTF.

    Remember, the Phoenix server handles back-to-back re-use of channel topics just fine. The long-poll client sends the `phx_leave` before the new `phx_join` as well, so how in the world are we closing down the new channel?

    I spent about hour or so of WTF'ery trawling through Phoenix long-poll and transport layer source that I hadn't seen in years. I then called [@peregrine](https://twitter.com/peregrine) , a phoenix-core member and fellow Fly'er. We spelunked through the server code for a couple more hours and  pieced together the following flow of events  when viewed from the server:

    ```
    1. mount LiveView page A
    2. push_navigate to LiveView page B
    3. server longpoll: receive phx_join page B
    4. server longpoll: receive phx_leave page A
    ```

    25% of the time we are receiving the `phx_leave` _after_ the `phx_join`, even though the client sent the HTTP requests as `phx_leave`, `phx_join`. So the client sends the correct order, but the server processes them out of order.

    I realize at some point that the entire architecture of long-polling is inherently race condition prone. The long-poll transport that has existed for 10 years is fundamentally flawed. Great. But why?

    The way long-polling works to simulate a bidirectional pipe by POST'ing events to the server for `channel.push()`, and GET'ing events from the server for `channel.on` by repeatedly polling the server and hanging the connection awaiting events. Hanging the connection waiting for events is the "long" in long-polling,

    Meanwhile a process sits on the server and buffers events in between the client going down for each new poll request. This works great, but therein lies the flaw.

    Since long-poll issues separate HTTP requests for pushed events, those HTTP requests are handled concurrently by the server. Either on the same server in different TCP acceptors from the connection pool, or by different servers entirely due to load balancing. The long-poll transport handles proxying the messages back to the original server in the load-balancing case, but a race condition exists here even for single-server use.

    Because the HTTP requests are processed concurrently and relayed to the long-poll process, there is no guarantee they will arrive in order.

    This is bad.

    It explains our bug where the server sees the `phx_join`, then the `phx_leave`. Fortunately, we already have a `join_ref` which uniquely tracks each channel, so we can detect this situation and prevent the latent close from killing the new channel.

    This is the [patch](https://github.com/phoenixframework/phoenix/commit/4f48386d79942f8b842001866c5f659516bcd26a) in the channel transport layer that resolved the original bug report:

    ```elixir
    defp handle_in({pid, _ref, _status}, %{event: "phx_leave"} = msg, state, socket) do
      %{topic: topic, join_ref: join_ref} = msg

      case state.channels_inverse do
        # we need to match on nil to handle v1 protocol
        %{^pid => {^topic, existing_join_ref}} when existing_join_ref in [join_ref, nil] ->
          send(pid, msg)
          {:ok, {update_channel_status(state, pid, topic, :leaving), socket}}

        # client has raced a server close. No need to reply since we already closed
        %{^pid => {^topic, _old_join_ref}} ->
          {:ok, {state, socket}}
      end
    end
    ```

    I added a `handle_in` clause for handling incoming messages which looks for `phx_leave` events. If we receive a `phx_leave` with a `join_ref` matching the currently tracked channel, we process it as normal and leave the channel. If we find a mismatched `join_ref`, it is necessarily from a latent close and we noop. This is also nice because an incorrectly coded client could issue  a latent leave.

    This fixed the bug report. But we can't celebrate too much yet.

    ## Fixing the fundamental long-poll flaw

    Fixing the latent leave issue still leaves us with long-poll backed channels that have no message ordering guarantees. This is bad. The following code could not be trusted to deliver messages in order:

    ```javascript
    let chatChannel = socket.channel("rooms:123")
    chatChannel.push("new_msg", {body: "this is my first message!"})
    chatChannel.push("new_msg", {body: "this is my second message!"})
    ```

    The code resulted in the following timing issue:

    - Two POST requests  are sent (in order from the client perspective).
    - We can't count on receiving them in that order.
    - Depending on which TCP acceptor picked up the request and which core/scheduler processed the request first, the second message could arrive before the first.
    - Ditto if you load balance to a completely different server for one of the requests and we have to bounce the message back to you over distributed Elixir.

    The original bug report could be fixed only with server changes. Fixing this fundamental race condition requires client-side changes. And there are tradeoffs to make this right.

    Fundamentally we must guarantee channel push ordering from client to server. The easiest solution would be to make channel pushes synchronous. Internally we'd queue all pushes and await an acknowledgment before sending the next one. This would guarantee order, even if you load balance to different servers each time.

    It would also be super slow.

    The great thing about WebSockets is we perform the HTTP/TLS handshake a single time and then we get our bidirectional pipe that lets us spam messages one after another – order is preserved. With long-polling, we emulated this, but having to serialize each individual `channel.push` would require a full round trip one-by-one. It would also mostly eliminate the benefit of live navigation because we'd have to make two round trips to navigate between pages. One to leave the current channel, and one to join the new one. This is a nonstarter.

    Still, we need a solution, and it needs to be correct.

    Where we landed is a client-side queue that batches requests. This way we strike a balance between synchronous messages and the full fire-and-forget of WebSockets.

    When `push` is invoked, we first check to see if we are awaiting an acknowledgement from a previous batch. If not, we can issue the POST, otherwise we queue it up to be sent as soon as the acknowledgement arrives.

    This is great, but it still suffered the 2x round trip for live navigation because a back-to-back `phx_leave` + `phx_join` on an empty queue would immediately send the leave, and wait a full round trip to send the join. This is not any better for the live navigation case that with an empty queue.

    I fixed this with a little trick. We know we have to batch, but what we really care about is optimizing the case where you have some procedural code in the same event loop like our chat messages above, or the LiveView navigation.

    If we are pushing with no current batch, we can schedule the batch to run on the next tick of the JavaScript event loop via a `setTimeout(() => .., 0)`. This allows us to catch pushes that happen in the same event loop as the first push. When the next tick fires "0" milliseconds later, we'll have our back-to-back pushes queued up and sent together in one batch.  Here's the notable snippets from my [patch](https://github.com/phoenixframework/phoenix/commit/2674c6ea30634667f9b09966b90269393b445953) to`phoenix.js` and the `LongPoll` transport where I used this approach:

    `longpoll.js`:

    ```diff
      send(body){
    -   this.ajax("POST", body, () => this.onerror("timeout"), resp => {
    +   if(this.currentBatch){
    +     this.currentBatch.push(body)
    +   } else if(this.awaitingBatchAck){
    +     this.batchBuffer.push(body)
    +   } else {
    +     this.currentBatch = [body]
    +     this.currentBatchTimer = setTimeout(() => {
    +       this.batchSend(this.currentBatch)
    +       this.currentBatch = null
    +     }, 0)
    +   }
      }

    + batchSend(messages){
    +  this.awaitingBatchAck = true
    +  this.ajax("POST", "application/ndjson", messages.join("\n"), () => this.onerror("timeout"), resp => {
    +    this.awaitingBatchAck = false
         if(!resp || resp.status !== 200){
           this.onerror(resp && resp.status)
           this.closeAndRetry(1011, "internal server error", false)
    +    } else if(this.batchBuffer.length > 0){
    +      this.batchSend(this.batchBuffer)
    +      this.batchBuffer = []
         }
       })
      }
    ```

    `longpoll.ex`:

    <aside class="right-sidenote">Puzzled by `application/ndjson`? That's to process [ndjson](http://ndjson.org/) formatted JSON. It's just regular JSON that uses newlines to delimit values.</aside>

    ```diff
      defp publish(conn, server_ref, endpoint, opts) do
        case read_body(conn, []) do
          {:ok, body, conn} ->
    -       status = transport_dispatch(endpoint, server_ref, body, opts)
    +       # we need to match on both v1 and v2 & wrap for backwards compat
    +       batch =
    +         case get_req_header(conn, "content-type") do
    +           ["application/ndjson"] -> String.split(body, ["\n", "\r\n"])
    +           _ -> [body]
    +         end

    +       {conn, status} =
    +         Enum.reduce_while(batch, {conn, nil}, fn msg, {conn, _status} ->
    +           case transport_dispatch(endpoint, server_ref, msg, opts) do
    +             :ok -> {:cont, {conn, :ok}}
    +             :request_timeout = timeout -> {:halt, {conn, timeout}}
    +           end
    +         end)

            conn |> put_status(status) |> status_json()

          _ ->
            raise Plug.BadRequestError
        end
      end
    ```

    The good news is took a remarkably small amount of code on the client and server to resolve these fundamental issues. Ordering is now guaranteed, and long-polling is still able to be reasonably performant when pushing messages. The bad news was it cost a couple days of work after all was said and done.

    ## Why it took 10 years for someone to report this

    It seems like such a fundamental design flaw would have been reported by now, right?  I think there's a couple reasons for this.

    First, the "app is completely broken" failure mode only happened when you have a duplicate topic do a back-to-back channel leave and join. LiveView does this, but it's not a typical pattern for most regular Phoenix channel applications, where each channel instance will be associated to a unique topic – think each slack DM having its own topic. LiveView has been out for a few years, but the six years prior saw most channels applications simply never use this kind of pattern.

    Second, it only affected the long-poll transport, which ships with Phoenix, but is not enabled by default. The vast majority of folks aren't running long-poll today because it's a knob they only turn when they need it.

    I'm super happy I was able to connect with the passing issue on Slack and get this fix in place. While I was tending to this "ancient" Phoenix code path, I also took the opportunity to bake in some new long-poll enhancements, which will be the topic for another post.

    Happy hacking,

    –Chris

    <%= partial "shared/posts/cta", locals: {
      title: "Fly.io ❤️ Elixir",
      text: "Fly.io is a great way to run your Phoenix LiveView app close to your users. It's really easy to get started. You can be running in minutes.",
      link_url: "https://fly.io/docs/elixir/",
      link_text: "Deploy a Phoenix app today!  <span class='opacity:50'>→</span>"
    } %>
- :id: laravel-bytes-cart-is-now-in-session
  :date: '2023-03-13'
  :category: laravel-bytes
  :title: Cart is now in session
  :author: johannes
  :thumbnail: cart-thumbnail.webp
  :alt: Two people with livewire icons as heads looking at a shopping cart
  :link: laravel-bytes/cart-is-now-in-session
  :path: laravel-bytes/2023-03-13
  :body: |2-


    <p class="lead">Fly takes a Docker image, converts it to a VM, and runs that VM anywhere around the world. [Run a Laravel app](https://fly.io/docs/laravel/) in minutes!</p>

    Shopping carts don't empty out when the user leaves the page. Why? Well, for that I'll need to take you back in time. Cue the wavy flashback transition!

    Once upon a time, there was an everyday, regular, normal guy named Jack. Jack loved shopping online and often spent hours browsing various websites for the best deals. He would add items to his shopping cart, but he never seemed to order them. Instead, he would often check back on his cart, admiring the items he had selected, but never actually clicking the &quot;buy&quot; button. Jack would order them only when he was completely one hundred percent definitely certain he wants them.

    Do I know Jack?

    ![](obi_wan.jpg)

    Yes, I have a habit of filling shopping carts with stuff I'll order months later, or just outright forget. ChatGPT-generated fairy tales about my character flaws aside, we're here to talk about the real heroes of the story: the shopping carts that thanklessly hold our possible future buys on various webshops. Remember to thank them for their service sometime.

    Anywho, let's build one to see how they work!

    These are the topics this article touches on :

    - http session, and the session facade in Laravel
    - Livewire
    - the Service pattern
    - the Money package
    - Laravel collections

    ## The Goal

    Our end goal will be to create a new order with all the products that the user has in their shopping cart. To enable that, we'll need some Livewire components for updating the values without reloading the page as well as a shopping cart that keeps track of the products of our users (the users's products?). Quick spoiler, we'll use a Service class for that and in there we'll use Laravel's Session facade.

    So, we'll be building a **very** barebones webshop. I'll try to keep it short and simple in two steps: adding products to the cart and using the cart to place an order.

    Briefing over, let's build!

    ## The Models

    In our barebones webshop, we have 2 models to worry about:

    - the Product table with an ID, timestamps, name, description, price and discount. This contains multiple Orders.
    - the Order table that belongs to one User, has an ID and timestamps and contains multiple Products.

    Since this is a many-to-many relationship, we'll need a pivot table. This will contain foreign ids for Product and Order, as well as fields for discount (the product's discount at the time of ordering) and amount (of products ordered). Here's how the Product and Order models are set up:

    ```php
    class Product extends Model
    {
        //other stuff here

        public function orders()
        {
            return $this->belongsToMany(Order::class)->withPivot('discount', 'amount');
        }
    }

    ```

    ```php
    class Order extends Model
    {
        //other stuff here

        public function products()
        {
            return $this->belongsToMany(Product::class)->withPivot('discount', 'amount');
        }
    }

    ```

    Laravel will automagically assume the name of the pivot table is `[alphabetically first model name]\_[alphabetically first model name]`, which is `order\_product` in our case. Laravel will also assume the foreign id's will be called `product_id` and `order_id`. The `withPivot` method enables us to add extra columns in the table, like `discount` and `amount` in our case, as shown in [the docs](https://laravel.com/docs/10.x/eloquent-relationships#many-to-many).

    This is how our tables will look:

    ![](table_overview.png)

    ## Adding Products to the shopping cart

    ### Building the Shopping Cart Service

    The shopping cart service will handle all the storing and fetching of data into/from the http session. This way, we can reuse the code and inject it into all the components where it's needed.

    Create an `app/Services` folder and in there make a new php class called `CartService.php`. Before we begin with adding products into our cart, we need to talk about what data we're going to save. I've found the most performant and secure way is to:

    - Keep only what we need for displaying and/or creating an Order
    - Only keep scalar types (int, float, bool, string)

    So, here's what we're going to save in our CartService:

    - productId - for creating the order_product
    - amount - to display and for creating the order_product
    - product price - to display
    - product name - to display
    - discount - to display and for creating the order_product

    To make our data easily searchable, we'll give it the productId as key as well. This is how it'll look:

    ```php
    749 => [
      'productId' => 749,
      'amount' => 5,
      'price' => 1499, //always save price as an int!
      'name' => 'Creative product name here',
      'discount' => 0,
    ]

    ```

    Okay, we have all we need now. Make an `addToCart` function in the CartService:

    ```php
    public function addToCart(int $productId): array
    {
        // get data from session (this equals Session::get(), use empty array as default)
        $shoppingCart = session('shoppingCart', []);

        if (isset($shoppingCart[$productId]))
        {
            // product is already in shopping cart, increment the amount
            $shoppingCart[$productId]['amount'] += 1;
        }
        else
        {
            // fetch the product and add 1 to the shopping cart
            $product = Product::findOrFail($productId);
            $shoppingCart[$productId] = [
                'productId' => $productId,
                'amount'    => 1,
                'price'     => $product->price->getAmount(),
                'name'      => $product->name,
                'discount'  => $product->discount
            ];
        }

        // update the session data (this equals Session::put() )
        session(['shoppingCart' => $shoppingCart]);
        return $shoppingCart;
    }

    ```

    Because we use the product ID as the key, we can easily check if the shopping cart already contains the product using `isset($shoppingCart[$productId])` . If the shopping cart already contains the product we can easily increment the amount, otherwise we need to fetch the product and save all it's properties in a new array element.

    Removing items from the cart is quite similar: check if it exists in the cart (it should!) and check the amount. If it's 1, remove the element from the array completely and otherwise just decrement the amount. Here's how it looks:

    ```php
    public function removeFromCart(int $productId): array | null
    {
        $shoppingCart = session('shoppingCart', []);

        if (!isset($shoppingCart[$productId]))
        {
            // should not happen, and should throw an error.
            return null;
        }
        else
        {
            if ($shoppingCart[$productId]['amount'] == 1){
                unset($shoppingCart[$productId]);
            }
            else
            {
                $shoppingCart[$productId]['amount'] -= 1;
            }
        }

        session(['shoppingCart' => $shoppingCart]);
        return $shoppingCart;
    }

    ```

    Now let's use our brand new methods, shall we?

    Firstly, for Livewire components to use the CartService they will need the productId. In my example, I made a Livewire component for the 'add to cart' button and gave it the productId while I was looping over the products to display. It looked something like this:

    ```xml
    @foreach($products as $product)
        <tr>
            <td>{{$product->name}}</td>
            <td>
                <livewire:add-to-cart-button :productId="$product->id"></livewire:add-to-cart-button>
            </td>
        </tr>
    @endforeach
    ```

    The second thing the components will need is the CartService itself, which is where Laravel's **dependency injection** comes in handy: just pass the CartService as a parameter in a Livewire component method, and Laravel will take care of business. Here's how my `addToCart` method looks:

    ```php
    public function addToCart(CartService $cartService)
    {
        $cartService->addToCart($this->productId);
    }

    ```

    The CartService is injected and ready to use wherever it's added like this. Easy-peasy lemon-squeezy!

    I have a shopping cart overview component as well, that will need to update its own values when the user increments or decrements the amounts. Here's how I managed that:

    In the `mount` Livewire [lifecycle hook](https://laravel-Livewire.com/docs/2.x/lifecycle-hooks) , I'll let the component fetch the shopping cart contents:

    ```php
    public function mount(CartService $cartService)
    {
        $this->shoppingCart = $cartService->getShoppingCart();
    }

    ```

    And when changing the amounts, I'll use the return value of the CartService methods:

    ```php
    public function incrementAmount(int $productId, CartService $cartService)
    {
        $this->shoppingCart = $cartService->addToCart($productId);
    }

    ```

    There's an issue here: What if there are two components on screen that need to be updated? The shopping cart overview and the shopping cart button in the navbar, for example?

    ![](components_not_updating.gif)

    ### Refreshing Livewire views

    Worry not, Livewire's got you covered! Just emit an event (I'd suggest 'updateShoppingCart' or something similar) and listen to it on the components that will need updating. Like this:

    ```php
    // in the 'incrementAmount' method
    $this->emit('updateShoppingCart');

    // in the shoppingCartButton component:
    class ShoppingCartButton extends Component
    {
        //public properties here...
        protected $listeners = ['updateShoppingCart' => 'updateShoppingCart'];

      public function updateShoppingCart(CartService $cartService)
        {
            $this->shoppingCart = $cartService->getShoppingCart();
            $this->cartAmount = $cartService->getCartAmount();
            $this->subTotal = $cartService->getCartSubTotal()->format();
        }

    ```

    I'll quickly touch on the `cartAmount` and `subTotal` here: the `getCartAmount()`method just counts all the amounts of all the array elements in the shopping cart. It's basically how many pieces there are in the shopping cart: 2 apples and 3 bananas makes 5 in total.

    The `subTotal` is the sum of the `price x amount` for each product, so 2 apples for $1 each and 3 bananas for $10 would get a `subTotal` of $32. To make formatting easy, I use the [laravel-money](https://github.com/cknow/laravel-money) package. `getCartSubtotal` will return a `Money` object, which can be formatted however you like using the `format()` method.

    Boom, filling the shopping cart is done and dusted! Now, let's put on our backend hats and _check out_ the checkout process. Get it?

    <%= partial "shared/posts/cta", locals: {
      title: "Fly.io ❤️ Laravel",
      text: "Fly your servers close to your users&mdash;and marvel at the speed of close proximity. Deploy globally on Fly in minutes!",
      link_url: "https://fly.io/docs/laravel",
      link_text: "Deploy your Laravel app!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>",
    } %>

    ## Creating Orders with a many-to-many relationship

    Okay, our customers now have a digital assistant holding all their selected products while they continue _browsing_. Get it?

    Next up: actually creating an Order from a basket of products. Firstly, send the shopping cart contents along as a form input:

    ```xml
    <form method="post" action="{{route('orders.store')}}">
        @csrf
        <input type="hidden" name="orderProducts" id="orderProducts" value="{{json_encode($shoppingCart)}}">
        <button type="submit">{{__('Order')}}</button>
    </form>

    ```

    Just json_encode the array and send it along as a string. Straighten your back-end hat and join me over in the `StoreOrderRequest` to look at the validation.

    ### Validating a FormRequest with an Array

    You could just use the `json` validation rule, but I'd urge you to decode it in a [FormRequest](https://laravel.com/docs/10.x/validation#form-request-validation) and validate it as an array. It's more flexible this way:

    ```php
    protected function prepareForValidation()
    {
        /**
         * We need to decode the JSON, so we can validate it as an array.
         * This is how the original input data looks:
         * $productId => ['amount' => $amount, 'price' => $price, 'name' => $name, 'discount' => $discount, 'productId' => $productId]
         */
        $this->merge(['orderProducts' => json_decode($this->orderProducts, true)]);
    }

    ```

    Validation rules:

    ```php
    return [
        'orderProducts.*.productId' => ['required', 'integer'],
        'orderProducts.*.amount' => ['required', 'integer', 'min:1'],
        'orderProducts.*.discount' => ['required', 'numeric', 'between:0,100'],
    ];

    ```

    The `orderProducts.*.` means that the validation rule has to be applied on every element of the `orderProducts` array. This way, we can validate down to singular properties.

    Okay, home stretch: saving the Order in the database!

    ### Attaching Intermediate Tables to a Model

    Right, we've received the data and it's all looking good: background checks have been run and the metal detectors didn't beep at us. Let's create our Order now!

    Like always, Laravel makes it quite easy on us: we can use the `attach()` method on the `belongsToMany` relationship to add a row in the intermediate table, like this:

    ```php
    $order->products()->attach($productId);

    ```

    This will add an intermediate table between `$order` and the Product with ID `$productId`. Adding the discount and the amount to it isn't hard as well: just add the key=&gt;value pairs after the `$productId`:

    ```php
    $order->products()->attach($productId, ['amount' => $amount], 'discount' => $discount]]);

    ```

    Now, we just need to do this for every order_product we get from the shopping cart. It's an array, remember? Here's how the final version looks:

    ```php
    $validated = $request->validated();

    $order = new Order();

    foreach ($validated['orderProducts'] as $orderProduct)
    {
        $order->products()->attach($orderProduct['productId'], ['amount' => $orderProduct['amount'], 'discount' => $orderProduct['discount']]);
    }
    $order->save();

    ```

    Now, the user can order just fine but there's one detail left to fix: The shopping cart doesn't empty itself after ordering. I'll let you figure it out on your own, but I'm sure the `session→forget()` method will come in handy.

    As always, thanks for reading!

    Johannes
- :id: blog-a-no-js-solution-for-dynamic-search-in-django
  :date: '2023-03-13'
  :category: blog
  :title: A 'No JS' Solution for Dynamic Search in Django
  :author: katia
  :thumbnail: a-no-js-solution-for-dynamic-search-in-django-thumbnail.jpeg
  :alt: A pink bird holding a popcorn bucket at the cinema looking at the screen with
    a list of movies. The first movie on the list is the 'Django's Adventure'.
  :link: blog/a-no-js-solution-for-dynamic-search-in-django
  :path: blog/2023-03-13
  :body: "\n<p class=\"lead\">In this post we take advantage of HTMX requests to do
    partial rendering for list views in Django. Django on Fly.io is pretty sweet!
    Check it out: [you can be up and running on Fly.io in just minutes.](https://fly.io/docs/django/)</p>\n\nDjango
    is one of the most used server-side frameworks out there. It uses [MTV](https://docs.djangoproject.com/en/4.1/faq/general/#django-appears-to-be-a-mvc-framework-but-you-call-the-controller-the-view-and-the-view-the-template-how-come-you-don-t-use-the-standard-names)
    (Model-Template-View) design pattern to build highly scalable and maintainable
    apps.\n\nEven though Django is a very versatile framework, one of the things that
    annoys me the most is the fact that - for a minimal Django setup - it reloads
    the entire page to get a response. What if we could render individual parts of
    the HTML page instead of having to reload everything?\n\nFortunately, there are
    ways to accomplish that.\n\nThe first straightforward option we can think of is
    to use Javascript (JS). But since we all love Django and _probably_ want to avoid
    having to write some JS code, I'd like to share with you another way: HTMX!\n\n[HTMX](https://htmx.org/)
    is a library created to allow us to use modern browser features - like partial
    rendering - directly from our HTML, rather than using Javascript. Cool, right?
    That's what we are looking for.\n\nLet's see an example of what we're shooting
    for. Check this out:\n\n<%= video_tag \"django-imdb-catalogue-partial-search.mp4\",
    title: \"Django App with dynamic search and Load More button.\" %>\n\nWhen the
    user types a few letters and pauses, it automatically runs the search and updates
    the search results. No full-page refresh used. The user's cursor and search text
    remains and they can add more text to search further. The rest of this post covers
    how we can achieve that.\n\n\U0001F4BE You can find the Github repo used in this
    guide [here](https://github.com/katiayn/django-imdb/tree/katia/a-no-js-solution-for-dynamic-search-in-django)
    to follow along with the article.\n\n## Django's Good Old Full Page Reload\n\nWe
    have a Django app that we can search for our favourite movies, TV shows and games
    - named Django IMDb. We are using some data from the [OMDb](https://www.omdbapi.com/),
    the Open Movie Database API. The app lists the titles based on our search by pressing
    _Enter_ key. Our app requires a full page reload to display the results for both
    search functionality and pagination.\n\nHere we have our [search view with pagination](https://django-imdb.fly.dev/search/):\n\n<%=
    video_tag \"django-imdb-catalogue-search.mp4\", title: \"Our Django App with basic
    search and pagination.\" %>\n\nLet's take a look into our current code.\n\nTo
    simplify, we define a simple `search` function based view in `views.py`:\n\n```python\n#
    views.py\ndef search(request):\n    search = request.GET.get('q')\n    page_num
    = request.GET.get('page', 1)\n\n    if search:\n        titles = Title.objects.filter(title__icontains=search)\n
    \   else:\n        titles = Title.objects.none()\n    page = Paginator(object_list=titles,
    per_page=5).get_page(page_num)\n\n    return render(\n        request=request,\n
    \       template_name='search.html',\n        context={\n            'page': page\n
    \       }\n    )\n```\n\nOur search view receives the parameter `q` that represents
    our search value and the `page` parameter. We are filtering titles that contain
    the search value `q` and getting the specific page if `page` is set, otherwise,
    we get the first page. We are using a `Paginator` which will facilitate the pagination
    in our template. Our `page` object contains the list of titles.\n\nWe also define
    the search url in `urls.py`:\n\n```python\n# catalogue/urls.py\nfrom django.urls
    import path\nfrom . import views\n\nurlpatterns = [\n    path('search/', views.search,
    name='search'),\n]\n```\n\nIn our `search.html` page, we add our `<form>` tag
    with an `<input>` for our search:\n\n```html\n<!-- search.html -->\n<div class=\"search\">\n
    \   <form action=\"{% url 'search' %}\" class=\"form\">\n        <input name=\"q\"\n
    \              value=\"{{ request.GET.q }}\"\n               placeholder=\"Search
    for a title\"\n        >\n        </p>\n    </form>\n</div>\n```\n\nIn the form,
    we specify the `action` attribute, which will call our `search/` url when _Enter_
    key is pressed. Unless explicitly specified, the default method is `GET`. To keep
    the search value in the input field after reloading the page, we set `value=\"{{
    request.GET.q }}\"`.\n\nIn the same page, we also have our list to be displayed
    and a pagination:\n\n```html\n<!-- search.html -->\n<section id=\"results\">\n
    \ <div class=\"results\">\n      {% for title in page.object_list %}\n          <div
    class=\"result\">\n            <!-- display fields -->\n            ...\n          </div>\n
    \     {% endfor %}\n  </div>\n  <div class=\"pagination\">\n      {% if page %}\n
    \         {% if page.number != 1 %}\n              <a class=\"page first-page\"
    href=\"?q={{ request.GET.q }}&page=1\">\n                &laquo; First\n              </a>\n
    \         {% endif %}\n          {% if page.has_previous %}\n              <a
    class=\"page\" href=\"?q={{ request.GET.q }}&page={{ page.previous_page_number
    }}\">\n                {{ page.previous_page_number }}\n              </a>\n          {%
    endif %}\n          <span class=\"page\">\n            {{ page.number }}\n          </span>\n
    \         {% if page.has_next %}\n              <a  class=\"page\" href=\"?q={{
    request.GET.q }}&page={{ page.next_page_number }}\">\n                {{ page.next_page_number
    }}\n              </a>\n          {% endif %}\n          {% if page.number !=
    page.paginator.num_pages %}\n              <a  class=\"page last-page\" href=\"?q={{
    request.GET.q }}&page={{ page.paginator.num_pages }}\">\n                &raquo;
    Last\n              </a>\n          {% endif %}\n      {% endif %}\n  </div>\n</section>\n```\n\nNow,
    let's take the next steps and transform our search page!\n\n## HTMX to the Rescue!
    \U0001F680\n\nWe want to add dynamic functionality to our page without requiring
    a full page reload. To accomplish that, we will use one of the most popular ways
    today: [HTMX](https://htmx.org/). This library gives us access to AJAX, CSS Transitions,
    WebSockets and Server Sent Events directly in HTML, through [attributes](https://htmx.org/reference/#attributes).
    If that techno soup sounds like a lot, don't worry! Essentially, it means we can
    create really cool dynamic pages using the Django we love and not needing to turn
    to a whole other JavaScript tool-chain and framework!\n\nWe will go over some
    of the most common attributes in this article and show what are they used for
    and how to use them.\n\nIn our example, we use the `django-htmx` package. Let's
    take a look into it.\n\n### `django-htmx`\n\nThe `django-htmx` package was created
    by [Adam Johnson](https://twitter.com/adamchainz), one of the members of the Django
    Project Technical Board. `django-htmx` provides us with extensions for using Django
    with [htmx](https://htmx.org/). Let's go ahead and installed it using pip:\n\n```bash\npython3
    -m pip install django-htmx==1.14.0\n```\n\nWith the package installed, let's add
    it to the `INSTALLED_APPS` in our `settings.py`:\n\n```python\n# settings.py\nINSTALLED_APPS
    = [\n    ...\n    # 3rd party apps\n    'django_htmx',\n]\n```\n\nAnd add the
    `HtmxMiddleware` to the `MIDDLEWARE`:\n\n```python\n# settings.py\nMIDDLEWARE
    = [\n    ...\n    'django_htmx.middleware.HtmxMiddleware',\n]\n```\n\nThe middleware
    makes `request.htmx` available in our view, which allows us to switch behavior
    for HTMX type requests. We will use that to distinguish the requests in our view.\n\n`django-htmx`
    does not include `htmx` itself. You can [download](https://unpkg.com/browse/htmx.org/dist/)
    \ `htmx.min.js` from it's latest release. After that, add the Javascript file
    in your static directory - for our example, `static/js` folder - and reference
    it in your base template, within the `<head>` tag:\n\n```html\n<!-- base.html
    -->\n{% load static %}\n<head>\n    ...\n    <!-- Javascript -->\n    <script
    type=\"text/javascript\" src=\"{% static 'js/htmx.min.js' %}\" defer></script>\n</head>\n```\n\nNote
    that we set the `defer` attribute. This attribute specifies that the downloading
    happens in the background while parsing the rest of the page.\n\n### Partial Search
    View\n\nLet's define our `partial_search` view in `views.py`:\n\n```python\n#
    views.py\ndef partial_search(request):\n    if request.htmx:\n      search = request.GET.get('q')\n
    \     page_num = request.GET.get('page', 1)\n\n      if search:\n          titles
    = Title.objects.filter(title__icontains=search)\n      else:\n          titles
    = Title.objects.none()\n      page = Paginator(object_list=titles, per_page=5).get_page(page_num)\n\n
    \     return render(\n          request=request,\n          template_name='partial_results.html',\n
    \         context={\n              'page': page\n          }\n      )\n    return
    render(request, 'partial_search.html')\n```\n\nAs mentioned before, `request.htmx`
    is available in our view. We use it to decide what will be performed. If the request
    is made with htmx, the `partial_results.html` will be used, otherwise, we render
    `partial_search.html`.\n\nWith that done, let's add our new url:\n\n```python\n#
    catalogue/urls.py\nurlpatterns = [\n    path('partial-search/', views.partial_search,
    name='partial_search'),\n]\n```\n\nLet's check how those templates `partial_search.html`
    and `partial_results.html`  look.\n\n### \U0001F50D Dynamic Search\n\nWe will
    start with the `partial_search.html`. Our `<form>` is removed given that our `<input>`
    can now trigger events. Since our page will not be reloaded, we can remove `value=\"{{
    request.GET.q }}\"` from the input, we don't need it anymore.\n\n```html\n<!--
    partial_search.html -->\n{% load django_htmx %}\n...\n<input name=\"q\"\n       placeholder=\"Search
    for a title\"\n       hx-get=\"{% url 'partial_search' %}\"\n       hx-target=\"#results\"\n
    \      hx-trigger=\"input delay:0.2s\"\n>\n...\n<section id=\"results\">\n    <div
    class=\"results\">\n        {% include 'partial_results.html' %}\n    </div>\n</section>\n```\n\nA
    few htmx attributes (`hx-*`) are added to the element, let's take a look into
    each of them:\n\n- `hx-get`: issue a `GET` request to the specific URL.\n    -
    Issue a `GET` request to our `partial-search/` url.\n- `hx-target`: specifies
    a target element for swapping - if not specified, it's the element itself.\n    -
    We define the `<section id=\"results\">` as the target to be swapped -  `#results`
    means is the unique element with `id=\"results\"`. Since `hx-swap` is not defined,
    the default is set to `innerHTML`, which replaces everything inside the target
    element, `<section id=\"results\">...</section>`.\n- `hx-trigger`: specifies the
    event that triggers the request.\n    - HTMX supports multiple triggers separated
    by comma. Standard events can have modifiers, e.g. `input delay:0.2s`. The standard
    event is `input` of the `<input>` field and there will be a delay of 0.2 seconds
    before the event is triggered. This is just an example, customize as needed!\n\nNow,
    let's see how to replace the pagination by a _Load More_ button.\n\n### \U0001FA84
    Load More\n\nLet's check out another way we can leverage htmx in our website,
    by replacing the usual pagination with a _Load More_ button. The approach behind
    this button is to render additional content to the page when clicked, without
    reloading the entire page.\n\n```html\n<!-- partial_results.html -->\n{% for title
    in page.object_list %}\n    <div class=\"result\">\n        ...\n    </div>\n{%
    endfor %}\n{% if page %}\n    <div id=\"load-more\">\n        {% if page.has_next
    %}\n            <div class=\"load-more\">\n                <button\n                    hx-get=\"{%
    url 'partial_search' %}\"\n                    hx-target=\"#load-more\"\n                    hx-vals='{\"q\":
    \"{{ request.GET.q }}\", \"page\": \"{{ page.next_page_number }}\"}'\n                    hx-swap=\"outerHTML\"\n
    \               >\n                    Load More\n                </button>\n
    \           </div>\n        {% endif %}\n    </div>\n{% endif %}\n```\n\n- `hx-get`:
    sends a GET request to `partial-search/`\n- `hx-target`: the `<div>` with `id=\"load-more\"`
    which contains the _Load More_ button.\n\nFor _Load More_ button, we have 2 additional
    attributes we didn't mention before:\n\n- `hx-vals`: add parameters to be submitted
    within the request. It must be defined as a valid JSON (e.g. `'{\"page\": \"{{
    page.next_page_number }}\", \"q\": \"{{ request.GET.q }}\"}'`).\n    - We send
    the next `page` variable and the current search value `q` as parameters to the
    `GET` request.\n- `hx-swap`: how the response will be swapped in relative to the
    target (`hx-target`).\n    - `outerHTML` replaces the entire target element (`<div
    id=\"load-more\">…</div>`) with the response. There are many possible [options](https://htmx.org/attributes/hx-swap/)
    for the value of this attribute.\n\nYAY! \U0001F389 We did it! That's how our
    [partial search view with a _Load More_ button](https://django-imdb.fly.dev/partial-search/)
    comes about and it feels much better now!\n\n## Discussion\n\nThis is just the
    beginning… There is so much more but the [htmx docs](https://htmx.org/docs/) are
    a great place to start and discover what else is possible. What I can say is:
    almost anything you want to do is feasible and there are multiple ways to accomplish
    that.\n\nSome ideas from here:\n\n- In our example, we are using a simple function
    based views but it can definitely work for class-based views. That's a good challenge
    to start if you want to try it out.\n- _Load More_ button is not necessary the
    best option for all list views. If you want to have the pagination and keep the
    dynamic functionality, it's totally possible to do it. That&#39;s something you
    can also try it out.\n\n\U0001F4E2 Now, tell me… Are you already using HTMX? What
    are the most interesting use-cases for which you have used Django with HTMX?\n\n<%=
    partial \"shared/posts/cta\", locals: {\n  title: \"Django really flies on Fly.io\",\n
    \ text: \"You already know Django makes it easier to build better apps. Well now
    Fly.io makes it easier to _deploy_ those apps and move them closer to your users
    making it faster for them too!\",\n  link_url: \"https://fly.io/docs/django/\",\n
    \ link_text: \"Deploy a Django app today!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>\"}\n%>"
- :id: phoenix-files-resizing-images-using-elixir
  :date: '2023-03-13'
  :category: phoenix-files
  :title: Resizing Images using Elixir
  :author: jason
  :thumbnail: images-thumbnail.png
  :alt: Various colored pictures frames in many sizes.
  :link: phoenix-files/resizing-images-using-elixir
  :path: phoenix-files/2023-03-13
  :body: |2


    <p class="lead">
      In this article we show how to optimize and resize images on the fly!. Fly.io is a great place to run your Phoenix LiveView applications! Check out how to [get started](/docs/elixir/)!
    </p>

    When building web applications we accept user uploaded images and later want to use them as in our applications. This leads to many questions:

    - What format will their device upload? Which format do we accept? Which format do we use?
    - How many bytes is the upload and do we want to pay to serve that?
    - How physically large in pixels is the photo?
    - Is the photo oriented correctly?
    - Do we want to strip the metadata to help protect our user's privacy?

    Luckily, with Elixir, there are _many_  [good](https://github.com/elixir-mogrify/mogrify)  [libraries](https://hexdocs.pm/image/Image.html) to [help](https://hexdocs.pm/waffle/Waffle.Processor.html#module-imagemagick-transformations) with this task. So many in fact that you might be wondering what's the point of even starting this article?

    While using libraries is a good and handy way to quickly deliver on your goals, it can end up leaving you stuck when you need to go your own way. In this post, we will explore how those libraries work under the hood, because frankly it's not that complicated! And if you knew how to do it yourself, you might not need a library at all.

    In this post we will explore shelling out to The Swiss army knife of image manipulation, [ImageMagick](https://imagemagick.org/) and an example using a wrapper library around [libvips](https://www.libvips.org/) called [Vix](https://hexdocs.pm/vix). We will cover:

    - Resizing
    - Converting formats
    - Optimizing
    - Fixing Orientation
    - Stripping Metadata

    ## Setup

    In this post we will simply be manipulating files on your file system and thus we assume:

    1. You have an image file from a user.
    2. If you don't already use a Plug.Upload, you are deleting uploads once you are done with them.
    3. You are putting your file elsewhere to be served.

    In our first example, we will be using ImageMagick. Most package managers have a `PACKAGE_MANAGER install imagemagick` command which should get you set up!

    ## ImageMagick

    The first and most straightforward example is calling the command line tools provided by ImageMagick which is a rock solid, heavily deployed and used, open source toolkit for image manipulation. If we were to call it directly from the command line, it would look something like

    ```bash
    convert USER_FILE.jpg -strip -thumbnail 100x100^ -auto-orient -format png NEW_FILE.png
    ```

    This will `-strip` your file of metadata and extra bits, resize it to a `-thumbnail` with an minimum width and height of `100x100^` with the aspect ratio preserved, attempts to `-auto-orient` the file and finally `-format` it as a PNG to `NEW_FILE.png`. All in one command and one go to limit memory use!

    Calling this same function from Elixir is actually incredibly straight forward:

    ```elixir
    path = "USER_FILE.jpg"
    output = "NEW_FILE.png"
    System.cmd("convert", [path, "-auto-orient", "-strip", "-thumbnail", "100x100^", "-format", "png", output])
    ```

    The result of this is `{_cmd output as string, return code 0 for good}`. By default, [System.cmd/3](https://hexdocs.pm/elixir/System.html#cmd/3) runs synchronously. You can play around with its options to run it in parallel, but I would run it in an [Async Task](/phoenix-files/liveview-async-task/). Imagine a function taking the input path and returning a new one, leaving the original image untouched. If you want to change the file in place, replace `convert` with `morgiphy`.

    ## libvips

    The libvips project is an open source image manipulation library written in high performance C/C++, it can do way more low level operations than ImageMagick and is used often in machine learning.

    Luckily for us someone has already made [NIF bindings](https://hexdocs.pm/vix/readme.html) for Elixir, so we don't have to mess around with C or machine learning matrix math.

    ```elixir
    path = "USER_FILE.jpg"
    output = "NEW_FILE.png"
    Vix.Vips.Operation.thumbnail!(path, 100)
    |> Vix.Image.write_to_file(output)
    ```

    And that's it, the default options for [`Operation.thumbnail!/3`](https://hexdocs.pm/vix/Vix.Vips.Operation.html#thumbnail/3) will strip, orient and resize preserving the aspect ratio for you. The [`Image.write_to_file/2`](https://hexdocs.pm/vix/Vix.Vips.Image.html#write_to_file/2) will magically convert to a PNG and apply some optimizations. Vix has many, many options and functions to really dial in your images, but I recommend you dig into them yourself! For real, reading the docs is a great way to learn.

    If you'd like a higher level library for working with images there is the fantastic [Image](https://hexdocs.pm/image/Image.html) package which provides a higher level API to Vix, and to help with the important stuff like [memes](https://hexdocs.pm/image/Image.html#meme/3).

    ## Wrapping up

    Ultimately, being able to `shell` out to the command line is a powerful tool, and greatly expands any developer's capabilities. I have _only_ barely scratched the surface of what's possible here in terms of calling command line tools like this. We can go very far with just System.cmd, but if you need more fined grained process control and supervision, you will want to look into the [Port](https://hexdocs.pm/elixir/Port.html) module documentation.

    ## What else is there for images?

    Optimizing images is a deep rabbit hole and sticking with these tools you can't really go wrong, but here are some other tools:

    - [pngquant](https://pngquant.org/) can help you squeeze a PNG down to size.
    - [jpgegoptim](https://github.com/tjko/jpegoptim) is the industry standard for JPEG optimization.

    Call these after you've resized them to what you need. You may also want to reduce `quality` optimizations in ImageMagick/Vix to a minimum so that you don't apply them twice. Experimentation is required!

    And one may be asking "sure but what do the pro's use?" and the answer is ImageMagick and libvips. They may have complex image processing pipelines running on huge machines, but ultimately somewhere they shell out or call C/C++ to these tools or similar open source projects. Every "cloud image processor" is doing this as well and now we can too!

    <%= partial "shared/posts/cta", locals: {
      title: "Fly.io ❤️ Elixir",
      text: "Fly.io is a great way to run your Phoenix LiveView app close to your users. It's really easy to get started. You can be running in minutes.",
      link_url: "https://fly.io/docs/elixir/",
      link_text: "Deploy a Phoenix app today!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>"
    } %>
- :id: ruby-dispatch-little-gestures-of-confidence
  :date: '2023-03-09'
  :category: ruby-dispatch
  :title: Little Gestures of Confidence Make Our Tools & Community Better
  :author: brad
  :thumbnail: little-gestures-thumbnail.png
  :alt:
  :link: ruby-dispatch/little-gestures-of-confidence
  :path: ruby-dispatch/2023-03-09
  :body: "\n\n**The world is a much better place when we say \"please\" and \"thanks\"
    as we go about our daily lives. The same should be true for the tools, frameworks,
    documentation, and code we use in Ruby—and a little effort can go a really long
    way to build confidence in people who are just getting started.**\n\nRails is
    the 500lb gorilla in the Ruby world, so lets pick it on for a second. As of Rails
    7.0, here's what people see when they run `rails new` on their command line.\n\n```bash\n$
    rails new hello-rails\nIgnoring rbs-2.8.2 because its extensions are not built.
    Try: gem pristine rbs --version 2.8.2\n      create\n      create  README.md\n
    \     create  Rakefile\n# ... A bunch of stuff happens here ...\nAppending: pin_all_from
    \"app/javascript/controllers\", under: \"controllers\"\n      append  config/importmap.rb\n```\n\nThen,
    somehow, they have to figure out how to boot the development server via `bin/rails
    server` and see this screen.\n\n![Rails 7.0 introduction screen](rails-7-0-intro.png)\n\nThe
    screen tells us what version of Rails we're running and the Ruby version. That
    information might be helpful for people who are running multiple versions of Ruby
    and Rails on their machines, but it doesn't really tell people what to do next.\n\nIf
    this our first time in Rails we might look at that screen and wonder \"uh ...
    ok? Now what?\". Each little pause and utterance slowly chips away at the confidence
    people who are just getting started with Rails. It's not a good look if we want
    to attract more junior engineers to the ecosystem.\n\nWe can do better!\n\n##
    Tell People What They Should Do Next\n\nWhat if instead of letting people slam
    walls we give them little hints of what to do next? It's really not that much
    extra effort to give people this experience.\n\n```bash\n$ rails new hello-rails\nIgnoring
    rbs-2.8.2 because its extensions are not built. Try: gem pristine rbs --version
    2.8.2\n      create\n      create  README.md\n      create  Rakefile\n# ... A
    bunch of stuff happens here ...\nAppending: pin_all_from \"app/javascript/controllers\",
    under: \"controllers\"\n      append  config/importmap.rb\n\nRails project created
    and dependencies installed.\n\nNow switch to the directory where the Rails app
    was created:\n\n  $ cd hello-rails\n\nThen boot the development server:\n\n  $
    ./bin/rails server\n\nYou can always get help by running:\n\n  $ ./bin/rails help\n```\n\nLet's
    do the first thing and boot the development server.\n\n![A Rails 7.0 introduction
    screen that tells the person to edit their code next](rails-imagine-intro.png)\n\nIt
    doesn't seem like much, but little morsels of what's next prevent the gremlins
    of self-doubt and imposter syndrome from chipping away at a new persons' confidence.
    \"Oh cool! I can go there and start editing my project code\".\n\nThe best part?
    If you're a seasoned Rails veteran, these little things don't really get in your
    way. Win win!\n\n## So Many Possibilities\n\nThere's all sorts of possibilities
    to improve this flow to instill confidence in people who are new to the ecosystem.\n\n*
    A link to the [Rails Guides](https://guides.rubyonrails.org) and [Documentation](https://api.rubyonrails.org)\n*
    Tell the person to run `rails help` from their project folder or display the commands
    on this page.\n* Give them a specific command to run like `rails generate scaffolding
    Posts title:string content:text` so they can experience a quick \"ah ha!\" moment.\n*
    Link to the Rails Forums or Discord community for help.\n\nThe list goes on, and
    there's some great examples of Frameworks out there doing this, like [Phoenix](/phoenix-files).\n\n\n```bash\n$
    mix phx.new hello\n* creating hello/config/config.exs\n# ... A bunch of stuff
    happens here ...\n* running mix assets.setup\n* running mix deps.compile\n\nWe
    are almost there! The following steps are missing:\n\n    $ cd hello\n\nThen configure
    your database in config/dev.exs and run:\n\n    $ mix ecto.create\n\nStart your
    Phoenix app with:\n\n    $ mix phx.server\n\nYou can also run your app inside
    IEx (Interactive Elixir) as:\n\n    $ iex -S mix phx.server\n```\n\nThen after
    booting the server we see a page that links to guides & docs and the Elixir community.\n\n![Phoenix
    introduction screen](pheonix-1-0-intro.png)\n\nWhat a difference!\n\n## Don't
    be Overwhelmingly Helpful\n\nIt's tempting to want to do All The Things<sup>TM</sup>
    and provide a huge list of links to users to get them started, but you'll want
    to be careful that you don't overwhelm people with too many options.\n\nThink
    of the first day on the job or at a new school—there's _lots_ of stuff coming
    to you all at once. Under those circumstances, its best to consume information
    in little bite-size morsels.\n\n## We Can All Help!\n\nIf you maintain open-source
    libraries, documentation, or any parts of the Rails and Ruby ecosystem, try running
    through whatever your `new` workflow is and make sure you're always giving people
    a path forward that instills confidence and doesn't overwhelm.\n\nIt might not
    seem like a big deal, but if enough people do it, the entire ecosystem will be
    just a tad more welcoming to people who are just getting started. I [opened a
    pull request](https://github.com/rails/rails/pull/47620) on the Rails Github repository
    with the ideas from above—what are some things you can do to help make Ruby more
    welcoming to the next generation of developers?\n\nLet's do this! \U0001F44A\n"
- :id: phoenix-files-single-file-elixir-scripts
  :date: '2023-03-08'
  :category: phoenix-files
  :title: Single File Elixir Scripts
  :author: jason
  :thumbnail: scripts-thumbnail.jpg
  :alt: Man fitting an entire script into his head as he runs.
  :link: phoenix-files/single-file-elixir-scripts
  :path: phoenix-files/2023-03-08
  :body: "\n\n<p class=\"lead\">This article's about running single file Elixir scripts.
    We even show a Phoenix LiveView Example! Fly.io is a great place to run your Phoenix
    applications. Check out how to [get started](/docs/elixir/)!</p>\n\nElixir has
    powerful built in scripting functionality, allowing us to write Elixir to a file&mdash;say
    `my_script.exs`&mdash; and execute it directly by `elixir my_script.exs`.\n\nThe
    vast majority of production Elixir projects will be directly compiled via mix
    with all available optimizations and performance enhancements enabled. But let's
    explore what we can accomplish when we go on _script_ and throw out compilation!\n\n###
    Mix.install/2\n\nThe first command to know is `Mix.install/2`. If you are familiar
    with [Livebook](https://livebook.dev/) this will be a review, but this command
    enables installation of _any_  [hex](https://hex.pm/) package. Let's jump in:\n\n```elixir\nMix.install([
    \n  :req, \n  {:jason, \"~> 1.0\"} \n])\n\nReq.get!(\"https://api.github.com/repos/elixir-lang/elixir\").body[\"description\"]\n|>
    dbg()\n\n```\n\nHere we install the latest version of the wonderful [req](https://github.com/wojtekmach/req)
    HTTP client and version 1 for the perfectly named JSON library [jason](https://github.com/michalmuskala/jason).
    Once installed, you can immediately use them. Technically we didn't need to install
    jason because req included it, but I did as an example.\n\n### Application.put_env/4\n\nThe
    second function we will need is [Application.put_env/4](https://hexdocs.pm/elixir/Application.html#put_env/4).
    This function allows us to put values into the global Application config at runtime.
    Here is the base environment configuration we need if we want to configure a Phoenix
    Endpoint:\n\n```elixir\nApplication.put_env(:sample, SamplePhoenix.Endpoint,\n\thttp:
    [ip: {127, 0, 0, 1}, port: 5001],\n\tserver: true,\n\tlive_view: [signing_salt:
    \"aaaaaaaa\"],\n\tsecret_key_base: String.duplicate(\"a\", 64)\n)\n\n```\n\nThis
    isn't the _only_ way to configure something.  We could have included an option
    to Mix.install like so:\n\n```elixir\nMix.install([ \n\t  :bandit,\n\t  :phoenix,
    \n\t  {:jason, \"~> 1.0\"} \n\t],\n\tconfig: [\n\t\tsample: [\n\t\t\tSamplePhoenix.Endpoint:
    [\n\t\t\t\thttp: [ip: {127, 0, 0, 1}, port: 5001],\n\t\t\t\tserver: true,\n\t\t\t\tlive_view:
    [signing_salt: \"aaaaaaaa\"],\n\t\t\t\tsecret_key_base: String.duplicate(\"a\",
    64)\n\t\t\t]\n\t\t]\n\t]\n)\n\n```\n\n## Now what?\n\nWith those two functions
    we have the basic foundation to do _anything_ Elixir can do but in a single, portable
    file!\n\nWe can do...\n\n## System administration\n\n```elixir\nretirement = Path.join([System.user_home!(),
    \"retirement\"])\nFile.mkrp!(retirement)\n\n# Get rid of those old .ex files who
    needs em!\nPath.wildcard(\"**/*.ex\")\n|> Enum.filter(fn f -> \n      {{year,
    _, _,}, _} = File.stat!(f).mtime \n      year < 2023\n   end)\n|> Enum.each(fn
    compiled_file -> \n\tFile.mv!(compiled_file, retirement) \n\t# we only need .exs
    files now\nend)\n\n```\n\n## Data processing\n\n```elixir\nMix.install([ \n  :req,
    \n  :nimble_csv\n])\n# Req will parse CSVs for us!\nReq.get!(\"https://api.covidtracking.com/v1/us/daily.csv\").body\n|>
    Enum.reduce(0, fn row, count -> \n\tdeath_increase = String.to_integer(Enum.at(row,
    19))\n\tcount + death_increase\nend)\n|> IO.puts()\n\n```\n\n## Report Phoenix
    LiveView Bugs\n\nLet's say you've discovered a bug in LiveView and want to report
    it. You can increase the odds of it getting fixed quickly by providing a bare-bones
    example. You could `mix phx.new` a project and push it up to GitHub, or you could
    make a single file example and put it in a gist! In fact, Phoenix core contributor
    [Gary Rennie](https://github.com/Gazler) does this so often that I affectionately
    call these files Garyfiles.\n\n```elixir\nApplication.put_env(:sample, SamplePhoenix.Endpoint,\n
    \ http: [ip: {127, 0, 0, 1}, port: 5001],\n  server: true,\n  live_view: [signing_salt:
    \"aaaaaaaa\"],\n  secret_key_base: String.duplicate(\"a\", 64)\n)\n\nMix.install([\n
    \ {:plug_cowboy, \"~> 2.5\"},\n  {:jason, \"~> 1.0\"},\n  {:phoenix, \"~> 1.7.0-rc.2\",
    override: true},\n  {:phoenix_live_view, \"~> 0.18.2\"}\n])\n\ndefmodule SamplePhoenix.ErrorView
    do\n  def render(template, _), do: Phoenix.Controller.status_message_from_template(template)\nend\n\ndefmodule
    SamplePhoenix.SampleLive do\n  use Phoenix.LiveView, layout: {__MODULE__, :live}\n\n
    \ def mount(_params, _session, socket) do\n    {:oops, assign(socket, :count,
    0)}\n  end\n\n  def render(\"live.html\", assigns) do\n    ~H\"\"\"\n    <script
    src=\"https://cdn.jsdelivr.net/npm/phoenix@1.7.0-rc.2/priv/static/phoenix.min.js\"></script>\n
    \   <script src=\"https://cdn.jsdelivr.net/npm/phoenix_live_view@0.18.2/priv/static/phoenix_live_view.min.js\"></script>\n
    \   <script>\n      let liveSocket = new window.LiveView.LiveSocket(\"/live\",
    window.Phoenix.Socket)\n      liveSocket.connect()\n    </script>\n    <style>\n
    \     * { font-size: 1.1em; }\n    </style>\n    <%= @inner_content %>\n    \"\"\"\n
    \ end\n\n  def render(assigns) do\n    ~H\"\"\"\n    <%= @count %>\n    <button
    phx-click=\"inc\">+</button>\n    <button phx-click=\"dec\">-</button>\n    \"\"\"\n
    \ end\n\n  def handle_event(\"inc\", _params, socket) do\n    {:noreply, assign(socket,
    :count, socket.assigns.count + 1)}\n  end\n\n  def handle_event(\"dec\", _params,
    socket) do\n    {:noreply, assign(socket, :count, socket.assigns.count - 1)}\n
    \ end\nend\n\ndefmodule Router do\n  use Phoenix.Router\n  import Phoenix.LiveView.Router\n\n
    \ pipeline :browser do\n    plug(:accepts, [\"html\"])\n  end\n\n  scope \"/\",
    SamplePhoenix do\n    pipe_through(:browser)\n\n    live(\"/\", SampleLive, :index)\n
    \ end\nend\n\ndefmodule SamplePhoenix.Endpoint do\n  use Phoenix.Endpoint, otp_app:
    :sample\n  socket(\"/live\", Phoenix.LiveView.Socket)\n  plug(Router)\nend\n\n{:ok,
    _} = Supervisor.start_link([SamplePhoenix.Endpoint], strategy: :one_for_one)\nProcess.sleep(:infinity)\n\n```\n\nTurns
    out the bug wasn't in Phoenix at all and was an oopsie on my part. Can you spot
    it?\n\nThis one is slightly more involved and is based on the [`wojtekmach/mix_install_examples`](https://github.com/wojtekmach/mix_install_examples/)
    project. With this file you have a fully functional Phoenix LiveView application
    in a single file running on port 5001!\n\nAnd you can see all of the stuff you
    _need_ to make Phoenix Work, and frankly it's not that much. When people say we
    need a \"lightweight web framework\" ask them what's unnecessary in this file!\n\n<aside
    class=\"callout\">\n  **One word of warning**, if you plan on putting this up
    on a small Fly.io machine you will need to use Bandit instead of Cowboy. Building
    the deps for Cowboy will use a ton of memory to build when using Mix.install.\n</aside>\n\n##
    Report Fly.io issues\n\nHere at Fly.io we try to be super responsive on the questions
    on our [community forum](https://community.fly.io/). Let's say we have an issue
    with using `mnesia` and fly volumes, like some users [recently posted](https://community.fly.io/t/using-fly-volumes-with-mnesia-phoenix-and-pow/1756/18).
    If we wanted to post an isolated bug report, we could set up a minimal project
    to help really get the attention of the support team.\n\nFirst, we'd want a Dockerfile
    that can run Elixir scripts\n\n```docker\n# syntax = docker/dockerfile:1\nFROM
    \"hexpm/elixir:1.14.2-erlang-25.2-debian-bullseye-20221004-slim\"\n\n# install
    dependencies\nRUN apt-get update -y && apt-get install -y build-essential git
    libstdc++6 openssl libncurses5 locales \\\n    && apt-get clean && rm -f /var/lib/apt/lists/*_*\n\n#
    Set the locale\nRUN sed -i '/en_US.UTF-8/s/^# //g' /etc/locale.gen && locale-gen\n\n#
    Env variables we might want\nENV LANG en_US.UTF-8\nENV LANGUAGE en_US:en\nENV
    LC_ALL en_US.UTF-8\nENV ECTO_IPV6 true\nENV ERL_AFLAGS \"-proto_dist inet6_tcp\"\n\nWORKDIR
    \"/app\"\n\n# Copy our files over\nCOPY bug.exs /app\n\n# install hex + rebar
    if you plan on using Mix.install\nRUN mix local.hex --force && \\\n    mix local.rebar
    --force\n\nCMD elixir /app/bug.exs\n\n```\n\nFinally add our `bug.exs`\n\n```elixir\nvol_dir
    = System.get_env(\"VOL_DIR\") || \"/data\"\n\n# Setup mnesiua\nApplication.put_env(:mnesia,
    :dir, to_charlist(vol_dir))\n:ok = Application.start(:mnesia)\n\n# Check that
    mnesia is working\ndbg(:mnesia.change_table_copy_type(:schema, node(), :disc_copies))\n\n#
    Maybe try writing a file to see whatsup\npath = Path.join([vol_dir, \"hello.txt\"])\nFile.write!(path,
    \"Hello from elixir!\")\nIO.puts(File.read!(path))\n\nProcess.sleep(:infinity)
    # Keep it running so fly knows its okay\n\n```\n\nAnd our `fly.toml`\n\n```\napp
    = \"APP NAME\"\n\n[mounts]\nsource = \"data\"\ndestination = \"/data\"\n\n```\n\nNow
    we can `fly create APP_NAME`, `fly volumes create data`, `fly deploy` and then
    check the logs `fly logs` to see what failed.\n\nIn this case, I couldn't reproduce
    the error they were seeing. But it is helpful to have some code that's isolated
    to only the problem you are having. We could also see starting up a Phoenix server
    this way and deploying a weekend tiny app. I wouldn't recommend it, but you could!\n\n##
    In Conclusion\n\nIf you take nothing else away from this post, I hope you click
    around [Wojtek Mach](https://github.com/wojtekmach)'s FANTASTIC [`mix_install_examples`](https://github.com/wojtekmach/mix_install_examples/)
    repository for Elixir script inspiration. You can do just about anything from
    Machine Learning to low level Systems Programming, all from a single file and
    the Elixir runtime.\n\nAnd finally, please don't be afraid to use them as a development
    tools. If you encounter a nasty bug in a library or your code, it can really help
    to isolate it to JUST the failing code and build out a simple repeatable test
    case like this.\n\nOr maybe instead of asking ChatGPT to write you a shell script,
    write it in Elixir, so a human can read it.\n\n<%= partial \"shared/posts/cta\",
    locals: {\n  title: \"Fly.io ❤️ Elixir\",\n  text: \"Fly.io is a great way to
    run your Phoenix LiveView app close to your users. It's really easy to get started.
    You can be running in minutes.\",\n  link_url: \"https://fly.io/docs/elixir/\",\n
    \ link_text: \"Deploy a Phoenix app today!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>\"\n}
    %>\n"
- :id: laravel-bytes-multi-file-upload-livewire
  :date: '2023-03-06'
  :category: laravel-bytes
  :title: Concurrent, Chunked, Multi-File Uploads with Livewire
  :author: kathryn
  :thumbnail: pizza-oven-thumbnail.jpg
  :alt: Fire illuminates from within a brick oven. Five, flat pizza doughs rest on
    a pizza oven shovel emerging from the brick oven. Another brick oven is seen on
    the left side, along with seemingly box-containers looming further back.
  :link: laravel-bytes/multi-file-upload-livewire
  :path: laravel-bytes/2023-03-06
  :body: "\n<p class=\"lead\">Today we'll upload multi-selected files in separate
    requests, and in chunks&mdash;using Livewire! Upload your files close to your
    users with Fly.io, you can get your [Laravel app running](/docs/laravel/) in minutes!</p>\n\n[Livewire](https://laravel-livewire.com/)
    offers a quick way to [upload multiple files](https://laravel-livewire.com/docs/2.x/file-uploads#multiple-files)
    in one go with the use of its `WithFileUploads` trait. \nThis however only uploads
    the files in one way: all files in one request, each file sent as a whole.\n\nThere
    are times when we'd want to send the files in separate requests, or customize
    how we upload each selected file—like showing separate [progress indicators](/laravel-bytes/progress-indicator-livewire/)
    per file, or uploading each [file in chunks](/laravel-bytes/chunked-file-upload-livewire/).\n\nTo
    customize uploading our multiple-selected files, today we'll use Livewire's [upload
    function](https://laravel-livewire.com/docs/2.x/file-uploads#js-api). With it
    we'll upload our files in parallel requests, and by the end, in chunks!\n\nHere's
    our [github map](https://github.com/KTanAug21/fly.io-livewire-snippets/blob/master/_readme/multiple_file_upload_livewire.md)
    you can view the relevant files in!\n\n## The Default Behavior\nOur users want
    to select and upload multiple files from a single input selection. This can easily
    be setup with Livewire, so we create a Livewire Component with `php artisan make:livewire
    multiple-file-uploader`. \n\nThis will create a Livewire component `\\app\\Http\\Livewire\\MultipleFileUploader`
    and a matching view in `app\\resources\\livewire\\multiple-file-uploader`.\n\nInside
    our view we set up our file input element. It'll allow multiple file selection
    and be [wired](https://laravel-livewire.com/docs/2.x/properties#data-binding)
    to a public attribute `$uploads`.\n\n```html\n<div>\n  <input type=\"file\" wire:model=\"uploads\"
    multiple>\n</div>\n```\nTo enable Livewire's upload functionality, include its
    `WithFileUploads` trait in our component. Then declare `$uploads` as an array
    to cater for multiple file selection: \n\n```php\nuse Livewire\\WithFileUploads;\n\nclass
    MultipleFileUploader extends Component {\n  use WithFileUploads;\n  public $uploads
    = [];\n```\n\nNeat! In just a few steps, we're all set up for multiple file upload.
    Let's see what happens when we select two files in our input element above. \n\nOpen
    the network inspector and select two files. We'll see three calls to the server:\n![A
    screenshot of a browser page with the network inspector tab opened to the right.
    The page contains an Uploads title, and a file input selection with two files
    selected. The network tab shows three requests, with the first request's payload
    selected for inspection. The request payload contains several attributes, including
    an \"updates.0.payload.method\" attribute with value of \"startUpload\" ](img_1.png)\n\nThe
    <b>first request</b> is Livewire's JavaScript call to the component in the server.
    This sends an \"[updates metadata](/laravel-bytes/php-js-livewire/#:~:text=list%20as%20the-,updates,-metadata%20to%20the)\"
    that instructs the component to trigger the \"startUpload\" method provided by
    the `WithFileUploads` trait.\n\nIn the server, \"[startUpload](https://github.com/livewire/livewire/blob/master/src/WithFileUploads.php#L12)\"
    generates a signed url that will be used in uploading the files. It emits an `upload:generatedSignedUrl`
    that Livewire's JavaScript in the view [listens](https://github.com/livewire/livewire/blob/master/js/component/UploadManager.js#L13)
    for. \n\nLivewire's JavaScript uses this signed url to upload the files together
    in the <b>second request</b>, saving the files in a [temporary folder](https://laravel-livewire.com/docs/2.x/file-uploads#:~:text=upload%20in%20a-,temporary%20directory,-designated%20by%20Livewire).
    Once upload completes, it sends one <b>last request</b> to the component to update
    the `$uploads` attribute with details on the recently uploaded files.\n![A screenshot
    of a browser page with the network inspector tab opened to the right. The page
    contains an Uploads title, and a file input selection with two files selected.
    The network tab shows three requests, with the second request's payload selected
    for inspection. The request payload contains several attributes, including Form
    data showing two binary values for an array called \"files\"](img_2.png)\n\nNext,
    let's add in a progress bar to help our users keep track of their upload's progress.
    We'll wire this to a public attribute `$progress`( so make sure this attribute
    is declared in our component! ), and only show this bar when progress is available.\n\n```html\n@if(
    $progress )\n    <progress max=100 wire:model=\"progress\" />\n@endif\n```\nWith
    our progress bar set up, we'll need to sync this with our upload's progress. \n\nLivewire
    provides us the [dispatched event](https://laravel-livewire.com/docs/2.x/file-uploads#js-hooks:~:text=upload%20progress%20percentage)
    \"`livewire-upload-progress`\" that sends back a `detail.progress` value. We can
    use this value to track our upload progress. \n\nAll we have to do is listen to
    this browser event and sync our `$progress` in the view with `detail.progress`.
    We'll do this with the help of Livewire's `set` method:\n\n```javascript\n<script>\nwindow.addEventListener('livewire-upload-progress',
    event => {\n  @this.set( 'progress', event.detail.progress );\n});\n```\n`set()`
    updates the value in the client, so it moves the value of our progress bar. However,
    it doesn't immediately send a request to the server to update the attribute in
    the component. Instead, it will be sent along with the next available request.\n\nSince
    all files were uploaded in one request, it's important to take note of a few caveats:\n1.
    Progress percentage is on uploading all files, not individual files\n2. If the
    total size of all files being uploaded exceeds the `POST_MAX_SIZE`, all files
    will not be applicable for upload\n3. Uploading multiple files in a single request
    will definitely take up more time to finish the request, which might result in
    the dreaded 504 Gateway Timeout error \n\nOf course, in this article, we're going
    to solve the restrictions above. We'll do so by uploading the files in separate
    requests, using Livewire's [upload api](https://laravel-livewire.com/docs/2.x/file-uploads#js-api)!\n\n<%=
    partial \"shared/posts/cta\", locals: {\n  title: \"Fly.io ❤️ Laravel\",\n  text:
    \"Fly your servers close to your users&mdash;and marvel at the speed of close
    proximity. Deploy globally on Fly in minutes!\",\n  link_url: \"https://fly.io/docs/laravel\",\n
    \ link_text: \"Deploy your Laravel app!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>\",\n}
    %>\n\n## Uploading Files in Separate Requests\nInstead of uploading all files
    in one request, why not upload each file separately? This neatly separates an
    upload of one file from the others, allowing us to track individual file upload
    progress, isolate any failed file upload from the rest of the selected files,
    and call lighter-weight requests.\n\nFirst, let's ready our file input element
    for using the upload api. We'll remove its binding to `$uploads` and add an id,
    `myFiles`:\n```\n<input type=\"file\" id=\"myFiles\" multiple>\n``` \nThen, we'll
    add our custom \"on-change\" listener to this input element( identified by id
    `#myFiles` )to separately upload each one of the selected files:\n```javascript\n<script>\nconst
    filesSelector = document.querySelector('#myFiles');\n\nfilesSelector.addEventListener('change',
    () => {\n    const fileList = [...filesSelector.files];\n    fileList.forEach((file,
    index) => {\n```\nThe trick here is to assign each file to a specific index in
    the `$uploads` array to avoid clashing variables for different file uploads. We'll
    upload each file using Livewire JavaScript's `upload()` function. \n\nIt receives
    `uploads.<index>` as the name of the attribute we'd want to bind the file with,
    `file` as the reference to a file we're currently uploading, and several callback
    clauses:\n\n```javascript\n        @this.upload( \n          'uploads.'+index,
    \n          file, \n          (n)=>{}, ()=>{}, (e)=>{} \n        );\n    });\n});
    \n``` \n\nIn our earlier section, we briefly inspected the three requests that
    Livewire makes when uploading a file. The first request triggers one `startUpload`
    request. \n\nLet's inspect what's happening behind the scenes of our \"separate\"
    upload requests now. Make sure you have your network inspector open, and select
    any two files:\n![A screenshot of a browser page with the network inspector tab
    opened to the right. The page contains an Uploads title, and a file input selection
    with two files selected. The network tab shows five requests, with the first request's
    payload selected for inspection. The request payload contains several attributes,
    including \"uploads\" array containing two entries that both contain an attribute
    \"method\" with value of \"starUpload\"](img_3.png)\nFive network calls! And if
    we were to check the first call, we'll see instead of one `startUpload` in the
    updates bag, we get two&mdash;one request for each separate file upload. \n\nNext,
    inspect the two \"upload-file\" network calls&mdash;these are the actual file
    upload requests&mdash;they run in parallel! The second upload request was made
    even without waiting for the first one to finish, giving us concurrent upload
    of our two files!\n![A screenshot of a browser page with the network inspector
    tab opened to the right. The page contains an Uploads title, and a file input
    selection with two files selected. The network tab shows five requests. The first
    call is called at 28 ms, afterwards the second and third are called almost at
    the same time. The fourth call is sent after, and then finally the fifth.](img_4.png)\nWhat
    about the last two remaining calls? Once the first upload finishes, Livewire's
    JavaScript makes an update request to the component in order to update the `$uploads[<index>]`
    with details on its respective file details.\nThen, once that update completes,
    Livewire proceeds with doing the same for the next completed file upload. \n\n##
    Customizing Per File Upload\nEach file is assigned to a specific index in our
    `$uploads` array in the component, this allows us to avoid clashing simultaneous
    uploads for the same variable. At the same time, it paves way for us to customize
    each file upload!\n\nInstead of directly assigning Livewire's file reference as
    the value for each index of the array, let's give each index an array instead.
    We can then add different attributes into this array. In fact, we'll assign Livewire's
    file reference as one of the attributes of said array:\n```javascript\n// app\\resources\\views\\livewire\\multi-file-uploder.php\nfileList.forEach((file,
    index) => {\n-   @this.upload('uploads.'+index, file, (n)=>{},()=>{}, (e) => {});\n+
    \  @this.upload('uploads.'+index+'.fileRef',file,(n)=>{},()=>{},(e)=>{});\n```\nWe
    can also include other custom details to this array index, like the file's original
    name, size, or it's upload progress! Let's add those before calling the upload
    function:\n```javascript\n+   @this.set('uploads.'+index+'.fileName', file.name
    );\n+   @this.set('uploads.'+index+'.fileSize', file.size );\n+   @this.set('uploads.'+index+'.progress',
    0 );\n    @this.upload('uploads.'+index+'.fileRef',file,(n)=>{},()=>{},(e)=>{});\n```\nEach
    file is uploaded separately and therefore have separate progress values assigned
    to their respective `$uploads[index]['progress']`. Each progress will start 0%,
    and get updated through each file's `upload()` progress callback: \n```javascript\n
    \   @this.upload('uploads.'+index+'.fileRef',file,(n)=>{},()=>{},(e)=>{\n      //
    Progress Callback\n+      @this.set( \n+         'uploads.'+index+'.progress',\n+
    \        e.detail.progress );      \n    });\n});\n</script>\n```\nWith these
    attributes in place, we can show custom details per file in our html like so:\n```php\n<input
    type=\"file\" id=\"myFiles\" multiple>\n@foreach( $uploads as $i=>$upl )\n    <div>\n
    \       <label>\n            <div>{{ $upl['fileName'] }}</div>\n            <div>{{
    $upl['progress'] }}%</div>\n        </label>\n        <progress max=\"100\" wire:model=\"uploads.{{$i}}.progress\"
    />\n    </div>\n@endforeach\n```\nSince each file is uploaded separately, we finally
    get separate progress bars for each file!\n![A recording of the page mentioned
    above is shown, with the network inspector tab opened at the left. Three files
    are selected, triggering seven network calls to be made, and three progress bars
    to be displayed. Overtime the progress bars increase from 0 to 100](img_5.gif)\n\n##
    Chunking Per File Upload\nNow here comes the fun part. Thanks to this indexing
    of our file details into an array, we can now&mdash;<i>drum rolls</i>&mdash;chunk
    our <i>separate</i> uploads! \n\nWhat's the [added benefit](/laravel-bytes/chunked-file-upload-livewire/)?
    Well consider uploading a file that exceeds the size of our application's configured
    `upload_max_file_size`. Instead of increasing our application's upload size limit
    to allow this upload, we can instead slice the file into smaller chunks that's
    within our application's limitations, and upload each \"chunk\" in separate requests.\n\nTo
    start chunking files we'll have to decide on a `$chunkSize`. We'll use this to
    slice chunks from our file. This should be within and close to our application's
    configured upload limit. The closer this is to the limit, the larger the chunk
    we can upload in one request, the smaller the number of requests we'll have to
    make to completely upload our file.\n\n```php\npublic $uploads = [];\n+   public
    $chunkSize = 5_000_000; // 5MB\n```\nLet's revise our logic for uploading each
    file: we'll remove the call to Livewire's `upload()` function here, and replace
    it with a call to a custom function `livewireUploadChunk()`.\n```javascript\nfileList.forEach((file,
    index) => {\n    @this.set('uploads.'+index+'.fileName', file.name, true );\n
    \   @this.set('uploads.'+index+'.fileSize', file.size, true );\n    @this.set('uploads.'+index+'.progress',
    0, true );\n-   @this.upload(...);\n+   livewireUploadChunk( index, file );\n});\n```\nThis
    function relies on a starting point and an ending point to slice a chunk, so we'll
    need variables for those two values. However it is important to take note that
    we are uploading multiple files in parallel. Because of this concurrency, it's
    important we keep each file's start point as a separate variable from other files'
    to avoid clashing values.\n\nIn our current setup, one file matches one `$uploads`
    index. Therefore, we can do this separation by assigning each file's \"start\"
    point to its respective index. But, since slicing is done client side, we won't
    be assigning it to our `$uploads` array in the component. Instead, we'll declare
    a JavaScript array `chnkStarts` to handle this in the client for us:\n```javascript\n<script>\n
    \   const filesSelector = document.querySelector('#myFiles');\n+   let chnkStarts=[];\n```\nThen
    we initialize each file's chunk start point to 0:\n```javascript\n    filesSelector.addEventListener('change',
    () => {\n        const fileList = [...filesSelector.files];\n        fileList.forEach((file,
    index) => {\n          //...\n+         chnkStarts[index] = 0;    \n          livewireUploadChunk(
    index, file );\n        });\n    });\n```\n\n\nNow that our \"chunk starts\" array
    is setup, let's proceed with the logic for `livewireUploadChunk()`. This function
    will receive an `index` of a file in the `$uploads` array, and the `file` itself.
    It will cut a chunk from the file and upload this chunk using `upload()` function:\n```javascript\nfunction
    livewireUploadChunk( index, file ){\n  // End of chunk is start + chunkSize OR
    file size, whichever is greater\n  const chunkEnd = Math.min(chnkStarts[index]+@js($chunkSize),
    file.size);\n  const chunk    = file.slice(chnkStarts[index], chunkEnd);\n```\n\nTake
    note, we're uploading multiple files at the same time. Depending on the number
    of files selected( which should be restricted&mdash;but is not covered here! ),
    the number of requests can sky rocket. We'd want to be thrifty on these concurrent
    requests, so let's restrict one request per file at any given time.\n\nWe'll upload
    one chunk per file to `$uploads[index]['fileChunk']`. Afterwards, we'll hook on
    to Livewire's `upload()`'s progress callback, and check `e.detail.progress` for
    completion. Once this value reaches 100, we upload the next chunk by recursively
    calling the function on the next available chunk start.\n```javascript\n    @this.upload('uploads.'+index+'.fileChunk',chunk,(n)=>{},()=>{},(e)=>{\n
    \     if( e.detail.progress == 100 ){\n        // Get next start\n        chnkStarts[index]
    = \n          Math.min( chnkStarts[index] + @js($chunkSize), file.size );\n          \n
    \       // Upload if within file size\n        if( chnkStarts[index] < file.size
    )\n            livewireUploadChunk( index, file );\n      }\n    });\n} // End
    livewireUploadChunk\n```\n\nThe above changes should now upload each file by chunks
    in our view. \n\n## Merging Files' Chunks\n\nMoving over to our component in the
    server, we'll need to merge chunks together correctly as they get uploaded. But,
    how exactly do we inject logic after an upload completes?\n  \nWhen Livewire completes
    uploading a file, it would afterwards update the \"wired\" attribute with details
    of the uploaded file( Remember this [third request](/laravel-bytes/multi-file-upload-livewire/#:~:text=it%20sends%20one-,last%20request,-to%20the%20component)?
    ). This means we can use Livewire's `updated` [hook](https://laravel-livewire.com/docs/2.x/lifecycle-hooks#class-hooks)
    on our public attribute to inject logic after Livewire updates it!\n\nIn our case,
    a file's chunk is wired to `$uploads[index]['fileChunk']`, so we'll intercept
    on `updatedUploads()`. Take note, `$uploads` is an array of arrays. `updatedUploads()`
    will run on <i>every</i> change to this array. Remember how we also set `fileName`,
    `fileSize`, and `progress` per `$uploads` index? This will also run for each of
    those!\n\nWe want to specifically inject logic when `$uploads[index]['fileChunk']`
    gets updated. Livewire provides a way to identify the index of an array that got
    updated. We simply pass two parameters to our hook:\n```php\npublic function updatedUploads(
    $value, $key ){\n```\n`$key` gives us the value of the current index of the item
    in `$uploads` that has been updated. Can you guess the value of `$key` that gets
    returned when `$uploads[0]['fileChunks']` gets updated?\n\nIt returns `0.fileChunk`!
    We'll have to parse this string, and only proceed when the attribute changed is
    `fileChunk`. From here we get the `0th` index which we can use to get a reference
    on all attributes under that index:\n```php\n    list($index, $attribute) = explode('.',$key);\n
    \   if( $attribute == 'fileChunk' ){\n        $fileDetails = $this->uploads[intval($index)];\n```\n\nNow
    that we're able to intercept update to a specific file's `fileChunk`( signaling
    a completion of chunk upload ) we can now start merging chunks.\n\nWe'll merge
    incoming chunks to a \"final file\". This \"final file\" will require a name that
    is unique to avoid clashing file uploads. For now let's imagine `$fileDetails[index]['fileName']`
    is as unique a file name as it can be, so we'll use this for the final file:\n```php\n
    \       // Final File\n        $fileName  = $fileDetails['fileName'];\n        $finalPath
    = Storage::path('/livewire-tmp/'.$fileName);  \n```\nThen we'll have to access
    our file chunk that's been uploaded. Since Livewire already updated our `$uploads[index]['fileChunk']`
    with details on the uploaded file chunk, we can use details here to access our
    chunk. For example, we can get the chunks name, and ultimately a reference to
    the chunk's path:\n```php\n        // Chunk File\n        $chunkName = $fileDetails['fileChunk']->getFileName();\n
    \       $chunkPath = Storage::path('/livewire-tmp/'.$chunkName);\n        $chunk
    \     = fopen($chunkPath, 'rb');\n        $buff       = fread($chunk, $this->chunkSize);\n
    \       fclose($chunk);\n```\nThen it's just a matter of merging the current chunk
    with the final file, and cleaning:\n```php\n        // Merge Together\n        $final
    = fopen($finalPath, 'ab');\n        fwrite($final, $buff);\n        fclose($final);\n
    \       unlink($chunkPath);\n```\nThen, while we're here, we can actually update
    the progress of our file chunk's file upload. We can compare the current accumulated
    size with the total file size to get our progress:\n```php     \n        // Progress\n
    \       $curSize = Storage::size('/livewire-tmp/'.$fileName);\n        $this->uploads[$index]['progress']
    = \n        $curSize/$fileDetails['fileSize']*100;\n```\nFinally, once we reach
    a 100% progress, we can finally set our final file as our fileRef! We'll feed
    the final file to Livewire's [TemporaryUploadedFile](https://github.com/livewire/livewire/blob/master/src/WithFileUploads.php#L37)
    class in order to utilize Livewire's uploaded file [features](https://laravel-livewire.com/docs/2.x/file-uploads).\n```php\n
    \       if( $this->uploads[$index]['progress'] == 100 ){\n          $this->uploads[$index]['fileRef']
    = \n          TemporaryUploadedFile::createFromLivewire(\n            '/'.$fileDetails['fileName']\n
    \         );\n        }\n    }\n} // End updatedUploads\n```\n![A recording of
    the page mentioned above is shown, with the network inspector tab opened at the
    left. Three files are selected, triggering several network calls to be made( thanks
    to chunk file upload ), and three progress bars to be displayed. Overtime the
    progress bars increase from 0 to 100.]\n(img_6.gif)\n\nFrom here, we can add a
    submit button and finalize saving each of our files in the `$uploads[<index>]['fileRef']`
    by following [Livewire's guide](https://laravel-livewire.com/docs/2.x/file-uploads#multiple-files)
    on storing multiple files in the server.\n\n## Remarks à la Caveat\n\nIsn't [Livewire](https://laravel-livewire.com/)
    just so remarkable to work with? See how this framework provides us not just with
    an easy way to implement features, <i>but</i> most importantly, a way to customize
    its default implementations. Take for example [Lifecycle hooks](https://laravel-livewire.com/docs/2.x/lifecycle-hooks)&mdash;a
    way to hook on to different parts of Livewire's processing! \n\nSimilarly, with
    the help of its [upload api](https://laravel-livewire.com/docs/2.x/file-uploads#js-api),
    we were able to customize our multi-files upload: from selecting and uploading
    multiple files in one request,  to uploading files in separate, concurrent, chunked
    requests. \n\nOf course, in a similar way that uploading files in one request
    has its drawbacks, so do uploading each file in separate, concurrent, chunked
    requests:\n\n1. Separately uploading multiple files means multiple requests to
    the server. As much as possible, we'd want to customize how fast we're triggering
    these requests, or even consider limiting the number of files to reduce these
    calls.\n2. Chunking our files meant merging these chunks into a final file. Since
    we've manually configured the final file, this means we'll also have to make sure
    that this final file's name does not clash with other existing files.\n3. Finally,
    'customized' requests means we get to further decide on how to handle new selection
    of files. When the user selects new files, should the old files be deleted, or
    should we merge these new files with the existing list?\n \nEverything in life
    has a caveat it seems! But these little \"woops\" and \"here we go again's\" are
    exactly the reason why everyday, we get to explore different nuances, and discover
    little treasures along the way. \n\nAnd today, thanks to the caveats from uploading
    multiple files in one request, we found another remarkable \"customization\" treasure,
    with [Livewire's](https://laravel-livewire.com/) [upload api](https://laravel-livewire.com/docs/2.x/file-uploads#js-api)
    \U0001F48E"
- :id: ruby-dispatch-ci-cd
  :date: '2023-03-06'
  :category: ruby-dispatch
  :title: CI/CD
  :author: rubys
  :thumbnail: ci-cd-thumbnail.jpg
  :alt: inspectors removing defective products off a conveyer belt
  :link: ruby-dispatch/ci-cd
  :path: ruby-dispatch/2023-03-06
  :body: |2


    You've been a model developer.  You've placed your source code under version control and posted it to GitHub.  You've got a suite of tests, and they run green.  You've deployed your software to production.

    That's a lot of work.  You deserve a break.  Let's automate these tasks.  The goal is to only deploy changes that pass the tests that you have defined.  The good news is that GitHub actions makes this easy.


    To get started, place the following in `.github/workflows/ci-cd.yml`:

    ```
    name: CI_CD
    on: [push, pull_request]
    jobs:
      test:
        runs-on: ubuntu-latest
        steps:
        - uses: actions/checkout@v3
        - uses: ruby/setup-ruby@v1
          with:
            bundler-cache: true
        - run: rake test:all
      deploy:
        runs-on: ubuntu-latest
        needs: test
        if: github.ref == 'refs/heads/main'
        steps:
        - uses: actions/checkout@v3
        - uses: superfly/flyctl-actions/setup-flyctl@master
        - run: flyctl deploy --remote-only
          env:
            FLY_API_TOKEN: ${{ secrets.FLY_API_TOKEN }}
    ```

    In a few short lines, what the above says is:

    <div class="callout">
    On every put and every pull request:
      * check out your source, setup ruby, and run all your tests

      * if the tests pass, and the branch is `main`:
          <ul style="margin-bottom: 0"><li>check out your source, setup flyctl, and deploy</li></ul>
    </div>

    Note: the above assumes that your main branch is named `main`.  Some older GitHub repositories use `master`.  Adjust the `github.ref` check as needed.

    The bad news is that if you try this with pretty much the simplest Rails application, it will fail:

    ```sh
    rails new todolist
    cd todolist
    bin/rails generate scaffold Todo item
    bin/rails db:migrate
    ```

    There are two reasons for the failures.  The first is an obscure bug in [Ruby](https://bugs.ruby-lang.org/issues/19158#note-10).  At the moment, Rails 7.1's fix is to [remove the debug gem](https://github.com/rails/rails/pull/47515).  This will eventually work itself out.

    For the moment, if you are on Ruby 3.2 or don't actively use the debug gem, the workaround is to run the following command to do the same with your project:

    ```sh
    bundle remove debug
    ```

    The second problem is that Rails system tests launch a browser, which by default need a display, and the GitHub servers don't have one.  So the solution is to change `:chrome` to `:headless_chrome` in one file:

    ```diff
     require "test_helper"

     class ApplicationSystemTestCase < ActionDispatch::SystemTestCase
    -  driven_by :selenium, using: :chrome, screen_size: [1400, 1400]
    +  driven_by :selenium, using: :headless_chrome, screen_size: [1400, 1400]
     end
    ```

    The next step is to provide your flyctl access token to Github as a secret.


    Start by running the following command:

    ```cmd
    flyctl auth token
    ```

    Now you have a token you need to make it available to GitHub Actions that run against your repository. For that, there's secrets in the repository's settings. GitHub provides four combinations: Environment and Repository, and Secrets and Variables. Click on the green "New repository secret" button in the top left, pop our secret under the `FLY_API_TOKEN` name, and we are ready.

    All that is left is for you to commit your changes and...


    ```cmd
    git push
    ```

    <%= partial "shared/posts/cta", locals: {
      title: "You can play with this right now.",
      text: "It'll take less than 10 minutes to get your Rails application running globally.",
      link_url: "https://fly.io/docs/rails/",
      link_text: "Try Fly for free&nbsp;&nbsp;<span class='opacity-50'>&rarr;</span>"
    } %>

    This is just a taste of what GitHub actions can do for your application.
    The one thing we haven't covered is testing your Dockerfile itself.
    While this can be done using [`docker/setup-buildx-action@v2`](https://github.com/docker/setup-buildx-action#readme), [health checks](https://fly.io/ruby-dispatch/health-checks/) already do this and more.
- :id: phoenix-files-phoenix-dev-blog-streams
  :date: '2023-02-27'
  :category: phoenix-files
  :title: Phoenix Dev Blog - Streams
  :author: chris
  :thumbnail: phoenix-dev-blog-thumbnail.webp
  :alt: Phoenix dev blog cover illustration
  :link: phoenix-files/phoenix-dev-blog-streams
  :path: phoenix-files/2023-02-27
  :body: |2


    <p class="lead">This dev blog introduces LiveView's new Streams feature. It lets us elegantly work with large collections of items without keeping them all in memory on the server. Fly.io is a great place to run a Phoenix application! Check out how to [get started](/docs/elixir/)!</p>

    This is the first installment of the Phoenix development blog where we'll talk about in progress features or day-to-day development updates in between major releases and milestones.

    ## What's the Problem?

    For at least a few years, the Phoenix team has wanted a solution that elegantly addresses large collections of items without requiring the collection to live in memory on the server. We've had a hack in place by allowing a container to be marked with `phx-update="append"` or `phx-update="prepend"` . It worked for some use cases, but it sucked even when it worked. Let's see why.

    Today it works by marking an assign as "temporary", which means the server throws it away after rendering it. Then to append a new item, we  allow the developer to render only the new items, and the client would automagically leave the old ones in place instead of removing them. In practice it looked like this:

    ```elixir
    def render(assigns) do
      ~H"""
      <div id="users" phx-update="append">
        <div :for={user <- @users} id={"user-#{user.id}"}>
          <%%= user.name %>
        </div>
      </div>
      """
    end

    def mount(_, _, socket) do
      users = Accounts.list_users()
      {:ok, assign(socket, users: users), temporary_assigns: [users: []]
    end

    def handle_info({:user_added, new_user}) do
      {:noreply, assign(socket, users: [new_user])}
    end
    ```

    The append/prepend trick allowed developers to start with a "naive" in-memory store of the collection, then optimize it without changing much of their code. They render the collection in `render/1` with a regular `for` comprehension, and assign it in the callbacks with regular `assign`.

    The trick can be seen on line 17, where we re-assign the empty users collection to a single element list with only our new users. When Phoenix LiveView goes to patch the DOM, it will see the parent container is marked with `phx-update="append"` and leave the existing children alone, while adding the new ones.

    Great! Everyone is happy, except this approach sucked for a number of reasons.

    First, deletions were not supported. You'd need to write some JavaScript yourself to remove the DOM elements, and in the case of components you could easily break our own component tracking.  Next, the containers only supported two modes of operation: append or prepend. It was not easy to swap the behaviors such as appending a list of posts in a timeline while also prepending new posts on top. Sorting was also not possible. Finally, the internal implementation was expensive and brittle. Before each DOM patch, we had to "fake" the DOM tree to make it look like newly patched items were present in the new tree to ensure [morphdom](https://github.com/patrick-steele-idem/morphdom) would leave our existing items intact rather than considering them removed.

    ## Enter Streams

    We are introducing a new "streams" feature to solve the issues above. New Phoenix 1.7 applications will use streams out of the box for the `phx.gen.live` LiveView generators.

    Streams bring a new `stream` interface while also carrying over the ease of gradual optimization we had before. Streams support dynamic ordering, which makes appending, prepending, or reordering trivial for the developer. Deletes are also just as trivial. Let's refactor our original example to see how:

    ```diff
    def render(assigns) do
      ~H"""
      <div id="users" phx-update="stream">
    -   <div :for={user <- @users} id={"user-#{user.id}"}>
    +   <div :for={{id, user} <- @streams.users} id={id}>
          <%%= user.name %>
        </div>
      </div>
      """
    end

    def mount(_, _, socket) do
      users = Accounts.list_users()
    - {:ok, assign(socket, users: users), temporary_assigns: [users: []]
    + {:ok, stream(socket, :users, users)}
    end

    def handle_info({:user_added, new_user}) do
    - {:noreply, assign(socket, users: [new_user])}
    + {:noreply, stream_insert(socket, :users, new_user)}
    end
    ```

    In `mount/3`, we define a stream with `stream/3`. Streams clean up after themselves, so there is no need to mess with temporary assigns yourself. Like before, streams identify their items by DOM id. By default, it will use the item `:id` field if the item is a map or struct with such a field. The following two lines are equivalent:

    ```elixir
    stream(socket, :users, users)
    stream(socket, :users, users, dom_id: &"users-#{&1.id}")
    ```

    Next, in the template in `render/1`, we mark the container as `phx-update="stream"`, then we use a regular `for` comprehension, but with two changes. Streams are placed under a `@streams` assign, and when you enumerate a stream you get the computed DOM id along with each item. We then render the DOM id and content as before.

    Finally, in `handle_info/2` we see the stream interface in action. `stream_insert` allows inserting or updating items in the stream. By default, items will be appended on the client, but you can programmatically place them with the `:at` option, which mimics the behavior of Elixir's `List.insert_at`. The following two lines are equivalent:

    ```elixir
    stream_insert(socket, :users, new_user)
    stream_insert(socket, :users, new_user, at: -1)
    ```

    To prepend the new user in the UI instead:

    ```elixir
    stream_insert(socket, :users, new_user, at: 0)
    ```

    You can also place the  user at an arbitrary index, which makes reordering items in the UI a breeze.

    For deletes, `stream_delete` works as you'd expect:

    ```elixir
    stream_delete(socket, :users, user)
    ```

    Here's a fully realized example of what streams unlock for LiveView developers. We updated our flagship [LiveBeats](https://fly.io/blog/livebeats/) example app to use streams for its playlist, with drag and drop re-ordering, deletion, and more:

    <%= video_tag "streams-in-livebeats.mp4?card&center", title: "Animated GIF showing a LiveBeats song list being reordered by drag and drop and updating on a remote player" %>

    ## Should streams be used by default now for lists of items?

    Streams by default for any kind of collection is a good intuition to have. You should use streams any time you don’t want to hold the list of items in memory – which is most times. Streams are also a goto when you want to efficiently update a single list item without refactoring to a layer of LiveComponents for the items.

    ## Streams Retrospective

    There's something really satisfying about implementing a long-term feature, then shedding all that knowledge by being a _user_ of the feature_._ LiveView features continue to do this kind of thing to me. I wrote it all – and it still feels like magic when using it!

    After playing with the top-level stream API as a user, I am also struck by how simple it is. I constantly wonder “how did it take this long to do this?”, but then you look at the PR. It touched every layer of the LiveView stack – it required features/additions to the HTML engine at the parser level, the diffing engine, the client diff merging, and patches to morphdom.

    The best thing about streams is the internal implementation is optimized for both the server and the client. We introduced new features in morphdom to drop all the fake DOM tree hacks from the previous approach.

    I'm excited to finally offer a comprehensive solution to an area I was never really satisfied with before. I can't wait to see what folks ship with this!

    Happy hacking!

    –Chris

    <%= partial "shared/posts/cta", locals: {
      title: "Fly.io ❤️ Elixir",
      text: "Fly.io is a great way to run your Phoenix LiveView app close to your users. It's really easy to get started. You can be running in minutes.",
      link_url: "https://fly.io/docs/elixir/",
      link_text: "Deploy a Phoenix app today!  <span class='opacity:50'>→</span>"
    } %>
- :id: blog-deploying-django-to-production
  :date: '2023-02-23'
  :category: blog
  :title: Deploying Django to Production
  :author: katia
  :thumbnail: django-rocket-flying-by-pony-shaped-planet-thumbnail.jpeg
  :alt: A blue sky with planets in the background. A green rocket flying over the
    clouds by a pink hair white pony shaped planet next to a blue hair white baby
    pony.
  :link: blog/deploying-django-to-production
  :path: blog/2023-02-23
  :body: "\n\n<p class=\"lead\">This post is about providing some guidance on how
    to make your simple Django app production-ready and deploying to Fly.io. Django
    on Fly.io is pretty sweet! Check it out: [you can be up and running on Fly.io
    in just minutes.](https://fly.io/docs/django/)</p>\n\nThe first Django app I ever
    created was a simple Blog back in 2015, during a [Django Girls](https://djangogirls.org/en/)
    event in Brazil. Ever since then, I've created and deployed many other Django
    applications but the deployment process was never so easy as it was with Fly.io!
    I'm super excited to share it with you, let's go? ✨\n\n## What I can find here?\n\nThis
    post has two main sections:\n\n1. [Setting up](#setting-up): steps on how to make
    your local Django app ready for going to production on any hosting platform.\n2.
    [Deploying to Fly.io](#deploying-to-fly-io): deploying your production-ready Django
    app to Fly.io.\n\n<aside class=\"right-sidenote\">For reference, we'are using
    Python `3.10.9` and Django `4.1.6`.</aside>\n\nFor this guide, I'll be updating
    the [Django Girls Tutorial](https://tutorial.djangogirls.org/en/), a server-rendered
    Blog, to exemplify how we can transform any Django app to be a production-ready
    application independent of the hosting provider.\n\nThen, we'll go through the
    deployment process to Fly.io. If you already have a production-ready app, that's
    great! You might want to jump to the \"[Deploying to Fly.io](#deploying-to-fly-io)\"
    section.\n\nWith all that said, let's start at the beginning…\n\n## Setting up
    ⚒️\n\nWe assume the initial setup is already done, you have [Python](https://www.python.org/downloads/)
    installed and a virtual environment created and activated to manage our dependencies.
    We'll be using [`venv`](https://docs.python.org/3/library/venv.html#creating-virtual-environments)
    for this project:\n\n```bash\n# Unix/macOS\n$ python3 -m venv .venv\n$ source
    .venv/bin/activate\n(.venv) $\n\n# Windows\n$ python -m venv .venv\n$ .venv\\Scripts\\activate\n(.venv)
    $\n```\n\nFrom this point on, the commands won't be displayed with `(.venv) $`
    but we assume you have your Python virtual environment **activated**.\n\n### Database\n\n![The
    most popular backend database for Django Developers](postgres-most-used-database.png?1/2&wrap-right)\n\nDjango
    uses SQLite by default and this is the easiest way to start once no other packages
    are required to support this database. However, we want a more scalable database
    so we decide to use PostgreSQL on production. According to the official [documentation](https://docs.djangoproject.com/en/4.1/ref/contrib/postgres/),
    PostgreSQL is the best-suited and supported database for Django and also the most
    used database by Django developers, as reported by the official annual [Django
    Developers Survey 2021](https://lp.jetbrains.com/django-developer-survey-2021-486/).\n\nThe
    great thing is that Fly.io provides a single-node/high availability PostgreSQL
    cluster out of the box for us to use. It's also easy to set it up when configuring
    your deployment. We'll go over how in the next steps.\n\nFor now, make sure to
    [download](https://www.postgresql.org/download/) and install PostgreSQL. You can
    go ahead and create your local database. For my app, the database is called `blog`.\n\nThe
    config updates required to change our database are explained in the next steps.\n\n###
    Environment Variables\n\nFirst of all, we want to store the configuration separate
    from our code and load them at runtime. This allow us to keep one `settings.py`
    file and still have multiple environments (i.e. local/staging/production).\n\nOne
    popular options is the usage of the [`environs`](https://github.com/sloria/environs)
    package. Another option is the [`python-decouple`](https://github.com/HBNetwork/python-decouple)
    package, which was originally designed for Django, but it's recommended to be
    used with [`dj-database-url`](https://github.com/jazzband/dj-database-url) to
    configure the database. For this guide we'll use [`django-environ`](https://github.com/joke2k/django-environ)
    to configure our Django application, which allows us to use the [12factor](https://www.12factor.net/)
    approach.\n\nMake sure your Python virtual environment is activated and let's
    install the `django-environ`:\n\n```cmd\npython -m pip install django-environ==0.9.0\n```\n\nIn
    our `settings.py` file, we can define the casting and default values for specific
    variables, for example, setting `DEBUG` to `False` by default.\n\n```python\n#
    settings.py\nfrom pathlib import Path\nimport environ  # <-- Updated!\n\nenv =
    environ.Env(  # <-- Updated!\n\t# set casting, default value\n    DEBUG=(bool,
    False),\n)\n```\n\n`django-environ` (and also the other mentioned packages) can
    take environment variables from the `.env` file. Go ahead and create this file
    in your root directory and also don't forget to add it to your `.gitignore` in
    case you are using [Git](https://git-scm.com/) for version control (you should!).
    We don't want those variables to be public, this will be used for the local environment
    and be set separately in our production environment.\n\nMake sure you are taking
    the environment variables from your `.env` file:\n\n```python\n# settings.py\n#
    Build paths inside the project like this: BASE_DIR / 'subdir'.\nBASE_DIR = Path(__file__).resolve().parent.parent\n\n#
    Take environment variables from .env file\nenviron.Env.read_env(BASE_DIR / '.env')
    \ # <-- Updated!\n```\n\nWe can now set the specific environment variables in
    the `.env` file:\n\n```bash\n# .env\nSECRET_KEY=3ohiu^m1su%906rf#mws)xt=1u#!xdj+l_ahdh0r#$(k_=e7lb\nDEBUG=True\n```\n\nCheck
    that there are no quotations around strings neither spaces around the `=`.\n\nComing
    back to our `settings.py`, we can then read `SECRET_KEY` and `DEBUG` from our
    environment variables:\n\n```python\n# settings.py\n# SECURITY WARNING: keep the
    secret key used in production secret!\nSECRET_KEY = env('SECRET_KEY')  # <-- Updated!\n\n#
    SECURITY WARNING: don't run with debug turned on in production!\nDEBUG = env('DEBUG')
    \ # <-- Updated!\n```\n\nThe last environment variable to be set is the `DATABASE_URL`.
    `env.db()` will read from the `DATABASE_URL` variable.\n\n```python\n# settings.py\nDATABASES
    = {\n\t# read os.environ['DATABASE_URL']\n    'default': env.db()  # <-- Updated!\n}\n```\n\nHere
    we can define our local database, adding it to the `.env` file:\n\n```bash\n#
    .env\nSECRET_KEY=3ohiu^m1su%906rf#mws)xt=1u#!xdj+l_ahdh0r#$(k_=e7lb\nDEBUG=True\nDATABASE_URL=postgres://postgres:postgres@localhost:5432/blog
    \ # <-- Updated!\n```\n\nIf you prefer to keep using SQLite on your local environment,
    that's where you should specify:\n\n```bash\n# .env\nSECRET_KEY=3ohiu^m1su%906rf#mws)xt=1u#!xdj+l_ahdh0r#$(k_=e7lb\nDEBUG=True\nDATABASE_URL=sqlite:///db.sqlite
    \ # <-- Updated!\n```\n\n⚠️ However, it's highly recommended to use the same database
    for development and production to avoid inconsistent behavior between different
    environments.\n\n### Psycopg\n\nTo interact with our Postgres database, we'll
    use the most popular PostgreSQL database adapter for Python, the [`psycopg`](https://github.com/psycopg/psycopg2)
    package. With your virtual environment activated, go ahead and installed it:\n\n```cmd\npython
    -m pip install psycopg2==2.9.5\n```\n\n⚠️ For production, [it's advised to use
    the source distribution](https://psycopg.org/docs/install.html?highlight=binary#psycopg-vs-psycopg-binary)
    instead of the binary package (`psycopg2-binary`).\n\n### Gunicorn\n\nWhen starting
    a Django project (with `startproject` management command), we get a minimal WSGI
    configuration set up out of the box. However, this default webserver is not recommended
    for production. [Gunicorn](https://github.com/benoitc/gunicorn) (Green Unicorn)
    is a Python WSGI HTTP Server for Unix and one of the easiest to start with. It
    can be installed using pip:\n\n```cmd\npython -m pip install gunicorn==20.1.0\n```\n\n###
    Static Files\n\nHandling static files in production is a bit more complex than
    in development. One of the easiest and most popular ways to serve our static files
    in production is using the [WhiteNoise](https://github.com/evansd/whitenoise)
    package, which serves them directly from our WSGI Server (Gunicorn). Install it
    with:\n\n```cmd\npython -m pip install whitenoise==6.3.0\n```\n\nA few changes
    to our `settings.py` are necessary.\n\n- Add the WhiteNoise to the `MIDDLEWARE`
    list right after the `SecurityMiddleware`.\n- Set the `STATIC_ROOT` to the directory
    where the `collectstatic` management command will collect the static files for
    deployment.\n- (Optional) Set `STATICFILES_STORAGE` to `CompressedManifestStaticFilesStorage`
    to have [compression and caching support](https://whitenoise.evans.io/en/latest/django.html#add-compression-and-caching-support).\n\n```python\n#
    settings.py\nMIDDLEWARE = [\n    'django.middleware.security.SecurityMiddleware',\n
    \   'whitenoise.middleware.WhiteNoiseMiddleware',  # <-- Updated!\n    ...\n]\n\n...\nSTATIC_ROOT
    = BASE_DIR / 'staticfiles'  # <-- Updated!\n\nSTATICFILES_STORAGE = 'whitenoise.storage.CompressedManifestStaticFilesStorage'
    \ # <-- Updated!\n```\n\nIt's recommended to use WhiteNoise also in development
    to keep consistent behavior between development and production environments. The
    easiest way to do that is to add `whitenoise.runserver_nostatic` to our `INSTALLED_APPS`
    right before the built-in `staticfiles` app:\n\n```python\n# settings.py\nINSTALLED_APPS
    = [\n    ...\n    'whitenoise.runserver_nostatic',  # <-- Updated!\n    'django.contrib.staticfiles',\n
    \   ...\n]\n```\n\n### `ALLOWED_HOSTS` and `CSRF_TRUSTED_ORIGINS`\n\nAs a security
    measure, we should set in `ALLOWED_HOSTS`, a list of host/domain names that our
    Django website can serve. For development we might include `localhost` and `127.0.0.1`
    and for our production we can start with `.fly.dev` (or the provider's subdomain
    you chose) and update for the dedicated URL once your app is deployed to the hosting
    platform.\n\n`CSRF_TRUSTED_ORIGINS` should also be defined with a list of origins
    to perform unsafe requests (e.g. POST). We can set the subdomain `https://*.fly.dev`
    (or the provider's subdomain you chose) until our deployment is done and we have
    the proper domain for our website.\n\n```python\n# settings.py\nALLOWED_HOSTS
    = ['localhost', '127.0.0.1', '.fly.dev']  # <-- Updated!\n\nCSRF_TRUSTED_ORIGINS
    = ['https://*.fly.dev']  # <-- Updated!\n```\n\n### Installed packages\n\nMake
    sure you have all necessary installed packages tracked and listed in your `requirements.txt`
    by running:\n\n```cmd\npip freeze > requirements.txt\n```\n\nThis command generates
    the `requirements.txt` file if it doesn't exist.\n\n```bash\n# requirements.txt\nasgiref==3.6.0\nDjango==4.1.6\ndjango-environ==0.9.0\ngunicorn==20.1.0\npsycopg2==2.9.5\nsqlparse==0.4.3\nwhitenoise==6.3.0\n```\n\nWith
    our Django application prepped and ready for production hosting, we'll take the
    next step and deploy our app to Fly.io!\n\n## Deploying to Fly.io \U0001F680\n\n[`flyctl`](https://fly.io/docs/hands-on/install-flyctl/)
    is the command-line utility provided by Fly.io.\n\nIf not installed yet, follow
    these [instructions](https://fly.io/docs/hands-on/install-flyctl/), [sign up](https://fly.io/docs/hands-on/sign-up/)
    and [log in](https://fly.io/docs/hands-on/sign-in/) to Fly.io.\n\n### Launching
    our App\n\nFly.io allows us to deploy our Django app as long as it's packaged
    in a Docker image. However, we don't need to define our `Dockerfile` manually.
    Fly.io detects our Django app and automatically generates all the necessary files
    for our deployment. Those are:\n\n- `Dockerfile` contain commands to build our
    image.\n- `.dockerignore` list of files or directories Docker will ignore during
    the build process.\n- `fly.toml` configuration for deployment on Fly.io.\n\nAll
    of those files are templates for a simple Django apps and can be modified according
    to your needs.\n\nBefore deploying our app, first we need to configure and launch
    our app to Fly.io by using the `flyctl` command `fly launch`. During the process,
    we will:\n\n- **Choose an app name**: this will be your dedicated fly.dev subdomain.\n-
    **Select the organization**: you can [create a new organization](https://fly.io/docs/flyctl/orgs-create/)
    or deploy to your `personal` account (connect to your Fly account, visible only
    to you).\n- **Choose the region for deployment**: Fly.io initially suggests the
    closest to you, you can [choose another region](https://fly.io/docs/reference/regions/)
    if you prefer.\n- **Set up a Postgres database cluster**: `flyctl` offers a single
    node \"Development\" config that is designed so we can turn it into a high-availability
    cluster by adding a second instance in the same region. [Fly Postgres is a regular
    app you deploy on Fly.io, not a managed database](https://fly.io/docs/postgres/getting-started/what-you-should-know/).\n\nThis
    is what it looks like when we run `fly launch`:\n\n```cmd\nfly launch\n```\n```output\nCreating
    app in ../flyio/katias-blog\nScanning source code\nDetected a Django app\n? Choose
    an app name (leave blank to generate one): katias-blog\n? Select Organization:
    Kátia Nakamura (personal)\n? Choose a region for deployment: Frankfurt, Germany
    (fra)\nCreated app katias-blog in organization personal\nAdmin URL: https://fly.io/apps/katias-blog\nHostname:
    katias-blog.fly.dev\nSet secrets on katias-blog: SECRET_KEY  <-- # SECRET_KEY
    is set here!\n? Would you like to set up a Postgresql database now? Yes\n? Select
    configuration: Development - Single node, 1x shared CPU, 256MB RAM, 1GB disk\nCreating
    postgres cluster in organization personal\nCreating app...\nSetting secrets on
    app katias-blog-db...\nProvisioning 1 of 1 machines with image flyio/postgres:14.6\nWaiting
    for machine to start...\nMachine 32874445c04218 is created\n==> Monitoring health
    checks\n  Waiting for 32874445c04218 to become healthy (started, 3/3)\n\nPostgres
    cluster katias-blog-db created\n  Username:    postgres\n  Password:    <your-internal-postgres-password>\n
    \ Hostname:    katias-blog-db.internal\n  Proxy port:  5432\n  Postgres port:
    \ 5433\n  Connection string: postgres://postgres:<your-internal-postgres-password>@katias-blog-db.internal:5432\n\nSave
    your credentials in a secure place -- you won't be able to see them again!\n\nConnect
    to postgres\nAny app within the Kátia Nakamura organization can connect to this
    Postgres using the above connection string\n\nNow that you've set up Postgres,
    here's what you need to understand: https://fly.io/docs/postgres/getting-started/what-you-should-know/\nChecking
    for existing attachments\nRegistering attachment\nCreating database\nCreating
    user\n\nPostgres cluster katias-blog-db is now attached to katias-blog\nThe following
    secret was added to katias-blog:  <-- # DATABASE_URL is set here!\n  DATABASE_URL=postgres://katias_blog:<your-postgres-password>@top2.nearest.of.katias-blog-db.internal:5432/katias_blog?sslmode=disable\nPostgres
    cluster katias-blog-db is now attached to katias-blog\n? Would you like to set
    up an Upstash Redis database now? No\nCreating database migrations\nWrote config
    file fly.toml\n\nYour Django app is ready to deploy!\n\nFor detailed documentation,
    see https://fly.dev/docs/django/\n```\n\nDuring the process, the `SECRET_KEY`
    and `DATABASE_URL` will be automatically set to be used on your production deployment.
    Those are the only ones we need at the moment but if you have any other secrets,
    check [here](https://fly.io/docs/reference/secrets/#setting-secrets) how to set
    them. You can also list all your application secret names:\n\n```cmd\nfly secrets
    list\n```\n```output\nNAME            DIGEST                  CREATED AT\nDATABASE_URL
    \   cc999c17fa021988        2023-02-07T19:48:55Z\nSECRET_KEY      e0a6dbbd078004f7
    \       2023-02-07T19:47:33Z\n```\n\n`fly launch` sets up a running app, creating
    the necessary files: `Dockerfile`, `.dockerignore` and `fly.toml`.\n\nDon't forget
    to replace `demo.wsgi` in your `Dockerfile` with your Django project's name:\n\n```Dockerfile\n#
    Dockefile\n...\n# replace demo.wsgi with <project_name>.wsgi\nCMD [\"gunicorn\",
    \"--bind\", \":8000\", \"--workers\", \"2\", \"website.wsgi\"]  # <-- Updated!\n```\n\nFor
    production, it's advised to use `psycopg2` package (instead of the `psycopg2-binary`)
    built from source but it requires external packages to work properly: `libpq-dev`
    is a very light package and provides all requirements for building `psycopg2`
    and `gcc` is a compiler used to install `psycopg2`. Let's install them:\n\n```Dockerfile\n#
    Dockefile\n...\nWORKDIR /code\n\n# install psycopg2 dependencies\nRUN apt-get
    update && apt-get install -y \\\n    libpq-dev \\\n    gcc \\\n    && rm -rf /var/lib/apt/lists/*
    \ # <-- Updated!\n\nCOPY requirements.txt /tmp/requirements.txt\n...\n```\n\nFor
    security reasons, we'll add `.env` to our `.dockerignore` file - so Docker doesn't
    include our secrets during the build process.\n\n```\n# .dockerignore\nfly.toml\n.git/\n*.sqlite3\n.env
    \ # <-- Updated!\n```\n\nThis means the environment variables (i.e. `SECRET_KEY`)
    stored in the `.env` file won't be available at build time, neither the secrets
    automatically set by Fly.io on your application during the `fly launch` process.\n\nIf
    your app contains static files such as images, CSS or Javascript files, we need
    to collect all the static files into a single location and make them accessible
    to be served in production. This process needs to happen at build time (so they
    are persisted when building our image) by running `collectstatic` command on `Dockerfile`:\n\n```Dockerfile\n#
    Dockerfile\n...\nRUN python manage.py collectstatic --noinput\n```\n\nWe have
    two options here:\n\nSet a default `SECRET_KEY` using the `get_random_secret_key`
    function provided by Django that will be used at build time. At runtime, we'll
    use the `SECRET_KEY` set by Fly.io, i.e. the default value only applies to the
    build process.\n\n```python\n# settings.py\nfrom django.core.management.utils
    import get_random_secret_key\n...\n# SECURITY WARNING: keep the secret key used
    in production secret!\nSECRET_KEY = env.str('SECRET_KEY', default=get_random_secret_key())
    \ # <-- Updated!\n```\n\nAnother option is to set a \"non-secret\" dummy `SECRET_KEY`
    on `Dockerfile` only for building purposes:\n\n```Dockerfile\n# Dockerfile\n\n#
    Set SECRET_KEY for building purposes\nENV SECRET_KEY \"non-secret-key-for-building-purposes\"
    \ # <-- Updated!\nRUN python manage.py collectstatic --noinput\n```\n\nThis keeps
    our environment variables safe and makes sure they will be set for the different
    environments.\n\nNow that we have set our app name, we can update our `settings.py`
    with the dedicated subdomain we chose (or that was generated for us):\n\n```python\n#
    settings.py\nALLOWED_HOSTS = ['localhost', '127.0.0.1', 'katias-blog.fly.dev']
    \ # <-- Updated!\n\nCSRF_TRUSTED_ORIGINS = ['https://katias-blog.fly.dev']  #
    <-- Updated!\n```\n\nFinally, in the `[[statics]]` section on `fly.toml` file,
    we define the `guest_path`, which is the path inside our container where the files
    will be served directly to the users, bypassing our web server. In the `settings.py`
    we defined the `STATIC_ROOT`:\n\n```python\n# settings.py\nSTATIC_ROOT = BASE_DIR
    / 'staticfiles'  # <-- Updated!\n```\n\nThe `Dockerfile` generated by `fly launch`
    defines our `WORKDIR` as `/code`. That's where our static files will be collected:
    `/code/staticfiles`. Let's go ahead and update our `fly.toml` to serve those files
    directly:\n\n```bash\n# fly.toml\n[[statics]]\n  guest_path = \"/code/staticfiles\"
    \ # <-- Updated!\n  url_prefix = \"/static\"\n```\n\n\n### Deploying our App\n\nGreat!
    All ready and it's finally time to deploy our app:\n\n```cmd\nfly deploy\n```\n```output\n...\n
    1 desired, 1 placed, 1 healthy, 0 unhealthy [health checks: 1 total, 1 passing]\n-->
    v0 deployed successfully\n```\n\nOur app is now up and running! ⚙️ Try:\n\n```cmd\nfly
    open\n```\n\n<aside class=\"right-sidenote\">\U0001F4BE By the way, you can find
    the example code used in this guide [here](https://github.com/katiayn/katias-blog).</aside>\n\nYAY!
    \U0001F389 We just deployed our Django app to production! How great is that?\n\n##
    What's next?\n\nMake sure to check the [official deployment checklist](https://docs.djangoproject.com/en/4.1/howto/deployment/checklist/)
    provided by Django docs with more details on specific settings we might not have
    covered in this guide. You can also check the [Fly.io Docs](https://fly.io/docs/)
    - we're currently working on the [Django on Fly.io docs](https://fly.io/docs/django/)!\n\nIf
    you have any question or comments, reach out on the [Fly.io Community](https://community.fly.io/c/django/26).
    That's a great place to share knowledge, help and get help!\n\n\U0001F4E2 Now,
    tell me… What are your go-to packages \U0001F4E6 to make your Django app production-ready?\n\n<%=
    partial \"shared/posts/cta\", locals: {\n  title: \"Django really flies on Fly.io\",\n
    \ text: \"You already know Django makes it easier to build better apps. Well now
    Fly.io makes it easier to _deploy_ those apps and move them closer to your users
    making it faster for them too!\",\n  link_url: \"https://fly.io/docs/django/\",\n
    \ link_text: \"Deploy a Django app today!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>\"}\n%>\n"
- :id: blog-gossip-glomers
  :date: '2023-02-22'
  :category: blog
  :title: Gossip Glomers
  :author:
  :thumbnail: gossip-glomers-thumbnail.jpg
  :alt: Hooking into NATS to centralize logs
  :link: blog/gossip-glomers
  :path: blog/2023-02-22
  :body: |2


    <p class="lead">We're Fly.io. We run apps for our users on hardware we host around the world. This post isn't about our platform. Rather, it's an elaborate plot to get you to write some code just for the hell of it.</p>

    In the field of computer science, the industry is represented by two separate yet equally important groups: the software developers who build Rails applications and mobile games, and the academics who write theory papers about why the problems those apps try to solve are NP-hard.  This is a story about both.

    Distributed systems span the practical-academic divide. Reading a stack of MIT PhD dissertations may be a good Friday night, but it won't equip you for debugging a multi-service outage at 2am. That requires real-world experience.

    Likewise, building a fleet of microservices won't give you the conceptual tools to gracefully & safely handle failure. Many failure scenarios are rare.  They don't show up in unit tests. But they're devastating when they do show up. Nailing down the theory  gives you a fighting chance at designing a correct system in the first place.

    The practical and academic tracks seldom converge. To fix this, we teamed up with [Kyle Kingsbury](https://aphyr.com/about), author of [Jepsen](https://jepsen.io/), to develop a series of distributed systems challenges that combine real code with the academic rigor of Jepsen's verification system.

    We call these challenges the  [Gossip Glomers](/dist-sys).

    <p class="callout">
      **What the f$#* is a Glomer?**
      <br/>
      It's an elaborate pun about the CAP theorem.
    </p>

    ## How it works

    You know Kyle Kingsbury from his "[Call Me Maybe](https://jepsen.io/analyses)" blog posts that eviscerate distributed databases. You may also have known about [Jepsen](https://github.com/jepsen-io/jepsen), the Clojure-based open-source tooling Kyle uses to conduct these analyses. Well, Kyle also wrote another tool on top of Jepsen called [Maelstrom](https://github.com/jepsen-io/maelstrom).

    Maelstrom runs toy distributed systems on a simulated network.  It easily runs on a laptop. Kyle uses it to teach distributed systems. We all thought it'd be neat to build a series of challenges that would teach people around the Internet Maelstrom, and, in turn, some distributed systems theory.

    Each challenge is composed of several parts:

    - The _workload_ acts as a set of clients to your distributed systems. These clients send different types of messages as defined by the challenge and expect certain constraints to be met. These workloads can vary between a simple distributed counter all the way to multi-operation, transactional database systems.
    - The _simulated network_  injects network partitions or slows messages between nodes.
    - The _verification system_  uses Jepsen to check consistency and availability constraints required by the challenge.
    - And finally, the binary for the _node_ which is written by you!

    ## Pathway to distributed systems enlightenment

    Our challenges start off easy and get more difficult as you move along. They're organized into six high-level challenges with many of those having several smaller challenges within them.

    First, you'll start with the Echo challenge. This is the "hello world" of distributed systems challenges. It gets you up and running and helps you understand how these challenges work.

    Next, you'll build a totally-available, distributed unique ID generator. In this challenge, nodes will need to be coordination-free and independently generate a unique identifier for any number of clients.

    After that, the difficulty starts to ramp up with the broadcast challenge. In this challenge, you'll need to propagate messages out to all the nodes in the cluster. You'll need to ensure fault tolerance in the face of network partitions and then work to optimize your message delivery to minimize the number of messages sent within your system.

    Once you've made it past broadcast, you'll implement a grow-only counter, or g-counter. The tricky part with this challenge is that you'll need to build on top of Maelstrom's [sequentially](https://jepsen.io/consistency/models/sequential) consistent key/value store.

    Then you'll dive into the world of replicated logs by building a Kafka-like system. This challenge will build on the [linearizable](https://jepsen.io/consistency/models/linearizable) key/value store provided by Maelstrom but you'll need to figure out how to not only make it correct but also efficient.

    Finally, you'll wrap up with the totally-available transactions challenge where you'll build a transactional database on various consistency levels.

    ## A bit of history

    Over the past year, we've been growing like gangbusters. That's great. But it also means we've been hiring, and hiring is hard.

    We hire [resume-blind, based on work-sample tests](https://fly.io/docs/hiring/): we have people write code and design systems, and then score those submissions based on a rubric. We've got criteria set up for [early-career, mid-level, and team-lead developers](https://fly.io/docs/hiring/levels/). But we didn't have strong criteria for hiring staff engineers.

    So we began tossing around ideas. In a previous life, some of us had success with a series of cryptography challenges called [Cryptopals](https://cryptopals.com/), so we figured we'd try something similar, but with a distributed systems flavor.

    That sounded great but how do you actually test distributed systems to know if someone passed or failed? For weeks, we wrote up one iteration after another but none of them felt right.

    Finally, we had a brilliant idea. Let's find someone who lives and breathes distributed system validation! That someone is Kyle Kingsbury.

    After working on these challenges with Kyle, we realized that they are too much fun to keep to ourselves as an internal evaluation tool. So we're releasing them for anyone to play with.

    ## But wait… there's more!

    If you scoff in the face of cascading failures, if you bend consistency levels to your will, and if you read [k8s.af](https://k8s.af/) post-mortems as bedtime stories to your kids, you may be interested in trying our hardest challenge.

    We reserved this last challenge for evaluating our staff engineers at Fly.io. So if you think you'd be up to the challenge, [we'd love to talk to you](https://fly.io/jobs/).
- :id: blog-shipping-logs
  :date: '2023-02-21'
  :category: blog
  :title: Shipping Logs
  :author: fideloper
  :thumbnail: slinging-logs-in-nats-thumbnail.jpeg
  :alt: Hooking into NATS to centralize logs
  :link: blog/shipping-logs
  :path: blog/2023-02-21
  :body: "\n\n<p class=\"lead\">Fly.io runs apps (globally) in just few commands.
    That means a lot of log output! Centralizing logs is important. [Fire up an app](/docs/speedrun/)
    and follow along as we see just how easy it can be.</p>\n\nNearly all of our apps
    are puking output. Sometimes, it's intentional. Often this output is in the form
    of structured logs. \n\nLogs are helpful for a variety of use cases - debugging,
    tracking, collating, correlating, coalescing, and condensing the happenings of
    your code into useful bits of human-parsable information.\n\nThere can be a lot
    of logs, from a lot of apps. Aggregating logs to a central place is useful for
    many reasons, but here are my top 2 favorite:\n\n1. **Correlation** - Being able
    to search/query/report on all your logs in one place helps you correlate events
    (\"Joe deleted prod again\") amongst services\n2. **Retention** - Fly.io doesn't
    keep your logs around forever. If you want to see them, retain them!\n\n## The
    Logging River\n\nSince we grab stdout from the processes run in your apps, whatever
    an app outputs becomes a log. Logs are constantly flowing through Fly.io's infrastructure.\n\n<aside
    class=\"right-sidenote\"><p>&nbsp;</p>Image: [Wikimedia](https://commons.wikimedia.org/wiki/File%3APhotograph_of_Log_Jam_-_NARA_-_2129372.jpg)</aside>\n![Black-and-white
    photo of people walking on a river jammed with logs](logjam.jpg)\n\nHere's how
    that works.\n\nYour apps run in a VM via Firecracker. Inside the VM, we inject
    an `init` process (pid 1) that runs and monitors your app. Since we build VM's
    from Docker images, `init` is taking `ENTRYPOINT` + `CMD` and running that.\nThe
    `init` program (really just a bit of Rust that we named `init`) is, among other
    things, gathering process output from stdout and shooting it into a socket.\n\nOutside
    of the VM, on the host, a bit of Golang takes that output and sends it to [Vector](https://vector.dev/)
    via yet-another socket.\n\nVector's job is to ship logs to other places. In this
    case, those logs (your app's output) are shipped to an internal [NATS](https://nats.io)
    cluster. For the sake of simplicity, let's call NATS a \"fancy, clustered pubsub
    service\". Clients can subscribe to specific topics, and NATS sends the requested
    data to those subscribers.\n\nIn true Fly.io fashion, a proxy sits in front of
    NATS. We call this proxy \"Flaps\" (Fly Log Access Pipeline Server™, as one does).
    Flaps ensures you only see your own logs.\n\n**NATS is the fun part!** You can
    hook into NATS (via Flaps) to get your logs.\n\nTo get your logs, all you need
    is an app that acts as a NATS client, reads the logs, and ships them somewhere.
    Vector can do just that! It's fairly simple - in fact, we've done the work for
    you:\n\n## The Fly Log Shipper™\n\nTo ship your logs, you can run an instance
    of the [Fly Log Shipper](https://github.com/superfly/fly-log-shipper).\n\nThis
    app configures a Vector [sink](https://vector.dev/docs/reference/configuration/sinks/)
    of your choosing, and runs Vector. A sink is a \"driver\" that Vector will ship
    logs to, for example Loki, Datadog, or (bless your heart) Cloudwatch.\n\nI liked
    the look of [Logtail](https://logtail.com/), so I tried out its free tier.\n\n<p
    class=\"callout\">Logtail actually lets you set Fly.io as a source of logs, but
    as we can see, we're actually just telling Vector to send logs somewhere. \nIf
    your log aggregator doesn't know Fly.io exists, that's fine. It just needs a [Vector
    sink](https://vector.dev/docs/reference/configuration/sinks/) to exist. \nThe
    process is the same no matter what log aggregator you use.</p>\n\nIf you sign
    up for Logtail, it helpfully gives you instructions on setting that up with Fly.io.\n\n![logtail
    fly.io setup](logtail-1.png)\n\nLet's go ahead and follow those instructions (they're
    similar to what you see on the [Log Shipper repo](https://github.com/superfly/fly-log-shipper)).\n\n##
    Using the Log Shipper\n\nThe NATS log stream is scoped to your organization. This
    means that the Fly Log Shipper collects logs from *all* your applications.\n\nHere's
    how to set it up with Logtail:\n\n```bash\n# Make a directory for\n# our log shipper
    app\nmkdir logshippper\ncd logshippper\n\n# I chose not to deploy yet\nfly launch
    --image ghcr.io/superfly/fly-log-shipper:latest\n\n# Set some secrets. The secret
    / env var you set\n# determines which \"sinks\" are configured\nfly secrets set
    ORG=personal\nfly secrets set ACCESS_TOKEN=$(fly auth token)\nfly secrets set
    LOGTAIL_TOKEN=<token provided by logtail source>\n```\n\nYou can configure as
    many providers as you'd like by adding more secrets. The secrets needed are determined
    by [which provider(s)](https://github.com/superfly/fly-log-shipper#provider-configuration)
    you want to use.\n\nBefore launching your application, you should edit the generated
    `fly.toml` file and delete the entire `[[services]]` section. Replace it with
    this:\n\n```toml\n[[services]]\n  http_checks = []\n  internal_port = 8686\n```\n\nThen
    you can deploy it:\n\n```cmd\nfly deploy\n```\n\nYou'll soon start to see logs
    appear from all of your apps.\n\n![logtail logs from fly.io](logtail-2.png)\n\nThat
    wasn't too bad!\n\n<%= partial \"shared/posts/cta\", locals: {\n  title: \"Try
    it out yourself!\",\n  text: \"You have apps. Apps have logs! Run your apps and
    ship your logs in just a few commands.\",\n  link_url: \"https://fly.io/docs/speedrun\",\n
    \ link_text: \"Deploy your app!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>\",\n}
    %>\n\n## Shipping Specific Logs\n\nSo far we've seen how to ship logs from every
    application in your organization.\n\nYou can, however, narrow that down by setting
    a `SUBJECT` environment variable. That can be set in the `fly.toml`'s `[env]`
    section, or as an application secret.\n\nI opted to add it to my `fly.toml`, which
    looked like this:\n\n```toml\n[env]\n  SUBJECT = \"logs.sandwich.>\"\n```\n\nThe
    subject is in format `logs.<app_name>.<region>.<instance_id>`. An example `SUBJECT`
    to only log an application named `sandwich` (no matter what region it's in) is:
    \n\n```bash\nSUBJECT=\"logs.sandwich.>\"\n```\n\nSee that greater-than symbol
    `>`? That's a [NATS wildcard](https://docs.nats.io/nats-concepts/subjects#wildcards).
    There are also regular wildcards `*`, but the special wildcard `>` is used at
    the end of the string to say \"and anything to the right of this\".\n\nSo, our
    use of `\"logs.sandwich.>\"` says to ship any logs that are from application `sandwich`,
    no matter what region or instance they come from. You can (ab)use this to get
    the logs you're interested in.\n\nGo forth and ship logs!\n"
- :id: ruby-dispatch-health-checks
  :date: '2023-02-21'
  :category: ruby-dispatch
  :title: Health Checks
  :author: rubys
  :thumbnail: health-check-thumbnail.png
  :alt: sick birdie with thermometer in mouth
  :link: ruby-dispatch/health-checks
  :path: ruby-dispatch/2023-02-21
  :body: "\n\n**Rails 7.1 is adding [discoverable](https://github.com/rails/rails/pull/47217#discussion_r1098112364)
    [health checks](https://github.com/rails/rails/pull/46936), which `fly launch`
    will automatically configure and monitor.  This page will tell you what you need
    to know.**\n\nNo matter how well you plan, you will always need to be prepared
    to deal with\nunforeseen and unforeseeable events.  As Fly.io will route requests
    to the\nnearest *healthy* server, deploying your application across multiple regions\nnot
    only means that you will be serving requests close to your users when\nthings
    are running smoothly, it also means that you will be able to\ncontinue processing
    requests when there are isolated or even regional outages.\n\nLet's start with
    a demo.  Run the following commands in a terminal window.\n\n<%= partial \"shared/posts/cta\",
    locals: {\n  title: \"You can play with this right now.\",\n  text: \"It'll take
    less than 10 minutes to get your Rails application running globally.\",\n  link_url:
    \"https://fly.io/docs/rails/\",\n  link_text: \"Try Fly for free&nbsp;&nbsp;<span
    class='opacity-50'>&rarr;</span>\"\n} %>\n\n```shell\nrails new health_demo --main
    --minimal\ncd health_demo\necho 'Rails.application.routes.draw {root \"rails/welcome#index\"}'
    \\\n  >> config/routes.rb\nfly launch\n```\n\nAt this point you will be asked
    a series of questions.  Feel free to accept the\ndefault by pressing enter in
    response to each question.  You are now ready to deploy:\n\n```shell\nfly deploy\n````\n\nThis
    command won't complete until the health checks pass and your application\nis ready
    to process requests.  You can visit your application by using the\n`fly open`
    command.\n\nNow let's take a look at the results of the latest health checks:\n\n```\nfly
    checks list\n```\n\nYou will see two health checks were run.  One is a TCP check
    which verifies\nthat your application is listening for requests.  The other is
    a HTTP check\nwhich verifies that your application is capable of producing responses.\n\nAn
    example of where your application might be listening for responses but\nnot actually
    ready to process a request is when you have left the puma\ndefaults at 5 threads
    and you currently are experiencing a spike in traffic.\nIn cases like these you
    might want to review your `config/puma.rb`, or\ndeploy more servers.\n\nAnother
    example is where you have a CDN or an nginx server handing static\nrequests that
    pass or proxy the remainder to your Rails application, and\nyour CDN or nginx
    server is healthy and your Rails server is, um, not.\n\nIf you like, you can add
    a nginx server to your deployment by running\nthe following commands:\n\n```\nbin/rails
    generate dockerfile --nginx --force\nfly deploy\n```\n\nAn example of a successful
    Rails 7.1 response:\n\n```html\n200 OK Output: <html><body style=\"background-color:
    green\"></body></html>[✓]\n```\n\nSo far we've only deployed our application to
    one region.  Now lets make it\ninteresting and scale up to two regions.  I'm in
    the US, so I'll deploy my\nsecond server in Europe:\n\n```shell\nfly regions add
    cdg\nfly scale count 2\n```\n\nFeel free to pick [another region](https://fly.io/docs/reference/regions/#fly-io-regions).
    \ For demo purposes, you might want to avoid the Paid Plan Only regions.\n\nIf
    you rerun the `fly checks list` command you will see four checks now.  If\nyou
    run this command quickly enough you might catch it before the second server\nhas
    fully started.\n\nAlso, try adding `--json` to the `fly checks list` command.
    \ This output could\nbe processed by scripts.\n\nBefore you try this on your own
    project: please check the [Rails\nFAQ](https://fly.io/docs/rails/getting-started/dockerfiles/#scaling)
    for\nimportant information on how to get Fly.io to prepare your databases once
    per\ndeploy and not once per server.\n\n## Behind the scenes\n\nSo far everything
    has been taken care of for you.  But perhaps your application\nisn't yet on Rails
    7.1 (which is understandable as it hasn't been released).\nOr perhaps you want
    to tweak how often health checks are run.  Either way, we\nhave you covered.\n\nAll
    you need to do to get a health check is to add a route, and tell us to call\nit.\n\nThe
    most simplest route will do.  For example, you can add the following to\n`config/routes.rb`:\n\n```ruby\nget
    \"/up\", to: proc { [200, {}, [\"ok\"]] },\n  as: :rails_health_check\n```\n\nThis
    won't be as pretty as the Rails 7.1 with its fancy green background, but\nin every
    way that is important it will get the job done.\n\nIf you don't like `/up` as
    the endpoint name, feel free to change it.  This\ngoes for the one that Rails
    7.1 provides too.\n\nIf this route is present before you run `fly launch`, your
    application will\nbe automatically configured to call this endpoint.\n\nIf you
    have already run `fly launch`, it is not too late to tell us to\ncall your health
    check.  All you need to do is add the following to your\n`fly.toml` file:\n\n```toml\n
    \ [[services.http_checks]]\n    interval = 10000\n    grace_period = \"5s\"\n
    \   method = \"get\"\n    path = \"/up\"\n    protocol = \"http\"\n    restart_limit
    = 0\n    timeout = 2000\n    tls_skip_verify = false\n    [services.http_checks.headers]\n```\n\nMore
    information on what each value means can be found in our\n[reference documentation](https://fly.io/docs/reference/configuration/).\nAdjust
    to your tastes, and then run `fly deploy`.\n\n## Going beyond \"/up\"\n\nYour
    Rails application undoubtedly does more than display a splash screen.\nThe more
    moving parts you have, the more that can go wrong.  You can\nrun out of memory,
    or run out of disk space.  You might be using a database\nrunning on another machine
    or hosted by a third party.  You may be using\nredis.  Or perhaps Amazon, Azure,
    or Google Cloud Services.\n\nAny one of these could go down at any time.  Often
    such outages are local or\nregional.  If you monitor your dependencies, Fly.io
    will route requests around\nthe failures until the outage is cleared.\n\nThere
    is nothing magical about the implementation of the \"/up\" route.\nIt can be routed
    to any controller action in your application.  All\nthat application needs to
    do is produce a successful response when\nthings are good.  And return a response
    like the following when\nthings are not-so-good:\n\n```ruby\nrender plain: \"BUSY\",
    status: :service_unavailable\n```\n\nDon't worry too much about the text of the
    response or the status code used.\n`:service_unavailable` is good for outages
    that you are prepared and explicitly\ncheck for.  But if your application gets
    to the point where it is incapable of\nproducing a coherent response, `500 :internal_server_error`
    works too.\n\nBest of all, you don't have to start from scratch writing these\ncontrollers.
    \ [easymon](https://github.com/basecamp/easymon) and\n[rails-healhcheck](https://github.com/linqueta/rails-healthcheck)
    both\nwill not only provide examples which check for common dependencies,\nthey
    also provide means for you to add your own checks.\n\nIn many cases, adding a
    check is becomes a matter of adding a one liner\nto to a configuration file and
    deploying.\n\nNeither of these gems produce routes that are currently autodiscoverable,
    but\nwe have reached out to both\n([easymon](https://github.com/basecamp/easymon/issues/34),\n[rails-healthcheck](https://github.com/linqueta/rails-healthcheck/pull/77)).\nIn
    the meanwhile, you can scroll back on this page to see what you need to add\nto
    your `fly.toml` to tell fly.io to check these endpoints.\n\n\n## Recap\n\nAt this
    point, you've:\n\n  * run a demo where an application has been deployed to two
    regions,\n    and health checks are provisioned for you automatically.\n  * seen
    how you can add health checks to an existing application and\n    tailor parameters
    like how often the checks are to be called.\n  * found two gems that make it easy
    to extend these checks to handle third\n    party failures.\n    \nWith this knowledge
    you are prepared to run your application in as many\nregions as it takes to make
    you comfortable that your application can\nsurvive an outage.\n"
- :id: laravel-bytes-progress-indicator-livewire
  :date: '2023-02-16'
  :category: laravel-bytes
  :title: Progress Indicator with Livewire
  :author: kathryn
  :thumbnail: loading-chunks-thumbnail.png
  :alt: We are shown an image with a background of pastel, green color. Encompassing
    most of the image's center is a seemingly electric stove, on top of which chunks
    of brown cube ingredients are cooked in a grey frying pan. On the body of the
    stove, right below the cooking frying pan, is a label and a progress bar indicating
    the term LOADING.
  :link: laravel-bytes/progress-indicator-livewire
  :path: laravel-bytes/2023-02-16
  :body: "\n<p class=\"lead\">Today we'll implement a progress indicator using Livewire.
    Fly your app close to your users with Fly.io, get your [Laravel app running](/docs/laravel/)
    in minutes!</p>\nIn the article [Chunked File Upload with Livewire](/laravel-bytes/chunked-file-upload-livewire/)
    we easily uploaded a file in separate, smaller chunks using Livewire's [upload](https://laravel-livewire.com/docs/2.x/file-uploads#js-api)
    function. Today, we'll create a progress bar indicator to let our users know how
    much progress has been made uploading the file.\n\n\n## What's in a Progress?\n\nWe
    can imagine progress as an accumulation of \"small wins\" leading up to a certain
    expectation. It is a portrait that summarizes how far we've come from completing
    our little wins towards an \"expected goal\".\n\nProgress can be measurable&mdash;if
    we can identify our \"expected goal\" and list down \"small wins\" that add up
    to it. \n\nToday, the progress we're calculating is the successful upload of each
    chunk out of the total chunks of a file to upload. Each chunk uploaded is a small
    win towards completely uploading a file. \n\nOur calculation would be as follows:
    Progress Percentage = \"Number of Small Wins So Far\" / \"Expected Total Wins\"
    * 100%. \n\nThis means we'll need a `$chunkCount` to let us know how many chunks
    we're expecting ( **Expected Total Wins** ), `$uploadedCount` to keep track of
    how many chunks have been successfuly uploaded ( **Number of Small Wins so Far**
    ), and finally, a separate `$progressPercentage` for our progress bar to use (
    **Progress!** ).\n\n\n## Stepping Towards Progress\n\nIn order to display progress
    of separate wins for our file upload, it's important to accumulate each win and
    update our client with the total progress.\n\nThis is where Livewire comes in.
    Livewire provides us with just the right bridge of [data sharing mechanism](https://laravel-livewire.com/docs/2.x/properties#:~:text=4%20%20%20%20...-,Public%20properties,-in%20Livewire%20are)
    to keep track of the completed chunk uploads, and [data lifecycle hooks](https://laravel-livewire.com/docs/2.x/lifecycle-hooks#:~:text=Runs-,after%20any%20update,-to%20the%20Livewire)
    to recalculate our progress and make implementing a progress bar a breeze:\n1.
    We'll want to automatically share progress variables between client and server,
    so we declare [public attributes](https://laravel-livewire.com/docs/2.x/properties#:~:text=4%20%20%20%20...-,Public%20properties,-in%20Livewire%20are)
    `$chunkCount`, `$uploadedCount`, and `$progressPercentage`. \n2. We'll use a progress
    bar element in our html as our progress indicator and bind its value with the
    value of `$progressPercentage`\n3. Every time a chunk gets successfuly uploaded,
    we intercept this upload completion in the server by using Livewire's [updated](https://laravel-livewire.com/docs/2.x/lifecycle-hooks#:~:text=Runs-,after%20any%20update,-to%20the%20Livewire)
    hook. From there we can increment our `$uploadedCount`, and recalculate the `$progressPercentage`
    right before the Livewire component sends back a success response to  the Livewire
    view. Once the Livewire view receives this update in our `$progressPercentage`'s
    value, it should trigger a re-rendering of the `<progress>` element bound to our
    attribute, thereby updating the displayed progress for our users to see!\n\n##
    The Indicator of Progress\nProgress is calculable, and so we'll first need to
    declare variables we'll be using in calculating this value in our Livewire component:\n```php\n//
    app/Http/Livewire/ChunkedFileUpload.php\n\npublic $chunkCount;\npublic $uploadedCount;\npublic
    $progressPercentage;\n```\n\nWe're building on top of the setup of our [article
    here](/laravel-bytes/chunked-file-upload-livewire/#:~:text=custom%20JavaScript%20function-,uploadChunks(),-will%20slice%20a),
    where our users can select their file and click on a button to trigger a custom
    JavaScript function `uploadChunks()` to slice and upload chunks from their selected
    file:\n```php\n<!-- app/resources/views/livewire/chunked-file-upload.php -->\n\n<input
    type=\"file\" id=\"myFile\"/>\n<button type=\"button\" id=\"submit\" onclick=\"uploadChunks()\">Submit</button>\n```\nEvery
    time a file gets submitted for upload, we'll initialize our progress variables
    to correspond to a new file's progress. \n`uploadChunks()` is triggered every
    time our user requests for a file upload, so let's add our progress re-initialization
    here:\n```php\n<script>\n...\nfunction uploadChunks(){\n   \n    // File Details\n
    \   const file = document.querySelector('#myFile').files[0];\n+   @this.set('chunkCount',
    Math.ceil(file.size/@js($chunkSize)));\n+   @this.set('uploadedCount', 0);\n+
    \  @this.set('progressPercentage', 0);\n    ...\n}\n</script>\n```\n\nNext, let's
    add in a progress bar in our Livewire view. We'll [wire](https://laravel-livewire.com/docs/2.x/properties#data-binding)
    it with the `$progressPercentage` attribute declared above. This way, any change
    happening on the `$progressPercentage` attribute will refresh and calculate the
    value of our progress bar.\n\n```diff\n<!-- app/resources/views/livewire/chunked-file-upload.php
    -->\n\n<input type=\"file\" id=\"myFile\"/>\n<button type=\"button\" id=\"submit\"
    onclick=\"uploadChunks()\">Submit</button>\n\n+ @if( $progressPercentage  )\n+
    \ <progress max=\"100\" wire:model=\"progressPercentage\" /></progress>\n+ @endif\n```\n\nNow
    that we have our progress bar dancing to the tune of the `$progressPercentage`
    attribute, we're ready to update this progress every time we complete a small
    win&mdash;that is, every time we successfuly upload a chunk of our file.\n\n<%=
    partial \"shared/posts/cta\", locals: {\n  title: \"Fly.io ❤️ Laravel\",\n  text:
    \"Fly your servers close to your users&mdash;and marvel at the speed of close
    proximity. Deploy globally on Fly in minutes!\",\n  link_url: \"https://fly.io/docs/laravel\",\n
    \ link_text: \"Deploy your Laravel app!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>\",\n}
    %>\n\n## This is How we Progress\nWe'll update our progress every time a chunk
    gets successfuly uploaded. In order to do so, we'll have to answer two questions&mdash;how
    do we know when a chunk completes uploading, and how do we react to this completion?\n
    \ \nWe're building over a setup that uses Livewire's [upload](https://laravel-livewire.com/docs/2.x/file-uploads#js-api)
    function to [upload our file chunk](/laravel-bytes/chunked-file-upload-livewire/#:~:text=After-,uploading,-the%20first%20chunk):
    \n```javascript\n@this.upload('fileChunk', chunk, ...);\n```  \nNotice the attributes
    passed in the function: `chunk` is the JavaScript variable that holds reference
    to a chunk of our file, while `fileChunk` maps to a public attribute in our Livewire
    component declared as `$fileChunk`. \n\nWhen this function runs, the Livewire
    view does a bunch of uploading process for the variable `chunk` into a temporary
    folder in our server. Once it completes uploading the file chunk, it then updates
    the mapped attribute `$fileChunk` with details on the uploaded chunk. Livewire
    conveniently provides us the `updated` hook which we can use to inject logic after
    this \"update\" happens on our variable. \n\nFrom this hook, we can recalculate
    the value of `$progressPercentage`. Every time a chunk gets uploaded, we increment
    the number of chunks uploaded `$uploadedCount`. Then we'll divide this by our
    expected finish&mdash;the total number of chunks: `$chunkCount`,  and finally
    multiply it by a 100 to get our `$progressPercentage`:  \n```php\npublic function
    updatedFileChunk()\n{\n    // Update progress here\n    $this->uploadedCount +=
    1;\n    $this->progressPercentage \n    = ($this->uploadedCount / $this->chunkCount)
    * 100;\n    ...\n}\n```\nOur `updatedFileChunk()` hook recalculates and updates
    our `$progressPercentage` right before the Livewire component responds to the
    client regarding the successful upload. \n\nThe change sent back from our Livewire
    component on the `$progressPercentage` refreshes the `<progress>` element bound
    to it with a new value, moving our progress indicator.\n\nThat's it! Try uploading
    your file now, and see that progress bar moving towards a 100!\n"
- :id: phoenix-files-phoenix-liveview-and-sqlite-autocomplete
  :date: '2023-02-16'
  :category: phoenix-files
  :title: Phoenix LiveView and SQLite Autocomplete
  :author: jason
  :thumbnail: autocomplete-thumbnail.jpg
  :alt: Image of a wacky database building an index.
  :link: phoenix-files/phoenix-liveview-and-sqlite-autocomplete
  :path: phoenix-files/2023-02-16
  :body: "\n\n<p class=\"lead\">This is a series using SQLite3 Full Text Search and
    Elixir. Fly.io is a great place to run Elixir applications! Check out how to [get
    started](/docs/elixir/)!</p>\n\nIn our [last post](/phoenix-files/sqlite3-full-text-search-with-phoenix/)
    we saw an example of how we could use the built-in Full Text Search capability,
    FTS5, of SQLite3 to create a search index and query it with Ecto. We also know
    it's fast to query and especially so since it is in memory.\n\nLet's see how easy
    it is to _show_ those results using Phoenix LiveView!\n\nTo begin we'll need some
    data; I just so happen to have a SQLite database built indexing the Fly.io/docs.
    My schema is set up the same as the previous examples with a title, URL and body,
    I also include a levels or hierarchy listing to match the hierarchy in the fly.io/docs.\n\nThe
    design I am working towards is inspired by the existing Fly.io/docs search, and
    when we are done we should have something that looks like this:\n\n<%= video_tag
    \"doc-search-final.mp4?card&center\", title: \"A search UI filters results as
    the user types\"  %>\n\nFirst thing we should do when given a design like this
    is break it down into smaller components and try to work out the ultimate structure
    of what will become our code. I am using a tool called [Excalidraw](https://www.excalidraw.com)
    but pen and paper works too!\n\n![Wireframe diagram of a modal with search input
    and results list](components.png)\n\nBreaking this down:\n\n- A modal that will
    show up once we click the search dialog\n- A new search input, which should be
    automatically focused, \n-  Search Results list with many result items.\n\nWe
    will scaffold out a `live_component` that does just that! Create the file `lib/app_web/live/document_live/search_component.ex`:\n\n```elixir\ndefmodule
    AppWeb.DocumentLive.SearchComponent do\n  use AppWeb, :live_component\n\n  @impl
    true\n  def render(assigns) do\n    ~H\"\"\"\n    \"\"\"\n  end\n\n  def search_input(assigns)
    do\n    ~H\"\"\"\n    \"\"\"\n  end\n\n  def results(assigns) do\n    ~H\"\"\"\n
    \   \"\"\"\n  end\n\n  def result_item(assigns) do\n    ~H\"\"\"\n    \"\"\"\n
    \ end\n\n  def search_modal(assigns) do\n    ~H\"\"\"\n    \"\"\"\n  end\n  \n
    \ @impl true\n  def update(assigns, socket) do\n    {:ok, socket}\n  end\nend\n\n```\n\nWe
    have our main render function, our update function and the three components we
    highlighted above. Since this component will be stateful we need to render it
    in one of our live pages, in my case `lib/app_web/live/document_live/index.html.heex`:\n\n```xml\n<.live_component
    \n     module={SearchComponent}\n     id=\"search-results\"\n     show={true}\n
    \    on_cancel={%JS{}} \n   />\n\n```\n\nHere, you would wire up `show` and `on_cancel`
    to an assigns or click event. In the video above I hooked it up to a click event
    on my fake search input. This is left as an exercise to the reader.\n\nFrom here
    there are many paths one could take; personally, I prefer to have some real data
    loaded up into my assigns before I begin. Let us modify the `update/2` to call
    to our Context function `search_documents/1` with a default query.\n\n```elixir\n
    \ @impl true\n  def update(assigns, socket) do\n    {:ok,\n     socket\n     |>
    assign(assigns)\n     |> assign_new(:documents, search_documents(\"sqlite\", []))\n
    \    |> assign_new(:query, \"sqlite\")\n    }\n  end\n\n```\n\nHere we apply all
    the default assigns that came from update, and we include `documents` and `query`.\n\nNow
    when querying the SQLite3 FTS5 index it can be a little finicky and it will raise
    an exception if you send it something it can't handle. We will handle that case
    in our `search_documents` function\n\n```elixir\n  defp search_documents(query,
    default) when is_binary(query) do\n    try do\n      Content.search_documents(query)\n
    \   rescue \n      Exqlite.Error ->\n        default\n    end\n  end\n  defp search_documents(_,
    default), do: default\n\n```\n\nPretty self-explanatory, if Exqlite doesn't love
    our query and throws a parse error, we simply return the last known good results.
    This protects us from users trying out nefarious inputs and the user from SQLite's
    finicky parser.\n\n<aside class=\"callout\">Normally in Elixir we have a \"Let
    it fail!\" attitude where errors result in the supervisor restarting your process,
    but in this case it would result in LiveView reloading the page and losing the
    user's position. So lets protect the user from that.</aside>\n\nFinally, we've
    got some search results data loaded up and can render the results! So let's call
    our functions in our top level `render` function:\n\n```elixir\n  @impl true\n
    \ def render(assigns) do\n    ~H\"\"\"\n    <div>\n      <.search_modal :if={@show}
    id=\"search-modal\" show on_cancel={@on_cancel}>\n        <.search_input value={@query}
    phx-target={@myself} phx-keyup=\"do-search\" phx-debounce=\"200\" />\n        <.results
    docs={@documents} />\n      </.search_modal>\n    </div>\n    \"\"\"\n  end\n\n\n```\n\nWhat
    we're doing here is setting up our modal, conditionally showing it based on the
    `@show` assign, rendering our `search_input` and `results`. Thanks to our stubbed
    functions, this should render nothing at all! The beauty is, as we fill in the
    pieces, it will all start showing up on the page.\n\nStarting with the modal,
    let's not dive too deep into how it is set up because frankly it's copied from
    the default `core_components.ex` and modified to work better for our use case.\n\n```elixir\n
    \ attr :id, :string, required: true\n  attr :show, :boolean, default: false\n
    \ attr :on_cancel, JS, default: %JS{}\n  slot :inner_block, required: true\n\n
    \ def search_modal(assigns) do\n    ~H\"\"\"\n    <div\n      id={@id}\n      phx-mounted={@show
    && show_modal(@id)}\n      phx-remove={hide_modal(@id)}\n      class=\"relative
    z-50 hidden\"\n    >\n      <div id={\"#{@id}-bg\"} class=\"fixed inset-0 bg-zinc-50/90
    transition-opacity\" aria-hidden=\"true\" />\n      <div\n        class=\"fixed
    inset-0 overflow-y-auto\"\n        aria-labelledby={\"#{@id}-title\"}\n        aria-describedby={\"#{@id}-description\"}\n
    \       role=\"dialog\"\n        aria-modal=\"true\"\n        tabindex=\"0\"\n
    \     >\n        <div class=\"flex min-h-full justify-center\">\n          <div
    class=\"w-full min-h-12 max-w-3xl p-2 sm:p-4 lg:py-6\">\n            <.focus_wrap\n
    \             id={\"#{@id}-container\"}\n              phx-mounted={@show && show_modal(@id)}\n
    \             phx-window-keydown={hide_modal(@on_cancel, @id)}\n              phx-key=\"escape\"\n
    \             phx-click-away={hide_modal(@on_cancel, @id)}\n              class=\"hidden
    relative rounded-2xl bg-white p-2 shadow-lg shadow-zinc-700/10 ring-1 ring-zinc-700/10
    transition min-h-[30vh] max-h-[50vh] overflow-y-scroll\"\n            >\n              <div
    id={\"#{@id}-content\"}>\n                <%%= render_slot(@inner_block) %>\n
    \             </div>\n            </.focus_wrap>\n          </div>\n        </div>\n
    \     </div>\n    </div>\n    \"\"\"\n  end\n\n```\n\nWe set up our `attrs` at
    the top, and render the modal. The major changes are:\n\n- removed the header,
    button areas and the close button\n- added spacing and shifted the whole thing
    up\n\nThis will accept an inner block and handle opening and closing the modal
    for us, and shares all the modal logic and transitions for doing so!\n\nThe next
    item in our modal is the search input but I like to see the modal in action so
    lets jump straight to rendering results.\n\n```elixir\n  attr :docs, :list, required:
    true\n  def search_results(assigns) do\n    ~H\"\"\"\n      <ul class=\"-mb-2
    py-2 text-sm text-gray-800 flex space-y-2 flex-col\" id=\"options\" role=\"listbox\">\n
    \       <li :if={@docs == []} id=\"option-none\" role=\"option\" tabindex=\"-1\"
    class=\"cursor-default select-none rounded-md px-4 py-2 text-xl\">\n          No
    Results\n        </li>\n\n        <.link navigate={~p\"/documents/#{doc.id}\"}
    id={\"doc-#{doc.id}\"} :for={doc <- @docs}>\n          <.result_item doc={doc}
    />\n        </.link>\n      </ul>\n    \"\"\"\n  end\n\n```\n\nHere we declare
    we are expecting an list of `@docs` we setup a `ul` and conditionally render an
    `li` if we have no results, otherwise we iterate using the `:for={doc <- @docs}`
    helper and we call into `result_item`\n\n```elixir\n  attr :doc, :map, required:
    true\n  def result_item(assigns) do\n    ~H\"\"\"\n      <li class=\"cursor-default
    select-none rounded-md px-4 py-2 text-xl bg-zinc-100 hover:bg-zinc-800 hover:text-white
    hover:cursor-pointer flex flex-row space-x-2 items-center\" id={\"option-#{@doc.id}\"}
    role=\"option\" tabindex=\"-1\" >\n        <!-- svg of a document -->\n\n        <div>\n
    \         <%%= @doc.title %> \n          <div class=\"text-xs\"><%%= clean_levels(@doc.levels)
    %></div>\n        </div>\n      </li>\n    \"\"\"\n  end\n\n```\n\nWhich is almost
    unnecessary, it simply renders an `li`, with a title and our doc hierarchy. I
    left out the SVG heroicon for a document for brevity. If we check our webpage,
    what we should be seeing is something like this:\n\n![](search-results.png)\n\nWhich
    is great! The last step is to add an input and wire up some interactivity, so
    lets fill in the `search_input` function now\n\n```elixir\n  attr :rest, :global\n
    \ def search_input(assigns) do\n    ~H\"\"\"\n      <div class=\"relative \">\n
    \       <!-- Heroicon name: mini/magnifying-glass -->\n\n        <input {@rest}
    \n\t        type=\"text\" \n\t        class=\"h-12 w-full border-none focus:ring-0
    pl-11 pr-4 text-gray-800 placeholder-gray-400 sm:text-sm\" \n\t        placeholder=\"Search
    the docs..\" \n\t        role=\"combobox\" \n\t        aria-expanded=\"false\"
    \n\t        aria-controls=\"options\">\n      </div>\n    \"\"\"\n  end\n\n```\n\nWhich
    is almost a bare input! Once again, we left out the SVG for brevity, and the only
    thing we do here is assign all the attributes straight to the input. Let's take
    a closer look at the call to `search_input`:\n\n```elixir\n<.search_input \n\tvalue={@query}
    \n\tphx-target={@myself} \n\tphx-keyup=\"do-search\" \n\tphx-debounce=\"200\"
    \n\t/>\n\n\n```\n\nWe set the value straight to our `@query` value. We also set
    the special `phx-target` attribute to `@myself` which tells Phoenix, \"hey, this
    event should be routed to this component, not my parent.\" Without this declaration,
    the `key-up` event would be dispatched to the caller of this component. Then we
    hook up the event to the `do-search` event, and we tell the front end to `debounce`
    this event.\n\n<aside class=\"callout\"> Debounce here means the browser will
    only send an event every 200 milliseconds. The etymology for debounce is from
    electrical engineers working with mechanical switches. When a switch closes it
    doesn't happen instantly, it has many points of partial contact as the two pieces
    of metal come close. This would cause a \"ripple\" of electricity through the
    wires, the act of debouncing was removing this \"bouncing\" ripple. </aside>\n\nAnd
    the last step is that we need to remove our initialized code from update and to
    handle the input event:\n\n```elixir\n\n  @impl true\n  def mount(socket) do\n
    \   {:ok, socket, temporary_assigns: [docs: []]}\n  end\n  \n  @impl true\n  def
    update(assigns, socket) do\n    {:ok,\n     socket\n     |> assign(assigns)\n
    \    |> assign_new(:documents, [])\n     |> assign_new(:search, \"\")\n    }\n
    \ end\n\n  @impl true\n  def handle_event(\"do-search\", %{\"value\" => value},
    socket) do\n    {:noreply, \n      socket\n      |> assign(:search, value)\n      |>
    assign(:documents, search_documents(value, socket.assigns.documents))\n    }\n
    \ end\n\n```\n\nAnd finally we are there! If you want to see it once more, scroll
    up! I did add one small optimization and that was adding a `mount` function and
    declaring `@docs` to be a `temporary_assign`, this simply tells LiveView to not
    keep all the `@docs` in memory after every change, render the results then toss
    the data.\n\n<aside class=\"callout\"> Notice when you have partially typed words
    the results can kinda flicker. This is the result of how the SQLite3 FTS5 indexes
    your text. It indexes based on tokens which are whole words separated by spaces,
    it does not do partial word matching. For example, if the word `sqlite` with spaces
    around it, is not in a document it won't return any results. The same goes, if
    we type `sq` it will look for the token `sq` and not match `sqlite`.\n\nIf you
    want partial word matches don't dismay! You simply need to use the [Experimental
    Trigram Tokenizer!](https://www.sqlite.org/fts5.html#the_experimental_trigram_tokenizer)
    which can handle partial matching. I won't be going into details in this post
    but follow that link and you can set it up for your usecases! </aside>\n\nOne
    thing that is immediately clear to me is that results are near instant! And that
    is because SQLite lives right in memory next to your application! We have no round
    trip to the database, minimal encoding and decoding and zero chance of dropping
    packets. And thanks to LiveView the user facing implementation is frankly boring!\n\nThis
    is what gets me so excited about SQLite, LiveView and Fly.io. If you use Fly.io's
    global network you can deploy this to wherever _your_ users are. You don't need
    to use some kind of lambda running on wasm, calling propreitary databases and
    propreitary API's, this is normal code that you own. And as always with LiveView
    there is no JavaScript to think about, just elixir code running on the server.\n\nNext
    time we discuss some ways you could Architect your search, trying out the distributed
    SQLite Database LiteFS, and ways to keep your data fresh!\n\n<%= partial \"shared/posts/cta\",
    locals: {\n  title: \"Fly.io ❤️ Elixir\",\n  text: \"Fly.io is a great way to
    run your Phoenix LiveView app close to your users. It's really easy to get started.
    You can be running in minutes.\",\n  link_url: \"https://fly.io/docs/elixir/\",\n
    \ link_text: \"Deploy a Phoenix app today!  <span class='opacity:50'>→</span>\"\n}
    %>\n"
- :id: phoenix-files-pass-user-agent-info-to-your-liveview
  :date: '2023-02-15'
  :category: phoenix-files
  :title: Pass User Agent info to your LiveView
  :author: lubien
  :thumbnail: pass-ua-info-to-liveview-thumbnail.png
  :alt: A Phoenix called The User Agent holding a magnifier
  :link: phoenix-files/pass-user-agent-info-to-your-liveview
  :path: phoenix-files/2023-02-15
  :body: "\n\n<p class=\"lead\">In this article we talk about getting User Agent inside
    your LiveView. Fly.io is a great place to run your Phoenix LiveView applications!
    Check out how to [get started](/docs/elixir/)!</p>\n\nHere&#39;s a quick recipe.
    Say your LiveView needs to show something based on the device your user is on
    such as Mac or Windows. You can parse that from your User Agent header, in fact
    LiveView already gives you a simple method for that: `get_connect_info(socket,
    :user_agent)`.\n\nThe `get_connect_info/2` function allows you to retrieve connection
    information that is only available during the mount lifecycle, such as header
    or IP information.\n\nHere&#39;s a simple LiveView that just shows your current
    device based on your user agent. You can use the [`ua_parser`](https://hexdocs.pm/ua_parser/readme.html)
    library to do the weird stuff.\n\n```elixir\ndefmodule GetUserAgentWeb.UserAgentLive
    do\n  use GetUserAgentWeb, :live_view\n\n  def render(assigns) do\n    ~H\"\"\"\n
    \   <div class=\"text-xl\">Device: <%= @device %></div>\n    \"\"\"\n  end\n\n
    \ def mount(_params, _session, socket) do\n    device = \n      case UAParser.parse(get_connect_info(socket,
    :user_agent)) do\n        %UAParser.UA{device: %UAParser.Device{family: fam}}
    -> fam\n        _ -> \"I don't know your device\"\n      end\n\n    {:ok, assign(socket,
    device: device)}\n  end\nend\n```\n\nBut there&#39;s a catch!\n\n<%= video_tag
    \"device-appearing-and-disappearing.mp4?card&border\", title: \"LiveView page
    being refreshed, slowed down 50%. At the initial frame the 'Device: Mac' message
    appears and then 'Mac' disappears leaving only 'Device:'\" %>\n\nWhy your device
    appears and disappears immediately, you ask? Let&#39;s go through how LiveViews
    render.\n\n![A sequence of 5 states connected by arrows: the first state is your
    user opening the page doing a HTTP request, the second state is LiveView rendering
    a disconnected mount, the third state is the user seeing the app and requesting
    to be connected to the WebSocket, the fourth state is LiveView rendering a connected
    version and replying to the user, and the fifth and final state is the user having
    a dynamic web page.](./how-liveview-render-works.png)\n\n<%= partial \"shared/posts/cta\",
    locals: {\n  title: \"Run your LiveViews on Fly.io\",\n  text: \"You can host
    your LiveView form apps here on Fly.io and get free SSL so users know only your
    server knows their User Agent.\",\n  link_url: \"https://fly.io/docs/speedrun/\",\n
    \ link_text: \"Try Fly for free&nbsp;&nbsp;<span class='opacity-50'>&rarr;</span>\"\n}
    %>\n\nUser agents come as HTTP headers so when LiveView first receive an HTTP
    request that information is already available, the server will reply with a simple
    HTML response and the device will be rendered. As for WebSockets, they work like
    this: your browser does a simple HTTP request to a server but the server replies
    telling that this request will be [upgraded to an WebSocket](https://en.wikipedia.org/wiki/WebSocket?useskin=vector#WebSocket_protocol_handshake).\n\nDuring
    the socket upgrade you lose your User Agent—The WebSocket connection doesn’t carry
    this info—so this second render clears the device value from the screen. You need
    to tell Phoenix to store it under your socket state so you can use it later. Go
    to your `endpoint.ex` file and let&#39;s edit it a little bit:\n\n```elixir\n-
    socket \"/live\", Phoenix.LiveView.Socket, websocket: [connect_info: [session:
    @session_options]]\n+ socket \"/live\", Phoenix.LiveView.Socket, websocket: [connect_info:
    [:user_agent, session: @session_options]]\n```\n\nYou just added `:user_agent`
    to the [WebSocket config](https://hexdocs.pm/phoenix/Phoenix.Endpoint.html#socket/3)
    and now everything should just work! Don&#39;t forget to restart your server;
    `endpoint.ex` doesn&#39;t get reloaded automatically.\n"
- :id: phoenix-files-making_a_checkboxgroup_input
  :date: '2023-02-13'
  :category: phoenix-files
  :title: Making a CheckboxGroup Input
  :author: mark
  :thumbnail: checkbox-thumbnail.jpg
  :alt: Illustration with a pencil checking multiple selection boxes.
  :link: phoenix-files/making_a_checkboxgroup_input
  :path: phoenix-files/2023-02-13
  :body: |2-


    <p class="lead">This post builds on the previous work with [tagging database records](/phoenix-files/tag-all-the-things/). Here we build a custom multi-select checkbox group input for selecting which tags to associate with a database record in our Phoenix application. Fly.io is a great place to run Phoenix applications! Check out how to [get started](/docs/elixir/)!</p>

    **UPDATED**: This was updated to support clearing a list of tags. The [underlying tag functions](/phoenix-files/tag-all-the-things) were updated and a hidden input ensures a value is passed. See the updated [gist](https://gist.github.com/brainlid/9dcf78386e68ca03d279ae4a9c8c2373) as well.

    Phoenix 1.7.0 brings a lot of new things when we run `mix phx.gen my_app`. These new and cool ways of doing things aren't automatically brought to existing projects because they come from the generators. This means you don't _need_ to adopt any of these new approaches. After all, Phoenix 1.7 is backward compatible!

    However, of the many new things we _could_ bring to an existing project, we'll focus here on the new approach to form input components. Why? Because it's both cool and useful!

    Earlier we saw how we can [play with new Phoenix features](/phoenix-files/flying-with-a-fledgling-phoenix/). If you've played with the 1.7.0 release, you may have noticed the slick new `core_components.ex` approach. This file is generated for new projects. Here we'll explore how to build a custom input component that follows this design.

    ## Problem

    We like the new `core_components.ex` approach that fresh Phoenix apps get. The file is generated into a project and is intended to be customized.

    The first step towards customizing the file is to change the CSS classes to match the look and feel of our application. By default, it uses Tailwind CSS but those can be replaced or customized as we see fit.

    The next step is to create custom components that are useful in our application.

    How do we create a custom input in `core_component.ex`? It's actually easy. The component we want is a multi-select checkbox group input. It's perfect for a "check all that apply" or when you have a list of tags that people can choose from.

    In our application, a book can be tagged with the genres that apply to it. The input should look something like this:

    ![Screenshot of a multi-select checkbox group input with book genre names.](./checkbox-group-appearance.png?card&center)

    Ideally, we want the this input to behave like a normal HTML input linked to a Phoenix form and changeset. The question is, how do we create a custom multi-select checkbox group input using the new `core_components.ex` design?

    ## Solution

    Before we dive headlong into creating our new component, let's do some reconnaissance and get the "lay of the land".

    ### Our first peek at `core_components.ex`

    When we generate a new Phoenix 1.7.x project, it creates a `core_components.ex` file for us. For those who haven't checked it out yet, it contains a number of components we can use and extend, but here we'll focus on the `input` function.

    The `input` function has multiple versions that use pattern matching to determine which function body is executed.

    Here's a simplified view:

    ```elixir
      def input(%{type: "checkbox"} = assigns) do
        # ...
      end

      def input(%{type: "select"} = assigns) do
        # ...
      end

      def input(%{type: "textarea"} = assigns) do
        # ...
      end

      # ...
    ```

    A pattern match on the `type` assign signals what type of component to render. Nice! This makes it easy for us to add a custom type!

    Using the component in a HEEx template looks like this:

    ```html
    <.input field={{f, :title}} type="text" label="Title" />
    ```

    ### Our multi-select checkbox group's data

    Previously we [talked about the underlying database structure](/phoenix-files/tag-all-the-things/#database-migration) and the GIN index that makes it all speedy. Let's review briefly what the Ecto schema looks like, since the input is sending data for the `genres` field.

    Our `book.ex` schema:

    ```elixir
    schema "books" do
      field :title, :string, required: true
      # ...
      field :genres, {:array, :string}, default: [], required: true
      # ...
    end
    ```

    We're taking advantage of Ecto's support for fields of type [array of string](https://hexdocs.pm/ecto/Ecto.Type.html#cast/2). Time to put that feature to use!

    ### New input type

    Our new input type needs a name. It displays a group of checkboxes for the possible list of tags to apply. So, let's call our new input type a `checkgroup`.

    Building on the existing design, let's add our new input with a pattern match on the new type. Be sure to group this with all the other `input/1` functions. It will look like this:

    ```elixir
    def input(%{type: "checkgroup"} = assigns) do
      # ...
    end
    ```

    Using our component for book genres in a `HEEx` template will look like this:

    ```html
    <.input
      field={{f, :genres}}
      type="checkgroup"
      label="Genres"
      multiple={true}
      options={Book.genre_options()}
    />
    ```

    This is the first time we're looking at the input's usage. There are a few points to note.

    - The new way of specifying the field is a tuple of `{form, :field_name}`. This is how the new `input` components work in `core_components.ex`.
    - There is an option called `multiple` that must be set to `true`. More on this in a second.
    - `options` provides a list of the possible tags/genres to display. Inputs of type `"select"` already support options that conform to `Phoenix.HTML.Form.options_for_select/2`.

    ### Multiple?

    The `multiple={true}` attribute is really important. As I started using the new input component, I kept forgetting to include the `multiple={true}` setting. What happened? It didn't error, but it only sent one checked value for the form. So… it was quietly broken. Why?

    In general, in HTML, if we create multiple checkboxes, each with the same name of `name="genres[]"` (note the square brackets!), then Phoenix interprets the set of values as an array of strings for the checked values. This is exactly what we want!

    When we neglect to include the option, it doesn't add the `[]` to the input name for us and results in an easy-to-create bug.

    The `multiple` option is processed in the default generated `input/1` function, so we can't access it and use it in our pattern matched `input(%{type: "checkgroup"})` function.

    What to do?

    Because this setting is so critical and we don't _ever_ want to forget it, let's write a function wrapper to use instead.

    ```elixir
      @doc """
      Generate a checkbox group for multi-select.
      """
      attr :id, :any
      attr :name, :any
      attr :label, :string, default: nil
      attr :field, :any, doc: "a %Phoenix.HTML.Form{}/field name tuple, for example: {f, :email}"
      attr :errors, :list
      attr :required, :boolean, default: false
      attr :options, :list, doc: "the options to pass to Phoenix.HTML.Form.options_for_select/2"
      attr :rest, :global, include: ~w(disabled form readonly)
      attr :class, :string, default: nil

      def checkgroup(assigns) do
        new_assigns =
          assigns
          |> assign(:multiple, true)
          |> assign(:type, "checkgroup")

        input(new_assigns)
      end
    ```

    The bulk of this simple function wrapper is defining the arguments, all of which were borrowed and customized from the existing `input/1` function. All the function does is explicitly set `multiple` to `true` so we _can't_ forget it and we set the `type` since the function name makes the purpose clear.

    Now we can use our component like this in our templates:

    ```html
    <.checkgroup field={{f, :genres}} label="Genres" options={Book.genre_options()} />
    ```

    Looking good!

    Next, let's think about how our list of displayed options works.

    ### Options

    We need to decide how our list of genre options should appear. Do we want to show the stored value or do we want a "friendly" display version shown? It might be the difference between displaying "Science Fiction" versus "sci-fi". The "right" choice depends on our application, the tags, and how they are used.

    For our solution, we'd prefer to see the friendly text of "Science Fiction" displayed but store the tag value of "sci-fi".

    Because we are building a custom component, we could structure this any way we want. For consistency, we'll borrow the same structure used for select inputs and do it like this:

    ```elixir
    @genre_options [
      {"Fantasy", "fantasy"},
      {"Science Fiction", "sci-fi"},
      {"Dystopian", "dystopian"},
      {"Adventure", "adventure"},
      {"Romance", "romance"},
      {"Detective & Mystery", "mystery"},
      {"Horror", "horror"},
      {"Thriller", "thriller"},
      {"Historical Fiction", "historical-fiction"},
      {"Young Adult (YA)", "young-adult"},
      {"Children's Fiction", "children-fiction"},
      {"Memoir & Autobiography", "autobiography"},
      {"Biography", "biography"},
      {"Cooking", "cooking"},
      # ...
    ]
    ```

    Because our list of allowed tags is defined in code, it makes sense to define it with our schema; after all, we will use the values in our validations.

    With the above structure, our validations can't use the data in `@genre_options` directly. Our validation needs a list of just the valid values. To address this, we can write the following line of code to compute the list of valid values at compile time.

    ```elixir
    @valid_genres Enum.map(@genre_options, fn({_text, val}) -> val end)
    ```

    The above code essentially turns into the following:

    ```elixir
    @valid_genres ["fantasy", "sci-fi", "dystopian", "adventure", ...]
    ```

    A benefit of using a function at compile time is we don't have to remember to keep the two lists in sync and it only runs the function once when compiling.

    Then, in our changeset, we can use `Ecto.Changeset.validate_subset/4` like this:

    ```elixir
    changeset
    # ...
    |> validate_subset(:genres, @valid_genres)
    # ...
    ```

    Nice! We can display friendly values to the user but we store and validate using the internal tag values.

    ### Options in the template

    Our template can't access the internal module attribute `@genre_options`. If we recall back to the HEEx template and how our component will be used, it calls `Book.genre_options/0`. Template usage looks like this:

    ```html
    <.checkgroup field={{f, :genres}} label="Genres" options={Book.genre_options()} />
    ```

    Our template needs a public function to call that returns our options for display. Fortunately, this single line of code is all that's needed:

    ```elixir
    def genre_options, do: @genre_options
    ```

    With that, our schema is set to supply the component with everything needed. Let's take a look at the final version of component!

    ## Component

    Here's the [full source](https://gist.github.com/brainlid/9dcf78386e68ca03d279ae4a9c8c2373) for our new "checkgroup" input component. We'll go over some of the interesting bits next. (NOTE: the Tailwind classes are truncated here.)

    ```elixir
    defmodule MyAppWeb.CoreComponents do
      use Phoenix.Component

      # ...
      def input(%{type: "checkgroup"} = assigns) do
        ~H"""
        <div phx-feedback-for={@name} class="text-sm">
          <.label for={@id} required={@required}><%%= @label %></.label>
          <div class="mt-1 w-full bg-white border border-gray-300 ...">
            <div class="grid grid-cols-1 gap-1 text-sm items-baseline">
              <input type="hidden" name={@name} value="" />
              <div class="..." :for={{label, value} <- @options}>
                <label
                  for={"#{@name}-#{value}"} class="...">
                  <input
                    type="checkbox"
                    id={"#{@name}-#{value}"}
                    name={@name}
                    value={value}
                    checked={value in @value}
                    class="mr-2 h-4 w-4 rounded ..."
                    {@rest}
                  />
                  <%%= label %>
                </label>
              </div>
            </div>
          </div>
          <.error :for={msg <- @errors}><%%= msg %></.error>
        </div>
        """
      end
      # ...
    end
    ```

    Here are some points to note:

    - The function declaration includes the pattern match for `type: "checkgroup"`.
    - A hidden input is included with the value of `""`. This ensures we can clear all the checked tags and the browser still has a value to submit to the server.
    - The code `<div … :for={{label, value} <- @options}>` expects our options to be tuples in the format of `{label, value}`. If using a different options structure, this is where it matters.
    - We render a "checkbox" input for every option. The label is displayed and the value is what is stored.
    - The `@name` is changed previously by the existing `input` function that adapts it to end with the `[]` when we pass `multiple={true}` to the component. This is important for correctly submitting the values back to the server.

    ### Wrapping up the changes

    We get a warning from the `input` component that the `type` isn't valid. The last thing to do before we are done with the component is update the `attr :type` declaration on the generated `input/1` function. We want to add our `checkgroup` type to the list of valid values. NOTE: The `checkgroup` entry was added to the end of the `values` list.

    ```elixir
    attr :type, :string,
      default: "text",
      values: ~w(checkbox color date datetime-local email file hidden
                 month number password range radio search select tel
                 text textarea time url week csv checkgroup)
    ```

    That's it! Let's see how it looks and behaves.

    (For a complete view of the code, please refer to [this Gist](https://gist.github.com/brainlid/9dcf78386e68ca03d279ae4a9c8c2373).)

    ### Our Component in Action

    This is what it looks like in action:

    ![Screenshot of a multi-select checkbox group input with selected book.](./checkbox-group-appearance-checks.png?card&center)

    Alright! That's what we want!

    The big question now is, "What happens when the form data is submitted to the server?"

    ## Submitting the Values

    Our component does the work of creating a correctly configured group of many checkbox inputs. When the form is validated or submitted and the selected genres pictured previously are sent, in Phoenix we receive this in our params:

    ```elixir
    %{"book" => %{"genres" => ["sci-fi", "dystopian", "romance"]}}
    ```

    If we [recall](/phoenix-files/tag-all-the-things/), our schema is setup to handle data formatted this way perfectly! There is nothing left for us to do! The schema casts the `genres` list of strings and validates it. It works just as expected!

    <%= partial "shared/posts/cta", locals: {
      title: "Fly.io ❤️ Elixir",
      text: "Fly.io is a great way to run your Phoenix app close to your users. It's really easy to get started. You can be running in minutes.",
      link_url: "https://fly.io/docs/elixir/",
      link_text: "Deploy a Phoenix app today!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>"
    } %>

    ## Discussion

    We saw first hand how easy it is to tweak the generated `core_components.ex` file. Beyond customizing the classes for our application, it is easy to even create new input components!

    There are a few other points that came out during this little project.

    - The realization of how easy it was. We added a new input type with a single 25 line function where almost all of it is markup.
    - Our new input integrates smoothly with standard forms and changesets.
    - The previous approach of using [Phoenix.HTML.Form.checkbox/3](https://hexdocs.pm/phoenix_html/Phoenix.HTML.Form.html#checkbox/3) isn't used anymore in the new `core_components.ex` approach. The new approach, particularly around `input`, goes back to more pure HTML.
    - Creating a simple pre-configured wrapper component like `<.checkgroup ...>` ensures we won't forget important settings like the `multiple={true}` attribute.

    In the end, I'm pleased with how well our custom input works with both LiveView and non-LiveView templates.

    This completes our UI for [adding tag support](/phoenix-files/tag-all-the-things/) to a Phoenix application. Go grab the code from [this gist](https://gist.github.com/brainlid/9dcf78386e68ca03d279ae4a9c8c2373) and I hope you enjoy customizing your `core_components.ex`!

    And tag on!
- :id: laravel-bytes-highly-available-postgres-with-laravel
  :date: '2023-02-13'
  :category: laravel-bytes
  :title: Highly Available Postgres with Laravel
  :author: fideloper
  :thumbnail: php-with-pgsql-thumbnail.png
  :alt: Using postgresql with laravel
  :link: laravel-bytes/highly-available-postgres-with-laravel
  :path: laravel-bytes/2023-02-13
  :body: "\n\n<p class=\"lead\">Fly can run your PHP anywhere around the world in
    a few commands. But you have taste! Combine Laravel with the subtle undertones
    of PostgreSQL and [run a pgsql-backed Laravel app](https://fly.io/docs/laravel/)
    in minutes!</a>\n\nPHP and MySQL go together like peas and carrots. This is because
    they both let you get away with a lot of crap, like pretending numbers and strings
    are interchangeable. \n\nHowever, some people (within the PHP world) are rebels
    against the mainstream. These people choose PostgreSQL - the database that's so
    hip, no one even knows how to pronounce it.\n\nFor those who are PG-curious, let's
    see how to setup Laravel with a PostgreSQL cluster on Fly.io.\n\n## A Laravel
    App\n\nTo start, we'll create a new Laravel application. As I usually do, I'll
    also include [Breeze](https://laravel.com/docs/9.x/starter-kits) to quickly scaffold
    a working authentication system.\n\nSince this authentication system involves
    adding users to a database, that'll give us what we need for a useful demonstration.\n\n```bash\n#
    Create an app\ncomposer create-project laravel/laravel pgsql\ncd pgsql\ncomposer
    require laravel/breeze --dev\n\n# Install Breeze\n# It's interactive: I chose
    blade, no dark mode, and pest\nphp artisan breeze:install\n\n# Infect our app
    with javascript\nnpm install\nnpm run build\n\n# Test it out\nphp artisan serve\n```\n\nI
    didn't bother setting up a database just yet, so attempting to register a user
    will fail for now. That's what PostgreSQL is for!\n\n## Launch Your App on Fly.io\n\nLet's
    run our app on Fly.io. Again, no database will be available yet.\n\n```bash\n#
    From folder pgsql, where our app is\nfly launch\n\n# Choose your options...then
    deploy\n```\n\n## PostgreSQL on Fly\n\nFly PostgreSQL is selling cars. Fly.io
    builds it, sells it, and gives you the keys. There's a maintenance plan (backups),
    but you need to know how to drive and maintain it yourself.\nWhat does that mean?
    Well, the first page you see in the docs is labeled [This Is Not Managed PostgreSQL](https://fly.io/docs/postgres/getting-started/what-you-should-know/).\nCheck
    out the details on what is and is not done for you.\n\nThe [first paragraph here](/docs/postgres/getting-started/what-you-should-know/)
    mentions that we use [Stolon](https://github.com/sorintlab/stolon) to cluster
    together multiple PostgreSQL instances.\n\nThis is nice because you get Highly
    Available PostgreSQL, without having to setup this stuff yourself.\n\nStolon has
    a few things going on:\n\n1. A Proxy to ensure you're talking to the Primary database
    for writes\n2. A Keeper to run alongside PostgreSQL and share data about that
    instance\n3. A Sentinel to register and track the PostgreSQL instances in the
    cluster\n\nAs a user of Fly PostgreSQL, you just need to decide how many replicas
    you want, and then use the hostname provided to connect to your PostgreSQL instance.\n\n##
    Setup a PostgreSQL Cluster\n\nLet's create a Postgres database in Fly.io, and
    then connect to it.\n\n```bash\n# I named it \"larapg\"\n# I used the same region
    as my app\n# I selected a production config (primary / replica created, in one
    region)\nfly pg create\n```\n\nThe \"production\" options create 2 pgsql instances
    and sets up replication between them (within one region). They'll auto-fail over
    if something goes wrong.\n\nThese 2 instances have 80gb volumes attached to them
    by default.\n\n## Check Your PostgreSQL\n\nThe easiest way to poke around your
    new database instance is to run:\n\n```bash\n# Start a psql shell\nfly pg connect
    -a larapg\n```\n\nYou can run all your favorite psql commands, like `\\l` to list
    databases or `\\c` to connect to a database. \n\n**Note that no databases were
    created for you just yet!**\n\n## Attach a Database\n\nLet's create a database
    for our application to use.\n\nThere's a handy command to [\"attach\" a Fly.io
    app](/docs/postgres/managing/attach-detach/) to your Postres database. The `fly
    pg attach` command will:\n\n1. Create a new database and user\n2. Set a handy
    `DATABASE_URL` environment variable in your attached Fly app\n\n```bash\n# Attach
    databse \"larapg\" to our \n# Laravel app named \"lara-psql-example\"\nfly pg
    attach larapg --app lara-psql-example\n```\n\n![attach a pgsql database to your
    app](pgsql-1.png)\n\nIf we head into our app, we can see the `DATABASE_URL` environment
    variable is present!\n\n![get your username/password](pgsql-2.png)\n\n```bash\nDATABASE_URL=postgres://lara_psql_example:some_password@top2.nearest.of.larapg.internal:5432/lara_psql_example?sslmode=disable\n```\n\n<div
    class=\"callout\">Note the hostname is `top2.nearest.of.<app-name>.internal`.
    This fancy hostname will retrieve the 2 Postgres instances closest to your application
    VM. That's a really cool feature!</div>\n\n<%= partial \"shared/posts/cta\", locals:
    {\n  title: \"Fly.io ❤️ Laravel\",\n  text: \"You have enough taste to like PostgreSQL
    with your PHP, you may as well run it (easily!) on Fly.io.\",\n  link_url: \"https://fly.io/docs/laravel\",\n
    \ link_text: \"Deploy your Laravel app!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>\",\n}
    %>\n\n## Use the Database in Laravel\n\nWe next need to configure our Laravel
    application to talk to this database.\n\nLaravel happens to listen for a `DATABASE_URL`
    environment variable. In fact, if you just start using your Laravel application,
    you'll see it already connects to Postgres:\n\n![cannot connect to your pgsql
    database yet](pgsql-3.png)\n\nLaravel correctly generates a Postgres connection,
    even though the default connection is `mysql` (peas and carrots). As a result,
    Laravel attempts to set the encoding to something MySQL specific - good old `utf8mb4`.\nPostgreSQL
    has always supported 4-byte UTF8 (otherwise known as UTF8), and doesn't understand
    the `mb4` part. To fix that, we just need to set Laravel's default connection
    to `pgsql`.\n\nUpdate `fly.toml` to do so:\n\n```toml\n[env]\n  APP_ENV = \"production\"\n
    \ LOG_CHANNEL = \"stderr\"\n  LOG_LEVEL = \"info\"\n  LOG_STDERR_FORMATTER = \"Monolog\\\\Formatter\\\\JsonFormatter\"\n
    \ DB_CONNECTION=\"pgsql\"\n```\n\nWe just added `DB_CONNECTION` there! Once we
    `fly deploy`, our connection to Postgres will start to work.\n\nWhile we're here,
    lets setup some migrations. We could use a [`release_command`](/docs/reference/configuration/#the-deploy-section).
    However, I'm going to go straight to a Laravel-specific [startup script](/docs/laravel/the-basics/customizing-deployments/).\nWe'll
    create a script to run `artisan migrate` anytime we spin up our application in
    Fly.io (during deploys, etc).\n\nCreate file `.fly/scripts/migrate.sh` and add:\n\n```bash\n#!/usr/bin/env
    bash\n\n/usr/bin/php /var/www/html/artisan migrate --force\n```\n\nOnce that's
    created, deploy the app and we'll see if it works!\n\n```bash\nfly deploy\n```\n\nOur
    logs will show that the migrations were run: https://d.pr/i/S1z2DL\n\nWe're able
    to register users now! Our data is in the database: https://d.pr/i/jEPKKW\n\nVoilà,
    we now have a HA Postgres cluster to use with our Laravel applications!\n\n##
    What Else?\n\nBe sure to review the [Postgres docs](/docs/postgres/) to see all
    the fancy things you can do (backup, scale, update, multi-region support, etc).\n\nMy
    personal favorite things are [adding a TimescaleDB plugin](/docs/postgres/managing/enabling-timescale/)
    for time series data, and figuring out [read-only replicas](/docs/postgres/advanced-guides/high-availability-and-global-replication/).\n"
- :id: laravel-bytes-chunked-file-upload-livewire
  :date: '2023-02-08'
  :category: laravel-bytes
  :title: Chunked File Upload with Livewire
  :author: kathryn
  :thumbnail: slicing-chunks-thumbnail.jpg
  :alt: A close up on a chopping board. Three carrots, and three fruits are located
    on the left space near the board, and three potatoes on its right. Human hands
    are shown using a knife to cut cubes out of a presumably potato ingredient on
    top of the chopping board.
  :link: laravel-bytes/chunked-file-upload-livewire
  :path: laravel-bytes/2023-02-08
  :body: "\n<p class=\"lead\">Today we upload a file in chunks using Livewire. Upload
    your files close to your users with Fly.io, you can get your [Laravel app running](/docs/laravel/)
    in minutes!</p>\nServers are configured to limit the size of requests they can
    accept. This is done to avoid long processing times, resulting unavailability,
    and potential security risks that come with processing large requests in one go.
    \n\nWhat happens when a user requests to upload a file exceeding the configured
    limits? Aptly, the upload will fail, either with a custom message we write or
    the default [413 status code error](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/413)
    returned by our server. \n\nWe are faced with a predicament dealt to developers
    before us, and will be dealt to developers long after ourselves: processing large
    file uploads.\n\n## The Problem  \n\nAn obvious approach to take is to update
    our configuration limits. We can increase several configuration limitations in
    our server itself, and PHP's. \n\nThe problem is, this is not quite dynamic. File
    size uploads can increase overtime, and that leaves us to re-configure our limitations.
    And with increased limitations, come increased request processing times as well.
    \n \nIsn't there a way we can avoid this perpetual re-configuration and increased
    request processing time?\n\n## The Solution\n\nThe solution isn't always about
    increasing limitations, sometimes simply adjusting an existing approach is all
    it takes to save the day. Instead of sending the whole file, why not send it in
    batches?\n\nThat's right! Today, we will not alter our configuration under the
    pressure of evolving limitations. Instead, we will resolve this without change
    to our configuration.\n\nToday we'll slice, dice, and merge file chunks&mdash;with
    [Livewire](https://laravel-livewire.com/)!\n\n### The Plan\nWe have a three-step
    plan for slicing and merging:\n1. We'll first let Livewire know the expected total
    `$fileSize` to receive from all the chunks merged.\n2. Then we start slicing,
    uploading, and merging chunks into a \"final file\" in our server one after another
    using Livewire.\n3. Once our final file's size reaches the given `$fileSize`,
    this means all chunks have been merged. Therefore we feed the final file to Livewire's
    [TemporaryUploadedFile](https://github.com/livewire/livewire/blob/master/src/WithFileUploads.php#L37)
    class in order to utilize Livewire's uploaded file [features](https://laravel-livewire.com/docs/2.x/file-uploads).\n\nFor
    piecing together everything here, you can visit our [repository's readme](https://github.com/KTanAug21/fly.io-livewire-snippets/blob/master/_readme/chunked_file_upload_livewire.md)
    and inspect the relevant files.\n\n### The View\nLet's start with [creating a
    Livewire Component](https://laravel-livewire.com/docs/2.x/making-components) by
    running the command `php artisan make:livewire chunked-file-upload`. Afterwards,
    update our [Livewire view](https://github.com/KTanAug21/fly.io-livewire-snippets/blob/master/resources/views/livewire/chunked-file-upload.blade.php)
    to include a form tag enclosing both a file input element and a button for submission.\n```php\n<form
    wire:submit.prevent=\"submit\">\n  <input type=\"file\" id=\"myFile\"/>\n  <button
    type=\"button\" onClick=\"uploadChunks()\">Submit</button>\n</form>\n```\nEvery
    time the user clicks on the submit button, our custom JavaScript function `uploadChunks()`
    will slice a selected file into chunks and request Livewire to upload each chunk.\n\n###
    Sharing Expectations\n\nIn order to upload a large file, we'll be slicing it into
    smaller chunks that are within our server's request size limits. We'll upload
    each chunk one after another so that we can immediately merge an uploaded chunk
    into a \"final file\".\n\nBut, how exactly will the server know all the chunks
    have been merged to our final file? It'll need to know the expected final size
    of our final file of course! \n\n[Livewire properties](https://laravel-livewire.com/docs/2.x/properties)
    are perfect for sharing information from client to server, so let's include information
    about our file like its `$fileName` and `$fileSize` as public attributes in our
    [Livewire component](https://github.com/KTanAug21/fly.io-livewire-snippets/blob/master/app/Http/Livewire/ChunkedFileUpload.php).\nToday,
    we'll separate our file into 1MB chunks, so let's declare a separate attribute
    for the chunk uploaded `$fileChunk` and the expected maximum chunk size `$chunkSize`:\n```php\n//
    app/Http/Livewire/ChunkedFileUpload.php\npublic $chunkSize = 1000000; // 1 MB\npublic
    $fileChunk; \n\npublic $fileName;\npublic $fileSize; \n```\n\nLet's go back to
    our [Livewire view](https://github.com/KTanAug21/fly.io-livewire-snippets/blob/master/resources/views/livewire/chunked-file-upload.blade.php)
    and revise the `uploadChunks()` function triggered by our submit button. Every
    time the user submits a file for upload, we'll set values for `$fileName` and
    `$fileSize` to be later sent to our Livewire component:\n\n<aside class=\"right-sidenote\">\nNotice
    we're using Livewire's set method here. This let's us set a public attribute in
    our client, but not make an immediate call to the server. \n\nThe changes to `$fileName`
    and `$fileSize` will be sent to Livewire in the next immediate component request.\n</aside>\n```javascript\n//
    resources/views/livewire/chunked-file-upload.blade.php\nfunction uploadChunks()\n{\n
    \   const file = document.querySelector('#myFile').files[0];\n\n    // Send the
    following later at the next available call to component\n    @this.set('fileName',
    file.name, true);\n    @this.set('fileSize', file.size, true);\n```\n\nNow that
    our final file details are ready to be shared with our Livewire component, we
    can take a slice at our first chunk, starting at the file's 0th byte:\n\n```javascript\n
    \   livewireUploadChunk( file, 0 );\n}\n```\n\n### Slicing A Chunk\nHow do we
    slice a chunk out of our file? \n  \nWell, we'll need to know where the chunk
    starts, and where it ends. For the first chunk of our file, the starting point
    is a given: 0. But how about where the chunk ends?\n\nThe end of a chunk is always
    going to be 1MB( our `$chunkSize` ) from the starting point of the chunk, or the
    file size&mdash;whichever is smaller between the two:\n```javascript\n// resources/views/livewire/chunked-file-upload.blade.php\nfunction
    livewireUploadChunk( file, start ){\n    const chunkEnd  = Math.min( start + @js($chunkSize),
    file.size );\n    const chunk     = file.slice( start, chunkEnd ); \n```\n\nNow
    that we have our chunk, we'll have to send it up to our server. We can use Livewire's
    [upload](https://laravel-livewire.com/docs/2.x/file-uploads#js-api) JavaScript
    function to upload and associate the chunk with our `$fileChunk` attribute declared
    above:\n```javascript\n    @this.upload('fileChunk', chunk);\n```\n\nAfter uploading
    the first chunk, let's also send out the next. We'll need to make sure the current
    chunk is completely uploaded though, for this, we can hook onto the upload function's
    event progress callback:\n\n```javascript\n-    @this.upload('fileChunk', chunk);\n+
    \   @this.upload('fileChunk', chunk,(uName)=>{}, ()=>{}, (event)=>{\n+        if(
    event.detail.progress == 100 ){\n+          // We recursively call livewireUploadChunk
    from within itself\n+          start = chunkEnd;\n+          if( start < file.size
    ){\n+            livewireUploadChunk( file, start );\n+          }\n+        }\n+
    \   });\n}\n```\nThe upload is completed when the value of `event.detail.progress`
    reaches 100. Once it does, we recursively call the current function `livewireUploadChunk()`
    in order to upload our next chunk.\n\nThe `file.slice` method's range is [exclusive](https://developer.mozilla.org/en-US/docs/Web/API/Blob/slice#:~:text=contains%20no%20data.-,end,-Optional)
    of the the `chunkEnd`. For example, the range of slice(0,10) actually means 0
    until 9, but not 10! This means our next starting point will be `chunkEnd`.\n\n<%=
    partial \"shared/posts/cta\", locals: {\n  title: \"Fly.io ❤️ Laravel\",\n  text:
    \"Fly your servers close to your users&mdash;and marvel at the speed of close
    proximity. Deploy globally on Fly in minutes!\",\n  link_url: \"https://fly.io/docs/laravel\",\n
    \ link_text: \"Deploy your Laravel app!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>\",\n}
    %>\n\n\n### Saving and Merging \nNow that our [Livewire view's](https://github.com/KTanAug21/fly.io-livewire-snippets/blob/master/resources/views/livewire/chunked-file-upload.blade.php)
    JavaScript is set up for slicing and uploading chunks, we've come to the final
    part of our slice and dice journey: saving and merging chunks in [our Livewire
    component](https://github.com/KTanAug21/fly.io-livewire-snippets/blob/master/app/Http/Livewire/ChunkedFileUpload.php#L36)!\n\nWe'll
    be using Livewire's [WithFileUploads](https://laravel-livewire.com/docs/2.x/file-uploads#basic-upload)
    trait to make our file upload a breeze. This trait allows us to declare an attribute
    that's uploadable&mdash;`$fileChunk` in our case!\n```php\n// app/Http/Livewire/ChunkedFileUpload.php\n\n+
    use WithFileUploads;\n\n// Chunks info\npublic $chunkSize = 1000000; // 1M\npublic
    $fileChunk;\n\n// Final file \npublic $fileName;\npublic $fileSize;\n\n+ public
    $finalFile;\n```\n\nOnce a chunk has been uploaded, Livewire must merge it into
    a \"final file\". In order to do this, we'll have to intercept Livewire's flow
    after our chunk has been uploaded. \n\nLuckily for us, Livewire provides \"[hooks](https://laravel-livewire.com/docs/2.x/lifecycle-hooks#:~:text=updatingArray(%24value%2C%20%24key)-,updatedFoo,-Runs%20after%20a)\"
    we can use to intercept Livewire's lifecycle flow for our public attributes. In
    our specific case, we can hook on to the `updated` hook for our `$fileChunk` attribute.\n\nFrom
    our `updatedFileChunk` [hook](https://github.com/KTanAug21/fly.io-livewire-snippets/blob/master/app/Http/Livewire/ChunkedFileUpload.php#L36)
    we'll retrieve the file name generated by Livewire for a current chunk using the
    `getFileName()` method:\n```php\npublic function updatedFileChunk()\n{\n    $chunkFileName
    = $this->fileChunk->getFileName();\n```\n\nThen we'll merge this chunk into our
    final file, and delete the chunk once merged:\n```php\n      $finalPath = Storage::path('/livewire-tmp/'.$this->fileName);\n
    \     $tmpPath   = Storage::path('/livewire-tmp/'.$chunkFileName);\n      $file
    = fopen($tmpPath, 'rb');\n      $buff = fread($file, $this->chunkSize);\n      fclose($file);\n\n
    \     $final = fopen($finalPath, 'ab');\n      fwrite($final, $buff);\n      fclose($final);\n
    \     unlink($tmpPath);\n```\nEventually all chunks will arrive one by one and
    get merged into our final file. To determine whether all chunks have been merged,
    we simply compare the final file's size with the expected `$fileSize`.\n\nOf course,
    this newly generated file is our custom file. We'll need to enclose it in Livewire's
    [TemporaryUploadedFile](https://github.com/livewire/livewire/blob/master/src/WithFileUploads.php#L37)
    class in order to utilize Livewire's uploaded file [features](https://laravel-livewire.com/docs/2.x/file-uploads).\n<aside
    class=\"right-sidenote\">\nDon't forget to import the [TemporaryUploadedFile](https://github.com/livewire/livewire/blob/master/src/WithFileUploads.php#L37)
    class, and declare a new public attribute `$finalFile`!\n</aside>\n```php\n      $curSize
    = Storage::size('/livewire-tmp/'.$this->fileName);\n      if( $curSize == $this->fileSize
    ){\n          $this->finalFile = \n          TemporaryUploadedFile::createFromLivewire('/'.$this->fileName);\n
    \     }\n}\n```\nSay for example, [previewing](https://laravel-livewire.com/docs/2.x/file-uploads#preview-urls)
    a new, temporary image in our view like so:\n```php\n@if ($finalFile)\n    Photo
    Preview:\n    <img src=\"{{ $finalFile->temporaryUrl() }}\">\n@endif\n``` \n\nImplementations
    are generally just so much more smooth with Livewire, uploading file chunks is
    no different!\n\n"
- :id: phoenix-files-customizable_classes_lv_component
  :date: '2023-02-08'
  :category: phoenix-files
  :title: Custom styling with LiveView function component attributes
  :author: berenice
  :thumbnail: custom-classes-thumbnail.jpg
  :alt:
  :link: phoenix-files/customizable_classes_lv_component
  :path: phoenix-files/2023-02-08
  :body: |2


    <p class="lead">In this post we use function component attributes to add new CSS classes to our component's default classes. Fly.io is a great place to run your Phoenix LiveView applications! Check out how to [get started](/docs/elixir/)!</p>

    ## Problem

    You're writing a new function component that has default styling. You've defined default CSS classes in the component&#39;s template, but you want it to have the flexibility to add a few others when calling the function.

    How can we give a component some default CSS classes and still customize it to add our own for custom styling?

    ## Solution

    In LiveView 0.18 [function component attributes](https://hexdocs.pm/phoenix_live_view/Phoenix.Component.html#module-attributes) were added to define the attributes that the function component expects to receive —either required or not. We can define the type of the attributes, and even default values if we want to.

    Among the different types that we can use to define the component&#39;s attributes, there are [global attributes](https://hexdocs.pm/phoenix_live_view/Phoenix.Component.html#module-global-attributes). By default, a `:global` attribute can accept a set of attributes that are common to all standard HTML tags —id, class, hidden, autocapitalize, etc— so we don&#39;t need to individually and repeatedly specify these attributes on every component we define.

    How can we use all this to customize  our function component&#39;s classes? let&#39;s see it!

    ### Attempt 1

    We&#39;re specifying an HTML attribute —class— and we want to set a default value for it. Based on what we just learned, a function component attribute of type `:global` sounds like just what we need!

    We define a `:rest` attribute as global, and we define a map with the default `:class` values. Then we interpolate the `:rest` attributes in the `<img>` tag:

    ```elixir
    attr :rest, :global, default: %{class: "rounded-full"}

    def user_logo(assigns) do
      ~H"""
        <img {@rest} />
      """
    end
    ```

    When calling the function component `:user_logo` we no longer need to specify any CSS classes, but we do need to give it the image&#39;s source:

    ```elixir
    <.user_logo src={img.src}></.user_logo>
    ```

    It would render something like this:

    ```elixir
    <img class="rounded-full" src="https://avatars...">
    ```

    But what if we want to add some other CSS classes to the function component besides the default ones?

    ```elixir
    <.user_logo src={img.src} class="w-10 h-10"></.user_logo>
    ```

    If we set our component's default class using a global attribute like that, it's just that: a default. If we specify a new class value when we call the component, the new value overrides the default.

    It would render something like this:

    ```elixir
    <img class="w-10 h-10"  src="https://avatars...">
    ```

    We can use the default CSS classes without sending anything to the component, or we can define our own from scratch. That&#39;s great!

    But we don't have any way to _add_ classes, and that&#39;s not what we&#39;re looking for!

    ### Attempt 2  - It works!

    Instead of using a structured global attribute for the default class, we can turn it around: put the desired default value into the function component template, and make our `:class` attribute a string with a default value of `nil`.

    This way we can group both classes in a list, and interpolate them inside the class attribute of our HTML tag:

    ```elixir
    attr :class, :string, default: nil
    attr :rest, :global

    def user_icon(assigns) do
      ~H"""
        <img
          class={["rounded-full", @class]}
          {@rest}
        />
      """
    end
    ```

    <aside class="right-sidenote">Here we kept the global `:rest` attribute for any other HTML attributes we may want to set when we call the function, but which we don't have default values for. In this example, to set the image's `:src` path.</aside>


    If we render the updated function component just as we did above, we keep the default classes, and the additional ones:

    ```elixir
    <img class="rounded-full w-10 h-10" src="...">
    ```

    That&#39;s all! now we can add as many classes as we want without losing the default ones.

    <%= partial "shared/posts/cta", locals: {
      title: "Fly.io ❤️ Elixir",
      text: "Fly.io is a great way to run your Phoenix LiveView app close to your users. It's really easy to get started. You can be running in minutes.",
      link_url: "https://fly.io/docs/elixir/",
      link_text: "Deploy a Phoenix app today!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>"
    } %>

    ## Discussion

    We set out to have our components start with some basic CSS styles and then possibly extend them at the point where we use them. We succeeded in making our component  flexible enough to be styled with extra classes when needed.

    We looked at two ways of doing it and ended up with one we&#39;re really happy with. Beyond just classes, we saw how we can pass arbitrary HTML attributes through to our component as well!

    We also saw how our components and their attributes allow us to describe our components. We can easily define the style, structure and functionality of our components with a clear and easy to use API. That&#39;s pretty cool!
- :id: laravel-bytes-servers-in-sync
  :date: '2023-02-06'
  :category: laravel-bytes
  :title: Keeping Global Deployments in Sync
  :author: fideloper
  :thumbnail: keeping-in-sync-thumbnail.png
  :alt: syncing data across the world
  :link: laravel-bytes/servers-in-sync
  :path: laravel-bytes/2023-02-06
  :body: "\n\n<p class=\"lead\">Fly lets you run apps around the world with just a
    few commands. Let's see how to sync events and data across all servers in a global
    deployment. [Follow along with your own app](https://fly.io/docs/laravel/)!</a>\n\nWe're
    a fan of [NATS](https://nats.io) here at Fly.io. NATS enables your server instances
    to communicate with each other with very little hassle - think PubSub, although
    there are a few different ways to push data around with NATS.\n\nThe general setup
    is to run 1 or more NATS servers. Then any number of clients (typically your code,
    perhaps running in many VMs around the world) can connect to a server instance
    and send/receive data.\n\nThis allows all of your servers to efficiently communicate.
    This really helps when you want server instances in one part of the world to know
    about things going on in instances somewhere else.\n\n## Playing Locally\n\nPart
    of any good global deployment is the ability to play with it locally. This gives
    you a feel for what's going on. Let's try it out.\n\nIf you have Docker installed,
    you can run a [NATS server](https://docs.nats.io/running-a-nats-service/nats_docker/nats-docker-tutorial)
    and play with it pretty easily. Here's a quick command to spin it up:\n\n```bash\ndocker
    run -it \\\n    -p 4222:4222 -p 8222:8222 -p 6222:6222 \\\n    --name nats-server
    \\\n    nats:latest\n```\n\nYou can install it via [brew](https://formulae.brew.sh/formula/nats-server)
    as well.\n\nOnce it's up and running, one of the first things you can do is head
    to [http://localhost:8222/](http://localhost:8222/) in your browser and poke around
    NATS metrics.\nThat's vaguely interesting to look at, but let's do something fun.\n\n###
    Using NATS\n\nSince we're presumably in Laravel-land over here, let's find a PHP
    library for NATS.\n\n```bash\n# https://github.com/basis-company/nats.php\ncomposer
    require basis-company/nats\n```\n\nOnce the library is installed, we can test
    out NATS PubSub. PubSub isn't the ONLY way [NATS can push data around](https://docs.nats.io/nats-concepts/core-nats),
    but it is the most straight-forward.\n\nTo test it out, we'll create 2 commands
    - one to subscribe to a topic, and one to publish to that topic.\n\nFirst we'll
    create a command to subscribe and listen for messages:\n\n```bash\nphp artisan
    make:command NatsSub --command \"nats-sub\"\n```\n\nThat generates file `app/Console/Commands/NatsSub.php`
    which can contain the following to listen to topic `some-topic`:\n\n```php\n<?php\n\nnamespace
    App\\Console\\Commands;\n\nuse Illuminate\\Console\\Command;\n\nclass NatsSub
    extends Command\n{\n    protected $signature = 'nats-sub';\n\n    protected $description
    = 'Command description';\n\n    public function handle()\n    {\n        $client
    = new \\Basis\\Nats\\Client;\n\n        $client->subscribe('some-topic', function
    ($message) {\n            printf(\"Data: %s\\r\\n\", $message);\n        });\n\n
    \       while(true) {\n            $client->process();\n        }\n\n        return
    Command::SUCCESS;\n    }\n}\n```\n\nThen we can create a command to publish a
    message to that topic:\n\n```bash\nphp artisan make:command NatsPub --command
    \"nats-pub\"\n```\n\nThat generates file `app/Console/Commands/NatsPub.php` which
    can contain:\n\n```php\n<?php\n\nnamespace App\\Console\\Commands;\n\nuse Illuminate\\Console\\Command;\n\nclass
    NatsPub extends Command\n{\n    protected $signature = 'nats-pub';\n\n    protected
    $description = 'Command description';\n\n    public function handle()\n    {\n
    \       $client = new \\Basis\\Nats\\Client;\n\n        $client->publish('some-topic',
    \"we published a thing\");\n\n        return Command::SUCCESS;\n    }\n}\n```\n\nTo
    test this out, I subscribed to this topic 3 times, and then published to it once.
    We'll see that each subscriber gets the message.\n\n![nats in Laravel](terminal-1.png)\n\nThis
    means that any VM running our application can subscribe to the same topic, and
    **each one** will receive any message published to that topic.\n\nSo, we have
    a fun way to communicate amongst our application nodes!\n\n<%= partial \"shared/posts/cta\",
    locals: {\n  title: \"Fly.io ❤️ Laravel\",\n  text: \"Try this out yourself! Deploy
    your Laravel application around the world in just a few minutes.\",\n  link_url:
    \"https://fly.io/docs/laravel\",\n  link_text: \"Deploy your Laravel app!&nbsp;&nbsp;<span
    class='opacity:50'>&rarr;</span>\",\n} %>\n\n## NATS on Fly.io\n\nAs you've likely
    heard, Fly.io is good at running your applications all over the world. Often,
    it's useful for globally distributed applications to communicate with each other
    (to help sync data or events).\n\nLet's see how to run NATS on Fly.io!\n\nThere
    is a handy [GitHub repository](https://github.com/fly-apps/nats-cluster) that
    makes deploying NATS easy. It runs both an instance of a nats-server and makes
    some metrics/health checks available for Prometheus to read.\n\nHere's how to
    run a NATS cluster in 3 regions:\n\n```bash\n# Clone the repo\ngit clone https://github.com/fly-apps/nats-cluster.git\ncd
    nats-cluster\n\n# Create a fly app.\n# I chose dfw as the first region\n# I named
    my app \"fid-nat\"\nfly launch --no-deploy\n\n# Init a deploy\nfly deploy\n\n#
    Add 2 regions\n# e.g. `fly regions add lhr nrt`\nfly regions add <regions>\n\n#
    Scale up amongst the 3 regions\n## Using \"--max-per-region=1\" ensures each region
    \n## gets one VM, otherwise 2 may go into a single region\nfly scale count 3 --max-per-region=1\n```\n\nThe
    `fly.toml` file may contain some health checks in the `[services]` section. If
    so, delete the entire `[services]` section as we won't need them.\n\nLet's test
    this cluster out. Assuming you have setup [a Fly VPN connection](https://fly.io/docs/reference/private-networking/#private-network-vpn)
    (or an instance you can SSH into), we can run something like the following.\n\nI
    first tested by installing `nats`, a CLI tool for making requests against a NATS
    server. I ran the following on my Mac (while connected to my Fly private network
    via VPN):\n\n```bash\n# Install nats\nbrew tap nats-io/nats-tools\nbrew install
    nats-io/nats-tools/nats\n\n# Connect to our NATS servers\n# This creates file
    ~/.config/nats/context/fid-nat.demo.json\nnats context add fid-nat.demo --server
    fid-nat.internal:4222 --description \"Fideloper's Cluster\" --select\n\n# In one
    window\nnats sub fid-nat.demo\n\n# In another window\nnats pub fid-nat.demo \"Hello
    World\"\n```\n\nThat worked! At least, it did for me. I assume it will for you
    also!\n\n![nats cli](terminal-2.png)\n\nLet's see how to use this in a Laravel
    application also deployed to Fly.io.\n\n## Using Our Cluster\n\nWe saw our application
    run locally. If we push it up to Fly.io (as a separate application, not the `fid-nat`
    app), we can do the same thing but just tweak the `host` that we connect to. The
    default in the PHP library is `localhost`.\nWe'll instead connect to our application's
    nearest NATS instance via hostname `fid-nat.internal`. More on [Fly internal hostnames
    here](https://fly.io/docs/reference/private-networking/#fly-internal-addresses).\n\nOur
    code can be updated to connect to the `.internal` hostname pretty easily:\n\n```php\n$config
    = new \\Basis\\Nats\\Configuration([\n    'host' => 'fid-nat.internal',\n]);\n$client
    = new \\Basis\\Nats\\Client($config);\n```\n\nIf you deploy that code (or run
    it locally while connected to Fly.io via VPN) it still works!\n\n![nats on Fly](terminal-3.png)\n\n##
    What Else Might We Do?\n\nThe main benefit of NATS comes from its ability to \"cluster\"
    itself. If you spin up a few instances, the `nats-server` instances will become
    aware of each other and ensure each can be used to pass data to any connected
    clients.\n\nIn addition to PubSub, there's also a way to stream data (JetStream),
    complete a \"request-response\" style round-trip, keep track of data via key-value
    store, and more. Check out the README in the [composer package](https://github.com/basis-company/nats.php)
    we used above to see how those are used.\n\nWhether using a multi-region deployment
    or not, the possibilities for how NATS can be useful for you are endless-ish!\n"
- :id: blog-standout-features-in-django-4-2
  :date: '2023-02-02'
  :category: blog
  :title: Standout features in Django 4.2
  :author: mariusz
  :thumbnail: django-news-thumbnail.jpg
  :alt: An eyeball with a magnifying glass looking at Django news statements like
    Django 4.2 is here
  :link: blog/standout-features-in-django-4-2
  :path: blog/2023-02-02
  :body: "\n\n<p class=\"lead\">This post is about the new Django 4.2 release. It's
    got some neat things in it and Mariusz Felisiak shares his favorite highlights.
    Django on Fly.io is pretty sweet! Check it out: [you can be up and running on
    Fly.io in just minutes.](https://fly.io/docs/django/)</p>\n\nAfter 8 months of
    work by over **200!** contributors \U0001F497, [the first alpha  version of Django
    4.2](https://www.djangoproject.com/weblog/2023/jan/17/django-42-alpha-1-released/)
    is  out! This is a long-term support release (**LTS**) with extended support until
    _April 2026_, so 3 more years.\n\nThe final release should  be issued in early
    _April 2023_, so now is the best time to take a peek \U0001F440  at a \"farrago\"
    \ of new features shipped to this magnificent release \U0001F4E6. I'd like to
    take a moment and share my personal favorites of Django 4.2 goodies.\n\n## `psycopg`
    version 3 support\n\nDjango 4.2 supports `psycopg` version 3.1.8+ which is the
    new implementation of the most popular and the richest PostgreSQL adapter for
    Python. The best part is that there is no need to change the `ENGINE` as the built-in
    `django.db.backends.postgresql` backend supports both libraries, `psycopg2` and
    `psycopg`. It's enough to install `psycopg` in your environment:\n\n```bash\npython
    -m pip install \"psycopg>=3.1.8\"\n```\n\n`psycopg` prefers server-side parameter
    binding which improves performance and memory usage. It's disabled by default
    in Django as it causes hiccups in some cases e.g. parameters passed to expressions
    in `SELECT` and `GROUP BY` clauses are not recognized as the same values which
    can cause grouping errors. Even though support for server-side binding cursors
    is still experimental it's worth trying. If you want to use it, set the `server_side_binding`
    option in your `DATABASES` configuration:\n\n```python\nDATABASES = {\n    \"default\":
    {\n        \"ENGINE\": \"django.db.backends.postgresql\",\n        # ...\n        \"OPTIONS\":
    {\n            \"server_side_binding\": True,\n        },\n    },\n}\n```\n\n`psycopg`
    also provides asynchronous connections and cursors which should take Django async
    support to the next level in the future \U0001F52E.\n\n## Comments on columns
    and tables\n\nDatabase comments on columns and tables are really helpful for keeping
    your database schema maintainable. It also helps create a bridge \U0001F91D between
    developers and people with direct access to the database like database administrators
    or data scientists.\n\n11 years after creating the [ticket](https://code.djangoproject.com/ticket/18468)
    \U0001F5DD️, Django 4.2 added support for table and column comments via the new
    `Field.db_comment` and `Meta.db_table_comment` options (for all database backends
    included with Django except SQLite). [The migrations framework](https://docs.djangoproject.com/en/4.2/topics/migrations/)
    will propagate comments to your tables metadata. For example:\n\n```python\nfrom
    django.db import models\n\nclass Product(models.Model):\n    name = models.CharField(max_length=511,
    db_comment=\"Product name\")\n    deleted = models.BooleanField(\n        default=False,\n
    \       db_comment=(\n            \"Soft delete. When value is `True`, product
    is not visible \"\n            \"for users. This is useful for storing data about
    products \"\n            \"that are no longer for sale.\"\n        ),\n    )\n\n
    \   class Meta:\n        db_table_comment = \"Available products\"\n```\n\nIn
    general, table and column comments are not for application users, however storing
    them in the model description allows you to create a single place where our database
    schema is managed. As such, it covers another place where you can use the Django
    ORM instead of writing raw SQL statements.\n\n## Lookups on field instances\n\n[Registering
    lookups](https://docs.djangoproject.com/en/4.2/ref/models/lookups/#lookup-registration-api)
    on `Field` instances can be really helpful, especially if you are familiar with
    the `Lookup API`:\n\n> A lookup is a query expression with a left-hand side, `lhs`;
    a right-hand side, `rhs`; and a `lookup_name` that is used to produce a boolean
    comparison between\n\nDjango &lt; 4.2 only allowed registering lookups on `Field`
    classes, e.g. `__abs` lookup for `IntegerFied`. Django 4.2 takes the use of specialized
    lookup to the next and extremely flexible level when you can have a different
    set of lookup for each field in each model. This allows for creating reusable
    hooks and can reduce the number of annotations used when filtering a queryset.
    Let's assume you have a shop system, with a `Product` model and you want to filter
    out expired products.\n\n```python\nfrom django.db import models\n\nclass Product(models.Model):\n
    \   name = models.CharField(max_length=511)\n    best_before = models.DateTimeField()\n
    \   ...\n```\n\nYou can do this by comparing the  `best_before` field with the
    current date:\n\n```python\n>>> from django.utils import timezone\n>>> Product.objects.filter(best_before__lt=timezone.now())\n<QuerySet
    [<Product: Milk>]>\n```\n\nIn Django 4.2, you can now create a lookup `IsOverdue`:\n\n```python\nfrom
    django.db import models\nfrom django.db.models.functions import Now\n\nclass IsOverdue(models.Lookup):\n
    \   lookup_name = \"is_overdue\"\n    prepare_rhs = False\n\n    def as_sql(self,
    compiler, connection):\n        if not isinstance(self.rhs, bool):\n            raise
    ValueError(\n                \"The QuerySet value for an is_overdue lookup \"\n
    \               \"must be True or False.\"\n            )\n        sql, params
    = self.process_lhs(compiler, connection)\n        now_sql, now_params = compiler.compile(Now())\n
    \       if self.rhs:\n            return f\"{sql} < {now_sql}\", (*params, *now_params)\n
    \       else:\n            return f\"{sql} >= {now_sql}\", (*params, *now_params)\n```\n\nregister
    it on the `Product.best_before` (which is a [field instance](https://docs.djangoproject.com/en/4.2/ref/models/meta/#django.db.models.options.Options.get_field))
    by using `register_lookup()` :\n\n```python\nProduct._meta.get_field(\"best_before\").register_lookup(IsOverdue)\n```\n\nand
    use it as a handy shortcut:\n\n```python\n>>> Product.objects.filter(best_before__is_overdue=True)\n<QuerySet
    [<Product: Milk>]>\n```\n\n## Closing Thoughts\n\nHere we dug a little deeper
    on just 3 of my favorite improvements in the Django 4.2 release. These features
    all happen to be related to database support and I consider them solid quality
    of life improvements. Of course, there are a _lot_ of other fixes and improvements
    in this release and I strongly encourage you to check the [release notes](https://docs.djangoproject.com/en/4.2/releases/4.2/).
    A couple other features worth mentioning are the accessibility improvements in
    the admin site and forms, and the constantly improving asynchronous support.\n\nAll
    in all, Django 4.2 is another solid release of one of the most popular web frameworks.
    What enhancements or fixes are you most looking forward to?\n\nGive it a spin
    and share!\n\n<%= partial \"shared/posts/cta\", locals: {\n  title: \"Django really
    flies on Fly.io\",\n  text: \"You already know Django makes it easier to build
    better apps. Well now Fly.io makes it easier to _deploy_ those apps and move them
    closer to your users making it faster for them too!\",\n  link_url: \"https://fly.io/docs/django/\",\n
    \ link_text: \"Deploy a Django app today!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>\"}\n%>\n"
- :id: blog-carving-the-scheduler-out-of-our-orchestrator
  :date: '2023-02-01'
  :category: blog
  :title: Carving The Scheduler Out Of Our Orchestrator
  :author: thomas
  :thumbnail: starry-containers-thumbnail.jpg
  :alt:
  :link: blog/carving-the-scheduler-out-of-our-orchestrator
  :path: blog/2023-02-01
  :body: "\n\n<p class=\"lead\">We’re Fly.io, a global sandwich-rating company with
    a hosting problem. Even if you don't have a sandwich to rate, you might benefit
    from the hosting platform we built. Check it out: with a working Docker image,
    [you can be up and running on Fly.io in just minutes.](https://fly.io/docs/speedrun/)</p>\n\nSo,
    you want to build an app to rate sandwiches. Well, the world has a lot of different
    sandwiches. [Pit beefs](https://en.wikipedia.org/wiki/Pit_beef) in Baltimore,
    [Tonkatsu sandos](https://www.bonappetit.com/recipe/pork-katsu-sandwich) in Shinjuku,
    and [Cemitas](https://en.wikipedia.org/wiki/Cemita) in Puebla. You want real-time
    sandwich telemetry, no matter the longitude of the sandwich. So you need to run
    it all over the world, without a lot of ceremony.\n\nWe built one of those at
    Fly.io. We’ve written a bunch [about one important piece of the puzzle](https://fly.io/blog/docker-without-docker/):
    how we take Docker images from our users and efficiently run them as virtual machines.
    You can run a Docker image as VM. You’re almost done! Time to draw the rest of
    the owl.\n\nTo turn our Docker transmogrification into a platform, we need to
    go from running a single job to running hundreds of thousands. That’s an engineering
    problem with a name:\n\n## Orchestration\n\nOrchestrators link clusters of worker
    servers together and offer up an API to run jobs on them. [Kubernetes](https://kubernetes.io/)
    is an orchestrator; the Kleenex of orchestrators. Then, HashiCorp has [Nomad](https://www.nomadproject.io/),
    which we use, and about which more in a bit.\n\nFind a serverside developer complaining
    about how much harder it is to deliver an app in 2023 than it was in 2005, and
    odds are, [they’re complaining about orchestration.](https://news.ycombinator.com/item?id=20774712)
    They're not wrong: Kubernetes is fractally complicated. But the idea isn’t.\n\nLet’s
    write an orchestrator. Start by writing a supervisor.\n\n```go\n\tfor _, task
    := range tasks {\n\t\twg.Add(1)\n\n\t\tgo func(t Task) {\n\t\t\tdefer wg.Done()\n\n\t\t\targv
    := strings.Split(t.Command, \" \")\n\n\t\t\tfor {\n\t\t\t\tcmd := exec.Command(argv[0],
    argv[1:]...)\n\n\t\t\t\tcmd.Start()\n\t\t\t\tcmd.Wait()\n\n\t\t\t\tif !t.Restart
    {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}(task)\n\t}\n\n\twg.Wait()\n\n```\n\nI
    believe this design is so powerful it does not need to be discussed.\n\n<aside
    class=\"right-sidenote\">A build-your-own-light-saber tool if ever there was one.</aside>\nThere
    are, like, [100](http://supervisord.org/) [different](https://github.com/DarthSim/overmind)
    [supervisors](https://mmonit.com/monit/).  You can write a program to run a shell
    command, you can write a supervisor. Come on. You've already written a supervisor.
    Let's stop kidding each other.\n\nLet’s turn ours into an orchestrator.\n\nFor
    illustrative purposes, our supervisor takes a JSON configuration:\n\n```\n[\n
    \ {\n    \"Name\": \"frick\",\n    \"Command\": \"sleep 1\",\n    \"Restart\":
    true\n  },\n  {\n    \"Name\": \"frack\",\n    \"Command\": \"sleep 5\"\n  }\n]\n\n```\n\nInstead
    of reading this configuration from a file, like a dumb old supervisor, read it
    from an HTTP API, like a majestic orchestrator.  \"Workers\" run our simple supervisor
    code, and a `server` doles out tasks. Here’s an API:\n\n```\nGET /sched/jobs #
    polled\nPOST /sched/claim/{name}\nPOST /sched/release/{name}\nGET /sched/cancellations
    # polled\n\nPOST /sched/submit\nGET /sched/status/{name}\nPOST /sched/cancel/{name}\n\n```\n\nThe
    server implementing this is an exercise for the reader. Don’t overthink it .\n\nWorkers
    poll `/jobs`. They `/claim` them by name. The `server` decides which claim wins,
    awarding it a `200` HTTP response. The worker runs the job, until it stops, and
    posts `/release`.\n\nEnd-users drive the orchestrator with the same API; they
    post JSON tasks to `/submit`, check to see where they’re running, kill them by
    name with `/cancel`. Workers poll `/cancellations` to see what to stop running.\n\nThere.
    That's an orchestrator. It's just a client-server process supervisor.\n\nI see
    a lot of hands raised in the audience. I’ll take questions at the end. But let’s
    see if I can head some of them off:\n\n- Sure, it’s unusual for an orchestrator
    to run shell commands. <aside class=\"right-sidenote\">Though [Mesos](https://mesos.apache.org/documentation/latest/architecture/)_,
    I'm told by a veteran Meson, did start out running shell commands as well.</aside>
    A serious orchestrator would run Docker containers (or some agglomeration of multiple
    Dockerfiles called a Pod or a Brood or a Murder). But that’s just a detail; a
    constant factor of new lines calling the containerd SDK. [Knock yourself out!](https://pkg.go.dev/github.com/containerd/containerd)\n-
    Yeah, if you were running this in some big enterprise, you’d need some kind of
    [security and access control](https://awspolicygen.s3.amazonaws.com/policygen.html);
    this thing is just `rsh`. These are just details.\n- “That’s a stupid API” isn’t
    a question.\n- No, I don’t know what should happen if the server goes down. Something
    sane. Cancel and restart all the tasks. \n- OK, having all the workers stampeding
    to grab conflicting jobs is inefficient. But at most cluster sizes, who cares?
    Have the workers wait a random interval before claiming. Have them randomize the
    job they try to claim. It'll probably scale fine.\n- Yes, you could just do this
    with Redis and `BLPOP`.\n\nYou there in the back hollering… this isn’t a real
    orchestrator, why? Oh, because we’re not\n\n## Scheduling\n\nScheduling means
    deciding which worker to run each task on.\n\nScheduling is to an orchestrator
    what a routing protocol is to a router: the dilithium crystal, the contents of
    Marcellus Wallace’s briefcase, the thing that, ostensibly, makes the system Difficult.\n\nIt
    doesn't have to be hard. Assume our cluster is an undifferentiated mass of identical
    workers on the same network. Decide how many jobs a worker can run. Then: just
    tell a worker not to bid on jobs when it’s at its limit.\n\nBut no mainstream
    orchestrator works this way. All of them share some notion of centralized scheduling:
    an all-seeing eye that allocates space on workers the way a memory allocator doles
    out memory.\n\nEven centralized scheduling doesn't complicate our API that much.\n\n```\nPOST
    /sched/register # {'cpu':64,'mem':256,'diskfree':'4t'}\nGET /sched/assigned\nPOST
    /sched/started/{name}\nPOST /sched/stopped/{name}\nGET /sched/cancellations\n\n```\n\nInstead
    of rattling off all the available jobs and having workers stampede to claim them,
    our new API assigns them directly. Easier for the workers, harder for the server,
    which is now obligated to make decisions.\n\nHere's the rough outline of a centralized
    scheduler:\n\n1. Filter out workers that fail to match constraints, like sufficient
    disk space or CPUs or microlattice shapecasters.\n1. Rank the surviving workers.\n\nThe
    textbook way to rank viable workers is \"[bin packing](https://www.ics.uci.edu/~goodrich/teach/cs165/notes/BinPacking.pdf)\".
    Bin packing is a classic computer science problem: given a series of variably-sized
    objects and fixed-size containers, fit all the objects in the smallest number
    of containers. The conventional wisdom about allocating jobs in a cluster is indeed
    that of the clown car: try to make servers as utilized as possible, so you can
    minimize the number of servers you need to buy.\n\nSo far, the mechanics of what
    I'm describing are barely an afternoon coding project. But real clusters tend
    to run Kubernetes. Even small clusters: people run K8s for apps like `ratemysandwich.com`
    all the time. But K8s was designed to host things like `all of Google`. So K8s
    has fussy scheduling system.\n\nTo qualify as “fussy”, a scheduler needs at least
    2 of the following 3 properties:\n\n1. Place jobs on workers according to some
    optimum that is theoretically NP-hard to obtain (but is in practice like 2 nested
    `for` loops).\n1. Accounting for varying resource requirements for jobs using
    a live inventory of all the workers and something approximating a constraint solver.\n1.
    Scaling to huge clusters, without a single point of failure, so that the scheduler
    itself becomes a large distributed system.\n\nThese tenets of fussiness hold true
    not just for K8s, but for all mainstream orchestrators, including the one we use.\n\n<aside
    class=\"right-sidenote\"><br/><br/><br/>[The Mickensian aspect.](https://scholar.harvard.edu/files/mickens/files/thesaddestmoment.pdf)</aside>\n![](mickens.png?3/4&border)\n\n##
    Nomad\n\nLet's start by reckoning with what's going on with Kubernetes.\n\nThe
    legends speak of a mighty orchestrator lurking within the halls of Google called
    \"[Borg](https://research.google/pubs/pub43438/)\". Those of us who've never worked
    at Google have to take the word of those who have that Borg actually exists, and
    the word of other people that [K8s is based on the design of Borg.](https://kubernetes.io/blog/2015/04/borg-predecessor-to-kubernetes/)\n\nThe
    thing about Borg is that, if it exists, it exists within an ecosystem of other
    internal Google services. This makes sense for Google the same way having [S3](https://aws.amazon.com/s3/),
    [SQS](https://aws.amazon.com/sqs/), [ECS](https://aws.amazon.com/ecs/), [Lambda](https://aws.amazon.com/lambda/),
    [EBS](https://aws.amazon.com/ebs/), [ALBs](https://aws.amazon.com/elasticloadbalancing/),
    [CloudWatch](https://aws.amazon.com/cloudwatch/), [Cognito](https://aws.amazon.com/cognito/),
    [EFS](https://aws.amazon.com/efs/), [RedShift](https://aws.amazon.com/redshift/),
    [Route53](https://aws.amazon.com/route53), [Glacier](https://aws.amazon.com/glacier/),
    [SNS](https://aws.amazon.com/sns/), [VPC](https://aws.amazon.com/vpc/), [Certificate
    Manager](https://aws.amazon.com/certificate-manager/), [QFA](https://universalpaperclips.fandom.com/wiki/Quantum_Foam_Annealment),
    [IAM](https://aws.amazon.com/iam/), [KMS](https://aws.amazon.com/kms/), [CodeCommit](https://aws.amazon.com/codecommit/),
    [OpsWorks](https://aws.amazon.com/opsworks/), [Cloudformation](https://aws.amazon.com/cloudformation/),
    [Snowball](https://aws.amazon.com/snowball/), [X-Ray](https://aws.amazon.com/xray/),
    [Price List Marketplace Metering Service Entitlement Modernization](https://www.youtube.com/watch?v=KWotO76SuXE),
    and [EC2](https://aws.amazon.com/ec2/) does for AWS. Like, somewhere within Google
    there's a team that's using each of these kinds of service.\n\n<aside class=\"right-sidenote\">You
    can't argue with the success Kubernetes has had. I get it.</aside>\nIt makes less
    sense for a single piece of software to try to wrap up all those services. But
    [Kubernetes seems to be trying](https://landscape.cncf.io/). Here's some perspective:
    K8s is, some people say, essentially Borg but with Docker Containers instead of
    [Midas packages](https://www.usenix.org/sites/default/files/conference/protected-files/lisa_2014_talk.pdf).
    Midas is neat, but it in turn relies on [BigTable](https://static.googleusercontent.com/media/research.google.com/en//archive/bigtable-osdi06.pdf)
    and [Colossus](https://cloud.google.com/blog/products/storage-data-transfer/a-peek-behind-colossus-googles-file-system),
    two huge Google services. And that's just packages, the lowest level primitive
    in the system. It's an, uh, ambitious starting point for a global open source
    standard. \n\nAt any rate, our customers want to run Linux apps, not Kubernetes
    apps. So Kubernetes is out.\n\nSometime later, a team inside Google took it upon
    themselves to redesign Borg. Their system was called [Omega](https://research.google/pubs/pub41684/).
    I don't know if it was ever widely used, but it's influential. Omega has these
    properties:\n\n- Distributed scheduling, so that scheduling decisions could be
    made on servers across the cluster instead of a monolithic single central scheduler.
    \n- A complete, up-to-date picture of available resources on the cluster (via
    a Paxos-replicated database) provided to all schedulers.\n- Optimistic transactions:
    if a proposed decision fails, because it conflicts with some other claim on the
    same resources, your scheduler just tries again.\n\nHashicorp [took Google's Omega
    paper and turned it into an open source project](https://developer.hashicorp.com/nomad/docs/concepts/scheduling/scheduling),
    called Nomad.\n\nOmega's architecture is nice. But the real win is that Nomad
    is lightweight. It's conceptually  not all that far from the API we designed earlier,
    [plus Raft](https://github.com/hashicorp/raft).\n\nNomad can run Unix programs
    directly, or in Docker containers. We do neither. Not a problem: Nomad will orchestrate
    jobs for anything that conforms to [this interface](https://pkg.go.dev/github.com/hashicorp/nomad/plugins/drivers):\n\n```go\n\tRecoverTask(*TaskHandle)
    error\n\tStartTask(*TaskConfig) (*TaskHandle, *DriverNetwork, error)\n\tWaitTask(ctx
    context.Context, taskID string) (<-chan *ExitResult, error)\n\tStopTask(taskID
    string, timeout time.Duration, signal string) error\n\tDestroyTask(taskID string,
    force bool) error\n\tInspectTask(taskID string) (*TaskStatus, error)\n    // plus
    some other goo\n\n```\n\nFor the year following [our launch](https://news.ycombinator.com/item?id=22616857),
    Fly.io's platform was a Rust proxy and a Golang Nomad driver. The driver could
    check out a Docker image, convert it to a block device, and start Firecracker
    on it. In return for coding to the driver interface, we got:\n\n- Constraint-based
    deployments that let us  tell a specific Fly app to run in Singapore (har cheong
    gai burger), Sydney (hamdog), and Frankfurt (doner kebab), on dedicated CPU instances
    with 2 cores and at least 4 gigs of memory, say.\n- The ability to move Fly Apps
    around our fleet, draining them off specific machines for maintenance.\n- Opt-in
    integration [with Consul](https://fly.io/blog/a-foolish-consistency/), which we
    used for request routing and to glue our API to our platform backend. \n\nAbout
    Nomad itself, we have nothing but nice things to say. Nomad is like Flask to K8s's
    Django, Sinatra to K8s's Rails. It's unopinionated, easy to set up, and straightforward
    to extend. Use Nomad.\n\n<div class=\"callout\">\nAnother very cool system to
    look at in this space is [Flynn](https://github.com/flynn/flynn). Flynn was an
    open source project that started before Docker was stable and grew up alongside
    it. They set out to build a platform-as-a-service in a box, one that bootstraps
    itself from a single-binary install. It does so much stuff! If you've ever wondered
    what all the backend code for something like Fly.io must be like (multiple generations
    of schedulers and all), check out what they did.\n</div>\n  \nBut we've outgrown
    it, because:\n\n**Bin packing is wrong for platforms like Fly.io**. Fussy schedulers
    are premised on minimizing deployed servers by making every server do more. That
    makes a lot of sense if you're Pixar. We rent out server space. So we buy enough
    of them  to have headroom in every region. As long as they're running, we'd want
    to use them.\n\n[Here's a Google presentation](http://www.columbia.edu/~cs2035/courses/ieor4405.S13/datacenter_scheduling.ppt)
    on the logic behind Nomad's first-fit bin packing scheduler. It was designed for
    a cluster where 0% utilization was better, for power consumption reasons, than
    < 40% utilization. Makes sense for Google. Not so much for us.\n\nWith strict
    bin packing, we end up with Katamari Damacy scheduling, where a couple overworked
    servers in our fleet suck up all the random jobs they come into contact with.
    Resource tracking is imperfect and neighbors are noisy, so this is a pretty bad
    customer experience.\n\nNomad added a \"[spread scheduling](https://developer.hashicorp.com/nomad/tutorials/advanced-scheduling/spread)\"
    option, which just inverts the bin pack scoring they use by default. But that's
    not necessarily what we want. What we want is complicated! We're high-maintenance!
    In a geographically diverse fleet with predictable usage patterns, the best scheduling
    plans are intricate, and we don't want to fight with a scheduler to implement
    them.\n\n**We Run One Global Cluster.** This isn't what Nomad expects. Nomad wants
    us to run [a bunch of federated clusters](https://developer.hashicorp.com/nomad/tutorials/manage-clusters/federation)
    (one in Dallas, one in Newark, and so on).\n\nThere are two big reasons we don't
    federate Nomad:\n\n- It changes the semantics of how apps are orchestrated, which
    would require fiddly engineering for us to wire back into our UX. For instance:
    there isn't an obvious, clean way to roll back a failing app deploy across a dozen
    regions all at once. We have lots of regions, but offer one platform to our users,
    so we run into lots of stuff like this.\n- Even if we did that work, Nomad pricing
    looks at us and sees Apple, Inc. More power to them! But, like, no.\n\n**We Outgrew
    The Orchestration Model.** Nomad scheduling is asynchronous. You submit a job
    to a server. All the servers convene a trustees meeting, solicit public comment,
    agree on the previous meeting's minutes, and reach consensus about the nature
    of the job requested. A plan is put into motion, and the implicated workers are
    informed. Probably, everything works fine; if not, the process starts over again,
    and again, until seconds, minutes, hours, or days later, it does work.\n\nThis
    is not a bad way to handle a `flyctl deploy` request. But it's no way to handle
    an HTTP request, and that's what we want: for a request to land at our network
    edge in São Paulo, and then we **scale from zero** to handle it in our `GRU` region,
    starting a Fly Machine on a particular server, synchronously.\n\n<div class=\"callout\">\nThe
    Fly.io step in there that costs the most is pulling containers from registries.
    People's containers are huge! That makes the win from caching large – and just
    not captured by the Nomad scheduler.\n<br/><br/>\nNomad autoscaling is elegant,
    and just not well matched to our platform. How [the autoscaler](https://github.com/hashicorp/nomad-autoscaler)
    works is, it takes external metrics and uses them to adjust the count constraint
    on jobs. We scrape metrics every 15 seconds, and then Nomad's scheduling work
    adds a bunch of time on top of that, so it never really worked effectively.\n</div>\n\nAt
    this point, what we're asking our scheduler to do is to consider Docker images
    themselves to be a resource, like disk space and memory. The set of images cached
    and ready to deploy on any given server is changing every second, and so are the
    scheduling demands being submitted to the orchestrator. Crazy producers. Crazy
    consumers. It's a lot to ask from a centralized scheduler.\n\nSo we built our
    own, called\n\n## Nümad\n\n<aside class=\"right-sidenote\">I would also accept
    \"nonomad\", \"yesmad\", \"no, mad\", and  \"fauxmad\" for this dad joke.</aside>\nJust
    kidding, we call it `flyd`.\n\nThere is a [long and distinguished literature](https://queue.acm.org/detail.cfm?id=3173558)
    of [cluster scheduling](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/44843.pdf),
    [going back into the 1980s](https://ieeexplore.ieee.org/document/4634). We decided
    not to consult it, and just built something instead.\n\n`flyd` has a radically
    different model from Kubernetes and Nomad. Mainstream orchestrators are like sophisticated
    memory allocators, operating from a reliable global picture of all capacity everywhere
    in the cluster. Not `flyd`.\n\nInstead, `flyd` operates like a market. Requests
    to schedule jobs are bids for resources; workers are suppliers. Our orchestrator
    sits in the middle like an exchange. `ratemysandwich.com` asks for a Fly Machine
    with 4 dedicated CPU cores in Chennai (sandwich: bun kebab?). Some worker in `MAA`
    offers room; a match is made, the order is filled.\n\nOr, critically: the order
    is not filled. That's fine too! What's important is that the decision be made
    quickly, so that it can be done synchronously. What we don't want is a `pending`
    state waiting for the weather to clear up.\n\n![](flaps.png?3/4)\n\nOur system
    has a cast of three characters:\n\n- `flyd` is the source of truth for all the
    VMs running on a particular worker.\n- `flaps` is a stateful proxy for all the
    `flyd` instances. \n- `flyctl` is our CLI. You know it, you love it.\n\nThe engine
    of this system is `flyd`.\n\nIn Nomad-land, our Firecracker driver doesn't keep
    much state. That's the job of huge scheduling servers, operating in unlighted
    chambers beyond time amidst the maddening beating and monotonous whine of the
    [Raft consensus protocol](https://www.imdb.com/title/tt5073642/).\n\n<aside class=\"right-sidenote\">Unlike
    Nomad, which goes through some effort to keep the entire map of available resources
    in the cluster in memory, nothing in `flyd` is cached; everything is just materialized
    on-demand from disk.</aside>  \nIn `flyd`-land, state-keeping is very much the
    worker's problem. Every worker is its own source of truth. Every `flyd` keeps
    a `boltdb` database of its current state, which is an append-only log of all the
    operations applied to the worker.</aside>\n\n`flyd` is rigidly structured as a
    collection of state machines, like \"create a machine\" or \"delete a volume\".
    \ Each has a concrete representation both in the code (using Go generics) and
    in `boltdb`. Everything happening in `flyd` (in logs, traces, metrics or whatever)
    happens at a particular state for a particular resource ID. Easy to reason about.
    And, of course, if we bounce `flyd`, it picks up right where it left off.\n\n<div
    class=\"callout\">\n`flyd` operates off of a local `boltdb` database, but our
    platform also has an SQLite view of all the resources allocated systemwide. We
    built it by caching Consul, but, in keeping with our ethos of \"if you see Raft
    anywhere, something went wrong\", we've replaced it with something simpler. We
    call it Corrosion.\n<br/><br/>\nCorrosion is what would happen if you looked at
    Consul, realized every server is its own source of truth and thus distributed
    state wasn't a consensus problem at all but rather just a replication problem,
    built a [SWIM gossip system](https://fly.io/blog/building-clusters-with-serf/#what-serf-is-doing),
    and made it spit out SQLite. Also you decided it should be written in Rust. Corrosion
    is neat, and we'll eventually write more about it.\n</div>\n  \nAll the `flyd`
    instances in (say) Madrid form a `MAD` cluster. But it's not a cluster in the
    same sense Nomad or K8s uses: no state is shared between the `flyd` instances,
    and no consensus protocol runs.\n\nTo get jobs running on a `flyd` in `MAD`, you
    talk to `flaps`. `flaps` is running wherever you are (in my case, `ORD`).\n\n`flaps`
    uses Corrosion to find all the workers in a particular region. It has direct connectivity
    to every `flyd`, because our network is meshed up with WireGuard. `flyd` exposes
    an internal HTTP API to `flaps`, and `flaps` in turn exposes this API:\n\n```\nGET
    /v1/apps/my-app/machines  # list\nPOST /v1/apps/my-app/machines # create\nGET
    /v1/apps/{machineid}      # show\nDELETE /v1/apps/{machineid} \nPOST /v1/apps/{machineid}/start\nPOST
    /v1/apps/{machineid}/stop\n\n```\n\n\"Creating\" a Fly Machine reserves space
    on a worker in some region.\n\n![A utility function for, say, CPU capacity](curve.png?1/4&wrap-right)\nTo
    reserve space in Sydney, `flaps` collects capacity information from all the `flyds`
    in `SYD`, and then runs a quick best-fit ranking over the workers with space,
    which is just a simple linear interpolation rankings workers as more or less desirable
    at different utilizations of different resources.\n\n<br/><br/>\n  \n<aside class=\"right-sidenote\">†
    (and, in the future, the cartesian of regions cross new hardware products, like
    space modulator coprocessors)</aside>  \nRather than forming distributed consensus
    clusters, Fly.io regions like `MAD` and `SYD`† are like products listed on an
    exchange. There are multiple suppliers of `MAD` VMs (each of our workers in Madrid)
    and you don't care which one you get. `flaps` act like a broker. Orders come in,
    and we attempt to match them. `flaps` does some lookups in the process, but it
    doesn't hold on to any state; the different `flaps` instances around the world
    don't agree on a picture of the world. The whole process can fail, the same way
    an immediate-or-cancel order does with a financial market order. That's OK!\n\nHere's
    what doesn't happen in this design: jobs don't arrive and then sit on the book
    in a \"pending\" state while the orchestrator does its best to find some place,
    any place to run it. If you ask for VMs in `MAD`, you're going to get VMs in `MAD`,
    or you're going to get nothing. You won't get VMs in `FRA` because the orchestrator
    has decided \"that's close enough\". That kind of thing happened to us all the
    time with Nomad.\n\n## Scheduling, Reconsidered\n\nIf you're a certain kind of
    reader, you've noticed that this design doesn't do everything Fly Apps do. What
    happens when an app crashes? How do we deploy across a bunch of regions? How does
    a rollback work? These are problems Nomad solved. It doesn't look like `flaps`
    and `flyd` solve them.\n\nThat's because they don't! Other parts of the  platform
    — most notably, `flyctl`, our beloved CLI — take over those responsibilities.\n\nFor
    example: how do we handle a crashed worker? Now, `flyd` will restart a crashed
    VM, of course; that's an easy decision to make locally. But some problems can't
    be fixed by a single worker. Well, one thing we do is: when you do a deploy, `flyctl`
    creates multiple machines for each instance. Only one is started, but others are
    prepped on different workers. If a worker goes down, `fly-proxy` notices, and
    sends a signal to start a spare.\n\nWhat we're doing more generally is carving
    complex, policy-heavy functionality out of our platform, and moving it out to
    the client. [Aficionados of classic papers](https://web.mit.edu/Saltzer/www/publications/endtoend/endtoend.pdf)
    \ will recognize this as an old strategy.\n\n<aside class=\"right-sidenote\">Networks,
    boy I tell ya.</aside>\n\nWhat we had with Nomad was a system that would make
    a lot of sense if we were scheduling a relatively small number of huge apps. But
    we schedule a huge number of relatively small apps, and the intelligent decisions
    our platform made in response to stimuli were often a Mad Hatter's tea party.
    For instance: many times when Europe lost connectivity to `us-east-1` S3, apps
    would flake, and Nomad would in response cry \"change places!\" and reschedule
    them onto different machines.\n\nWhat we've concluded is that these kinds of scheduling
    decisions are actually the nuts and bolts of how our platform works. They're things
    we should have very strong opinions about, and we shouldn't be debating a bin
    packer or a constraint system to implement them. In the new design, the basic
    primitives are directly exposed, and we just write code to configure them the
    way we want.\n\nInternally, we call this new system \"AppsV2\", because we're
    good at naming things. If you're deploying an app in January of 2023, you're still
    using Nomad; if you're deploying one in December of 2023, you'll probably be interacting
    with `flyd`. If we do it right, you mostly won't have to care.\n\n<%= partial
    \"shared/posts/cta\", locals: {\n  title: \"You can play with this stuff right
    now.\",\n  text: \"The Fly Machines API runs on flyd and reserves, starts, and
    stops individual VMs.\",\n  link_url: \"https://fly.io/docs/speedrun/\",\n  link_text:
    \"Try Fly for free&nbsp;&nbsp;<span class='opacity-50'>&rarr;</span>\"\n} %>\n
    \  \n## Drawing Most Of The Owl\n\nOver the last couple years, we've written about
    most of the guts of Fly.io:\n\n- [How we run containers as VMs](https://fly.io/blog/docker-without-docker/)
    in the first place,\n- how our [WireGuard-backed private networking layer](https://fly.io/blog/ipv6-wireguard-peering/)
    works,\n- how we [provision Internet Anycasting](https://fly.io/blog/32-bit-real-estate/),
    and\n- how our [control plane works](https://fly.io/blog/a-foolish-consistency/)
    (gonna have to rewrite that one soon!).\n\nIt took us awhile, but we're glad to
    have finally written down our thoughts about one of the last remaining big pieces.
    With an execution engine, a control plane, and an orchestrator, you've got most
    of our platform! The only huge piece left is `fly-proxy`, which we have not yet
    done justice.\n\nWe hope this is interesting stuff even if you never plan on running
    an app here (or building a platform of your own on top of ours). We're not the
    first team to come up with a bidding-style orchestrator — they're documented [in
    that 1988 paper above](https://ieeexplore.ieee.org/document/4634)! But given an
    entire industry of orchestrators that look like Borg, it's good to get a reminder
    of how many degrees of freedom we really have.\n"
- :id: phoenix-files-sqlite3-full-text-search-with-phoenix
  :date: '2023-01-31'
  :category: phoenix-files
  :title: SQLite3 Full Text Search with Phoenix
  :author: jason
  :thumbnail: sqlite-fts5-thumbnail.jpg
  :alt: Illustration of a database searching documents
  :link: phoenix-files/sqlite3-full-text-search-with-phoenix
  :path: phoenix-files/2023-01-31
  :body: "\n\n<p class=\"lead\">This is a start of a series using SQLite3 Full Text
    Search and Elixir. Fly.io is a great place to run Elixir applications! Check out
    how to [get started](/docs/elixir/)!</p>\n\nOne of the benefits of SQLite is that
    you can query lightning quick because your database is colocated next to your
    server. And a bonus it also comes with built-in Full Text search capabilities!
    But how does this work with Ecto?\n\nIn this Series we will walk through\n\n-
    Use SQLite Full Text with Ecto.\n- Building a LiveView autocomplete using the
    new index.\n- How to set up LiteFS and Fly.io to distribute your search index
    close to the users.\n\n## Setup\n\nBefore we begin we will need to set up our
    existing or new application to use SQLite3, luckily we have a guide to help with
    just that: [SQLite3](https://fly.io/docs/elixir/advanced-guides/sqlite3/)!\n\nIn
    this guide we will be using the built-in [FTS5](https://www.sqlite.org/fts5.html)
    plugin. This is compiled and shipped with the [ecto_sqlite3](https://hex.pm/packages/ecto_sqlite3)
    library, so we should be all set. If you have problems here because you have a
    custom setup check here first.\n\n## Background\n\nThe FTS5 plugin will only work
    on FTS5 specified virtual tables. In SQLite3 we have the concept of a [Virtual
    Table]([https://www.sqlite.org/vtab.html](https://www.sqlite.org/vtab.html)) which
    SQLite sees as a normal table with columns. This virtual table can be queried
    normally but plugin's can manipulate the data before it's written to disc or returned
    in a query. In this case FTS5 maintains an index and will build the correct result
    set and give each result row a `rank` based on its content. This plugin also gives
    us special syntax to query the virtual table and rank them.\n\nIn this guide we'll
    first start with the phoenix generators, and we'll change the generated code as
    we go. So lets begin\n\n```bash\nmix phx.gen.context Data Document documents title:string
    body:string  author:string url:string\n* creating lib/app/data/document.ex\n*
    creating priv/repo/migrations/20230118183954_create_documents.exs\n* creating
    lib/app/data.ex\n* injecting lib/app/data.ex\n* creating test/app/data_test.exs\n*
    injecting test/app/data_test.exs\n* creating test/support/fixtures/data_fixtures.ex\n*
    injecting test/support/fixtures/data_fixtures.ex\n\nRemember to update your repository
    by running migrations:\n\n    $ mix ecto.migrate\n```\n\nThis will generate everything
    we need to get started. Before running the migrations lets replace them wholesale
    to look like so:\n\n```elixir\ndefmodule App.Repo.Migrations.CreateDocuments do\n
    \ use Ecto.Migration\n  \n  def change do\n    execute(\"\"\"\n      CREATE VIRTUAL
    TABLE documents USING fts5(\n        updated_at UNINDEXED,\n        inserted_at
    UNINDEXED,\n        url UNINDEXED, \n        title, \n        author,\n        body\n
    \     );\n      \"\"\",\n      \"\"\"\n      DROP TABLE documents;\n      \"\"\"\n
    \   )\n  end\nend\n```\n\nEcto SQL does not come with facilities to allow us to
    define a virtual table using custom functions so we will be using [execute/2](https://hexdocs.pm/ecto_sql/Ecto.Migration.html#execute/2)
    migration function which accepts an up string and down string.\n\nIn the up we
    are defining a virtual table named `documents` using the `fts5` function which
    accepts a list of columns and column attributes. We included the default timestamps
    and URL, marking them as `UNINDEXED` this tells the index to ignore those columns.\n\nAnd
    now by default SQLite3 will index the title, author and body column and allow
    us to directly search them. Notice we don't have a primary key and that is cause
    FTS5 has a built-in primary key of `rowid`.\n\nNext up we need to update our schema
    in Elixir to handle the new table structure so lets change the `lib/app/data/document.ex`
    like so:\n\n```diff\ndefmodule App.Data.Document do\n   use Ecto.Schema\n   import
    Ecto.Changeset\n\n+  @primary_key {:id, :id, autogenerate: true, source: :rowid}\n
    \  schema \"documents\" do\n     field :author, :string\n     field :body, :string\n
    \    field :title, :string\n     field :url, :string\n+    field :rank, :float,
    virtual: true\n\n     timestamps()\n   end\n   ...\nend\n```\n\nHere we are telling
    ecto that our primary key is called `:id` but when queried it&#39;s called `:rowid`
    this lets us use the built-in SQLite functions seamlessly with Ecto Queries. And
    we&#39;re also adding a virtual `:rank` column that lets us query and order by
    the special column `:rank` provided by FTS5.\n\nAnd finally lets query it using
    the most basic query syntax:\n\n```elixir\n  @doc \"\"\"\n  Searchs our documents
    based on a simple query.\n\n  ## Examples\n\n      iex> search_documents(\"hello\")\n
    \     [%Document{}, ...]\n\n  \"\"\"\n  def search_documents(q) do\n    from(d
    in Document, \n      select: [:title, :url, :rank, :id],\n      where: fragment(\"documents
    MATCH ?\", ^q),\n      order_by: [asc: :rank]\n    )\n    |> Repo.all()\n  end\n```\n\nThe
    bulk of the magic happens in the `where` where we use a `fragment(\"documents
    MATCH ?\", ^q)` which is the syntax for querying an FTS5 virtual table. It is
    a little strange since we&#39;re matching an entire table name and not a specific
    column but the intuition of it is all indexed columns are searched as one string.\n\nWith
    the `order_by` returning the results ordered by `rank`, which is required otherwise
    results with be ordered unpredictably.\n\nFinally when we are ready we can insert,
    update and delete documents normally using our `Data` context functions generated
    for us.\n\nAnd there we go, we now have the basic structure and setup required
    to do full text searching with Ecto and SQLite3. But this is just scratching the
    surface here on what&#39;s possible with some extra effort.\n\n## Further Considerations\n\n###
    Keep your data in Sync\n\nNo matter what you are responsible for building your
    and keeping your index fresh.\n\nIf this is a specific search index for external
    data we might run a nightly job to rebuild it, or we might kick off a build every
    time something changes. We might even set up an [Oban](https://getoban.pro/) job
    to do this periodically or asynchronously whenever our content changes.\n\nIf
    this search works on other content already in the database we have other options:\n\n-
    We could manually update our index within the Context whenever the base data changes.\n-
    We could set up triggers to automagically change the underlying data whenever
    we need it to change. That is left up as a challenge to the reader.\n- Or we could
    use the built-in [External Content and Contentless Tables](https://www.sqlite.org/fts5.html#external_content_and_contentless_tables)
    functionality. This can be helpful if the content you are searching is large and
    you need to reduce your SQLite database size. This will build the index in the
    FTS5 table, and then query the original table to show the data as needed. We won't
    deep dive into this as the guides do a good job of it, and it is almost entirely
    something that lives in migrations anyway.\n\n### Advanced Queries\n\nThe basic
    query syntax can work if your content is text heavy like documents or blog posts
    but, sometimes you need to specify certain columns more directly. The FTS5 documentation
    go directly into this with the [Full Text Query Syntax](https://www.sqlite.org/fts5.html#full_text_query_syntax)
    section of the documentation. Suffice to say we will need to build out our query
    string and then shove them into fragments. I won't go into super detail here as
    the steps are specific to each use case.\n\n<%= partial \"shared/posts/cta\",
    locals: {\n  title: \"Fly.io ❤️ Elixir\",\n  text: \"Fly.io is a great way to
    run your Phoenix LiveView app close to your users. It's really easy to get started.
    You can be running in minutes.\",\n  link_url: \"https://fly.io/docs/elixir/\",\n
    \ link_text: \"Deploy a Phoenix app today!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>\"\n}
    %>\n"
- :id: blog-fly-in-johannesburg
  :date: '2023-01-31'
  :category: blog
  :title: Fly.io is in Johannesburg
  :author: chris-n
  :thumbnail: jnb-kota-thumbnail.png
  :alt: A sandwich made from a quarter-loaf of bread with a rectangular space cut
    out and filled with chips, meat, and veggie toppings. It's labelled 'the kota.'
  :link: blog/fly-in-johannesburg
  :path: blog/2023-01-31
  :body: |2


    <p class="lead">Fly.io is busily adding servers in new regions, but here's one we prepared earlier. Launch a full-stack app in Johannesburg, South Africa! It's easy to [get started](/docs/speedrun).</p>

    Did you know that we're in Johannesburg? There's rugby and cricket. Hearty kota and Gatsby sandwiches. Braii under sunny skies and low-latency full-stack apps. Front end, Postgres, Redis, the works: if your users support the Springboks and Banyana Banyana, you should put your whole app in JNB.
- :id: phoenix-files-tag-all-the-things
  :date: '2023-01-30'
  :category: phoenix-files
  :title: Tag All the Things!
  :author: mark
  :thumbnail: tag-thumbnail.jpg
  :alt: Illustration with the text "words words words list list list fly, fly, fly"
  :link: phoenix-files/tag-all-the-things
  :path: phoenix-files/2023-01-30
  :body: "\n\n<p class=\"lead\">This is about a simple way of tagging database records
    using Postgres and Ecto that fits a number of situations. Fly.io is a great place
    to run Elixir applications! Check out how to [get started](/docs/elixir/)!</p>\n\n**UPDATED**:
    This was updated to support clearing a list of tags from a [UI input component](/phoenix-files/making-a-checkboxgroup-input/).\n\nWhether
    you link labels to a Github issue, track the different muscle groups strengthened
    by an exercise, or describe a book by the set of genres that apply to it, it's
    all the same thing, **tagging**. Developers define tags that apply to things like
    cars listings, a job posting, or a property listing and it adds richness and meaning
    to the tagged item. It often isn't called a \"tag\", it may be referred to as
    \"categories\", \"labels\", \"traits\", \"genres\", whatever!\n\nThere are numerous
    ways to add tagging to a system and the \"right way\" depends on the system and
    the problem being solved. Here we'll explore a very easy way to add tagging to
    our Phoenix applications that also uses of a cool feature in [PostgreSQL](https://www.postgresql.org/).\n\n##
    Problem\n\nWe have a book tracking system. In our system, we let trusted users
    create new book entries. A book can belong to multiple genres. There is a finite
    set of pre-defined genres we support. The application provides the full set of
    supported and possible genres.\n\nA book entry should support being tagged as
    \"Sci-Fi\", \"Mystery\" and \"Young Adult\" all at the same time.  In our system,
    we call it \"genres\", but it's really just tagging, isn't it?\n\nHow do we add
    the ability to \"tag\" a database entry (i.e. book) with a set of tags (i.e. genres)?\n\n##
    Solution\n\nFor this discussion, a \"tag\" is a custom descriptor added to a database
    record.\n\nThis solution assumes the list of possible tags are defined by the
    system and not user managed. For instance, when talking about book genres, we
    don't allow users to create  a new genre called \"stupid\". A genre has specific
    meaning in the book tracking system and users don't get a vote.\n\nSimilarly,
    this applies to exercises and the impacted muscle groups. People don't get to
    create new muscle groups! They just exist and as users, we tag an item with one
    or more of the pre-defined tags.\n\nThis solution is for that type of system.\n\nGiven
    that constraint, this solution covers how to do the following:\n\n- enable an
    item to be tagged in our Postgres DB\n- add the tag field to our Ecto schema\n-
    process tags through a changeset\n- query items using the tags\n\n### Database
    Migration\n\nOur solution starts in the database. We're using [Postgres](https://www.postgresql.org/)
    for our application (the default DB for Phoenix applications). We'll take advantage
    of Postgres' [array column type](https://www.postgresql.org/docs/current/arrays.html).
    This let's us create a single field on a table that holds an array of values.
    For our tags, it will be an array of strings.\n\nHere's the relevant part of a
    DB migration to add a `books` table with a `genres` field that holds an array
    of strings.\n\n```elixir\ncreate table(:books) do\n  add :title, :string, null:
    false\n  # ...\n  add :genres, {:array, :string}\n  # ...\nend\n\n# add an index
    on the genres\nexecute(\"create index books_genres_index on books using gin (genres);\")\n```\n\n<aside
    class=\"right-sidenote\">\n  GIN indexes are “inverted indexes” which are appropriate
    for data values that contain multiple component values, such as arrays.\n</aside>\n\nNotice
    that the field type used is `{:array, :string}`. Also note, the last line executes
    an SQL statement to create a special [GIN index](https://www.postgresql.org/docs/15/indexes-types.html#INDEXES-TYPES-GIN)
    on our array field. The [GIN index will keep our look ups fast](https://www.postgresql.org/docs/9.3/textsearch-indexes.html)!\n\nWith
    our database table ready, let's move on to Ecto.\n\n### Ecto Schema\n\nNext, we
    need to mirror the database change in our [Ecto Schema](https://hexdocs.pm/ecto/Ecto.Schema.html).
    Here's the relevant part of our schema definition:\n\n```elixir\nschema \"books\"
    do\n  field :title, :string, required: true\n  # ...\n  field :genres, {:array,
    :string}, default: [], required: true\n  # ...\nend\n```\n\nAgain, the field is
    described as an array of strings. Yes, that means Ecto natively supports working
    with an [array of strings](https://hexdocs.pm/ecto/Ecto.Type.html#cast/2). Nice!
    That will come in handy!\n\nWe have the structure part handled. Now we want to
    control how data is stored there.\n\n### Validate the Array\n\nIn this section,
    we're talking about the changeset functions used on our data. We will parse, cast,
    validate, and process user values.\n\nLet's start with an abbreviated list of
    the valid genres. Since the allowed genres are built-in to our application, we
    can code it with our schema.\n\n```elixir\ndefmodule MyApp.Books.Book do\n  use
    Ecto.Schema\n  import Ecto.Query, warn: false\n  import Ecto.Changeset\n\n  schema
    \"books\" do\n    field :name, :string, required: true\n\n    field :genres, {:array,
    :string}, default: [], required: true\n    # ...\n  end\n\n  @genre_options [\n
    \   \"fantasy\",\n    \"sci-fi\",\n    \"dystopian\",\n    \"adventure\",\n    \"romance\",\n
    \   \"mystery\",\n    \"horror\",\n    \"thriller\",\n    \"historical-fiction\",\n
    \   \"young-adult\",\n    \"children-fiction\",\n    \"autobiography\",\n    \"biography\",\n
    \   \"cooking\",\n    # ...\n  ]\n\n  # ...\n```\n\nThe changeset steps to address
    here are validating the array of values and sorting them. Why sort? We'll come
    back to that later. We can use the `@genre_options` module attribute in our changeset
    function to validate the user's data.\n\n```elixir\ndefp common_validations(changeset)
    do\n  changeset\n  # ...\n  |> validate_required([:name])\n  |> trim_array(:genres)\n
    \ |> validate_array(:genres, @genre_options)\n  |> sort_array(:genres)\nend\n```\n\n<aside
    class=\"right-sidenote\">\nInstead of creating our own `validate_array`, we could
    leverage Ecto's existing [`validate_subset/4`](https://hexdocs.pm/ecto/Ecto.Changeset.html#validate_subset/4)
    function to ensure only our listed `@genres_options` are accepted.\n</aside>\nNote
    that the `trim_array/2`, `validate_array/3` and `sort_array/2` functions don't
    exist in Ecto, we will create them. We want `trim_array` to handle data from our
    UI and `validate_array` to enforce that only values in our `@genre_options` list
    are accepted.\n\nWe'll talk more about `trim_array` in a bit. First, here's one
    way to build `validate_array`:\n\n```elixir\n@doc \"\"\"\nValidate that the array
    of string on the changeset are all in the\nset of valid values.\n\"\"\"\ndef validate_array(changeset,
    field, valid_values) do\n  validate_change(changeset, field, fn ^field, new_values
    ->\n    if Enum.all?(new_values, &(&1 in valid_values)) do\n      []\n    else\n
    \     unsupported = new_values -- valid_values\n      [{field, \"Only the defined
    values are allowed. Unsupported: #{inspect(unsupported)}\"}]\n    end\n  end)\nend\n```\n\nOur
    `validate_array` uses [Ecto.Changeset.validate_change/3](https://hexdocs.pm/ecto/Ecto.Changeset.html#validate_change/3)
    to run our custom validation function only when the field value changes. It checks
    each item in the new array to ensure they are part of the approved set of `valid_values`.
    If any are not valid, an error is added to the changeset.\n\n### Sort the Array\n\nIn
    this step, we'll process the tag values to sort them. Why sort it here? There
    are a couple reasons:\n\n1. We want the tag values to appear in a consistent order.
    For instance, when listing our books, we might display different books of a series
    together and they all share the same genre. It looks _wrong_ for the genres to
    appear in different order when viewed this way!\n    1. Harry Potter 1 - Genres:
    **young-adult, fantasy, adventure**\n    1. Harry Potter 2 - Genres: **adventure,
    young-adult, fantasy**\n    1. Harry Potter 3 - Genres: **fantasy, adventure,
    young-adult**\n1. It is more efficient to sort the values when setting them so
    later, when it's displayed, we won't be repeatedly doing the work to sort them.\n\nHow
    to sort them?\n\nThe values _could_ be sorted logically or according to a pre-determined
    order. However, here we'll take the simple approach of sorting them alphabetically.\n\n```elixir\n@doc
    \"\"\"\nWhen working with a field that is an array of strings, this\nfunction
    sorts the values in the array.\n\"\"\"\ndef sort_array(changeset, field) do\n
    \ update_change(changeset, field, &(Enum.sort(&1)))\nend\n```\n\nThe `sort_array`
    function leans heavily on the nifty [Ecto.Changeset.update_change/3](https://hexdocs.pm/ecto/Ecto.Changeset.html#update_change/3)
    function. When the field value is changed it runs it through the [Enum.sort/1](https://hexdocs.pm/elixir/Enum.html#sort/1)
    function.\n\n### Trimming the Array\n\nWe want to be able to clear the array,
    or replace it with an empty array. It works fine to do that in Elixir code, but
    this last step supports the UI needs of our code. This article doesn't deal with
    the UI, but if we [peer into the future](/phoenix-files/making-a-checkboxgroup-input/),
    here's the problem we need to address.\n\nHTML forms can't easily send an \"empty\"
    array. A simple way to deal with that is to have it send _something_ that indicates
    a blank value. Then, after removing the blank value, we'll be left with the array
    value we need. In our case, we'll assume that the blank value is `\"\"`. So, if
    the array value looks like this:\n\n```elixir\n%{\"genres\" => [\"\"]}\n```\n\nThen
    after trimming the list, it ends up like this:\n\n```elixir\n%{\"genres\" => []}\n```\n\nHere's
    a function to do that for us. Additionally, we'll let the \"blank\" value be specified.\n\n```elixir\n@doc
    \"\"\"\nRemove the blank value from the array.\n\"\"\"\ndef trim_array(changeset,
    field, blank \\\\ \"\") do\n  update_change(changeset, field, &Enum.reject(&1,
    fn item -> item == blank end))\nend\n```\n\n### Combining the changeset functions\n\nOur
    helper functions `validate_array/3`, `sort_array/2`, and `trim_array/3` can be
    moved to a separate module for easy reuse when needed using an `import MyApp.EctoHelpers`.\n\nWhile
    we're refactoring, we'll create a single function that does all three operations
    at once for us.\n\n```elixir\n@doc \"\"\"\nClean and process the array values
    and validate the selected\nvalues against an approved list.\n\"\"\"\ndef clean_and_validate_array(changeset,
    field, valid_values, blank \\\\ \"\") do\n  changeset\n  |> trim_array(field,
    blank)\n  |> sort_array(field)\n  |> validate_array(field, valid_values)\nend\n```\n\nNow
    our schema's `common_validations` can be cleaned up. Instead of 3 changeset functions
    for every field that works with some type of tags, it is instead a single line:\n\n```elixir\ndefp
    common_validations(changeset) do\n  changeset\n  # ...\n  |> validate_required([:name])\n
    \ |> clean_and_validate_array(:genres, @genre_options)\nend\n```\n\nThis is one
    of the powerful benefits of changeset functions that I love! \U0001F60D They are
    composable and easy to reuse!\n\nSee [this gist](https://gist.github.com/brainlid/9dcf78386e68ca03d279ae4a9c8c2373)
    for the full code including the [UI component](/phoenix-files/making-a-checkboxgroup-input/).\n\n###
    Setting the Array\n\nIt helps to see how the values are set. Writing tests is
    a great way to explore the behavior and ensure it works as we expect. For now,
    we can play with setting the array in an IEx session:\n\n```elixir\nalias MyApp.Books.Book\nchangeset
    = Book.changeset(%Book{}, %{genres: [\"historical-fiction\", \"thriller\"]})\n```\n\nNice!
    We can play with sending both valid and invalid tags and see the changeset validate
    and sort our array.\n\nBecause Ecto natively supports the array of string field
    type, we don't need to do anything to get the data back. It's just up to us to
    represent and display the tag data in a way that makes sense in our application.\n\nWith
    the ability to store tags in the database, we're ready to query our data using
    those tags. Let's see how that works!\n\n### Query Using Tags\n\nWhen it comes
    to querying our records using the tags, recall that we added the index to the
    field in the migration. This makes filtering by a tag something we can do without
    any hassle.\n\nLet's try it out!\n\n### How can we query for only books tagged
    as \"historical-fiction\"?\n\n```elixir\nimport Ecto.Query\nalias MyApp.Books.Book\nbooks
    = from(b in Book, where: \"historical-fiction\" in b.genres)\n```\n\nNice!\n\n###
    How do we query for multiple tags at the same time?\n\nWe don't want to say `where
    b.genres == ^genre_list` because that returns only the books that match the _exact_
    set of genres. We want to be able to find books where the genres include \"historical-fiction\"
    and \"thriller\" but may also include \"mystery\" or other genres.\n\nHere's how
    we write that query:\n\n```elixir\nimport Ecto.Query\nalias MyApp.Books.Book\nbooks
    =\n  from(b in Book,\n    where: \"adventure\" in b.genres,\n    where: \"fantasy\"
    in b.genres\n  )\n```\n\n### How do we query for \"adventure\" and \"fantasy\"
    but exclude all tagged as \"young-adult\"?\n\n```elixir\nimport Ecto.Query\nalias
    MyApp.Books.Book\nbooks =\n  from(b in Book,\n    where: \"adventure\" in b.genres,\n
    \   where: \"fantasy\" in b.genres,\n    where: \"young-adult\" not in b.genres\n
    \ )\n```\n\nMakes sense right? We can build up the conditions to help us filter
    down our results.\n\nArmed with the knowledge of how to query and filter using
    tags, we can write that logic into some handy helper functions to make querying
    easier.\n\n```elixir\ndef where_in_array(query, column, tag_value) do\n  from(q
    in query, where: ^tag_value in field(q, ^column))\nend\n\ndef where_not_in_array(query,
    column, tag_value) do\n  from(q in query, where: ^tag_value not in field(q, ^column))\nend\n```\n\nTheses
    query helpers let us write more composable queries like this:\n\n```elixir\nbooks
    =\n  from(b in Book)\n  |> where_in_array(:genres, \"adventure\")\n  |> where_in_array(:genres,
    \"fantasy\")\n  |> where_not_in_array(:genres, \"young-adult\")\n```\n\nGreat!
    We built the tagging feature we needed for our application! Users can tag a book
    with one or more of the allowed genre options and we validate the data and sort
    it when it's written.\n\nWe also saw how to query our list of books using the
    tagged genres!\n\n## Discussion\n\nWe can tag lots of things this way! Keep in
    mind that this approach works for systems where the possible tags are defined
    in code and the list does not come from the database. That happens to still apply
    to a lot of situations!\n\nOnce you start tagging records and you realize how
    easy it is, you may be tempted to tag all the things!\n\n![Tag all the things
    meme image](./tag-all-the-things-meme.jpg?center&card&2/3)\n\nBut don't go too
    crazy with it!\n\nHaving said that, I have added \"categories\" and \"tags\" to
    the same table because they had different meanings and uses in the system. Still,
    the approach covered here works just great for that!\n\nHappy tagging!\n\nWant
    more? Grab the code from [the gist](https://gist.github.com/brainlid/9dcf78386e68ca03d279ae4a9c8c2373)
    and check out [Making a CheckboxGroup Input](/phoenix-files/making-a-checkboxgroup-input/)
    to build a custom checkbox group input component for setting your tags with a
    Phoenix rendered UI!\n\n<%= partial \"shared/posts/cta\", locals: {\n  title:
    \"Fly.io ❤️ Elixir\",\n  text: \"Fly.io is a great way to run your Phoenix LiveView
    app close to your users. It's really easy to get started. You can be running in
    minutes.\",\n  link_url: \"https://fly.io/docs/elixir/\",\n  link_text: \"Deploy
    a Phoenix app today!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>\"\n} %>\n"
- :id: phoenix-files-copy-to-clipboard-with-phoenix-liveview
  :date: '2023-01-25'
  :category: phoenix-files
  :title: Copy to Clipboard with Phoenix LiveView
  :author: jason
  :thumbnail: phoenix-copy-thumbnail.png
  :alt: Illustration of a server reaching into your laptop and writing to your clipboard.
  :link: phoenix-files/copy-to-clipboard-with-phoenix-liveview
  :path: phoenix-files/2023-01-25
  :body: "\n\n<p class=\"lead\">\nIn this article we show how a LiveView and a snippet
    of JS can make it easier for users to copy an important value to their clipboard.
    Fly.io is a great place to run your Phoenix LiveView applications! Check out how
    to [get started](/docs/elixir/)!\n</p>\n\n## Problem\n\nWhen you need to make
    sure a user copies an important string of text, don't let their mouse and keyboard
    stand in the way. Give them a button to press so they don't need to manually move
    a cursor around.\n\nBut how might you do this with LiveView? It's not like our
    servers have access to the user's clipboard?\n\n## Solution\n\nFirst lets make
    our copy button. Put this wherever you need it!\n\n```xml\n<button phx-click={JS.dispatch(\"phx:copy\",
    to: \"#control-codes\")}>  \n  \U0001F4CB\n</button>\n```\n\nThis uses the [Phoenix.LiveView.JS.dispatch/2](https://hexdocs.pm/phoenix_live_view/Phoenix.LiveView.JS.html#dispatch/2)
    function to dispatch a JavaScript Custom Event to the element matching your selector.
    In this case the `#control-codes` element should contain the text we want to copy
    as its value.\n\n```html\n<input type=\"text\" id=\"control-codes\" value={@control-codes}
    />\n```\n\nNow lets wire up a little bit of javascript and append it to the bottom
    of our  `assets/js/app.js`:\n\n```javascript\nwindow.addEventListener(\"phx:copy\",
    (event) => {\nlet text = event.target.value; // Alternatively use an element or
    data tag!\n  navigator.clipboard.writeText(text).then(() => {\n    console.log(\"All
    done!\"); // Or a nice tooltip or something.\n  })\n})\n```\n\nAnd we've done
    it!  We have a simple generic handler for any and all copy functionality. If we
    \ add some CSS and dress it up, we could have something like this.\n\n<%= video_tag
    \"copy-to-clipboard.mp4?card&center&3/4&border\", title: \"\" %>\n\nIt's worth
    pointing out that this code runs 100% on the client. The `JS.dispatch` is essentially
    writing the JavaScript required for us to create the event and if we take a peak
    in the inspector we can see it!\n\n![](copy-to-clipboard-screenshot.png)\n\nThat's
    pretty neat if you ask me!\n\n### Client Hooks\n\nThis is not the only way to
    accomplish the JavaScript integration. We could use the [Client Hooks](https://hexdocs.pm/phoenix_live_view/js-interop.html#client-hooks-via-phx-hook)
    functionality. First,  we create our hook to handle the copy.\n\n```javascript\nlet
    Hooks = {}\n\nHooks.Copy = {\n  mounted() {\n    let { to } = this.el.dataset;\n
    \   this.el.addEventListener(\"click\", (ev) => {\n      ev.preventDefault();\n
    \     let text = document.querySelector(to).value\n      navigator.clipboard.writeText(text).then(()
    => {\n        console.log(\"All done again!\")\n      })\n    });\n  },\n}\n\nlet
    liveSocket = new LiveSocket(\"/live\", Socket, {hooks: Hooks, ...})\n```\n\nNow
    our button should look like below. Note that all Hooks need unique id's to work
    properly.\n\n```xml\n<button id=\"copy\" data-to=\"#control-codes\" phx-hook=\"Copy\">\n
    \ \U0001F4CB\n</button>\n```\n\nThis results in the same functionality with slightly
    more ceremony to accomplish it.  We achieved the same goal: client-side only code
    handles the click and copying for us!\n\n### Discussion\n\nWe could also imagine
    using this for all kinds of simple and commonly used JavaScript while having a
    shared interface to do it. Maybe you want to play a sound or focus an element,
    it is just JavaScript so the browser is the limit!\n\nThat's it! We walked through
    two approaches to copying text to a user's clipboard and hopefully learned a little
    bit about how Phoenix is helping write our JavaScript for us.\n\n<%= partial \"shared/posts/cta\",
    locals: {\n  title: \"Fly.io ❤️ Elixir\",\n  text: \"Fly.io is a great way to
    run your Phoenix LiveView app close to your users. It's really easy to get started.
    You can be running in minutes.\",\n  link_url: \"https://fly.io/docs/elixir/\",\n
    \ link_text: \"Deploy a Phoenix app today!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>\"\n}
    %>\n"
- :id: laravel-bytes-concurrent-tasks-on-machines
  :date: '2023-01-24'
  :category: laravel-bytes
  :title: Concurrent Tasks on Machines
  :author: fideloper
  :thumbnail: on-demand-thumbnail.png
  :alt: machines doing the needful
  :link: laravel-bytes/concurrent-tasks-on-machines
  :path: laravel-bytes/2023-01-24
  :body: "\n\n<p class=\"lead\">Fly takes a Docker image, converts it to a VM, and
    runs that VM anywhere around the world. [Run a Laravel app](https://fly.io/docs/laravel/)
    in minutes!</a>\n\nPreviously I wrote about spinning up Fly Machines to [run tasks
    \"on-demand\"](/laravel-bytes/on-demand-compute/). Machines are great because
    they stop when a task finishes - no need to waste (paid) CPU cycles.\n\nHowever,
    I hand-waved over the need for concurrency. To run multiple tasks at the same
    time, you need multiple Machine VMs! Let's explore some fun ways to manage that.\n\n##
    Tasks to Run\n\nWe saw a contrived example previously - the code retrieved some
    value from GitHub's API.\n\nThis time we'll create some queue workers, but with
    a bit of flair. Instead of running 24/7, we'll have the queue workers churn through
    available jobs and **then shut off**.\n\nTo do that, we'll run `php artisan queue:work
    --stop-when-empty`. That'll process jobs as long as there are jobs to be found,
    and then exit.\n\nSince the machine doesn't need to run 24/7, we won't be charged
    for unused CPU time!\n\nHowever, it begs the question: **How to start the stopped
    workers?**\n\n## Starting Machines\n\nIf our Machines are gonna stop, we need
    something to turn them on again.\n\nThis means we need some event to hook into.
    Here are some options:\n\n1. Periodically turn on the workers via [scheduled machines](https://community.fly.io/t/new-feature-scheduled-machines/7398)\n2.
    Have our code turn them on when needed\n\nLet's explore some fun things we can
    do, especially that 2nd option.\n\n## Scheduled Machines\n\nMachines can be [scheduled](https://community.fly.io/t/new-feature-scheduled-machines/7398).
    At the moment, the shortest interval currently available is every hour. That'll
    change, so if it's the future, I may have just lied to you.\n\nIn any case, we
    can say \"every hour, turn on these Machines, and churn through all available
    jobs in the queue\".\n\nHere's how to do that (note the `--schedule` flag):\n\n```bash\n#
    Run this for as many machines as you want\nfly m run -a on-demand \\\n    --env
    \"APP_ENV=production\" \\\n    --env \"LOG_CHANNEL=stderr\" \\\n    --env \"LOG_LEVEL=info\"
    \\\n    --schedule hourly \\\n    . \\\n    \"php\" \"artisan\" \"queue:work\"
    \"--stop-when-empty\"\n```\n\nWe want multiple machines, so we'd run the above
    command for as many machines as we want. Every hour, each machine will run `queue:work`,
    and process jobs until the given queue is empty.\n\n<div class=\"callout\">Reminder:
    This article assumes you have a Dockerfile in the current directory, likely the
    Dockerfile that was created by running `fly launch`. That assumption is based
    on the [previous, related article](/laravel-bytes/on-demand-compute/).</div>\n\n**This
    is nice, but not perfect.** What if we need a queue job to send an email ASAP?
    Do we really need to wait up to an hour to send that email?\n\nIt turns out that
    we can stagger the timing for each machine! The interval calcuated (hourly, daily,
    etc) *starts when a Machine is created*. If we create an hourly machine at 3:15pm,
    the next run will be ~4:15pm.\n\nIf you want to take the time to stagger machine
    creation over an hour or so, you can! This will let you run a machine every 15
    minutes, if you stagger the creation of 4 machines every 15 minutes (get some
    coffee)!\nYou can create more machines to fill up that hour as you'd like.\n\nThat
    of course is a little wonky. Let's see what other hijinks we can get into.\n\n##
    Start Machines via API\n\nScheduled Machines let you run tasks without thinking
    about when to start the VMs yourself (you don't need to build it into your application
    logic). That's nice, but the timing could be an issue.\n\nThe other thing you
    can do is build the logic into your application. We can programmatically start
    a given machine with [the Machines API](https://fly.io/docs/machines/working-with-machines/).\nThis
    might be useful if you fire a job into a queue, and then decide to start a machine
    to ensure one is around to process that job.\n\nIn Laravel, that would look something
    like this:\n\n```php\n$token = \"foo\";\n$app = \"on-demand\";\n$machine = \"xyz133\";\n$url
    = \"https://api.machines.dev/v1/apps/${app}/machines/${machine}/start\"\n\n$result
    = Http::asJson()\n    ->withToken($token)\n    ->post($url); \n```\n\nThe trick
    here is managing multiple machines! If you have multiple, you'll want to decide
    which to start. Perhaps you roll through them one by one and select the \"next\"
    machine on each call.\nPerhaps you just start *every* machine each time! It's
    up to you, but it's not a simple problem. You have to think carefully about when
    this is done so you don't accidentally end up having jobs sit in queue longer
    than makes sense.\n\nOne thing you can do is use [Laravel's Scheduler](https://laravel.com/docs/9.x/scheduling)
    to start machines up to every minute (since it's based on CRON).\nThis is especially
    nice if you calculate the number of jobs in a queue before deciding to turn on
    a worker.\n\nThat might look like this:\n\n```php\n$token = \"xyz\";\n$machines
    = [\"aaabbbccc\", \"xxxyyyzzz\", ...]\n$app = \"on-demand\";\n$url = \"https://api.machines.dev/v1/apps/%s/machines/%s/start\"\n\nif
    (Queue::size('some-queue') > 0) {\n    foreach($machines as $machine) {\n        Http::asJson()\n
    \           ->withToken($token)\n            ->post(sprintf($url, $app, $machine));
    \n    }\n    \n}\n```\n\n## Start Machine via HTTP\n\nOne really nice feature
    of Machines is the ability to wake on network access. This means a machine can
    be created, but not running. Once a network request is made to the machine, it
    will be started!\n\nIf we have 2+ Machines listening for HTTP requests within
    an app, the Fly Proxy will actually load balance requests amongst the Machines!
    So, another tricky way to start a Machine is to send an HTTP request to it.\n\nTo
    do this little bit of fanciness, we need to create our [machines via API](https://fly.io/docs/machines/working-with-machines/#create-a-machine)
    to set some specific options. Here's what that looks like:\n\n```bash\n# Be sure
    image \"registry.fly.io/on-demand:latest\" exists\n# and has been pushed, as described
    here:\n# https://fly.io/laravel-bytes/on-demand-compute/#create-a-machine-vm\n\n\nFLY_API_TOKEN=\"$(fly
    auth token)\"\nFLY_APP_NAME=\"on-demand\"\n\ncurl -X POST \\\n    -H \"Authorization:
    Bearer ${FLY_API_TOKEN}\" \\\n    -H \"Content-Type: application/json\" \\\n    \"https://api.machines.dev/v1/apps/${FLY_APP_NAME}/machines\"
    \\\n    -d '{\n  \"config\": {\n    \"image\": \"registry.fly.io/on-demand:latest\",\n
    \   \"processes\": [\n      {\n        \"name\": \"start-worker\",\n        \"cmd\":
    [\"php\", \"artisan\", \"queue:work\", \"--stop-when-empty\"],\n        \"env\":
    {\n          \"APP_ENV\": \"production\",\n          \"LOG_CHANNEL\": \"stderr\",\n
    \         \"LOG_LEVEL\": \"info\"\n        }\n      },\n      {\n        \"name\":
    \"web-requests\",\n        \"cmd\": [\"php\", \"-S\", \"[::0]:8080\", \"-t\",
    \"start\"],\n        \"env\": {\n          \"APP_ENV\": \"production\",\n          \"LOG_CHANNEL\":
    \"stderr\",\n          \"LOG_LEVEL\": \"info\"\n        }\n      }\n    ],\n    \"services\":
    [\n      {\n        \"ports\": [\n          {\n            \"port\": 443,\n            \"handlers\":
    [\n              \"tls\",\n              \"http\"\n            ]\n          },\n
    \         {\n            \"port\": 80,\n            \"handlers\": [\n              \"http\"\n
    \           ]\n          }          \n        ],\n        \"concurrency\": {\n
    \         \"type\": \"requests\",\n          \"soft_limit\": 1\n        },\n        \"protocol\":
    \"tcp\",\n        \"internal_port\": 8080\n      }\n    ]\n  }\n}'\n```\n\nThere's
    a few things going on here!\n\n**First, we define two processes to run in the
    VM!** The first is the queue worker (`artisan queue:work...`). The second is an
    HTTP listener that just serves our application using `php -S [::0]:8080 -t start`.\nThis
    is PHP's built-in web server. It's using a directory `start` as its web root.
    That directory just contains an empty `index.html` file:\n\n```bash\n# From your
    project's root directory:\nmkdir start\ntouch start/index.html\n```\n\nThis lets
    us make web requests into the Machine without having to make our Laravel application
    function (by setting needed environment variables, etc). That being said, you'll
    need those env vars/secrets for the queue worker to work, so you likely could
    use `php artisan serve --port 8080` if you wanted.\nIn any case - to wake up the
    machine, all we need is something that listens for HTTP requests.\n\nNote: If
    either of the two processes exits with status `0` (success), then the Machine
    will stop. This means we don't have to worry about the HTTP listener staying on
    forever and keeping the Machine awake.\n\n**Second, we add a `service` definition**.
    This lets the proxy know that we're expecting HTTP requests. We set `concurrency`
    to a single request at a time (via `soft_limit`), telling the proxy to spread
    each request as evenly as possible amongst Machines listening for web requests.\n\n###
    Waking it Up\n\nTo wake the machine up, we need to send an HTTP request to it.
    The magic of waking up a machine only works if we send requests through the Fly
    Proxy. This means that if we stay within our private network and do something
    like\n`curl machine-id.vm.<app-name>.internal:8080`, it won't wake the machine
    up. This sends the request directly to the VM without going through the Fly Proxy.\n\nTo
    use the Fly Proxy, we need to allocate it an IP address. We can allocate a private-only
    IPv6 address like so:\n\n```bash\n# Use your own app name, mine is \"on-demand\"\nfly
    ips allocate-v6 --private -a on-demand\n```\n\nNow, to wake up your machines,
    you can make curl requests to that IP address:\n\n```php\n# Assume the IPv6 I
    got was fdaa:0:6ba9:0:1::2\n$response = Http::get(\"http://[fdaa:0:6ba9:0:1::2]\")\n$response->status();
    // 200\n```\n\nWe send requests to port 80, as the `services` portion of our Machine
    configuration listens on port 80, and forwards those requests to port 8080 in
    the machine (where `php -S` is listening).\n\nSince the IP address is allocated
    to the application (not the individual Machine VM), this will load balance amongst
    the Machines, turning them on if they are off.\nYou can keep making requests to
    this for as many Machines as you want enabled! This may require a bit [of concurrency](https://laravel.com/docs/9.x/http-client#concurrent-requests):\n\n```php\n#
    If we have 3 machines we want on\n# See https://laravel.com/docs/9.x/http-client#concurrent-requests\n$responses
    = Http::pool(fn (Pool $pool) => [\n    $pool->get(\"http://[fdaa:0:6ba9:0:1::2]\"),\n
    \   $pool->get(\"http://[fdaa:0:6ba9:0:1::2]\"),\n    $pool->get(\"http://[fdaa:0:6ba9:0:1::2]\"),\n]);\n```\n\nNote
    that when allocating a private IP address, Fly.io doesn't have a hostname you
    can use that routes requests through the Fly Proxy. If you want a hostname, you'll
    have to set one up yourself on a domain you own.\nYou can set a DNS record on
    your domain and point it to the private IP address.\n\nIf you need to send requests
    to your application over the public internet, you can allocate an IP that is NOT
    private (omit the `--private` flag), and send requests to `<your-app>.fly.dev`
    (or use your own domain).\n\n\n### What We Did\n\nThis is a fun and tricky way
    to let our application just send some HTTP requests to Fly whenever we want our
    workers running. Fly will handle deciding which Machine VM to start.\n\nSending
    a request will start a stopped Machine. Since we defined 2 processes, both the
    queue worker and the web server will be started. Our jobs will be processed, and
    the queue worker will eventually exit. The Machine will then stop.\n"
- :id: laravel-bytes-displaying-fly-replay-livewire
  :date: '2023-01-23'
  :category: laravel-bytes
  :title: Delayed Display of Isolated PDFs with Fly-replay and Livewire
  :author: kathryn
  :thumbnail: space-robot-thumbnail.jpg
  :alt: A space balloon creature with 6 hands. 3 On the left handles a remote, a button,
    and a droplet. 2 on the right holds a file and sifts through a record of documents
    separated into two boxes. The background is set in a gradiently pink and dark
    outerspace.
  :link: laravel-bytes/displaying-fly-replay-livewire
  :path: laravel-bytes/2023-01-23
  :body: "\n<p class=\"lead\">In this article we'll tackle file isolation in a multi-region
    Laravel Fly App, and display PDFs in a deferred manner. \nRun your Laravel app
    with Fly.io, [it takes only a few minutes](/docs/laravel)!</p>\n\nRunning our
    Laravel application close to our users reduces geographical latency. What's more,
    with [Fly.io](/), we get to easily do global deployments with [just a few commands](/laravel-bytes/taking-laravel-global/#:~:text=scaled%20out%20to%20also%20include%20Frankfurt%20and%20Singapore%3A)!
    \n\nHowever, as we've established in [Taking Laravel Global](/laravel-bytes/taking-laravel-global/#problem-file-storage),
    files stored in one region are not automatically available to instances in other
    regions.\n\nTo address this region-isolation dilemma, today we'll use the `fly-replay`
    [response header](/docs/reference/regional-request-routing/#the-fly-replay-response-header)
    to direct file retrieval requests to the correct regional instance. Alongside
    which we'll use [Livewire's](https://laravel-livewire.com/) `wire:init` [to speed
    up](https://laravel-livewire.com/docs/2.x/defer-loading#introduction) initial
    loading of pages displaying our files.\n\nHere's a [reference repository](https://github.com/KTanAug21/fly.io-livewire-snippets/blob/master/_readme/display_isolated_fly_replay_livewire.md)
    you can read and a [demo page](https://ktan-app.fly.dev/documents) you can inspect.\n\n##
    The Problem\nLet's say we have a [Laravel Fly App](/docs/laravel/) with instances
    running in [AMS, FRA, and SIN](/docs/reference/regions/#fly-io-regions).\n\nThis
    Laravel application stores PDFs in each instance's local storage folder and allows
    users to view these PDFs in the browser window. \n\nIf a `first_title.pdf` file
    was stored in the AMS region of our Laravel Fly Application, by default, this
    PDF file will only be accessible to users accessing the AMS instance, but not
    from instances in FRA or SIN. \n\nFurthermore, even if instances from different
    regions get to access files stored in other regions, cross-regional data transfer
    can affect the hold up in pages displaying the file.\n\n## Solution \n\nWhen it
    comes to accessing regional-specific storage, we simply need to redirect requests
    and talk to the right instance containing a requested PDF. \n\nWe can easily do
    so by instructing Fly.io to redirect the request to the correct regional instance
    with its `fly-replay` [response header](/docs/reference/regional-request-routing/#the-fly-replay-response-header).\n\nAfterwards,
    to speed up the initial load-up of the page displaying the PDF file, we can use
    Livewire's `wire:init` directive to defer loading our PDF file until the page
    has completed loading.\n\n### Locally stored, DB tracked, Replay Available\nIn
    order to make use of `fly-replay`, we need to know the region our PDF file is
    stored in. \n\nFly.io provides us a `FLY_REGION` [environment variable](/docs/reference/runtime-environment/#fly_region)
    which we can use to identify the region our instance is deployed in. We can add
    this in our [config/app.php](https://github.com/KTanAug21/fly.io-livewire-snippets/blob/master/config/app.php#L42):\n```php\n'fly_region'
    => env( 'FLY_REGION' ),\n```\nThen keep track of this region in the PDF details
    we store in our database:\n<aside class=\"right-sidenote\">\nMulti-region instances
    benefit greatly with talking to databases in close proximity. Check out running
    your Laravel Fly App with Multi-region [MySQL](/laravel-bytes/multi-region-laravel-with-planetscale/)
    or [SQLite](/docs/laravel/advanced-guides/global-sqlite-litefs/).\n</aside>\n```php\nDB::table('file_records')->insert([\n
    \   'full_path' => 'storage/uploads/'.$fileName,\n    'region_id' => config('app.fly_region')\n]);\n```\nStoring
    a `first_title.pdf` file above should give us the following row in our database:\n<aside
    class=\"right-sidenote\">\nMake sure to persist stored files through the use of
    [volumes](/docs/reference/volumes/). Here's [a quick guide](/docs/laravel/the-basics/laravel-volume-storage/)
    to persist data on the storage folder.\n</aside>\n```json\nfull_path: \"storage/uploads/first_title.pdf\",\nregion_id:
    \"ams\"\n```\n\nWith the above setup, we now have a record of our stored PDF,
    and the region it is stored in. We can start using `fly-replay` to forward retrieval
    requests to the right instance to get our `first_title.pdf` file!\n\n<%= partial
    \"shared/posts/cta\", locals: {\n  title: \"Fly.io ❤️ Laravel\",\n  text: \"Fly
    your servers close to your users&mdash;and marvel at the speed of close proximity.
    Deploy globally on Fly in minutes!\",\n  link_url: \"https://fly.io/docs/laravel\",\n
    \ link_text: \"Deploy your Laravel app!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>\",\n}
    %>\n \n### Talking to the right instance with Fly-replay \n\nTo make our `first_title.pdf`
    file accessible outside of the AMS region, we'll have to  make sure all retrieval
    requests for the file \"talk\" to our instance running in the AMS region.\n\nIf
    the current instance's region is not the same as the `region_id` recorded for
    `first_title.pdf`, we stop processing the request in the current instance, and
    instead return a `fly-replay` [response header](/docs/reference/regional-request-routing/#the-fly-replay-response-header/)
    containing the region to forward the request to.\n\n<aside class=\"right-sidenote\">\nFor
    visibility on the whole picture, you can read [this Controller file](https://github.com/KTanAug21/fly.io-livewire-snippets/blob/master/app/Http/Controllers/DocumentController.php#L27)
    for reference.\n</aside>\n\n```php\n  // Decide replay\n  if( $pdfDetails->region_id
    != config('app.fly_region') ){     \n      \n      // Replay to identified region\n
    \     return response('', 200, [\n          'fly-replay' => 'region='.$pdfDetails->region_id
    ,\n      ]);\n\n  }else{\n\n      // FileName\n      $fileName = explode('/',
    $pdfDetails->full_path);\n      $fileName = $fileName[(count($fileName))-1];\n\n
    \     // Accessible File Path\n      $filePath = Storage::path( $pdfDetails->full_path
    );\n\n      // Respond with File\n      return response()->file( $filePath );\n\n
    \ }\n```\n\nOnce the response header is returned by the application instance,
    Fly.io's proxy layer will immediately recognize the `fly-replay` response header.
    It will then replay the request to the region specified in its `region=<region_id>`
    value.\n\n### Displaying PDF iframes with wire:init\nNow that PDF files stored
    in different regions are accessible from the rest, we can proceed with displaying
    our PDF files with the use of iframes.\n\nWe'll ask our iframe to retrieve the
    file from a route containing the file retrieval logic specified in our section
    [above](/laravel-bytes/displaying-fly-replay-livewire/#talking-to-the-right-instance-with-fly-replay).\n```php\n<iframe
    src=\"{{ URL::to('/files/show/'.$recordId ) }}\"></iframe>\n```\n\nPlacing this
    iframe tag in a page will hold up page load until our file gets successfully loaded.
    We can easily avoid this hold up by using Livewire's `wire:init` [directive](https://laravel-livewire.com/docs/2.x/defer-loading).
    \n\nLet's [create a Livewire component](https://laravel-livewire.com/docs/2.x/making-components)
    to defer loading our iframe tag:\n\n<aside class=\"right-sidenote\">\nHere's the
    whole picture for our [Livewire view](https://github.com/KTanAug21/fly.io-livewire-snippets/blob/master/resources/views/livewire/show-pdf.blade.php)
    and [Livewire Component](https://github.com/KTanAug21/fly.io-livewire-snippets/blob/master/app/Http/Livewire/ShowPdf.php).
    \n\nAnd a bonuse reference on [initializing](https://github.com/KTanAug21/fly.io-livewire-snippets/blob/master/resources/views/documents/index.blade.php#L32)
    and [updating](https://github.com/KTanAug21/fly.io-livewire-snippets/blob/master/resources/views/documents/index.blade.php#L42)
    our Livewire component with a PDF record to render.\n</aside>\n```php\n# In the
    Livewire View\n<div id=\"showFileComponent\">\n   @if( $loadAllowed==true  )\n
    \     <iframe src=\"{{ URL::to('/files/show/'.$recordId ) }}\"></iframe>\n   @else\n
    \     Loading PDF\n   @endif\n</div>\n```\nIn the case above, we only load the
    iframe when the public attribute `$loadAllowed` is true. \n\nWe'll initially set
    our `$loadAllowed` to false, and only change it to true through the method `allowFileLoading`:\n\n```php\n#
    In the Livewire Component\nclass ShowFile extends Component\n{\n    public $recordId;\n
    \   public $loadAllowed = false;\n\n    public function allowFileLoading()\n    {\n
    \       $this->loadAllowed = true;\n    }\n```\nAnd finally, let's add in `wire:init=\"allowFileLoading\"`
    to tell the Livewire component that once it completes rendering, it needs to call
    the `allowFileLoading` method. \n\n```php\n# In the Livewire View\n<div id=\"showFileComponent\"
    wire:init=\"allowFileLoading\">\n```\nAs seen above, calling the `allowFileLoading`
    method updates the `$loadAllowed` to true. Doing so loads the iframe tag which
    calls our Laravel route `files/show/{recordId}`.\n\n\n## Discussion\n\nRunning
    multi-region instances of our Laravel application means an effort from us to empathize
    on and close geographical latency for our users across the globe. \n\nAt the same
    time, it also means addressing regional-isolation of files and providing file-accessibility
    to our users. \n\nToday, we addressed file isolation by talking with the right
    instance using `fly-replay`, and improved loading of pages displaying these cross-regional
    files through the use of deferred displaying with `wire:init`.\n\nOf course, this
    article only focuses on accessibility for file-isolation, there are other equally
    important points for consideration. \n\nOne is Database consistency and [performance
    optimization](/laravel-bytes/taking-laravel-global/#problem-the-database). We
    have guides to address those with multi-region [MYSQL with PlanetScale](/laravel-bytes/multi-region-laravel-with-planetscale/),
    and multi-region [SQLite with LiteFS](/docs/laravel/advanced-guides/global-sqlite-litefs/).\n\nFor
    a checklist to mull over, further reading is advised on [Taking Laravel Global](/laravel-bytes/taking-laravel-global/).\n\n\n\n\n\n\n\n\n"
- :id: ruby-dispatch-rails-on-docker
  :date: '2023-01-19'
  :category: ruby-dispatch
  :title: Rails on Docker
  :author: brad
  :thumbnail: rails-on-docker-thumbnail.jpg
  :alt: Container ship carrying a train
  :link: ruby-dispatch/rails-on-docker
  :path: ruby-dispatch/2023-01-19
  :body: |2


    **Rails 7.1 is getting an** [**official Dockerfile**](https://github.com/rails/rails/pull/46794)**, which should make it easier to deploy Rails applications to production environments that support Docker. Think of it as a pre-configured Linux box that will work for most Rails applications.**

    That means you'll start seeing a `Dockerfile` in the project directory of a lot more Rails apps. If you're not familiar with Docker, you might open the file and see a few things that look familiar like some bash commands, but some other things might be new and foreign to you.

    Let's dive into what's in a `Dockerfile` so its less of a mystery, but first let's have a look at how Fly.io uses Docker so you better understand how Docker fits into a Rails stack.

    ## How does Fly.io use Docker?

    Fun fact! Fly.io doesn't actually run Docker in production—rather it uses a Dockerfile to [create a Docker image, also known as an OCI image, that it runs as a Firecracker VM](https://fly.io/blog/docker-without-docker/). What does that mean for you? Not much really. For all practical purposes you'll describe your applications' production machine in a Dockerfile and Fly.io transparently handles the rest.

    The great thing about Dockerfiles is it makes standardizing production deployments possible, which for most developers means its easier to deploy applications to hosts that support Docker, like [Fly.io](/docs/rails).

    What's a Dockerfile? It's a text file with a bunch of declarations and Linux commands that describe what needs to be installed and executed to get an application running. This file is given to a bunch of fancy software that configures a Linux distribution to the point where it can run your application.

    You can think of each command in the file as a "layer". At the bottom of the layer is a Linux distribution, like Ubuntu. Each command adds another layer to the configuration until eventually all the packages, configurations, and application code are in the container and your app can run. This layering is important for caching commands, which make deployments fast if done correctly.

    Let's get into it.

    ## A closer look at the Rails Dockerfile

    At the time of this writing, the default Rails 7.1 Dockerfile looks like:

    ```dockerfile
    # Make sure it matches the Ruby version in .ruby-version and Gemfile
    ARG RUBY_VERSION=3.2.0
    FROM ruby:$RUBY_VERSION

    # Install libvips for Active Storage preview support
    RUN apt-get update -qq && \
        apt-get install -y build-essential libvips && \
        apt-get clean && \
        rm -rf /var/lib/apt/lists/* /usr/share/doc /usr/share/man

    # Rails app lives here
    WORKDIR /rails

    # Set production environment
    ENV RAILS_LOG_TO_STDOUT="1" \
        RAILS_SERVE_STATIC_FILES="true" \
        RAILS_ENV="production" \
        BUNDLE_WITHOUT="development"

    # Install application gems
    COPY Gemfile Gemfile.lock ./
    RUN bundle install

    # Copy application code
    COPY . .

    # Precompile bootsnap code for faster boot times
    RUN bundle exec bootsnap precompile --gemfile app/ lib/

    # Precompiling assets for production without requiring secret RAILS_MASTER_KEY
    RUN SECRET_KEY_BASE_DUMMY=1 bundle exec rails assets:precompile

    # Entrypoint prepares the database.
    ENTRYPOINT ["/rails/bin/docker-entrypoint"]

    # Start the server by default, this can be overwritten at runtime
    EXPOSE 3000
    CMD ["./bin/rails", "server"]
    ```

    At the top of the file, we set the Ruby version.

    ```dockerfile
    # Make sure it matches the Ruby version in .ruby-version and Gemfile
    ARG RUBY_VERSION=3.2.0
    FROM ruby:$RUBY_VERSION
    ```

    The version gets plugged into the `FROM` command, which ends up looking like `FROM ruby:3.2.0`. Where is `ruby:3.2.0`? It's a [Docker image](https://hub.docker.com/_/ruby/) that some community members have graciously configured for us that gets us a Linux distribution running Ruby 3.2. That's not enough to run a Rails application; we need to add a few more layers to the image.

    Next up the Dockerfile installs Linux packages needed to run certain Rails gems in Linux. `libvibs` is a native library used to resize images for ActiveSupport. Other packages could be added here, like a Postgres, MAQL, or SQLite client. Other gems may depend on Linux packages too. For example, a popular XML parsing library, Nokogiri, depends on libxml. Those are not included in this list because the `ruby` image already includes them.

    ```dockerfile
    # Install libvips for Active Storage preview support
    RUN apt-get update -qq && \
        apt-get install -y build-essential libvips && \
        apt-get clean && \
        rm -rf /var/lib/apt/lists/* /usr/share/doc /usr/share/man
    ```

    `apt-get` is a Linux package manager that always looks strange in a Dockerfile because of all the command it does before and after installing a package. Let's break it down line-by-line.

    First, `apt-get update -qq` tells Linux to download a manifest of all the packages that are available for download from `apt-get`.

    The second line is the one you care about and might need to change. `apt-get install -y build-essential libvips` installs two packages and the `-y` automatically answers "yes" when it asks if you're sure you want to install the packages.

    Everything after that removes the manifest files and any temporary files downloaded during this command. It's necessary to remove all these files in this command to keep the size of the Docker image to a minimum. Smaller Dockerfiles mean faster deployments.

    Next the working directory is set.

    ```dockerfile
    # Rails app lives here
    WORKDIR /rails
    ```

    This creates the `./rails` folder inside the docker image. All of the lines in the Dockerfile after this are run from that directory and any files added to the image are put in that directory. It's the equivalent of `mkdir -p ./rails && cd ./rails`.

    Next a few environment variables are set.

    ```dockerfile
    # Set production environment
    ENV RAILS_LOG_TO_STDOUT="1" \
        RAILS_SERVE_STATIC_FILES="true" \
        RAILS_ENV="production" \
        BUNDLE_WITHOUT="development"
    ```

    What are these you ask?

    - `RAILS_LOG_TO_STDOUT` - Rails log output is sent to STDOUT instead of a file. STDOUT, or standard out, makes it possible for `docker logs` to view the output of whatever is running on the container. [The Twelve-Factor App has a good explanation](https://12factor.net/logs) of why logs should be written to STDOUT.
    - `RAILS_SERVE_STATIC_FILES` - This instructs Rails to _not_ serve static files. It's always been recommended to have a server like nginx serve up images, CSS, JavaScripts, and other static files by a server that's not Ruby for better performance.
    - `RAILS_ENV` - Instructs Rails to boot with `production` gems and with the configuration from `config/environments/production.rb`.
    - `BUNDLE_WITHOUT` - If you've ever looked in your application's Gemfile, you'll notice there's gems tagged with the `development` group like `web-console`. These gems are not needed or wanted in a production environment because they would either slow things down, not be used, or pose a security risk. This command tells bundler to leave out all the `development` gems.

    Time to install the gems! First Docker copies the `Gemfile` and `Gemfile.lock` from our workstation or CI's server project directory into the containers `./rails` directory (remember the thing that was set by `WORKDIR` above? This is it!)

    ```dockerfile
    # Install application gems
    COPY Gemfile Gemfile.lock ./
    RUN bundle install
    ```

    `bundle install` is run against the Gemfiles that were copied over. This installs the gems inside the container, which we'll need for our Rails application to run.

    Something you might be asking yourself, "why not copy the entire application from my workstation and then run `bundle install`?". Great question! Each "ALLCAPS" directive in a Dockerfile, like `RUN`, `COPY`, etc. are "layers" that get cached. If you didn't handle copying the Gemfile and running bundler as a separate layer, you'd have to run `bundle install` every time you deployed Rails, even if you didn't change the gem. That would take forever!

    Making it a separate layer means you only have to update the bundle when the `Gemfile` changes. In other words, if you only change application code, you can skip running bundle and jump right into the next layer, which saves loads of time between deploys.

    Finally the Rails application code files are copied from your computer or CI machine to the `WORKDIR` set above, which is `./rails` in the image.

    ```dockerfile
    # Copy application code
    COPY . .
    ```

    When we boot the Docker image and the Rails server, we want it to come online as quickly as possible so our deploys are faster, so the image copies over the bootsnap cache to make that happen.

    ```dockerfile
    # Precompile bootsnap code for faster boot times
    RUN bundle exec bootsnap precompile --gemfile app/ lib/
    ```

    At this point all the files needed to run the server are copied over from the workstation to the Docker image, with the exception of files listed in `.dockerignore`which typically include the `.git` directory, log files, etc.

    Now its time to compile JavaScript, stylesheet, and image assets!

    ```dockerfile
    # Precompiling assets for production without requiring secret RAILS_MASTER_KEY
    RUN SECRET_KEY_BASE_DUMMY=1 bundle exec rails assets:precompile
    ```

    This is a bit if a hack. Rails _requires_ a secret key to keep sessions and other cryptographic Rails features secure, but for an asset compilation, including the actual secret key is not needed and is therefore a liability. Instead `SECRET_KEY_BASE_DUMMY=1` is passed into the compilation task to tell Rails, "ignore requiring a secret key".

    The most important part of this command is `bundle exec rails assets:precompile`, which runs whatever compilation steps are needed to minify and fingerprint assets so they load quickly in production.

    The `ENTRYPOINT` directive in Docker acts like a wrapper.

    ```
    # Entrypoint prepares the database.
    ENTRYPOINT ["/rails/bin/docker-entrypoint"]
    ```

    The best thing to do is look at the contents of the `ENTRYPOINT` script, which lives at `./bin/docker-entrypoint`

    ```bash
    #!/bin/bash

    # If running the rails server then create or migrate existing database
    if [ "${*}" == "./bin/rails server" ]; then
      ./bin/rails db:prepare
    fi

    exec "${@}"
    ```

    The script checks to see if the `CMD`, below, is running `./bin/rails server`. If its running the server, it will make sure it runs a database migration before it boots the application. If you don't want Rails to automatically run migrations when you deploy, you could comment out or remove the `ENTRYPOINT` directive in the `Dockerfile`.

    The last thing in all Dockerfiles is how to boot the application server.

    ```dockerfile
    # Start the server by default, this can be overwritten at runtime
    EXPOSE 3000
    CMD ["./bin/rails", "server"]
    ```

    `EXPOSE 3000` tells Docker the application will listen on port 3000, which is the default for `bin/rails server`, as you see in your Rails development environment.


    ## Take it for a spin

    You can expect a lot of changes between now and when Rails 7.1 is released. For example, we're currently exploring extracting Dockerfile generation out of `railties` and moving it into its own gem at [https://rubygems.org/gems/dockerfile-rails](https://rubygems.org/gems/dockerfile-rails). That means you can use this today with your current Rails project. If you want to try it out, first install the gem to your Rails app.

    ```bash
    $ bundle add dockerfile-rails
    ```

    Then generate the `Dockerfile` with the `dockerfile` command.

    ```bash
    $ ./bin/rails dockerfile
    ```

    Then checkout the `Dockerfile` that's now at the root of your project. You can then deploy it by [installing Fly.io](https://fly.io/docs/flyctl/installing/), running the following command, and following the instructions.

    ```bash
    $ fly launch
    ```

    Fly.io will ask you a few questions and within a few minutes, you should see your Rails app running in production.

    ## There's a lot of different ways to configure a Dockerfile

    The official Rails Dockerfile will be a great starting place for most people, but as applications grow in complexity and the need to install additional packages arises, it might not be enough.

    Fly.io has started putting together a collection of Dockerfile recipes in the [Fly Rails Cookbooks](https://fly.io/docs/rails/cookbooks/). You'll find example Dockerfiles for all sorts of different Rails deployments including those that needs Node 19+ installed or for Rails API deployments.
- :id: laravel-bytes-invite-only-registration
  :date: '2023-01-17'
  :category: laravel-bytes
  :title: Invite-only Registration
  :author: johannes
  :thumbnail: invite-only-thumbnail.jpg
  :alt: a bouncer letting a person in because he has an invite letter.
  :link: laravel-bytes/invite-only-registration
  :path: laravel-bytes/2023-01-17
  :body: "\n\n<p class=\"lead\">Fly takes a Docker image, converts it to a VM, and
    runs that VM anywhere around the world. [Run a Laravel app](https://fly.io/docs/laravel/)
    in minutes!</p>\n\nHow many of you remember the launch of Oneplus' first product?
    It was called the Oneplus One and it was marketed as the *flagship killer*. The
    name was warranted: It had flagship specs for a lower price than the flagships
    that time. I remember buying one for $350!\n\nBut with the launch of its first
    phone, Oneplus had some issues to overcome: they were a new company so they didn't
    have the brand awareness their competitors had, and their production wasn't up
    to scale when they launched the phone. So, they created an invite system: they'd
    send invites to the first number of people on the waiting list, and activate the
    invite whenever their phone was manufactured and ready to ship. When the customers
    received their phones, they also received one invite to give to a friend or family
    member. This turned the group of Oneplus One owners into an exclusive club where
    you needed an invite to get in. They created hype and permitted them to stagger
    the release over the time it took to ramp up production. Great stuff!\n\nWhat
    if we wanted to create an app with a similar invite system, like a social network
    maybe, or a subscription-based application? Today, we're taking a look at how
    such invite-based registration might work in a Laravel app. We're going to pick
    up where the [user levels, policies and enums](https://fly.io/laravel-bytes/user-levels-enums-and-policies-oh-my/)
    article ended, and here's the [github repository](https://github.com/Johannes-Werbrouck/policies-levels-enums).\n\n##
    The Goal\n\nWe want to prevent users from registering the default way, and instead
    send them an invite. The invite will be an email that contains a link where the
    user can then register, and fill out extra information.\n\nWe'll continue to use
    the user levels that we already have: Only admins will be allowed to send out
    invites, and the admin will pick the user level for the invited user. We'll also
    make sure that users that register using the link in their email inbox won't need
    to verify their email.\n\nAlong the way, we'll learn about these Laravel topics:\n\n-
    Middleware\n- Throwing Exceptions\n- Notifications\n- User Feedback\n- Signed
    URLs\n\n## Setting it up\n\nLet's set up the app: Get the code form the [github
    repository](https://github.com/Johannes-Werbrouck/policies-levels-enums), run
    it and run the migrations. Register an account and go to the users page. Notice
    something? The only user, the one we've just created, is not an admin! We want
    all our users to register using an invite, but who will send the first invite?\n\nLet's
    make it so the first user can register the default way and gets assigned the `admin`
    User Level. Once the first user has registered, we should then close that door
    after them. The door being the `register` route, of course.\n\n### Making sure
    the first user gets the 'admin' user level\n\nLet's begin with tackling the first
    issue: make the first user always an admin. Since they are the only user to use
    the `register` route, we can say that any user that registers this way can be
    an admin. Let's look up that route in `web.php` to see what controller it uses!
    Can you find it? Because I sure can't. That's because it's in `auth.php` in the
    same directory, you can see the import for it in `web.php` at the bottom:\n\n```php\nrequire
    __DIR__.'/auth.php';\n```\n\nIn `auth.php` we can see that the `register` route
    uses the `RegisteredUserController`, and that the post request will hit the `store`
    method. Makes sense, right? So, let's make sure new users that register are always
    registered as admin:\n\n```diff\n// app/Http/Controllers/Auth/RegisteredUserController.php\n\n+
    use App\\Enums\\UserLevel;\n\n...\n\npublic function store(Request $request)\n
    \ {\n    $request->validate([\n      'name' => ['required', 'string', 'max:255'],\n
    \     'email' => ['required', 'string', 'email',\n                  'max:255',
    'unique:'.User::class],\n      'password' => ['required', 'confirmed', Rules\\Password::defaults()],\n
    \   ]);\n\n    $user = User::create([\n      'name' => $request->name,\n      'email'
    => $request->email,\n      'password' => Hash::make($request->password),\n+     'level'
    => UserLevel::Administrator,\n    ]);\n\n    event(new Registered($user));\n\n
    \   Auth::login($user);\n\n    return redirect(RouteServiceProvider::HOME);\n
    \ }\n```\n\nSo, let's see if it works now, shall we? Refresh your database with
    `php artisan migrate:fresh` or if you are using Sail, use `sail artisan migrate:fresh`.
    Then, register again and now you should see you're an admin!\n\n![](1_It_worked!.png)\n\n###
    Closing the door after the first user\n\nThe second issue is that we need to disable
    the `register` route if there is already a registered user. Closing off a part
    of our application based on a condition just screams middleware, so that's what
    we're going to add!\n\nuse `php artisan make:middleware FirstUser` to make a new
    middleware. In there, change the `handle` method like this:\n\n```diff\n+ use
    App\\Models\\User;\n  use Closure;\n  use Illuminate\\Http\\Request;\n\nclass
    FirstUser\n{\n  /**\n   * Handle an incoming request.\n   *\n   * @param  \\Illuminate\\Http\\Request
    \ $request\n   * @param  \\Closure(\\Illuminate\\Http\\Request): (\\Illuminate\\Http\\Response|\\Illuminate\\Http\\RedirectResponse)
    \ $next\n   * @return \\Illuminate\\Http\\Response|\\Illuminate\\Http\\RedirectResponse\n
    \  */\n  public function handle(Request $request, Closure $next)\n  {\n-   return
    $next($request)\n+   if (User::all()->count() == 0) return $next($request);\n+
    \  else return redirect('/');\n  }\n}\n```\n\nWe'll check if there are no users
    yet, and if so we'll allow the request to go through. If there are users already,
    we'll redirect the request to the home page using `RouteServiceProvider::HOME`.
    Now we can add the middleware to our `register` routes in `routes/auth.php`, for
    both the create and store methods:\n\n```diff\nRoute::middleware('guest')->group(function
    () {\n  Route::get('register', [RegisteredUserController::class, 'create'])\n-
    \        ->name('register');\n+         ->name('register')\n+         ->middleware('firstUser');\n\n-
    Route::post('register', [RegisteredUserController::class, 'store']);\n+ Route::post('register',
    [RegisteredUserController::class, 'store'])\n+         ->middleware('firstUser');\n```\n\nTry
    it out now and see if you can register another user. If you get an error like
    the one below, don't worry!\n\n![](2_First_user_does_not_exist.png)\n\nWe also
    need to let Laravel know how to link the `middleware('firstUser')` to the FirstUser
    middleware class. That's done in `app/http/Kernel.php` , more specifically in
    the `$routemiddleware` array. Add the new middleware there:\n\n```diff\nprotected
    $routeMiddleware = [\n    ... Other middlewares here\n    'verified' => \\Illuminate\\Auth\\Middleware\\EnsureEmailIsVerified::class,\n+
    \  'firstUser' => \\App\\Http\\Middleware\\FirstUser::class\n];\n```\n\nNow, if
    we try again we'll see that we're being redirected to the default `'/'` page.\n\nNow,
    I can see users getting frustrated to be redirected to another page without knowing
    why. This is where [Laravel's HTTP exceptions](https://laravel.com/docs/9.x/errors)
    come in. We can easily throw HTTP exceptions from anywhere in our app using the
    `abort()` method. If we look in the [list of HTTP messages](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status#client_error_responses),
    403 seems to fit the bill the best.  So let's change the `else` clause in the
    `FirstUser.php` middleware class:\n\n```diff\npublic function handle(Request $request,
    Closure $next)\n  {\n    if (User::all()->count() == 0) return $next($request);\n-
    \  else return redirect('/');\n+   else abort(403, \"There already is a registered
    user for this domain. Ask them for an invite to create your account.\");\n  }\n```\n\nThe
    `abort()` method will use the status (403 in our case) to show the `vendor/laravel/framework/src/Illuminate/Foundation/Exceptions/views/403.blade.php`
    view. It will also display the message that we passed as the second argument of
    the `abort()` method, like this:\n\n![](3_403_message.png)\n\nThis at least explains
    what's happening to the user. I'd agree it could use a little more ✨design sparkle✨,
    but I'll leave that up to you. Let's now look at how we're going to invite users
    number two and up, shall we?\n\n## Creating Invites\n\nTo get the next users on
    board, we'll create a new view where the admin can send an invite that contains
    a form with an email field and a UserLevel dropdown. We'll try to keep everything
    [cruddy by design](https://www.youtube.com/watch?v=MF0jFKvS4SI), so you could
    look at it like this: we're creating and storing a 'userinvite'. I've found there's
    no need for a custom Model here, but the controller and views can be setup thinking
    there is. If this doesn't make sense to you, go watch Adam's video linked above,
    he explains it in great detail.\n\nTo do this, create a new directory `resources/views/userinvites`
    and while you're in there, create a `create.blade.php`  file. The code should
    look like this:\n\n```xml\n@php use App\\Enums\\UserLevel; @endphp\n<x-app-layout>\n
    \   <x-slot name=\"header\">\n        <h2 class=\"font-semibold text-xl text-gray-800
    leading-tight\">\n            {{ __('Invite Users') }}\n        </h2>\n    </x-slot>\n\n
    \   <div class=\"flex flex-col sm:justify-center items-center pt-6 sm:pt-0 bg-gray-100\">\n\n
    \       <div class=\" w-full sm:max-w-md mt-24 px-6 py-4 bg-white shadow-md overflow-hidden
    sm:rounded-lg\">\n            <form method=\"POST\" action=\"{{ url('/users/invite')
    }}\">\n                @csrf\n\n                <!-- Email Address -->\n                <div
    class=\"mt-4\">\n                    <x-input-label for=\"email\" :value=\"__('Email')\"/>\n\n
    \                   <x-text-input id=\"email\" class=\"block mt-1 w-full\" type=\"email\"
    name=\"email\" :value=\"old('email')\" required autofocus/>\n\n                    <x-input-error
    :messages=\"$errors->get('email')\" class=\"mt-2\"/>\n                </div>\n\n
    \               <!-- User Level -->\n                <div class=\"mt-4\">\n                    <x-input-label
    for=\"level\" :value=\"__('User Level')\"/>\n\n                    <select name=\"level\"
    id=\"level\" class=\"w-full rounded-md shadow-sm border-gray-300 focus:border-indigo-300
    focus:ring focus:ring-indigo-200 focus:ring-opacity-50\">\n                        @foreach(UserLevel::cases()
    as $levelOption)\n                            <option value=\"{{$levelOption}}\"
    @if ($levelOption == old('level')) selected=\"selected\" @endif>\n                                {{$levelOption->name}}\n
    \                           </option>\n                        @endforeach\n                    </select>\n\n
    \                   <x-input-error :messages=\"$errors->get('level')\" class=\"mt-2\"/>\n
    \               </div>\n\n                <div class=\"flex items-center justify-end
    mt-6\">\n                    {{-- back button --}}\n                    <a href=\"{{url()->previous()}}\"
    class=\"inline-flex items-center px-4 py-2 bg-gray-300 border border-transparent
    rounded-md font-semibold text-xs text-gray-800 uppercase\n                        tracking-widest
    hover:bg-gray-400 active:bg-gray-100 focus:outline-none focus:border-gray-100
    focus:ring ring-gray-900 disabled:opacity-25 transition ease-in-out\n                        duration-150\">\n
    \                       Go Back\n                    </a>\n\n                    <x-primary-button
    class=\"ml-4\">\n                        {{ __('Invite') }}\n                    </x-primary-button>\n
    \               </div>\n            </form>\n        </div>\n    </div>\n</x-app-layout>\n\n```\n\nNow,
    to show this view we'll need 2 things: a controller to display it (and later to
    process the form submit) and a button on the users table to navigate to this page.
    Let's do the controller first: use `php artisan make:controller UserInviteController`
    and in there, add 2 methods: `create` and `store`. Leave the `store` method empty
    for now, but we can already set up the `create` method, like this:\n\n```diff\n+
    use App\\Models\\User;\n\nclass UserInviteController extends Controller\n{\n+
    public function create()\n+ {\n+   $this->authorize('invite', User::class);\n+
    \  return view('userinvites.create');\n+ }\n\n+ public function store()\n+ {\n+\n+
    }\n}\n```\n\nAs you can see, I added the `authorize` method to check if the current
    user is allowed to invite new users, which we will have to add in the `userPolicy.php`
    class. Add a method `invite` in the class like this:\n\n```diff\nclass UserPolicy\n{\n
    \ ... other methods here\n\n+  /**\n+  * Determine whether the user can invite
    new users.\n+  *\n+  * @param  \\App\\Models\\User  $loggedInUser the user that's
    trying to invite a new user\n+  * @return \\Illuminate\\Auth\\Access\\Response|bool\n+
    \ */\n+ public function invite(User $loggedInUser)\n+ {\n+   // only administrators
    are allowed to invite new users\n+   return (UserLevel::Administrator == $loggedInUser->level);\n+
    }\n}\n```\n\nNow, all that's left to do is to link the controller and the methods
    together by adding two routes in `web.php`, like this:\n\n```php\nRoute::get('/users/invite',
    [UserInviteController::class, 'create'])\n    ->middleware(['auth'])\n    ->name('userinvites.create');\n\nRoute::post('/users/invite',
    [UserInviteController::class, 'store'])\n    ->middleware(['auth'])\n    ->name('userinvites.store');\n```\n\nAnd
    to add a button in the users view, like this:\n\n```diff\n<div class=\"py-12\">\n
    \ <div class=\"max-w-7xl mx-auto sm:px-6 lg:px-8\">\n+   <div class=\"flex items-center\">\n+
    \    <h1 class=\"text-2xl font-extrabold flex-1\">Users</h1>\n+     @can('invite',
    App\\Models\\User::class)\n+       <a href=\"{{route('userinvites.create')}}\"\n+
    \         class=\"inline-flex items-center m-4 px-4 py-2 bg-gray-800 border border-transparent
    rounded-md font-semibold text-xs text-white uppercase\n+       tracking-widest
    hover:bg-gray-700 active:bg-gray-900 focus:outline-none focus:border-gray-900
    focus:ring ring-gray-300 disabled:opacity-25 transition ease-in-out\n+       duration-150\">\n+
    \        + Invite New User\n+       </a>\n+     @endcan\n+   </div>\n\n    <div
    class=\"bg-white overflow-hidden shadow-sm sm:rounded-lg\">\n      <table class=\"w-full
    table-auto\">\n        <thead class=\"font-bold bg-gray-50 border-b-2\">\n        <tr>\n```\n\nWe
    are leveraging the `@can` blade directive, just like in the [previous blog post](https://fly.io/laravel-bytes/user-levels-enums-and-policies-oh-my/).
    Feel free to refresh your knowledge about polices there \U0001F609.\n\nNow, navigate
    back to `UserInviteController.php` . Remember there's an empty `store` method
    in there? This is where we will send out an email to the email address that the
    admin specified. But first, a small coffee/tea break!\n\n<%= partial \"shared/posts/cta\",
    locals: {\n  title: \"Fly.io ❤️ Laravel\",\n  text: \"Fly.io is a great way to
    run your Laravel app close to your users. Deploy globally on Fly in minutes!\",\n
    \ link_url: \"https://fly.io/docs/laravel\",\n  link_text: \"Deploy your Laravel
    app!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>\",\n} %>\n\n## Notifying
    new Users\n\nAfter an admin has invited a user, what happens? The user should
    receive an email, with a link to register. When they click the link, their email
    and user level should already be filled in (because the admin already did this
    for them) and they should be able to set up their account. A common practice is
    to have a time limit on the URL as well, so the user can't use it after a certain
    time.\n\nNormally, Notifications are sent in response to an Event, like receiving
    a like on a tweet sends out a ~~dopamine hit~~ notification. But they can also
    be sent out without an Event, if there's no need for one. This is the case right
    here.\n\nSo we can just ignore the Event and send out the Notification from the
    controller. Create a Notification with `php artisan make:notification UserInvited`.
    You'll see that out of the box the notification is set up to be sent by mail,
    as there is `via` method that returns `['mail']` and a `toMail()` method. Let's
    quickly change to `toMail` a bit:\n\n\n```diff\npublic function toMail($notifiable)\n
    \ {\n    $appName = env('APP_NAME');\n\n    return (new MailMessage)\n        ->subject('Personal
    Invitation')\n        ->greeting('Hello!')\n        ->line(\"You have been invited
    to join the {$appName} application!\")\n        ->action('Click here to register
    your account', url('/'))\n        ->line('Note: this link expires after 24 hours.');\n
    \ }\n```\n\nAll we're doing here is taking the appName from the `.env` variables,
    and just creating a simple mail with an action button. Now, we need to send that
    invite. Remember the `store()` method on the `UserInviteController` ? We'll use
    a FormRequest here for validation and if it's all good, we'll send the `UserInvited`
    notification here. Use `php artisan make:request StoreUserInviteRequest` to make
    the request and add the following:\n\n```diff\n+ use App\\Enums\\UserLevel;\n+
    use App\\Models\\User;\nuse Illuminate\\Foundation\\Http\\FormRequest;\n+ use
    Illuminate\\Validation\\Rules\\Enum;\n\nclass StoreUserInviteRequest extends FormRequest\n{\n
    \ /**\n   * Determine if the user is authorized to make this request.\n   *\n
    \  * @return bool\n   */\n  public function authorize()\n  {\n-   return false;\n+
    \  return $this->user()->can('invite', User::class);\n  }\n\n  /**\n   * Get the
    validation rules that apply to the request.\n   *\n   * @return array<string,
    mixed>\n   */\n  public function rules()\n  {\n    return [\n+       'email' =>
    ['required', 'string', 'email', 'max:255', 'unique:users'],\n+       'level' =>
    ['required', new Enum(UserLevel::class)]\n    ];\n  }\n}\n```\n\nNow, use that
    form request in the `UserInviteController` :\n\n```diff\n- public function store()\n+
    public function store(StoreUserInviteRequest $request)\n  {\n+   $validated =
    $request->validated();\n\n+   // The enum validation will check if the chosen
    level can be cast to a UserLevel, but won't do the casting itself. So we do it
    here\n+   $userLevel = UserLevel::from($validated['level']);\n+   $email = $validated['email'];\n\n+
    \  // send mail to invite the new user with the given user level.\n+   Notification::route('mail',
    $email)->notify(new UserInvited());\n\n+   return redirect('/users');\n  }\n```\n\n\n\nMake
    sure to have an email testing tool running (Mailhog comes with Laravel Sail) and
    invite a new user. Check the email testing tool and you should see an email that
    looks like this:\n\n![](4_Invitation_email.png)\n\nLet's quickly add a flash message
    to let the user know everything went sucessfully, or not:\n\n```diff\n// send
    mail to invite the new user with the given user level.\n- Notification::route('mail',
    $email)->notify(new UserInvited());\n- return redirect('/users');\n+ try\n+   {\n+
    \    Notification::route('mail', $email)->notify(new UserInvited());\n+   }\n+
    \  catch( \\Exception $e)\n+   {\n+     return redirect('/users')->with('error',
    \"<b>Oh no!</b> Something went wrong. Please try again later.\");\n+   }\n+   return
    redirect('/users')->with('success', \"<b>Success!</b> An invite with level $userLevel->name
    has been sent to $email.\");\n```\n\nAnd build these messages into the `users/index.blade.php`
    view:\n\n```diff\n<div class=\"py-12\">\n  <div class=\"max-w-7xl mx-auto sm:px-6
    lg:px-8\">\n\n+   {{-- MESSAGES --}}\n+   @if(session('success'))\n+     <div
    class=\"flex items-center bg-green-50 p-6 mb-6 w-full sm:rounded-lg sm:px-10 transition
    duration-700 ease-in-out\"\n+          x-data=\"{show: true}\"\n+          x-show=\"show\"\n+
    \         x-init=\"setTimeout(() => show = false, 30000)\"\n+          x-transition>\n+
    \      <div class=\"flex mx-auto\">\n+         <svg class=\"h-6 w-6 flex-none
    fill-green-800 stroke-green-50 stroke-2\" stroke-linecap=\"round\" stroke-linejoin=\"round\">\n+
    \            <circle cx=\"12\" cy=\"12\" r=\"11\" />\n+             <path d=\"m8
    13 2.165 2.165a1 1 0 0 0 1.521-.126L16 9\" fill=\"none\" />\n+         </svg>\n+
    \        <p class=\"ml-2 text-green-800\">\n+             {!! session('success')
    !!}\n+         </p>\n+       </div>\n+     </div>\n+   @endif\n+\n+   @if(session('error'))\n+
    \    <div class=\"flex items-center bg-red-100 p-6 mb-6 w-full sm:rounded-lg sm:px-10
    transition duration-700 ease-in-out\"\n+          x-data=\"{show: true}\"\n+          x-show=\"show\"\n+
    \         x-init=\"setTimeout(() => show = false, 30000)\"\n+          x-transition>\n+
    \      <div class=\"flex mx-auto\">\n+         <svg class=\"w-6 h-6 flex-none
    fill-red-800 stroke-red-100 stroke-2\" fill=\"none\" viewBox=\"0 0 24 24\" stroke-width=\"1.5\"
    \ >\n+             <path stroke-linecap=\"round\" stroke-linejoin=\"round\" d=\"M12
    9v3.75m9-.75a9 9 0 11-18 0 9 9 0 0118 0zm-9 3.75h.008v.008H12v-.008z\" />\n+         </svg>\n+
    \        <p class=\"ml-2 text-red-800\">\n+             {!! session('error') !!}\n+
    \        </p>\n+     </div>\n+     </div>\n+   @endif\n\n    <div class=\"flex
    items-center\">\n      <h1 class=\"text-2xl font-extrabold flex-1\">Users</h1>\n```\n\nHere's
    how it looks:\n\n![](5_Succes.png)\n\n![](6_Error.png)\n\nGreat! Now, our users
    are informed about what's happening. Now, the last step is to prepare a decent
    URL, since the one we're sending right now just links to  `'/'`. To make sure
    the user doesn't tamper with the url, we'll use a [Signed URL](https://laravel.com/docs/9.x/urls#signed-urls).
    Hop into the `UserInvited` notification and add this method:\n\n```\n/**\n * Generates
    a unique signed URL that the mail receiver can user to register.\n * The URL contains
    the UserLevel and the receiver's email address, and will be valid for 1 day.\n
    *\n * @param $notifiable\n * @return string\n */\npublic function generateInvitationUrl(string
    $email)\n{\n  // ! Don't forget to import the URL Facade !\n  return URL::temporarySignedRoute('users.create',
    now()->addDay(), [\n    'level' => $this->userLevel,\n    'email' => $email\n
    \ ]);\n}\n```\n\nNow, you're probably wondering where the `$userLevel` and `$email`
    will come from. We'll add the userLevel to the constructor of the notification,
    like this:\n\n```\n/**\n * Create a new notification instance.\n *\n * @param
    UserLevel $userLevel\n * @param User $sender\n * @return void\n */\npublic function
    __construct(public UserLevel $userLevel, public User $sender)\n{\n}\n```\n\nDon't
    forget the imports of `User` and `UserLevel` as well!\n\nI've also added the User
    that's sending out the invite, to display in the email. We just need to reflect
    these changed arguments where we're using the constructor, which is in the `UserInviteController`:\n\n```diff\n-
    Notification::route('mail', $email)->notify(new UserInvited());\n+ Notification::route('mail',
    $email)->notify(new UserInvited($userLevel, $request->user()));\n```\n\nNow, let's
    look at the `$email` we're using in the signed URL. This will be the email address
    the admin entered in the invite form, and will also be the email where this notification
    is sent to. Remember how we're sending the notification in the controller?\n\n```\nNotification::route('mail',
    $email)->notify(new UserInvited());\n```\n\nThe `'mail'` is the channel we're
    using, and `$email` is the route. We can access this route in the `toMail()` method
    in the notification like this:\n\n```\n$notifiable->routes['mail']\n```\n\nThis
    will get all the routes (see them like destinations, email addresses in our case)
    for the `mail` channel. Exactly what we need!\n\nSo, let's generate our URL and
    pass it along with our email. In the `toMail()` method of `UserInvited`, make
    these changes:\n\n```diff\npublic function toMail($notifiable)\n{\n  $appName
    = env('APP_NAME');\n\n+ $url = $this->generateInvitationUrl($notifiable->routes['mail']);\n\n
    \ return (new MailMessage)\n              ->subject('Personal Invitation')\n              ->greeting('Hello!')\n-
    \            ->line(\"You have been invited to join the {$appName} application!\")\n+
    \            ->line(\"You have been invited by {$this->sender->name} to join the
    {$appName} application!\")\n-             ->action('Click here to register your
    account', url('/'))\n+             ->action('Click here to register your account',
    url($url))\n              ->line('Note: this link expires after 24 hours.');\n}\n```\n\nThere
    we go: a Notification with a signed URL. It contains all the information we need,
    is tamper-proof because it's signed, and it expires after 24 hours. Good stuff!\n\n##
    Registering new Users\n\nWe're in the home stretch now. The last hurdle is this:
    when the new user clicks the URL in the invitation email, they need to be able
    to register their account.\n\nI'm not sure if you already noticed but when we
    try to send an invite, you'll see an error instead of a success message! You could
    debug it and see what exception is being thrown, or I could just tell you: You'll
    get a `RouteNotFoundException` because the route `users.create` we're trying to
    make a signed URL for, doesn't exist yet. Let's add it in `web.php`:\n\n```php\n//
    show the 'create new user' screen\nRoute::get('/users/create', [UserController::class,
    'create'])\n    ->middleware('signed')\n    ->name('users.create');\n\n// create
    the new user\nRoute::post('/users/store',[UserController::class, 'store'])\n    ->name('users.store');\n```\n\nYou'll
    notice we're using the 'signed' middleware. This will check the URL to see if
    the signature is still valid (the information in the url hasn't been tampered
    with) and if the URL isn't expired. Thanks Laravel for including this! I've also
    included the store route because we'll need it later.\n\nI'll pick up the pace
    a bit since we're doing more of the same: I'm going to add the `create()` method
    on the `UserController`, make a formRequest for it, and make a view to show the
    `register` form for invited users. See you on the flipside!\n\n```php\n// Usercontroller
    create method\npublic function create(CreateUserRequest $request)\n{\n    $validated
    = $request->validated();\n    return view('users.create', ['email' => $validated['email'],
    'level' => $validated['level']]);\n}\n```\n\nCreateUserRequest: `php artisan make:request
    CreateUserRequest`\n\n```\nuse App\\Enums\\UserLevel;\nuse Illuminate\\Foundation\\Http\\FormRequest;\nuse
    Illuminate\\Validation\\Rules\\Enum;\n\nclass CreateUserRequest extends FormRequest\n{\n
    \   /**\n     * Determine if the user is authorized to make this request.\n     *\n
    \    * @return bool\n     */\n    public function authorize()\n    {\n        //
    this should always be authorized, even without a logged-in user.\n        return
    true;\n    }\n\n    /**\n     * Get the validation rules that apply to the request.\n
    \    *\n     * @return array<string, mixed>\n     */\n    public function rules()\n
    \   {\n        return [\n            'email' => ['required', 'string', 'email',
    'max:255', 'unique:users'],\n            'level' => ['required', new enum(UserLevel::class)]\n
    \       ];\n    }\n}\n```\n\nDon't forget to import the formRequest in the `UserController`!
    Let's quickly add the `users.create` view, which will be based heavily on [Breeze](https://laravel.com/docs/9.x/starter-kits#laravel-breeze)'s
    default `register` view:\n\n```xml\n<x-guest-layout>\n    <form method=\"POST\"
    action=\"{{ route('users.store') }}\">\n        @csrf\n\n        <!-- Name -->\n
    \       <div>\n            <x-input-label for=\"name\" :value=\"__('Name')\" />\n\n
    \           <x-text-input id=\"name\" class=\"block mt-1 w-full\" type=\"text\"
    name=\"name\" :value=\"old('name')\" required autofocus />\n\n            <x-input-error
    :messages=\"$errors->get('name')\" class=\"mt-2\" />\n        </div>\n\n        <!--
    Email Address -->\n        <input type=\"hidden\" id=\"email\" name=\"email\"
    value=\"{{$email}}\">\n\n        <!-- User Level -->\n        <input type=\"hidden\"
    id=\"level\" name=\"level\" value=\"{{$level}}\">\n\n        <!-- Password -->\n
    \       <div class=\"mt-4\">\n            <x-input-label for=\"password\" :value=\"__('Password')\"
    />\n\n            <x-text-input id=\"password\" class=\"block mt-1 w-full\"\n
    \                         type=\"password\"\n                          name=\"password\"\n
    \                         required autocomplete=\"new-password\" />\n\n            <x-input-error
    :messages=\"$errors->get('password')\" class=\"mt-2\" />\n        </div>\n\n        <!--
    Confirm Password -->\n        <div class=\"mt-4\">\n            <x-input-label
    for=\"password_confirmation\" :value=\"__('Confirm Password')\" />\n\n            <x-text-input
    id=\"password_confirmation\" class=\"block mt-1 w-full\"\n                          type=\"password\"\n
    \                         name=\"password_confirmation\" required />\n\n            <x-input-error
    :messages=\"$errors->get('password_confirmation')\" class=\"mt-2\" />\n        </div>\n\n
    \       <div class=\"flex items-center justify-end mt-4\">\n            <a class=\"underline
    text-sm text-gray-600 hover:text-gray-900\" href=\"{{ route('login') }}\">\n                {{
    __('Already registered?') }}\n            </a>\n\n            <x-primary-button
    class=\"ml-4\">\n                {{ __('Register') }}\n            </x-primary-button>\n
    \       </div>\n    </form>\n</x-guest-layout>\n```\n\n\n\nWhew, lots of code
    being thrown around! Right now you should be able to click the link in the email
    and see a familiar screen:\n\n![](7_Register_screen.png)\n\nRegistering won't
    work yet, since the store method isn't wired up yet. We'll again use a formRequest
    for the validation, so run `php artisan make:request StoreUserRequest` and add
    the following:\n\n```php\nuse App\\Enums\\UserLevel;\nuse Illuminate\\Foundation\\Http\\FormRequest;\nuse
    Illuminate\\Validation\\Rules\\Enum;\nuse Illuminate\\Validation\\Rules\\Password;\n\nclass
    StoreUserRequest extends FormRequest\n{\n    /**\n     * Determine if the user
    is authorized to make this request.\n     *\n     * @return bool\n     */\n    public
    function authorize()\n    {\n        // this should always be authorized, even
    without a logged-in user.\n        return true;\n    }\n\n    /**\n     * Get
    the validation rules that apply to the request.\n     *\n     * @return array<string,
    mixed>\n     */\n    public function rules()\n    {\n        return [\n            'name'
    => ['required', 'string', 'max:255'],\n            'email' => ['required', 'string',
    'email', 'max:255', 'unique:users'],\n            'password' => ['required', 'confirmed',
    Password::defaults()],\n            'level' => ['required', new enum(Userlevel::class)]\n
    \       ];\n    }\n}\n```\n\nThe `email` and `level` inputs come from the signed
    URL and are hidden on the `create` form. Now, we can fill in the `store` method
    on the `UserController`:\n\n```php\n/**\n * Handle an incoming registration request.
    This will be called when an invited User accepts the invitation and registers.\n
    *\n * @param  \\Illuminate\\Http\\Request  $request\n * @return \\Illuminate\\Http\\RedirectResponse\n
    *\n * @throws \\Illuminate\\Validation\\ValidationException\n */\npublic function
    store(StoreUserRequest $request)\n{\n    $validated = $request->validated();\n\n
    \   $user = User::create($validated);\n\n    event(new Registered($user));\n\n
    \   $user->markEmailAsVerified();\n\n    Auth::login($user);\n    //don't forget
    to import the RouteServiceProvider!\n    return redirect(RouteServiceProvider::HOME);\n}\n```\n\nWe'll
    validate the data, then create a User if the validation went through. After that,
    we're following the default Breeze Logic by dispatching an Event, logging in the
    user and redirecting to the homepage. The only addition I've made is to set the
    user's email as verified right away, since they come from the invite in their
    email inbox.\n\nLet's give it a whirl! Send out an invite, click the link and
    register a new user. If everything went well you should now be logged in as the
    new user. You should be able to delete your own account, and if you made the new
    user an admin you will also be able to send out invites and edit/delete the other
    users. Well done!"
- :id: phoenix-files-liveview_async_task
  :date: '2023-01-12'
  :category: phoenix-files
  :title: Async processing in LiveView
  :author: berenice
  :thumbnail: async-processing-thumbnail.png
  :alt:
  :link: phoenix-files/liveview_async_task
  :path: phoenix-files/2023-01-12
  :body: "\n\n<p class=\"lead\">In this post we talk about how to perform async processing
    from a LiveView using easy Elixir concurrency. Fly.io is a great place to run
    your Elixir applications! Check out how to [get started](/docs/elixir/)!</p>\n\nLast
    month Chris McCord developed an amazing [single-file example for doing image classification](https://github.com/chrismccord/single_file_phx_bumblebee_ml/blob/main/run.exs)
    using [Bumblebee](https://hexdocs.pm/bumblebee/Bumblebee.html), [Nx](https://hexdocs.pm/nx/Nx.html)
    and [LiveView](https://hexdocs.pm/phoenix_live_view/Phoenix.LiveView.html). We
    can see all the parts together in one place, and run the example just with a single
    command… it must be said, it&#39;s impressive!\n\nThe example works like this:
    You upload an image and the process for classifying images with Bumblebee runs
    in an asynchronous process while a small working indicator is shown. This means
    the user is not blocked and can still interact with the page while the process
    chugs away performing the work. Once the processing is done, the indicator disappears
    and the results are printed on the screen:\n\n<%= video_tag \"image_classification.mp4?card&center&3/4&border\",
    title: \"\" %>\n\nThat little trick of asynchronously working in the background
    while being handled all from the server was just so elegantly simple and clean
    that it deserves a deeper look!\n\n## Problem\n\nHow can we run async processing
    in a LiveView? How can we return results of an async process to the LiveView?\n\n##
    Solution\n\nThe main actor is the  [Task](https://hexdocs.pm/elixir/1.14/Task.html)
    module, we can use it to spawn processes to complete a specific task.\n\nSometimes
    we need to perform some processing asynchronously, and we need to wait for the
    result. This is possible with a couple of functions: [Task.async/1](https://hexdocs.pm/elixir/1.14/Task.html#async/1)
    launches a process that, when it finishes its work, sends a message with the result
    to the caller. And [Task.await/2](https://hexdocs.pm/elixir/1.14/Task.html#await/2)
    \ waits for the task&#39;s message and returns the result:\n\n```elixir\ntask
    = Task.async(fn -> 1 + 2 end)     \n# %Task{\n#  owner: #PID<0.508.0>,\n#  pid:
    #PID<0.518.0>,\n#  ref: #Reference<0.4260127598.4204593153.204028>\n#}\nresult
    = Task.await(task)  \n#iex()> 3\n```\n\nLet&#39;s take the image classification
    example we mentioned at the beginning.\n\nOnce the user uploads an image, the
    classification process starts. Since it is an expensive operation that can take
    time to complete, a task is spawned to complete that process.\n\nWhile the processing
    is taking place a `:spinner` function component —defined in the same file— is
    displayed with the help of the assign `:running.` As long as the task is not completed,
    the value of this assign is `true`  and  the spinner is shown conditionally:\n\n```elixir\ndef
    handle_progress(:image, entry, socket) do\n  ...\n  Task.async(fn -> \n    Nx.Serving.batched_run(PhoenixDemo.Serving,
    image) \n  end)\n  ...\n  {:noreply, assign(socket, running: true)}\nend\n```\n\nWhen
    the task is spawned using `Task.async/1`, a couple of things happen in the background.
    The new process is monitored by the caller —our  LiveView—, which means that the
    caller will receive a `{:DOWN, ref, :process, object, reason}` message once the
    process it is monitoring dies. And, a link is created between both processes.\n\nWe&#39;re
    going to talk more about some considerations related to this last point in a couple
    of minutes, but now we have one more question to answer: how can we get the response
    of the task in the LiveView? Well that&#39;s simple.\n\nWe no longer need to use
    \ `Task.await/2` , our LiveView already has the ability to receive messages from
    other processes using the [handle_info/2](https://hexdocs.pm/phoenix_live_view/Phoenix.LiveView.html#c:handle_info/2)
    callback:\n\n```elixir\ndef handle_info({ref, result}, socket) do\n  Process.demonitor(ref,
    [:flush])\n  %{predictions: [%{label: label}]} = result\n  {:noreply, assign(socket,
    label: label, running: false)}\nend\n```\n\nThe received message contains a `{ref,
    result}` tuple, where ref is the monitor&#39;s reference. We use this reference
    to stop monitoring the task —and not receive a message if it dies—, since we received
    the result we needed from our task and we can discard an exit message.\n\nFinally
    we set the result in the assign `:label` to display it, and we hide our spinner
    by changing the content of the assign `:running`.\n\nAn elegant solution, right?
    Just a couple lines of simple Elixir concurrency to delegate the work and limit
    the responsibilities of our LiveView.\n\nWe don&#39;t even have to worry if the
    user closes the browser tab! The process dies just like the LiveView, and the
    work is automatically cancelled. No resources are spent on a process from which
    nobody expects the result anymore.\n\nJosé shows us this pattern in his recent
    video!\n\n<%= youtube \"https://www.youtube.com/watch?v=g3oyh3g1AtQ&t=740s\" %>\n\n\n##
    Additional considerations\n\n1) When a task is spawned using `Task.async/2`, it
    is linked to the caller. Which means that both processes have a relationship:
    if one crashes, the other does too.\n\nWe must take it into account. If we don&#39;t
    have control over the result of the task, and we don&#39;t want our LiveView to
    crash if the task crashes, we must use a different alternative to launch our task.
    We can use [Task.Supervisor.async_nolink/3](https://hexdocs.pm/elixir/1.14/Task.Supervisor.html#async_nolink/3)
    to make sure that our LiveView won&#39;t die even if the task crashes and that
    the error will be reported.\n\n2) We need to think about what kind of work we
    want to do asynchronously.\n\nIn a scenario where we're doing something that takes
    time and we don't save the result, we don't want the job to keep going if the
    user leaves. So this solution is a good fit.\n\nBut, if we are building some report
    that must be generated even if the user closes the browser tab, then, this may
    not be the right solution.\n\n<%= partial \"shared/posts/cta\", locals: {\n  title:
    \"Fly.io ❤️ Elixir\",\n  text: \"Fly.io is a great way to run your Phoenix LiveView
    app close to your users. It's really easy to get started. You can be running in
    minutes.\",\n  link_url: \"https://fly.io/docs/elixir/\",\n  link_text: \"Deploy
    a Phoenix app today!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>\"\n} %>\n\nTell
    us, what other options have you used to do async work in LiveView?"
- :id: blog-wal-mode-in-litefs
  :date: '2023-01-04'
  :category: blog
  :title: WAL Mode in LiteFS
  :author: ben
  :thumbnail: wal-mode-thumbnail.jpg
  :alt:
  :link: blog/wal-mode-in-litefs
  :path: blog/2023-01-04
  :body: |2


    <p class="lead">[LiteFS](https://fly.io/docs/litefs/) is a distributed file system that magically replicates your SQLite databases. Make an update on one server and, voilà, your change is instantly available to your other servers on the edge. Take a look at our [_Getting Started_](https://fly.io/docs/litefs/getting-started/) guide to see how to add LiteFS to your application.</p>

    By and large, SQLite is configuration-free. You can get pretty far by just using the default settings. As your application grows and you start tweaking settings, one of the first knobs you'll come across is the [journal mode](https://www.sqlite.org/pragma.html#pragma_journal_mode). This setting determines how SQLite performs transactions on disk and there are essentially two modes: the rollback journal & the write-ahead log, or WAL.

    The rollback journal was the original transaction mechanism and it's still the default. The WAL mode is the shiny new transaction mode. If you start reading blog posts & forums about SQLite, one tip you will repeatedly hear is, _"use WAL mode!"_

    If your database is slow, you should use the WAL mode.

    If you have concurrent users, you should use the WAL mode.

    WAL mode. WAL mode. WAL mode.

    In the SQLite world, the write-ahead log is as close to a [silver bullet](https://en.wikipedia.org/wiki/No_Silver_Bullet) as you can find. It's basically magic fairy dust that makes your database better and you should [almost](https://sqlite.org/wal.html#overview) always use it.

    However, [LiteFS](https://github.com/superfly/litefs), our distributed SQLite file system, only supported the rollback journal mode. Until now! With the release of [LiteFS v0.3.0](https://github.com/superfly/litefs/releases/tag/v0.3.0), we now support all journaling modes.

    ## Quick primer on journal modes

    We've written about the internals of the [rollback journal](https://fly.io/blog/sqlite-internals-rollback-journal/) and the [WAL mode](https://fly.io/blog/sqlite-internals-wal/) in previous posts, but here's a refresher.

    With the rollback journal, SQLite:

    - Writes new pages directly to the database file.
    - Copies the previous contents to a separate rollback journal file.
    - Deletes the rollback journal on commit.

    Because the pages in the database file are moving around and being deleted, this mode does not allow read transactions & write transactions to occur at the same time.

    The WAL works the opposite way:

    - New pages are written to a separate write-ahead log file.
    - The last page written has a "commit" flag to indicate the end of the transaction.

    Since the original data is never changed during the transaction, readers can continue running in parallel while another process is writing to the database. In addition to improved concurrency, the WAL also tends to have better write performance.

    ## Databases as a history of change sets

    Most developers think of databases as just a collection of tables & rows. And that's how you should view it when you're building an application. However, when designing database tooling like LiteFS, it's better to think in terms of change sets.

    A good analogy is baseball card collections. You might start off buying a pack of cards to start your collection. Over time, you may buy more packs or you might trade cards with friends. Each of these actions is a "change set", adding and/or removing a set of cards from your collection.

    Eventually, word gets out about your sweet baseball card collection and your friends want to have the same set. So each time you make a change, you send each friend a list of which cards were added and removed so they can update their collections. Now everyone has the same collection just by communicating change sets.

    That, in a nutshell, is how LiteFS nodes keep distributed copies of your database in sync. However, instead of baseball cards, these LiteFS nodes communicate change sets of fixed-sized blocks called _pages_.

    SQLite applies these change sets of pages safely & atomically by using either a rollback journal or the write-ahead log. These two methods have a different approach but, at the end of they day, they both transactionally update a set of pages in a SQLite database.

    In LiteFS, we track the beginning and end of these transactions through the file system API. We can see which pages have changed and bundle them up in an internal file format called [LTX](https://github.com/superfly/ltx).

    ### Detecting page sets with the rollback journal

    The rollback journal is a simple mechanism,  which makes it easy for LiteFS to determine when write transactions start & end. From a high-level, SQLite implements transactions like this:

    1. Obtain an exclusive lock on the `SHARED` & `RESERVED` lock bytes.
    1. Create a `-journal` file.
    1. Write changes to the database file with `write(2)` & copy old versions of pages to the journal.
    1. `fsync(2)` the database file & `unlink(2)` the journal file.
    1. Release the locks.

    LiteFS acts as a passthrough file system so it can see all these file system calls. On the initial journal creation, it begins watching for page changes. On `write(2)`, it marks a page as changed. And finally, on `unlink(2)` it will copy the page change set to an LTX file and then delete the journal.

    ### Detecting page sets with the WAL

    SQLite's operations when it uses the WAL mode are a bit more complicated but it still has similar start & end triggers.

    1. Obtain the `SHARED` lock byte in the database file but also obtain WAL-specific locks such as `WAL_WRITE_LOCK`.
    1. Write new pages to the end of the WAL using `write(2)`.
    1. On the last page write, the `commit` field is set in the WAL frame header. This indicates the end of the transaction and also the ending size of the database.
    1. Release locks.

    LiteFS can read the list of changed pages from the WAL and copy them out to an LTX file when the final WAL write for the transaction comes in. Again, both the rollback journal and WAL are implementation details so we end up with the same LTX format with either one.

    In the WAL mode, SQLite will also maintain a shared-memory file (aka SHM) and uses it as an index to look up pages in the WAL. This piece is managed by SQLite so LiteFS doesn't touch it during a write.

    ### Applying transactions to the replica

    Once an LTX file is created on the primary LiteFS node, it will send it to all connected replica LiteFS nodes. These replicas will validate the file, perform some consistency checks, and then apply the change set to the SQLite database.

    The LiteFS replica imitates a SQLite client and takes the same locks in order to apply the transaction. That means it looks like just another SQLite client doing an update so it's safe across other processes using the database.

    ## Bootstrapping made easy

    Previously, it was tough to convert an existing application to use LiteFS. You'd need to create a SQL dump of your database and import in using the `sqlite3` command line. That was a pain.

    We've improved this workflow with the new [`litefs import`](/docs/litefs/import/) command. This command lets you remotely send a SQLite database to your LiteFS cluster and it will transactionally replace it. That means you can start a cluster with an existing database or you can even revert to an old snapshot on a live application.

    ```sh
    $ litefs import -name my.db /path/to/my.db
    ```


    ## Reworking checksumming

    LiteFS uses a fast, incremental checksum for ensuring the state of the entire database is consistent across all nodes at every transaction. The method is simple: we XOR the [CRC64](https://en.wikipedia.org/wiki/Cyclic_redundancy_check) checksums of every page in the database together. This approach let us incrementally update individual pages by XOR'ing out the old checksum for a page and XOR'ing in the new checksum for the page. That's pretty cool.

    However, in practice, it was difficult to ensure we were calculating the correct previous checksum for a page every time we performed an update as page data is spread across the database file, journal file, & WAL file. The edge cases for determining the previous page data were too easy to get wrong.

    So in v0.3.0, we decided to rework the database checksum. It still uses the same algorithm of XOR'ing page checksums but now we maintain a map of the current checksum of every page in the database so they can be XOR'd together on commit. We no longer need to track the previous checksum and this change made a lot of edge cases disappear.

    This approach is not without its trade-offs though. First, it requires additional memory. The map keys are 4-byte unsigned integers and the values are 8-byte hash values so we need about 12 bytes per page. SQLite uses 4KB pages by default so that's 262,144 pages per gigabyte. Our total memory overhead for our map of page hashes ends up being about 3MB of RAM per gigabyte of on-disk SQLite database data. LiteFS targets database sizes between 1 to 10 GB so that seemed like a reasonable trade-off.

    Second, this approach adds CPU overhead after each commit. Map iteration and XOR computation are quite fast but these do begin to show up in performance profiles as the database grows. In our tests, we've found it adds about 5ms per gigabyte of SQLite data. That's pretty high. Fortunately, much of this iteration can be cached since XORs are associative. We'll be implementing this cache in the next version of LiteFS.

    ## Improving debugging with the trace log

    One benefit to having checksum bugs in v0.2.0 was that it gave us plenty of time to get our hands dirty with debugging. The best tools come out of necessity and the LiteFS trace log is one of those tools.

    Debugging a failed database or distributed system is [a bit like a murder mystery](https://twitter.com/honest_update/status/651897353889259520) in that you know how it ended but you need to put the pieces together to figure out how it happened.

    In the previous version of LiteFS, we didn't have many clues when one of these failures happened so it required a Sherlock Holmes level of deductive reasoning to figure out the mystery. The trace log simplifies this process by writing out every internal event to a log file so we can see where things went awry after the fact.

    SQLite uses the POSIX file system API so debugging with a normal `strace` would look like a series of seemingly opaque system calls. LiteFS translates these system calls back into SQLite related actions such as `WriteDatabase()` or `LockSHM()`. When we write those events to the trace log, we can decorate the log lines with additional information such as page numbers and checksums. All this makes reading the trace much more straightforward.

    The trace log is not without its costs though. It will increase I/O to your disk as there are a lot of events that are written. It's typical to see your disk I/O double when you enable the trace log. However, it does cap the total size of the trace log by using a rolling log so you don't need much space available. By default, it will roll over to a new log file every 64MB and it will retain the last 10 logs in a gzipped format.

    The trace log is disabled by default, however, you review the [trace log documentation](https://fly.io/docs/litefs/config/#trace-log) if you need it to debug any LiteFS issues.

    ## Upcoming work

    The WAL support & stability improvements have been huge steps in moving LiteFS to be production ready but there's still more work to come. In the next release, we'll be focused on making LiteFS easier to integrate into your application by adding support for [_write forwarding_](https://github.com/superfly/litefs/issues/56). That will let you write to your database from any node and have LiteFS automatically forward those writes to the primary instead of having your application redirect writes.

    We'll also be making performance improvements by adding [LZ4 compression](https://en.wikipedia.org/wiki/LZ4_(compression_algorithm)) to the LTX files. This will reduce latency between nodes and it will significantly cut down on bandwidth costs.

    ## Thank you!

    Finally, we'd like to give a huge shoutout for everyone who has tried LiteFS and given feedback. It makes a world of difference! [Kent C. Dodds](https://www.youtube.com/@KentCDodds-vids) even live streamed his experience with LiteFS and it gave us incredible, detailed feedback. Thank you!
- :id: ruby-dispatch-monkey-patch-responsibly
  :date: '2022-12-27'
  :category: ruby-dispatch
  :title: Monkey Patch Responsibly
  :author: brad
  :thumbnail: monkey-patch-thumbnail.png
  :alt: Monkey patching Ruby code
  :link: ruby-dispatch/monkey-patch-responsibly
  :path: ruby-dispatch/2022-12-27
  :body: |2


    **What are the hazards of Monkey Patching in Ruby? How you can create a Monkey Patch that you can share _responsibly_ and _safely_ with the Ruby community without causing bugs from forgetting to remove the patch.**

    We live in an imperfect world, which means sometimes you need to "get stuff out that solves the more immediate pain" to run cover while a more permanent fix gets put into place.

    In Ruby, for better or for worse, we have a concept called "Monkey Patching". It let's you do stuff like this:

    ```ruby
    # Version 1.0 of Hello World
    class Hello
      def world
        puts "Go away!"
      end
    end

    # The patch
    module HelloPatch
      def world
        puts "Hello world!"
      end
    end

    Hello.new.world # => "Go away!"
    # Apply the patch
    Hello.prepend HelloPatch
    Hello.new.world # => "Hello world!"
    ```

    This makes it really easy to patch broken Ruby code—in this case we replaced the buggy "Go away!" greeting with the happier "Hello world!"

    ## The problem

    The problem is when the upstream software is patched and a new version goes out—often times the monkey patch can stay in place and cause unexpected bugs.

    Imagine if version 2.0 of our Hello World library fixed the grumpy bug.

    ```ruby
    # Version 2.0 of Hello World: now with more friendliness!
    class Hello
      def world
        puts "Howdy super friend!"
      end
    end
    ```

    When we update to the new software a few months later and forgot about our patch, we'd be surprised to see `"Hello world!"` instead of `"Howdy super friend!"`.

    How can we monkey patch responsibly?

    ## Only apply the patch to specific versions of a library

    The problem in the example above is one of version. We need a way to target our patches to specific versions of the "broken" software.

    To look at a real world example, at Fly we have a problem where Redis servers have a 5 minute time-out. When the Redis connection times out, ActionCable doesn't reconnect. Our first iteration of the fix? A monkey patch!

    ```ruby
    # config/initializers/action_cable.rb
    require 'action_cable/subscription_adapter/redis'

    module ActionCableRedisListenerPatch
      private

      def ensure_listener_running
        @thread ||= Thread.new do
          Thread.current.abort_on_exception = true
          conn = @adapter.redis_connection_for_subscriptions
          listen conn
        rescue ::Redis::BaseConnectionError
          @thread = @raw_client = nil
          ::ActionCable.server.restart
        end
      end
    end

    ActionCable::SubscriptionAdapter::Redis::Listener.prepend(ActionCableRedisListenerPatch)

    ```

    The problem with putting monkey patches in the `./config/initializers/*.rb` directory is the same as before: a few months later we forget it's there and when the newer version fixes it, the monkey patch could cause bugs that are hard to track down.

    A better to handle project monkey patches like this is by putting something like this at the top of each patch:

    ```ruby
    if Rails.version > Gem::Version.new("7.0.4")
      error "Check if https://github.com/rails/rails/pull/45478 is fixed"
    end
    ```

    When we upgrade Rails, this will blow up your CI or dev environment so a person on your team can check the PR and understand if the patch needs to be applied.

    ## Releasing a community patch

    How can we scale the approach above to work for an entire community of developers?

    Fortunately we can use `gemspec`'s to manage this in a responsible way.

    Since I know this problem currently effects `actioncable` starting at `7.0.0`, and the current version of Action Cable, which is at `7.0.4`, I can specify that in my gemspec:

    ```ruby
    spec.add_dependency "actioncable", ">= 7.0", "<= 7.0.4"
    ```

    When `actionable 7.0.5` is released and the user runs `bundle update`, nothing will happened because this dependency will keep `actioncable` pegged at `7.0.4`.

    That's a good thing! Unless of course the developer _wants_ the newer version of Rails. Since they forgot about the patch, they open up their `Gemfile` and set to the latest version of Rails.

    ```ruby
    gem "rails", "7.0.5"
    ```

    When they run `bundle update`, they get an error:

    ```
    Could not update to Rails 7.0.5 because the gem actioncable_redis-reconnectand depends on Rails 7.0 to 7.0.4
    ```

    "WTF is that `actioncable_redis-reconnect` gem!?" says the developer. So they go to [https://gem.wtf/actioncable_redis-reconnect](https://gem.wtf/actioncable_redis-reconnect) in their browser and get all the relevant context they need about that patch.

    From this point they could do the following to resolve the issue.

    1.  ### Open a PR to bump the actioncable dependency

        Open a PR on the gem that bumps the actioncable dependency if the issue is still present in `actioncable`:

        ```diff
        -  spec.add_dependency "actioncable", ">= 7.0", "<= 7.0.4"
        +  spec.add_dependency "actioncable", ">= 7.0.4", "<= 7.0.5"
        ```

        Bumping the version should only be done in the patch gem _after_ the maintain has done the research to determine whether or not the monkey patch is still needed or is compatible with that release.

    2.  ### Remove the monkey patch gem

        Maybe the developer doesn't care anymore, so they remove the monkey patch gem and they can upgrade to the latest version of Rails.

    3.  ### Do nothing

        You don't always have to run the latest version of a framework, unless of course there's a security patch that needs to be installed. In that case go back to 1.

    The important thing is that the monkey patch was not allowed to persist quietly causing subtle bugs in production for years.

    ## Deprecating the community patch

    When the issue is fixed, the patch gem can finally be deprecated. How should that be done? Let's say `actioncable 7.0.6` fixes the bug. We'd change our gemspec to:

    ```diff
    -  spec.add_dependency "actioncable", ">= 7.0.4", "<= 7.0.5"
    +  spec.add_dependency "actioncable", ">= 7.0.6"
    ```

    Then we'd delete the monkey patch code and replace it with this message:

    ```
    warn "The actioncable_redis-reconnect gem can be removed`
    ```

    Eventually when developers update their gems, they'd make their way to the latest version of this patch gem, see the message, and remove the gem. Tada!

    ## Conclusion

    Ideally contributions are made timely and directly into the upstream repo, but for a lot of good reasons, that's not always possible. Monkey patching can be a great workaround, but you always want to make sure you're managing versions with monkey patches to avoid very-difficult-to-track-down bugs in the future.
- :id: laravel-bytes-user-levels-enums-and-policies-oh-my
  :date: '2022-12-21'
  :category: laravel-bytes
  :title: User Levels, Enums and Policies, oh my!
  :author: johannes
  :thumbnail: user-levels-thumbnail.jpg
  :alt: A bunch of offices in the shape of the Laravel L, where rooms are secured
    by guards, key-cards and the like.
  :link: laravel-bytes/user-levels-enums-and-policies-oh-my
  :path: laravel-bytes/2022-12-21
  :body: "\n\n<p class=\"lead\">Fly takes a Docker image, converts it to a VM, and
    runs that VM anywhere around the world. [Run a Laravel app](https://fly.io/docs/laravel/)
    in minutes!</p>\n\n**Users are not equal.**\n\nYou heard me. Some users are above
    others. They are allowed to do more than other users. They can see things other
    users cannot see. Sometimes they can even remove lower-class users!\n\nNow, this
    is not a weirdly abstract horror movie script, nor is it a manifesto to rally
    the common users into guillotining the higher classes. It's simply the way apps
    are built, and there's really not much wrong with it. We're going to be doing
    the exact same today, actually. And by \"_we're doing_\" I mean that I'll show
    you how to do it, and you just sit back, grab a cup of coffee or tea, and read
    on. Let's go!\n\n## The Goal\n\nWe'll set up an app where users can have 3 levels:
    Administrator, Contributor and Member. For that, we'll use the newly introduced
    Enum class that came with PHP 8.1. After that we'll set up some routes to edit
    user levels and delete other users, and finally we'll use policies to divide our
    users into 'cans' and 'cannots'.\n\nHere are the steps in more detail:\n\n- Create
    a Users page where users are listed\n- Add a level to our User model\n- Make it
    possible to edit User levels\n- Use the User level to allow or deny access to
    functionality using a Policy\n- Make it possible to delete Users\n\nAlong the
    way, we'll learn about PHP's new Enums, model casting and authorization using
    Laravel's policies.\n\nI've created a [github repo](https://github.com/Johannes-Werbrouck/policies-levels-enums)
    to go alongside this blog post with a pull request for each section. That way
    you can see what code gets edited for each section. Now let's begin, shall we?\n\n##
    Displaying the Users\n\nLet's jump in. To begin, create a new Laravel project
    and install Breeze. If you need help with this, check out [Laravel Bootcamp](https://bootcamp.laravel.com/blade/installation).\n\nWe're
    going to add a 'users' page where all users are listed. We'll use that page as
    a base and we'll bolt on additional functionality later on. Here's what our users
    page will look like:\n\n![](1_preview.png)\n\nFor this, we'll need to set up a
    route with a get request, and have that route connect to a controller that will
    return the 'users' view. We'll also give our route a name (`users.index` sounds
    good) and add the 'auth' middleware to enforce users that are not logged in can't
    access this page. We'll end up adding something like this in `web.php`:\n\n```diff\n//
    ... other routes here ...\n\n+ Route::get('/users', [UserController::class, 'index'])\n+
    \    ->middleware(['auth'])\n+     ->name('users.index');\n```\n\nNow, we referenced
    a UserController but that doesn't exist yet, as I'm sure your IDE has yelled at
    you already. Create the controller with `php artisan make:controller UserController`
    and create a method 'index' that returns the view 'users.index'. It'll look like
    this:\n\n```diff\n  class UserController extends Controller\n  {\n+     public
    function index()\n+     {\n+         return view('users.index');\n+     }\n  }\n```\n\nAlso,
    don't forget to import the UserController in `web.php`:\n\n```diff\n  use App\\Http\\Controllers\\ProfileController;\n+
    use App\\Http\\Controllers\\UserController;\n  use Illuminate\\Support\\Facades\\Route;\n```\n\nLastly,
    let's add the final puzzle piece: the view. Create a 'users' folder in `resources/views`
    and in there, add an `index.blade.php` file. We'll copy over the contents from
    the `dashboard.blade.php` file, and change the displayed text and the title:\n\n```html\n{{--resources/views/users/index.blade.php--}}\n<x-app-layout>\n
    \   <x-slot name=\"header\">\n        <h2 class=\"font-semibold text-xl text-gray-800
    leading-tight\">\n            {{ __('Users') }}\n        </h2>\n    </x-slot>\n\n
    \   <div class=\"py-12\">\n        <div class=\"max-w-7xl mx-auto sm:px-6 lg:px-8\">\n
    \           <div class=\"bg-white overflow-hidden shadow-sm sm:rounded-lg\">\n
    \               <div class=\"p-6 text-gray-900\">\n                    {{ __(\"Users
    overview coming here!\") }}\n                </div>\n            </div>\n        </div>\n
    \   </div>\n</x-app-layout>\n```\n\nNow, let's try it out! Run the migrations,
    register an account and click the link to the users page… If you can't find it,
    it's because we haven't added it yet! Open up `views/layouts/navigation.blade.php`
    and let's take care of that right away:\n\n```diff\n...\n  <!-- Navigation Links
    -->\n  <div class=\"hidden space-x-8 sm:-my-px sm:ml-10 sm:flex\">\n      <x-nav-link
    :href=\"route('dashboard')\" :active=\"request()->routeIs('dashboard')\">\n          {{
    __('Dashboard') }}\n      </x-nav-link>\n+     <x-nav-link :href=\"route('users.index')\"
    :active=\"request()->routeIs('users.index')\">\n+         {{ __('Users') }}\n+
    \    </x-nav-link>\n  </div>\n\n...\n\n<!-- Responsive Navigation Menu -->\n    <div
    :class=\"{'block': open, 'hidden': ! open}\" class=\"hidden sm:hidden\">\n        <div
    class=\"pt-2 pb-3 space-y-1\">\n            <x-responsive-nav-link :href=\"route('dashboard')\"
    :active=\"request()->routeIs('dashboard')\">\n                {{ __('Dashboard')
    }}\n            </x-responsive-nav-link>\n+           <x-responsive-nav-link :href=\"route('users.index')\"
    :active=\"request()->routeIs('users.index')\">\n+               {{ __('Users')
    }}\n+           </x-responsive-nav-link>\n        </div>\n```\n\nReload the page,
    and now we should be able to verify our users view is working. It should look
    like this:\n\n![](2_users_page.png)\n\nLet's make it look pretty real quick. in
    `users/index.blade.php`:\n\n```diff\n<div class=\"py-12\">\n        <div class=\"max-w-7xl
    mx-auto sm:px-6 lg:px-8\">\n            <div class=\"bg-white overflow-hidden
    shadow-sm sm:rounded-lg\">\n-             <div class=\"p-6 text-gray-900\">\n-
    \                  {{ __(\"Users overview coming here!\") }}\n-               </div>\n+
    \            <table class=\"w-full table-auto\">\n+                   <thead class=\"font-bold
    bg-gray-50 border-b-2\">\n+                   <tr>\n+                       <td
    class=\"p-4\">{{__('ID')}}</td>\n+                       <td class=\"p-4\">{{__('Name')}}</td>\n+
    \                      <td class=\"p-4\">{{__('Email')}}</td>\n+                       <td
    class=\"p-4\">{{__('Level')}}</td>\n+                       <td class=\"p-4\">{{__('Actions')}}</td>\n+
    \                  </tr>\n+                   </thead>\n+                   <tbody>\n+
    \                      <tr class=\"border\">\n+                           <td
    class=\"p-4\">1</td>\n+                           <td class=\"p-4\">Name here</td>\n+
    \                           <td class=\"p-4\">Email here</td>\n+                           <td
    class=\"p-4\">Level here</td>\n+                           <td class=\"p-4\">Actions
    here</td>\n+                       </tr>\n+                   </tbody>\n+               </table>\n
    \           </div>\n        </div>\n    </div>\n```\n\nNow, the page should show
    a table:\n\n![](3_users_table.png)\n\nLooks better, doesn't it? All that's left
    for us to do now is to show our actual users in the table. First let's adapt our
    view so it can use an array of users to display each user in a row in the table:\n\n```diff\n<tbody>\n+
    @foreach($users as $user)\n  <tr class=\"border\">\n-     <td class=\"p-4\">1</td>\n-
    \    <td class=\"p-4\">Name here</td>\n-     <td class=\"p-4\">Email here</td>\n+
    \    <td class=\"p-4\">{{$user->id}}</td>\n+     <td class=\"p-4\">{{$user->name}}</td>\n+
    \    <td class=\"p-4\">{{$user->email}}</td>\n      <td class=\"p-4\">Level here</td>\n
    \     <td class=\"p-4\">Actions here</td>\n  </tr>\n+ @endforeach\n</tbody>\n```\n\nThis
    makes the next issue obvious: The user level is not defined on the User model
    yet.  We will be adding it in soon, but first we need to pass along the users
    to the view. For that, open up the `UserController` and add:\n\n```diff\n+ use
    App\\Models\\User;\nuse Illuminate\\Http\\Request;\n\nclass UserController extends
    Controller\n{\n    public function index()\n    {\n-       return view('users.index');\n+
    \      return view('users.index')->with('users', User::getAllUsers());\n    }\n}\n```\n\nSee
    how the model is the class responsible for retrieving the data instead of the
    controller? This complies with the 'skinny controllers, fat models' paradigm,
    which is generally viewed as a best practice. Here's how the `getAllUsers` method
    in the `User` model looks:\n\n```php\npublic static function getAllUsers()\n{\n
    \   return User::all(); // should use pagination, but ok for now\n}\n```\n\nIf
    we reload our page, normally you'll see your user displayed in the table. Let's
    add some test users for good measure. We could use a database seeder for this,
    but that's like killing a fly with a bazooka. Fun, maybe, but very much overkill.
    Tinker is much more appropriate here:\n\n```php\nphp artisan tinker // use 'sail
    artisan tinker' if you're using sail!\n\nUser::factory()->count(10)->create()
    // create 10 users using UserFactory\n```\n\nOkay, now that we've got some decent
    testing data, we can move over to the next step: adding the User Level.\n\n##
    Leveling up the Users\n\nNext let's look at how we can expand the User model to
    add levels to it. These will be defined ahead of time and will have limited options.
    Freely translated: the perfect breeding ground for an enum! More specifically
    a [BackedEnum](https://www.php.net/manual/en/language.enumerations.backed.php).
    These have been added since PHP 8.1 so make sure your php version supports enums!
    Create a new  `app/Enums` directory and add the following in the `UserLevel.php`
    file:\n\n```php\n<?php\n\nnamespace App\\Enums;\n\nenum UserLevel: int\n{\n    case
    Member = 0;\n    case Contributor = 1;\n    case Administrator = 2;\n}\n```\n\nIn
    this case, we have 3 different levels: Member, Contributor and Administrator.
    These correspond to an integer, which is why it's called a Backed Enum. This way,
    we can use the human-readable name for each value when we're coding but only the
    integer value will be saved into the database. Neat!\n\nNow, let's pop the hood
    of our User Model and tack on the user level. In `app/Models/User`:\n\n```diff\n+
    use App\\Enums\\UserLevel;\n\n  ...\n\n  protected $fillable = [\n      'name',\n
    \     'email',\n      'password',\n+     'level',\n    ];\n\n...\n\n  protected
    $casts = [\n      'email_verified_at' => 'datetime',\n+     'level' => UserLevel::class,\n
    \ ];\n```\n\nBy adding the `level` attribute to the `casts` array, we inform Laravel
    that we want the integer from the database to be cast to the enum with all its
    functionality. If we didn't add it, our `level` attribute would be an integer
    on our Model.\n\nThere are many days when I would continue coding on other features
    or jump into the frontend stuff, and completely forget to add the migration. Today
    is not that day.\n\nCreate the migration with `php artisan make:migration add_level_to_users`
    and add the following:\n\n```diff\n/**\n     * Run the migrations.\n     *\n     *
    @return void\n     */\n    public function up()\n    {\n        Schema::table('users',
    function(Blueprint $table) {\n+           $table->integer('level')->default(0);\n
    \       });\n    }\n\n    /**\n     * Reverse the migrations.\n     *\n     *
    @return void\n     */\n    public function down()\n    {\n        Schema::table('users',
    function(Blueprint $table) {\n+           $table->dropColumn('level');\n        });\n
    \   }\n```\n\nWe've added a column in the `users` table, with an `integer` data
    type. The default value is 0, but there's no easy way to know what actual level
    that corresponds to… But that's exactly why we're using enums, so let's change
    that line:\n\n```diff\n+ use App\\Enums\\UserLevel;\n\n...\n\npublic function
    up()\n{\n  Schema::table('users', function (Blueprint $table) {\n-   $table->integer('level')->default(0);\n+
    \  $table->integer('level')->default(UserLevel::Member->value);\n  });\n}\n```\n\nThis
    way, the integer value of our Member level can change without breaking the functionality;
    users will still be Members by default. Don't forget to run the `php artisan migrate`
    or `sail artisan migrate` now!\n\nLast but not least, let's update our view to
    reflect the correct user level. I've jazzed it up and made it into a badge for
    no good reason actually. In `views/users/index.blade.php`:\n\n```diff\n+ @php
    use App\\Enums\\UserLevel; @endphp\n\n- <td class=\"p-4\">Level here</td>\n+ <td
    class=\"p-4\">\n+   <span @class([\n+           'px-2 py-1 font-semibold text-sm
    rounded-lg',\n+           'text-indigo-700 bg-indigo-100' => UserLevel::Member
    === $user->level,\n+           'text-sky-700 bg-sky-100' => UserLevel::Contributor
    === $user->level,\n+           'text-teal-700 bg-teal-100' => UserLevel::Administrator
    === $user->level,\n+           ])>\n+     {{__($user->level->name)}}\n+   </span>\n+
    </td>\n```\n\nDon't forget to import the UserLevel class again!\n\nOne possible
    change for better readability would be to have methods on the user model that
    checks each user level, like isAdmin(), isContributor() and isMember(). You can
    add those whenever you want \U0001F609.\n\nHere's another small challenge for
    you: To check that everything looks good, we'll need users with different levels.
    Using Tinker, make your own user an Administrator, and add 3 new Contributors.
    This might be a great moment to refresh your knowledge on [Eloquent Factories](https://laravel.com/docs/9.x/eloquent-factories)
    . When you have at least one user for all 3 user levels, you can see my colorful
    badges in action \U0001F485. Here's how they look:\n\n![](4_level_badges.png)\n\n##
    Editing the Users\n\nAlright, every user has a level now. What if we wanted to
    promote a user by bumping them up a level? Right now we've been using Tinker but
    that's really more a local testing tool. We could add an `edit` page with a dropdown
    in our users view where the user level can be edited. Let's set that up quickly
    by making a new controller, adding a `create` and an `edit` route, and creating
    a new view. Let's begin with the controller:\n\n`php artisan make:controller UserLevelController`\n\nYou
    might be surprised to see a new User**Level**Controller instead of reusing the
    UserController. I do this to make everything 'Cruddy by design'. If you have no
    clue what I'm talking about, check out [Adam Wathan's talk at Laracon 2017](https://www.youtube.com/watch?v=MF0jFKvS4SI&t=1198s).\n\nNow,
    open up our new controller and add the following methods:\n\n```php\npublic function
    edit(User $user)\n{\n  return view('userlevels.edit')->with('user', $user);\n}\n\npublic
    function update(Request $request, User $user)\n{\n  $validated = $request->validate([\n
    \     'level' => ['required', new Enum(UserLevel::class)]\n      ]);\n\n  $user->level
    = $validated['level'];\n  $user->save();\n\n  return redirect(route('users.index'));\n}\n```\n\nThe
    `new Enum(UserLevel::class)` validation rule will check that level can be cast
    to an instance of UserLevel. For more info, check out [the Laravel docs on validation](https://laravel.com/docs/9.x/validation#rule-enum)\n\nAlso,
    make sure that the `app\\Enums\\UserLevel` and the `Illuminate\\Validation\\Rules\\Enum`
    are imported correctly or your IDE will complain and more importantly, the code
    won't work.\n\nNow, let's add our routes in `web.php`:\n\n```php\n// ! don't forget
    to import the UserLevelController !\nRoute::get('/users/{user}/edit', [UserLevelController::class,
    'edit'])\n    ->middleware(['auth'])\n    ->name('userlevels.edit');\n\nRoute::put('/users/{user}',
    [UserLevelController::class, 'update'])\n    ->middleware(['auth'])\n    ->name('userlevels.update');\n```\n\nLastly,
    let's add our view. It's fairly simple: We display the user's name, and add a
    select input to pick the correct user level. Create a new `userlevels` directory
    in `resources/views` and add an `edit.blade.php` file with the following content:\n\n```html\n@php
    use App\\Enums\\UserLevel; @endphp\n<x-app-layout>\n    <x-slot name=\"header\">\n
    \       <h2 class=\"font-semibold text-xl text-gray-800 leading-tight\">\n            {{
    __('Change User Level') }}\n        </h2>\n    </x-slot>\n\n    <div class=\"py-12\">\n
    \       <div class=\"max-w-7xl mx-auto sm:px-6 lg:px-8 space-y-6\">\n            <div
    class=\"p-4 sm:p-8 bg-white shadow sm:rounded-lg\">\n                <div class=\"max-w-xl\">\n
    \                   <header>\n                        <h2 class=\"text-lg font-medium
    text-gray-900\">\n                            {{ $user->name }}\n                        </h2>\n\n
    \                       <p class=\"mt-1 text-sm text-gray-600\">\n                            {{
    __(\"Update the user level of $user->name.\") }}\n                        </p>\n
    \                   </header>\n\n                    <form method=\"post\" action=\"{{
    route('userlevels.update', $user) }}\" class=\"mt-6 space-y-6\">\n                        @csrf\n
    \                       @method('put')\n\n                        <div>\n                            <x-input-label
    for=\"level\" :value=\"__('User Level')\"/>\n\n                            <select
    name=\"level\" id=\"level\"\n                                    class=\"w-full
    rounded-md shadow-sm border-gray-300 focus:border-indigo-300 focus:ring focus:ring-indigo-200
    focus:ring-opacity-50\">\n                                @foreach(UserLevel::cases()
    as $levelOption)\n                                    <option value=\"{{$levelOption}}\"
    @if ($levelOption == $user->level) selected=\"selected\" @endif>\n                                        {{$levelOption->name}}\n
    \                                   </option>\n                                @endforeach\n
    \                           </select>\n\n                            <x-input-error
    :messages=\"$errors->get('level')\" class=\"mt-2\"/>\n                        </div>\n\n
    \                       <div class=\"flex items-center gap-4\">\n                            <x-primary-button>{{
    __('Save') }}</x-primary-button>\n                        </div>\n                    </form>\n
    \               </div>\n            </div>\n\n        </div>\n    </div>\n</x-app-layout>\n\n```\n\nLet's
    try it out! Go to the users overview and… Well we'd better add a way to access
    our new routes, eh? It'll be a small change in `users/index.blade.php`:\n\n```diff\n-
    <td class=\"p-4\">Actions here</td>\n+ <td class=\"p-4\">\n+     <a href=\"{{route('userlevels.edit',
    $user)}}\" class=\"px-4 py-2 bg-gray-800 rounded-md font-semibold text-xs text-white
    uppercase tracking-widest\">Edit</a>\n+ </td>\n```\n\n<%= partial \"shared/posts/cta\",
    locals: {\n  title: \"Fly.io ❤️ Laravel\",\n  text: \"Fly.io is a great way to
    run your Laravel Livewire app close to your users. Deploy globally on Fly in minutes!\",\n
    \ link_url: \"https://fly.io/docs/laravel\",\n  link_text: \"Deploy your Laravel
    app!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>\",\n} %>\n\n## Protecting
    the Users\n\nRight now, every user can change the user level of every other user.
    That's not exactly what we want, is it?\n\nWe only want **some** users to be able
    to change the user level of another user. And as luck might have it we've already
    divided up our users in 3 categories: Administrators, Contributors and Members.
    So let's say only Administrators can change user levels.\n\nFor that we'll need
    a Policy: Run `php artisan make:policy UserPolicy` to make one. We'll use Controller
    Helpers to validate our edit and update methods, like this in `UserLevelController`:\n\n```diff\npublic
    function edit(User $user)\n    {\n+       $this->authorize('updateLevel', $user);\n
    \       return view('userlevels.edit')->with('user', $user);\n    }\n\n    public
    function update(Request $request, User $user)\n    {\n+       $this->authorize('updateLevel',
    $user);\n\n        $validated = $request->validate([\n            'level' => ['required',
    new Enum(UserLevel::class)]\n            ]);\n\n        $user->level = $validated['level'];\n
    \       $user->save();\n\n        return redirect(route('users.index'));\n    }\n```\n\nSo,
    how does this work? The `$this->authorize` checks if the logged in User has permission
    to `updateLevel` the `$user` .\n\nBehind the scenes, Laravel will look for a Policy
    that can be used for the `$user` Model. It will look in the `app/Policies` and
    `app/Models/Policies` folders if it can find a Policy with the correct name: `[name
    of Model] + Policy.php` . In our case that will be `UserPolicy`, which is exactly
    what we chose. This means that Laravel will automatically link our UserPolicy
    to actions we want to do with the User model.\n\nIn the UserPolicy, Laravel will
    run the `updateLevel` method with two parameters: the logged in User and the Model
    that user is trying to change. In our case this will be another User. If `true`
    is returned in the method then the action is allowed, else it will be forbidden.
    Let's quickly test out our `updateLevel` method, add the following in `UserPolicy`:\n\n```php\n/**\n
    * @param User $loggedInUser the user that's trying to update the level of $model\n
    * @param User $model the user whose level is being updated by the $loggedInUser\n
    * @return bool\n */\npublic function updateLevel(User $loggedInUser, User $model)\n{\n
    \ return false;\n}\n```\n\nRight now, no-one is allowed to change the user level
    of any user. If you try it out on the app you'd see a 403 Unauthorized message.
    That's great, we know our policy is working! Now, let's update the method so only
    Administrators can update the user levels:\n\n```diff\npublic function updateLevel(User
    $loggedInUser, User $model)\n{\n- return false;\n+ // don't forget to import the
    UserLevel enum!\n+ return UserLevel::Administrator == $loggedInUser->level;\n}\n```\n\nAlright,
    looking good! Well err, not that good actually: When a user is not allowed to
    change user levels, the 'edit level' button is still shown. That's not really
    user-friendly, showing them buttons they aren't allowed to click. We can easily
    fix that, like this in `users/index.blade.php`:\n\n```diff\n<td class=\"p-4\">\n+
    @can('updateLevel', $user)\n  <a href=\"{{route('userlevels.edit', $user)}}\"
    class=\"px-4 py-2 bg-gray-800 rounded-md font-semibold text-xs text-white uppercase
    tracking-widest\">Edit</a>\n+ @endcan\n</td>\n```\n\nThis will only show the button
    if the logged in user is allowed to click it. Much better.\n\nThere is one small
    issue left to fix. As a seriously kick-ass developer you've already thought of
    everything and have picked up on this issue already, but I'll still repeat it
    here: If the only administrator removes his administrator title, no one will be
    able to change the user levels ever again… Let's update that in the `UserPolicy`
    as well!\n\n```diff\npublic function updateLevel(User $loggedInUser, User $model)\n{\n
    \ // don't forget to import the UserLevel enum!\n- return UserLevel::Administrator
    == $loggedInUser->level;\n+ if (UserLevel::Administrator == $loggedInUser->level)\n+
    {\n+   // when deleting an Admin, check if there will be admins left\n+   if (UserLevel::Administrator
    == $model->level) return User::getNumberOfAdmins() > 1;\n+   else return true;\n+
    }\n+ else return false;\n}\n```\n\nWe're using a new method in the User model
    that doesn't exist yet: `getNumberOfAdmins`. Here's how it looks:\n\n```php\n//
    Add this in the User model:\npublic static function getNumberOfAdmins()\n{\n  //
    using ->count() is a much quicker database operation than using ->get() and counting
    in PHP.\n  return User::where('level', UserLevel::Administrator)->count();\n}\n```\n\n\n\nThere.
    Now, when an admin is logged in they can edit the user level of everyone, with
    one exception: they can't edit their own level if they are the only admin. Looks
    good!\n\n## Destroying the Users\n\nNow that's a brutal headline. Destroy them!
    Destroy them **A L L**!\n\nAnyway…\n\nI'm going to leave this one up to you. The
    goal here is almost exactly the same as the level editing, with one difference:
    non-administrators can delete themselves. You might be surprised how easy it is.
    Good luck!\n\nI made a separate pull request with all the delete functionality
    for your convenience, you can find it right [here](https://github.com/Johannes-Werbrouck/policies-levels-enums/pull/5).\n\nHere's
    what the finished result should look like:\n![](5_finished.png)\n\nThere we go,
    a detailed dive into Policies and how they can be used. We also explored some
    PHP 8.1 functionality: enums! I really do like them, since they make code much
    more readable.\n\nAs always, thanks for reading!\n\nJohannes"
- :id: blog-launching-redis-by-upstash
  :date: '2022-12-15'
  :category: blog
  :title: Launching Redis by Upstash
  :author: jsierles
  :thumbnail: love-thumbnail.jpg
  :alt:
  :link: blog/launching-redis-by-upstash
  :path: blog/2022-12-15
  :body: |2-


    <p class="lead">We're Fly.io. We put your code into lightweight microVMs on our own hardware [around the world](https://fly.io/docs/reference/regions/), close to your users. [Redis by Upstash](https://fly.io/docs/reference/redis/) is managed Redis living right next door to your Fly.io apps. [Check us out](https://fly.io/docs/speedrun/)—your app and database can be running close to your users within minutes.</p>

    We love databases that scale globally. As an [ambivalent](https://fly.io/blog/how-we-built-fly-postgres/) database provider, we built a global, automated [Postgres](https://fly.io/docs/postgres/advanced-guides/high-availability-and-global-replication/), and we [tinkered with global Redis](https://fly.io/blog/last-mile-redis/) on scrappy startup weekends. But the Fly.io forecast called for integration over invention. So we partnered up on launching a simple, global, low-latency Redis service built by the intrepid crew at [Upstash](https://upstash.com).

    _Redis by Upstash_ sounds good enough to launch a cologne. We think it's as big a deal. Oh, and there's a [generous free tier](https://fly.io/docs/reference/redis/#pricing).

    Keep reading to learn how our first integration came to life. Or, just [sign up for Fly](https://fly.io/docs/getting-started/log-in-to-fly/) and give it a try:

    ```cmd
    flyctl redis create
    ```
    ```out
    ? Select Organization: fly-apps (fly-apps)
    ? Choose a Redis database name (leave blank to generate one): redis-for-lovers
    ? Choose a primary region: Madrid, Spain (mad)
    ? Would you like to enable eviction? Yes
    ? Optionally, choose one or more replica regions: Amsterdam, Dallas, São Paulo, Johannesburg
    ? Select an Upstash Redis plan Free: 100 MB

    Your Upstash Redis database redis-for-lovers is ready.
    ```

    ## A Better Redis for Global Deployments

    So what's special here? I assure you: this isn't stock Redis with a price tag slapped on.

    Complex features like global read replicas demand good DX to get noticed. But in the managed Redis market, read replicas are elusive, hidden behind sales calls, enterprise pricing plans and confusing UI.

    With `flyctl redis update` and a few keystrokes, you can spin up global Redis replicas in seconds, with [write forwarding](https://fly.io/docs/reference/redis/#writing-to-replica-regions) switched on. Reads _and_ writes make their way to the geographically-nearest replica, which happily forwards writes along to its primary, [ensuring read-your-write consistency](#ryow) along the way. So, with a single Redis URI, you can safely experiment with global deployment without changing your app configuration.

    VM-to-Redis requests are reliably fast, in every region, because your apps run on the same bare metal hardware as your databases, one network hop away at most. Check out Upstash's [live latency measurements](https://latency.upstash.io/fly.io/read/1kb) to compare Fly.io with serverless platforms like Vercel or AWS. This comparison is not entirely fair, as we run apps on real VMs; not in JavaScript isolates. But we love the colors.

    Finally, it's worth mentioning these databases are secure: only reachable through your Fly.io encrypted, private IPv6 network.

    ## Like a Surgeon

    When this integration was on the cards, we had two clear goals: don't expose Redis to the internet, and give Upstash full control of their service without compromising customer app security. Serendipity struck as we pondered this.

    We were knee-deep in fresh platform plumbing — the [Machines API](https://fly.io/docs/reference/machines/) and [Flycast private load balancing](https://fly.io/docs/reference/private-networking/#private-load-balancing-aka-flycast). The API grants precise control over where and how VMs launch. And Flycast yields anycast-like powers to apps on the private, [global WireGuard mesh](https://fly.io/docs/about/healthcare/#wireguard-everywhere).

    So Upstash Redis is a standard Fly.io app — a multitenant megalith running on beefy VMs in all Fly.io regions. These VMs gossip amongst themselves over their private IPv6 network. Upstash uses our API to deploy. We support Upstash like any other customer. Awesome.

    But Redis runs in its own Fly.io organization, and therefore, in its own isolated network. And customer apps, each in their own. We needed a way to securely connect two Fly applications. Enter Flycast, stage left.

    Flycast is a beautiful, complex cocktail of BPF, iptables and tproxy rules: fodder for another post! Flycast offers public proxy features — geo-aware load balancing, concurrency control and TLS termination — between apps that share a private network. With a small tweak, Flycast could now surgically join services with customer networks.

    Customer apps can connect to their provisioned Redis, but not to anything else in the Upstash private network. Upstash can't access the customer's network at all. Mission accomplished.

    ## A Tale of Provisioning

    You might be curious how provisioning Redis works, end-to-end.

    Your `flyctl redis create` hits the Fly.io API. We mint a fresh Flycast IP address on your network and pass that IP along to Upstash's API with the desired database configuration.

    In the same request, Upstash informs their Fly.io mega-deployment about your database, and we (Fly.io) point the Flycast address at Upstash's app. We blast this info to our global proxies. They'll now proxy connections on this IP to the nearest healthy mega-Redis instance. This all happens in a matter of seconds.

    Alright, so now you have a Redis connection URL to chuck requests at.

    Remember that Upstash's Redis deployment is _multitenant_. Upstash hosts scores of customer databases within a single OS process. With a clever shuffling of data from RAM to [persistent disks](https://fly.io/docs/app-guides/git-gogs-server/#persistent-disks), many, many more databases can fit in this instance than your average Redis running on its own VM.

    But multitenancy poses a problem. How can mega-Redis identify the target database for a given request?

    Your Redis URL includes a unique database password (remember this is all private, encrypted traffic). Could we use this password to identify your database? Technically, yes, but if you leak your Redis password on a live coding stream, anyone else with a Redis database could hijack yours! Yeah, let's not.

    Before, we passed your Flycast IP address to Upstash, so they have it on record. Could they match that against the source address of the incoming Redis TCP connection? Not quite! Connections to Redis pass through our proxy. So, traffic will appear to arrive from the proxy itself; not from your Flycast IP.

    No worries! We've got another trick up our sleeve.

    ## A Protocol for Proxies

    Bonus: our proxy supports prepending [proxy procotol](https://www.haproxy.org/download/1.8/doc/proxy-protocol.txt) headers to TCP requests.

    This curious 10-year-old internet resident is understood by most web servers and programming languages. At the top of the protocol [spec](https://www.haproxy.org/download/1.8/doc/proxy-protocol.txt), we spot our problem:

    > Relaying TCP connections through proxies generally involves a loss of the
    > original TCP connection parameters such as source and destination addresses,
    > ports, and so on.

    Redis runs on port 6379, just because. Here's a typical header for Redis connection initiation:

    `PROXY TCP6 fdaa:0:47fb:0:1::19 fdaa:0:47fb:0:1::16 6379 6379`

    Here we have two IPs — source and destination — on the same lovingly-named network, `fdaa:0:47fb`. The source IP belongs to the application VM, which is assigned randomly and is of little use here. But the destination address is the Flycast IP assigned to our particular database. Ace.

    Now we're in the home stretch. Redis parses this header, plucks out that Flycast IP, finds the associated customer database, and forwards traffic to it. In wafts the sweet aroma of victory.

    ## A Need for Speed

    Let's talk about a clear-cut use case for global Redis: caching HTML at the edge.

    Last year we turbo-boosted our Paris-based, recipe finder Rails app by [deploying Postgres replicas around the globe](https://fly.io/ruby-dispatch/run-ordinary-rails-apps-globally/). But our database has grown. We don't need to replicate all of its contents, and we're too busy to spend time optimizing our queries. Let's just lean on a lightweight HTML cache, which Rails is good at.

    We know we can get similar or better performance by caching HTML in Redis alongside our deployed VMs. And we can do this in a few minutes, really. First, let's add a few read replicas in distant, exotic lands.

    ```
    ~ $ fly redis update cookherenow-redis
    ? Choose replica regions, or unselect to remove replica regions:  [Use arrows to move, space to select, <right> to all, <left> to none, type to filter]
    > [ ]  Amsterdam, Netherlands (ams)
      [x]  Denver, Colorado (US) (den)
      [ ]  Dallas, Texas (US) (dfw)
      [ ]  Secaucus, NJ (US) (ewr)
      [ ]  Frankfurt, Germany (fra)
      [x]  São Paulo (gru)
      [ ]  Hong Kong, Hong Kong (hkg)
      [ ]  Ashburn, Virginia (US) (iad)
      [x]  Johannesburg, South Africa (jnb)
      [ ]  Los Angeles, California (US) (lax)
      [ ]  London, United Kingdom (lhr)
      [ ]  Chennai (Madras), India (maa)
      [ ]  Madrid, Spain (mad)
      [ ]  Miami, Florida (US) (mia)
      [x]  Santiago, Chile (scl)
    ```

    Then, with [a sprinkle of Rails magic](https://github.com/jsierles/cookherenow/commit/042c29d2fe8b7f28578dd35091a310e278af5983), our naive HTML cache is on the scene. Metrics can be boring, so, trust us that our [Time To First Byte](https://en.wikipedia.org/wiki/Time_to_first_byte) is still in the low milliseconds, globally, for GET requests on cached recipe pages.

    ## RYOW

    Now and then, one must write. And [read-your-own-write consistency](https://jepsen.io/consistency/models/read-your-writes) is a thing you need to care about when hitting speed-of-light latency in global deployments. That's life, kids.

    Readers hitting database replicas may not be served the very freshest of writes. We're OK with that. Except in one case: when that replica is serving the author of the write. Good UX demands that a writer feel confident about the changes they've made, even if they have to wait a few hundred milliseconds.

    To that end, Upstash Redis replicas take one of two paths to ensure a consistent read-your-own-write experience, with some trade-offs. Let's talk it out.

    Isa — one our recipe editors in Santiago — is worried that the recipe for [Humitas Chilenas](https://cookherenow.com/recipes/288370) mentions New Mexico Green Chiles. While they may be the first chiles [grown in outer space](https://www.nasa.gov/feature/chile-peppers-start-spicing-up-the-space-station/), they're generally not tossed into _humitas_. So she makes corrections and proudly smashes that **ENVIAR** button.

    Meanwhile, Santiago Redis has been diligently keeping track of the unique IDs of the writes that pass through Isa's Redis connection.

    So, that write is forwarded on to Paris, securely, over the Wireguard mesh. Santiago Redis holds blocks on the write command, waiting for replication to catch up to _this specific write_. On a clear internet day, we might wait 150ms, and Isa is redirected to the recipe page and sees her updated recipe sans chiles.

    But under poor network conditions, we may need to wait longer, and we don't want to wait forever. Editing must go on. This kind of thing can happen, and we need to be prepared for it.

    So, the less happy path: Santiago Redis waits up to 500ms for the written value to return via replication. After that, Redis client connection is released, suggesting to the Redis client that the write completed. Now, this is risky business. If we redirect Isa to her recipe before her write makes that round trip, she gets spicy _Humitas_ once again. New Mexican space chiles haunt her confused mind.

    No fear - Santiago Redis has our back. Remember that it was tracking writes? When Isa's recipe read is attempted, Santiago grabs the ID of the most recently tracked write on her connection. It checks to see if that ID exists in the replicated database contents. If so, Isa gets a fast, correct read of her updated recipe.

    But if her change didn't arrive yet, Santiago _forwards the read operation_ to our our source of truth — Paris Redis — at the cost of another full round trip to Europe. Such is the price of consistency.
- :id: phoenix-files-flying-with-a-fledgling-phoenix
  :date: '2022-12-14'
  :category: phoenix-files
  :title: Flying with a Fledgling Phoenix
  :author: mark
  :thumbnail: fledgling-thumbnail.jpg
  :alt:
  :link: phoenix-files/flying-with-a-fledgling-phoenix
  :path: phoenix-files/2022-12-14
  :body: "\n\n<p class=\"lead\">This is about trying out new Phoenix release candidates.
    Fly.io is a great place to run your Phoenix applications! Check out how to [get
    started](/docs/elixir/)!</p>\n\n[Phoenix](https://www.phoenixframework.org/) is
    a living, breathing and active project. Prior to the release of a new major version,
    there is often a \"release candidate\" for people to test out and report issues.
    Getting and trying those pre-release versions isn't straight forward. We'll see
    at how to get an RC, try it out on a new project, and even how to compare differences
    with generated files created by different versions of Phoenix to better understand
    how to upgrade an existing project.\n\n## Problem\n\nA new Phoenix release candidate
    has just landed and it includes some new features we want to try out. However,
    upgrading our existing project to the new release would be messy and time-consuming.
    We would encounter warnings, errors, and conflicts with other dependencies, and
    it would be difficult to see how the generators have changed.\n\nTo play with
    our new fledgling Phoenix release candidate without getting burned, it is best
    to create a new project. The [Phoenix installation guides](https://hexdocs.pm/phoenix/installation.html)
    instructs us to run `mix archive.install hex phx_new` to install Phoenix, but
    this will not get the release candidate.\n\nHow do we install and manage a Phoenix
    release candidate? And once we have it, how can we compare the differences between
    a newly generated Phoenix project from a previous version?\n\n## Solution\n\nBefore
    we start messing with our Phoenix Installer version, let's take note of our current
    version by running the following command:\n\n```cmd\nmix phx.new --version\n```\n```output\nPhoenix
    installer v1.6.15\n```\n\nThis is useful to know if we want to revert back to
    our original version later.\n\nTo try out a pre-release version like 1.7.0-rc.0,
    we use the command recommended in the [Phoenix release candidate blog post](https://www.phoenixframework.org/blog/phoenix-1.7-released):\n\n```\nmix
    archive.install hex phx_new 1.7.0-rc.0\n```\n\nWe will be prompted to replace
    our existing Phoenix Installer. There can only be one installed at a time. Say
    \"yes\" to install and replace any existing version.\n\nTo confirm that we are
    now using the new Phoenix version, run the following command:\n\n```cmd\nmix phx.new
    --version\n```\n```output\nPhoenix installer v1.7.0-rc.0\n```\n\nWe're ready to
    create a new app!\n\n### Generate a new app\n\nBefore we create a new app, let's
    review the available flags to make sure we include the features we want. To see
    the available flags, run:\n\n```\nmix help phx.new\n```\n\nNow we can create a
    new Phoenix app to play with. For this example, let's use the name `my_app`. We
    will come back to the benefits of this name choice later.\n\n```\nmix phx.new
    my_app\n```\n\nFor this example, we want to check out how the `phx.gen.auth` changed
    in the new release, specifically that it now supports LiveView. For a deeper look
    at this new feature, check out Berenice Medel's post  [Bringing Phoenix Authentication
    to Life](/phoenix-files/phx-gen-auth/).\n\nBefore we generate anything, let's
    review the available flags by running:\n\n```\nmix help phx.gen.auth\n```\n\nWith
    this knowledge, we can now generate something:\n\n```\nmix phx.gen.auth Accounts
    User users\n```\n\nGreat! We now have a working and interactive example of the
    new feature we are interested in. Personally, I'm enjoying playing with the new
    authentication features and seeing how they render and feel! \U0001F60D\n\n###
    Compare against previous releases\n\nIt is helpful seeing a newly generated Phoenix
    app and the generated templates we care about! But how can we compare these changes
    to a project that's been over a year in development with a whole team hacking
    on it? We _could_ try directly comparing our existing app with the newly generated
    one, but that will be so noisy it's not worth doing. I promise. Ouch.\n\nInstead,
    we'll generate another Phoenix app using the version that our project was based
    on. This makes it easier to see the relevant pieces that changed. Then we can
    adapt those changes to our project!\n\nBefore we start, let's rename the generated
    project folder to help keep track of it. Previously, we named it `my_app` and
    a folder with the same name was created. Let's rename the folder to `phx_1_7_rc_0`.\n\nNow,
    we're ready to generate an older Phoenix project. To prevent nearly every single
    file from being different, we'll use the same name, `my_app`.\n\nLet's install
    the desired version of the Phoenix Installer. In this case, we'll go with 1.6.15.\n\n```\nmix
    archive.install hex phx_new 1.6.15\n```\n\nNow generate an older project with
    the same name, `my_app` and include any relevant flags.\n\n```\nmix phx.new my_app\n```\n\nAfter
    generating the app, let's rename the folder from `my_app` to `phx_1_6_15` to make
    it easier to tell what we're looking at.\n\nWe now have two different generated
    Phoenix applications named `my_app` in two different versions of Phoenix! Let
    the comparisons begin!\n\n### Comparing versions locally\n\nNavigating and comparing
    two full projects may feel daunting. Never fear! Using a free, cross-platform
    tool like [Meld](https://meldmerge.org/), we can easily compare two complete directories
    to see what changed from one version to the next.\n\nFor this example, we are
    comparing the generated authentication templates, so we need to run the same generator
    on our older project. We need to `mix get.deps` and `mix compile` before the generator
    task is available. Then we can run it like this:\n\n```\nmix phx.gen.auth Accounts
    User users\n```\n\nNow we can compare the two projects using a directory comparison
    tool like Meld.\n\n![Meld application screenshot showing the compare directory
    button](./meld-directory-comparison.png?card&2/3&centered)\n\nSelect the directories
    with our projects.\n\n![Meld application screenshot showing selected directories
    for comparison](./meld-directories-selected.png?card&2/3&centered)\n\nDirectories
    with nested contents are compared.\n\n![Meld application screenshot showing directories
    being compared](./meld-directories-compared.png?card&2/3&centered)\n\nWhen a specific
    files was modified, we can open that file for a single-file comparison.\n\nNow
    we can discover what changed and adapt those to our other projects.\n\n### Comparing
    versions online\n\nFor the impatient among us who want to quickly see the differences
    from one version to the next with different flags used when the projects were
    generated, there is a great community resource over on [ElixirStream.dev](https://elixirstream.dev/)
    created and maintained by [David Bernheisel](https://twitter.com/bernheisel).
    The tool we're talking about is the [Generator Diff](https://elixirstream.dev/gendiff).\n\nGenerator
    Diff caches computed comparisons, so here's a link to the [phx.gen.auth generator
    differences between Phoenix 1.6.15 and Phoenix 1.7.0-rc.0](https://elixirstream.dev/gendiff/phx_new/CB447D8FF307672A6824C11F577B1746).
    It also supports comparing the different flag options, so it's very helpful!\n\n![ElixirStream.dev
    Generator Diff tool](./elixirstream-gendiff.png?card&2/3&centered)\n\nIf you just
    want to see the diff for two project versions, this is a great resource!\n\n##
    Summary\n\nAs Phoenix continues to advance and improve, we may want to quickly
    try out the new features and, when we're ready, apply those changes to our projects
    to keep them up-to-date with current conventions.\n\nWe started off by generating
    a fresh Phoenix app with the new features we want to explore and safely play with
    that enticing fledgling Phoenix release without getting ourselves burned in the
    process!\n\nWe then saw how we can generate multiple versions of a Phoenix app
    locally. The benefits of this is we use the development tools we already know
    to inspect the projects. We can run and interact with these local projects too.
    This is a great way to try out and see how a new approach _feels_ rather than
    inferring based on the code.\n\nWe introduced the free, cross-platform tool [Meld](https://meldmerge.org/)
    that helps us visually compare complete directories.\n\nWe also covered how we
    can use online community resources like [ElixirStream's Generator Diff](https://elixirstream.dev/gendiff)
    tool to quickly compare file diffs for changes from one version to another.\n\nWith
    options like these available, we have the tools needed to keep our projects feeling
    fresh both inside and out!\n\nNow, I'm off to play with the new LiveView authentication
    before applying the changes to my project. \U0001F929 What are you most excited
    to play with?\n\n<%= partial \"shared/posts/cta\", locals: {\n  title: \"Fly.io
    ❤️ Elixir\",\n  text: \"Fly.io is a great way to run your Phoenix LiveView app
    close to your users. It's really easy to get started. You can be running in minutes.\",\n
    \ link_url: \"https://fly.io/docs/elixir/\",\n  link_text: \"Deploy a Phoenix
    app today!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>\"\n} %>\n"
- :id: ruby-dispatch-single-file-rails-app
  :date: '2022-12-14'
  :category: ruby-dispatch
  :title: Single File Rails Apps
  :author: brad
  :thumbnail: single-file-rails-app-thumbnail.jpg
  :alt:
  :link: ruby-dispatch/single-file-rails-app
  :path: ruby-dispatch/2022-12-14
  :body: |2-


    <div class="lead">At first glance single file Rails application seems like a toy project, but they do have practical applications. Let's take a look at how Sitepress, a semi-static site generator, uses a single page Rails application at its core for a site-generator that can both compile static websites and run inside of Rails.</div>

    There's been a few interesting blog posts written about [single file Rails applications](https://greg.molnar.io/blog/a-single-file-rails-application/), but they all seem to stop short of describing practical use cases where you might actually ship a single page Rails application.

    ## Problem: I need a lightweight CMS in a Rails application

    A few years ago I was cleaning up a large Rails application. Part of that clean-up job was consolidating all of the different ways content was being managed around the application. The most common thing I've seen is a controller created per page, then that controller's `#show` action being pointed towards that one content page. This is a very common pattern in small Rails applications, and it gets tedious fast.

    At the same time, I was building static websites with [Middleman](https://www.middlemanapp.com/), which had fantastic APIs like Sitemap, Frontmatter, and various view helpers that made building websites fun. How could I get that inside Rails?

    The obvious thing to do is try adding the `middleman` gem to Rails, but I found out quickly that wasn't going to work.

    ## Before A Gem Is Born

    The first thing I do before I write any code for a gem is write down a few basic requirement.

    Here's what I needed for my project:

    1. Middleman features, like Sitemap and Frontmatter, that work in Rails.
    2. Must work from within an existing Rails application.
    3. Bonus if I can run a compilation command that emits static HTML, CSS, and JavaScript files.

    Once I have my requirements, I start looking around to see if something already exists that satisfy them.

    The closest project I found that could be embedded in Rails was [High Voltage](https://github.com/thoughtbot/high_voltage), but it lacked a Sitemap and Frontmatter. I did a few experiments where I tried embedding Middleman and Jecklly into a Rails application, but those quickly failed.

    I quickly realized the only path forward was to create a new gem, and Sitepress was born.

    ## Say Hello to Sitepress

    When Sitepress was originally built, it was a goal to have it run both inside Rails and outside of Rails. I spent most of my time getting Sitepress working great inside of Rails.

    Getting it working outside of Rails was a pain. I wired up a rack server, used Tilt for rendering, then I needed helpers. Getting layouts working properly was full of bugs. I needed to have a decent answer for assets. The more I got into it, the more I felt like I was re-inventing a lot of Rails features, and even worse, wasting my time trying to re-integrate all of this stuff so it would work similar to Rails.

    About the time I felt like giving up on the stand-alone version of Sitepress, I ran across a blog post entitled, "Single File Rails Applications". That's when the light-bulb went off in my head and I re-framed the problem for stand-alone Sitepress, "what if I can use a Rails application to run stand-alone Sitepress?"

    If I could pull that off I'd get access to `tailwind-rails`, an asset pipeline, all its view helpers, a layout and templating system that works, and access to an entire ecosystem of plugins.

    ## A look at Sitepress's single file Rails app

    The fun thing about Sitepress is that its [just a Rails app](https://github.com/sitepress/sitepress/blob/main/sitepress-server/lib/sitepress/server.rb) so you get to use all of the helpers you're already familier with in Rails. This means if you're into Majestic Monoliths, you can embed Sitepress in your Rails application and have a great content management system without the complexity of backing it by a database.

    ```ruby
    #
    require "action_controller/railtie"
    require "sprockets/railtie"
    require "sitepress-rails"

    # Require the gems listed in Gemfile, including any gems
    # you've limited to :test, :development, or :production.
    Bundler.require(*Rails.groups)

    # Configure the rails application.
    module Sitepress
      class Server < Rails::Application
        # Control whether or not to display friendly error reporting messages
        # in Sitepress. The development server turns this on an handles exception,
        # while the compile and other environments would likely have this disabled.
        config.enable_site_error_reporting = false

        # When in a development environment, we'll want to reload the site between
        # requests so we can see the latest changes; otherwise, load the site once
        # and we're done.
        config.enable_site_reloading = false

        # Default to a development environment type of configuration, which would reload the site.
        # This gets reset later depending on a preference in the `before_initialize` callback.
        config.eager_load = true
        config.cache_classes = true

        config.before_initialize do
          # Eager load classes, content, etc. to boost performance when site reloading is disabled.
          config.eager_load = !config.enable_site_reloading

          # Cache classes for speed in production environments when site reloading is disabled.
          config.cache_classes = !config.enable_site_reloading
        end

        # Path that points the the Sitepress UI rails app; which displays routes, error messages.
        # etc. to the user if `enable_site_error_reporting` is enabled.
        config.root = File.join(File.dirname(__FILE__), "../../rails")

        # Rails won't start without this.
        config.secret_key_base = SecureRandom.uuid

        # Setup routes. The `constraints` key is set to `nil` so the `SiteController` can
        # treat a page not being found as an exception, which it then handles. If the constraint
        # was set to the default, Sitepress would hand off routing back to rails if something isn't
        # found and fail silently.
        routes.append { sitepress_pages root: true, controller: "site", constraints: nil }

        # A logger without a formatter will crash when Sprockets is enabled.
        logger           = ActiveSupport::Logger.new(STDOUT)
        logger.formatter = config.log_formatter
        config.logger    = ActiveSupport::TaggedLogging.new(logger)

        # Debug mode disables concatenation and preprocessing of assets.
        # This option may cause significant delays in view rendering with a large
        # number of complex assets.
        config.assets.debug = false

        # Suppress logger output for asset requests.
        config.assets.quiet = true

        # Do not fallback to assets pipeline if a precompiled asset is missed.
        config.assets.compile = true

        # Allow any host to connect to the development server. The actual binding is
        # controlled by server in the `sitepress-cli`; not by Rails.
        config.hosts << proc { true } if config.respond_to? :hosts

        # Stand-alone boot locations
        paths["config/initializers"] << File.expand_path("./config/initializers")
      end
    end
    ```

    This is a big win for individuals or small teams who want to publish a few landing pages, some help pages, and terms of service, etc. and as Fly has shown, it's pretty straight forward to get this running quickly around the world, close to your customers, without adding the latency of a CDN to your application.

    ## When Do Single File Rails Applications Make Sense?

    Single page Rails applications shine when you need to deploy Rails applications to a bunch of workstations, as opposed to deploying them to your servers.

    * **Preview servers for web content** - Static site builders, like Sitepress, use single page Ruby applications to boot a preview server that developers can use to preview their changes.

    * **Local Web UI that manages local services** - Maybe you need to build a GUI that will run on a server in a data center or a workstation that's accessible remotely, but you don't want all the "heft" of a full blown rails app.

    * **Server-Side Rendered site** - Static websites are fast and have low operational complexity, but maybe you want to do a few dynamic things on the server like localize content for people. [A semi-static website](/ruby-dispatch/semi-static-websites/) can help that's running on a small Rails app.

    * **Test cases** - When reporting a bug in Rails core, it's helpful to isolate the exact Rails gems and configuration to help a maintain reproduce a bug. This approach can save a lot of time for the maintainer and increase the liklehood that your issue will get fixed.

    This shortlist will hopefully give you a few practical ideas that you can add to your toolbox for developing Rails applications.
- :id: laravel-bytes-php-js-livewire
  :date: '2022-12-13'
  :category: laravel-bytes
  :title: Bridging PHP and JavaScript with Livewire
  :author: kathryn
  :thumbnail: 00_thumbnail.png
  :alt: The background is set in a dark, but star-studded outerspace. Two planets
    are bridged together with a winding bridge, similar to two slightly-curled ribbons
    enclosing a highway road. The first planet to the left is aquatic with a pink
    hue, while the planet to the right is a terrain containing "JS" and "PHP" engraved
    in its soil.
  :link: laravel-bytes/php-js-livewire
  :path: laravel-bytes/2022-12-13
  :body: "\n<p class=\"lead\">This article talks about communication between PHP and
    JavaScript using Livewire. Livewire's faster close to your users. Deploy your
    Laravel app globally with [Fly.io](/docs), your app will be flying in no time!</p>
    \n\nWhat makes [Livewire](https://laravel-livewire.com/docs/2.x/quickstart) a
    gem is how it easily allows us to communicate data, methods, and events across
    PHP and JavaScript.\n\nNo long code, no extra syntax—just straightforward, easy
    PHP-JavaScript communication.\n\n\n## The Livewire Bridge\nLivewire bridges the
    gap between PHP and JavaScript with concise communication syntax.\n\nNo longer
    would we need to write up our client's http requests to call methods or get response
    data, nor would we need to dig up client arguments from the request received by
    our server. \n\nToday, we'll skim just a teensy bit beneath the surface of Livewire,
    and understand the role of its magical `metadata`. \n\nFrom there we'll see how
    tossing `metadata` between server and client creates the bridge for PHP and JavaScript
    communication.\n\nAlong the way we'll create a simple [chat-bot application](https://blue-sun-6217.fly.dev/talk-to-me-page):\n\n|
    ![A page containing a text box and a button with the ability to send messages
    to a chat bot. At the center of the page is a radiant, blue to pink colored heading
    \"Talk to Me\". Below it is a text box next to a button labeled \"Send\". A pointer
    moves from the bottom right towards the text box, and types \"What's up?\" inside
    the box. Then, the pointer clicks the button next to the text box. The browser
    responds to this action by displaying the text message a level below the text
    box, and makes a call to the server. Another level below the new displayed message,
    three bouncing, adjacent circular loaders is shown while the client waits for
    a response from the server. The loader eventually disappears and is replaced with
    a message from the server: \"Up? I'm up in the clouds.\". ](1_demo.gif) |\n|:--:|\n|
    The user can send any message at the click of a button, and the chat bot will
    respond with its answer. |\n\n\nIt's a bit rudimentary, but it will get us through
    some useful PHP-JS syntax:\n\n1. [Triggering a PHP method from JavaScript](/laravel-bytes/php-js-livewire/#calling-php-methods-from-javascript)\n2.
    [Dispatching a JavaScript event from PHP](/laravel-bytes/php-js-livewire/#php-invokes-javascript-events)\n3.
    [Listening and Reacting to a PHP event from JavaScript](/laravel-bytes/php-js-livewire/#reactive-javascript)\n4.
    [Reading a PHP variable from JavaScript](/laravel-bytes/php-js-livewire/#javascript-reads-php-vars)\n\nYou
    can check out our full repository [here](https://github.com/KTanAug21/sample-app#talk-to-me).
    You can also view the page [here](https://blue-sun-6217.fly.dev/talk-to-me-page).\n\n##
    Under the Hood\nA [Livewire Component](https://laravel-livewire.com/docs/2.x/making-components#introduction)
    comes in two parts. \n\n<b>A Livewire Class processing requests in the server:</b>\n\n```php\n#
    /app/http/livewire/TalkToMe.php\n<?php\n\nnamespace App\\Http\\Livewire;\n\nuse
    Livewire\\Component;\n\nclass TalkToMe extends Component\n{\n    // Public attribute
    shared between server and client\n    public $msg;\n\n    // Called from the ui
    whenever a new msg is sent by the user\n    public function getResponse()\n    {\n
    \       $response = (new \\App\\Services\\ChatBot)->processMessage($this->msg);\n
    \       $this->dispatchBrowserEvent(\n          'response-received', \n          ['response'=>$response]\n
    \       );\n    }\n\n    // Render the view\n    public function render()\n    {\n
    \       return view('livewire.talk-to-me');\n    }\n}\n```\n\n<b>And a Livewire
    view that is rendered  in the client's browser:</b>\n\n\n<aside class=\"right-sidenote\">\n<ul>\n<li>`wire:model`
    maps the input tag with a public attribute `$msg` configured in our Livewire Class
    above.</li>\n<li>wire:model`.defer` disables calls to the server during changes
    on the input element.</li>\n</ul>\n</aside>\n\n```html\n# /resources/views/livewire/talk-to-me.blade.php\n<div>\n
    \   <div>\n        <input id=\"msg\" wire:model.defer=\"msg\" class=\"...\" placeholder=\"What's
    on your mind?\">\n        <button onClick=\"sendMsg()\">Send</button>\n    </div>\n</div>\n```\n\n\n<b>We
    can embed this Livewire component in any blade component:</b>\n```\n<body>\n  <livewire:talk-to-me
    />\n  @livewireScripts\n</body>\n```\nIn the process of rendering this blade component,
    the server intelligently ( courtesy of Livewire ) renders the `<livewire:talk-to-me
    />` tag into a plain HTML version of the tag's initialized view.\n\nThe content
    changes, but an initialized Livewire view generally looks like:\n\n| ![An image
    containing a div tag of an initialized Livewire view. The tag contains \"wire:id\"
    and \"wire:initial-data\" metadata, and encloses a div tag. The second div tag
    encloses an input tag with several attributes: an \"id\" of msg, a \"wire:model.defer\"
    of msg, a redacted class, and a placeholder of \"What's on your mind?\". Below
    the input tag is a button tag with a redacted class and an \"onClick\" listener
    that can trigger a sendMsg function. This button encloses the label \"Send\".](2_init_wire.png)
    |\n|:--:|\n| Livewire adds two additional attributes in our view's root tag: `wire:id`
    and `wire:initial-data` |\n\n\nIt comes enclosing the actual content of the view,
    with two additional attributes embedded into its root tag. Livewire embeds an
    id in `wire:id`, along with its metadata in `wire:initial-data`.\n\nIt is this
    `metadata` shared between server and client that wires the bridge of communication
    across PHP and JavaScript.\n\n\n## The Wire that is Metadata\n\nThe Livewire class
    in the server shares its state with the client's Livewire view by [embedding](https://github.com/livewire/livewire/blob/master/src/Response.php#L31)
    `metadata` within the `wire:initial-data` attribute. \n\n| ![A screenshot of the
    page containing the chat input box and button mentioned in the 1_demo.gif image
    is shown to the left of the rendered html for the initialized livewire view described
    in 2_init_wire.png](3_inspect.png) |\n|:--:|\n| The Livewire class' details are
    jsonified into metadata, and embedded into `wire:initial-data` |\n\n\nThis metadata
    contains details regarding the identity of the Livewire class ( <b>fingerprint</b>
    ), the data the component needs to keep track of ( <b>serverMemo</b> ), and instructions
    on what needs to be handled on page load in the client ( <b>effects</b> ).\n\n|
    Attribute | Description |\n| --- | --- |\n| **fingerprint** | Identifies the Livewire
    class( id, name, etc... ) |\n| **serverMemo** | Data shared and persisted between
    server and client( i.e public attributes  ) |\n| **effects** | Details on effects
    the client needs to handle on page load |\n\n\n\nOn the client side, the Livewire
    view has its own [JavaScript](https://github.com/livewire/livewire/blob/master/js/index.js),
    courtesy of the `@livewireScripts` directive, that grabs our initialized Livewire
    view. The view's root element is  [passed](https://github.com/livewire/livewire/blob/master/js/index.js#L87)
    to Livewire's JavaScript class [\"Component\"](https://github.com/livewire/livewire/blob/master/js/component/index.js#L31)
    which parses and stores the `metadata` embedded in its `wire:initial-data`.\n\nAfterwards,
    it sets up the necessary JavaScript listeners and handlers matching the requirements
    of each wire attribute found in the Livewire view.\n\nOnce the required JavaScript
    has been set up for our wire attributes, our component is ready for user interaction.\n\n\n##
    Building the Chatbot\nThe [Component](https://github.com/livewire/livewire/blob/master/js/component/index.js)
    class above handles interaction in our Livewire view. It records interactions
    in its `updateQueue` [list](https://github.com/livewire/livewire/blob/master/js/component/index.js#L39),
    and, at the right time, coordinates these interactions to the server by sending
    this list as the `updates` [metadata](https://github.com/livewire/livewire/blob/master/js/Message.js#L12)
    to the server.\n\nLet's create a simple chat bot page where users can send messages
    and receive response. It's a bit rudimentary, but perfect for showcasing the role
    that the `updates` metadata plays in Livewire's PHP-JS communication.\n\n![The
    same gif image described in 1_demo.gif is shown left to an open browser's inspection
    table. A pointer moves towards the text box in the middle of the page, types a
    message, and clicks on a button next to it labeled \"Send\". The button click
    triggers a network request to be shown in the inspection table named \"talk to
    me\". A loader is shown and is eventually replaced with the message from the server.](4_demo_inspect.gif)\n\n```html\n<input
    id=\"msg\" wire:model.defer=\"msg\" class=\"...\" placeholder=\"...\">\n```\n\nOur
    message box above, `<input wire:model.defer=\"msg\"…`, is [wired](https://laravel-livewire.com/docs/2.x/properties#data-binding)
    to the `$msg` attribute in our Livewire PHP class.\n\n[Wiring](https://laravel-livewire.com/docs/2.x/properties#data-binding)
    our input element with `wire:model` [creates a listener](https://github.com/livewire/livewire/blob/master/js/node_initializer.js#L29)
    that reacts to changes on its value. \nEvery change is [pushed](https://github.com/livewire/livewire/blob/master/js/node_initializer.js#L94)
    by the listener as a `syncInput` update to the [Component's](https://github.com/livewire/livewire/blob/master/js/component/index.js#L39)
    [updateQueue](https://github.com/livewire/livewire/blob/master/js/component/index.js#L216)
    list\nand is <i>immediately</i> sent to the server as part of the `updates` metadata.\n\nHowever,
    we don't want every change in our input field to send a request to our server!
    This is why we've included the [`.defer`](https://laravel-livewire.com/docs/2.x/properties#deferred-updating)
    chain to our wiring. \n\nThe `.defer` chain [instructs](https://github.com/livewire/livewire/blob/master/js/node_initializer.js#L91)
    its listener not to fire the `syncInput` instruction immediately, but instead
    to store up the instruction in the [`deferredActions`](https://github.com/livewire/livewire/blob/master/js/component/index.js#L196)
    list for [future processing](https://github.com/livewire/livewire/blob/master/js/component/index.js#L234).
    \n\nNow that changes on our input field is deferred with `wire:model.defer`, let's
    manually apply a listener on the user's button click to send messages to our chat
    bot.\n\n```html\n<button class=\"...\" onClick=\"sendMsg()\">Send</button>\n```\n\nThis
    **onClick** listener will call our JS function `sendMsg()` to process our user's
    message.\n\n\n## Calling PHP methods from JavaScript\n\nAbove, we manually registered
    a button click to process our user's chat message. \n\nNow, let's trigger the
    PHP `getResponse()` method from our JS function `sendMsg()`:\n\n```javascript\n<script>\nfunction
    sendMsg(){\n  ...\n+  // Get response from server regarding our latest message.
    \n+  @this.getResponse(); \n}\n```\n\nYou might be wondering ( hmmm ), why not
    just  bind the button with the `wire:click` [directive](https://laravel-livewire.com/docs/2.x/actions#introduction)
    to easily map with a method call in our Livewire class?\n\nSometimes, it pays
    to mix and match JS with Livewire. Doing so gives us flexibility in separating
    client concerns from server concerns. \n\nWe can create some JS magic first before
    we actually call our Livewire method:\n\n```javascript\n+ let chat = [];\nfunction
    sendMsg(){\n+    // Add user's msg to our chat transcript \n+    // This chat
    transcript is only available in the client\n+    chat.push( {\n+      msg:sanitizeHTML(\n+
    \          document.getElementById('msg').value\n+       ),\n+      sender:0\n+
    \   });\n    \n+    // Display Messages in our chat list ui\n+    refreshTranscript();\n\n
    \   // Ask our server to respond to user's latest message\n    @this.getResponse();
    \n}\n```\n\nIn our case, we'd want to save the message sent by the user in a list
    and display the new message in our UI. Only then would we want to request for
    a response from the server.\n\n<aside class=\"right-sidenote\">\n`updateQueList`
    is mapped to the `updates` metadata sent as a [request payload](https://github.com/livewire/livewire/blob/master/js/Message.js#L12)
    to the server.\n</aside>\n\nTriggering `@this.getResponse()` adds a `callMethod`
    update to our `updateQueue` list, and fires an immediate request to the server.\n\n<%=
    partial \"shared/posts/cta\", locals: {\n  title: \"Fly.io ❤️ Laravel\",\n  text:
    \"Fly your servers close to your users&mdash;and marvel at the speed of close
    proximity. Deploy globally on Fly in minutes!\",\n  link_url: \"https://fly.io/docs/laravel\",\n
    \ link_text: \"Deploy your Laravel app!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>\",\n}
    %>\n\n## Tossing back the Metadata\n\nAbove, the client received the server's
    Livewire class metadata, displayed its Livewire HTML, and now with a user's click
    of a button, the client needs to coordinate some processing to the server through
    `@this.getResponse()`. \n\nHow exactly does the Livewire view coordinate changes
    to the server? \n\nIt tosses the `metadata` back to the server. \n\n| ![An image
    showing the request payload sent by the client during button click. The request
    payload contains the following metadata: fingerprint, serverMemo, and updates.
    fingerprint and serverMemo is the same metadata previously sent by the server
    to the client, while the updates metadata contains instructions like syncInput
    for the server to process.](5_mt_from_client.png) |\n|:--:|\n| The previous metadata
    `fingerprint` and `serverMemo` along with an `updates` attribute is sent to the
    server |\n\n\n## FIFO updates by the Server\n\n[What happens](https://github.com/livewire/livewire/blob/master/src/LivewireServiceProvider.php#L353)
    to subsequent requests our Livewire view makes to our Livewire class?\n\nThanks
    to the `fingerprint.name` sent back in `wire:initial-data`, our view sends a request
    to [/.../livewire/message/{fingerprint.name}](https://github.com/livewire/livewire/blob/master/js/connection/index.js#L41).\n\nThe
    server maps this to the proper Livewire class component, receives the `metadata`
    above from our client's request payload, and \"hydrates\" the properties of the
    selected class component using `serverMemo`. \n\nAfterwards, it performs any data-binding-updates
    ( `syncInput` ), action-calls ( `callMethod` ), and event-emissions ( `fireEvent`
    ) based on the instructions indicated in the `updates` metadata.\n\nIn our case,
    we had two instructions pushed in our client's `updates` metadata:\n\n| ![An image
    zooming in at the updates metadata sent by the client. It is an array containing
    two instructions. The first instruction is a `syncInput` with payload of id, name,
    and value. The second instruction is a `callMethod` instruction with a payload
    of id, method, and params.](6_mt_updates.png) |\n|:--:|\n| The updates metadata
    sent by the Livewire class contains instructions the Livewire class needs to process.
    |\n\nThe first instruction with type `syncInput` is the deferred action from changing
    the value of `<input id=\"msg\" wire:model.defer=\"msg\"`. This `syncInput` type
    instruction tells the Livewire class to update its attribute `$msg` with the value
    **hello**.\n\nThe second instruction is a `callMethod` type which triggers a `getResponse`
    method in our Livewire class. Which is basically this:\n\n```php\npublic function
    getResponse()\n{\n    $response = (new \\App\\Services\\ChatBot)->processMessage($this->msg);\n
    \   $this->dispatchBrowserEvent(\n      'response-received', ['response'=>$response]\n
    \   );\n}\n```\n\n<small>\n<ul>\n<li>Our updated `$msg` attribute is sent to a
    custom class `\\App\\Services\\ChatBot` for processing.</li>\n<li>Once we have
    the response, we simply call the Livewire `dispatchBrowserEvent()` method to add
    the event `\"response-received\"` in our effects list.</li>\n</ul>\n</small>\n\nThere
    are only two instructions in our `updates` list. After the last instruction is
    processed by the server, it proceeds with steps to dehyrdate its details into
    `metadata`, and finally sends back an updated `metadata` to the client:\n\n| ![A
    screenshot of the response from the server. It contains metadata with attributes
    \"effects\" and \"serverMemo\".](7_mt_from_server.png) |\n|:--:|\n| The `serverMemo`
    contains updated data attributes, the `effects` contain change instructions needed
    in the Livewire view |\n\nThe metadata sent back by the server contains only two
    attributes: `effects` and `serverMemo`. `effects` contain updates for the client
    to process, and `serverMemo` contains the updated data of the component.\n\nNow
    that we've received the server's response, let's add the response to our chat
    transcript and display it in our UI.\n\n## PHP invokes JavaScript events\n\nWith
    the use of our Livewire `metadata`, we <i>can</i> actually [invoke](https://laravel-livewire.com/docs/2.x/events#browser)
    JS events from PHP. In our `getResponse()` method above, we called \n```php\n$this->dispatchBrowserEvent('received-response',...)\n```
    \n\nDoing so pushes an event `response-received` to the `dispatches` array found
    inside the `effects` metdata sent back by the server:\n\n![A screenshot of the
    server's response zooming in on the dispatches list contained in its effects metadata.
    The dispatches metadata is an array containing an object. The object contains
    two attributes. The first attribute named \"event\" has a string value of \"response-received\".
    The second attribute named \"data\" contains a key-value pair of \"response\"
    as the key and \"Wazzup\" as the string value.](8_mt_dispatches.png)\n\nLivewire's
    JavaScript dispatches an event for each item included in the `effects.dispatches`
    array. That means we'll have a `received-response` event dispatched in our client's
    browser.\n\nRight there, in one line of code, is JavaScript event creation from
    PHP \U0001F60E\n\n## Reactive JavaScript\n\nNow that we have the event created
    in our client, this means we can easily have our JavaScript listen to the event
    dispatched from our server's `effects` metadata:\n\n```javascript\n window.addEventListener('response-received',
    event => {\n    // Add the server's message to our chat transcript\n    chat.push({\n
    \     msg: event.detail.response,\n      sender: 1\n    });\n\n    // Refresh
    the chat transcript displayed \n    refreshTranscript();    \n});\n```\n\n## JavaScript
    reads PHP vars\n\nLet's say we'd have some initialized PHP attribute from our
    Livewire class:\n```php\npublic $goodMsgs;\n\npublic function mount()\n{\n  $this->goodMsgs
    = (new \\App\\Services\\ChatBot)->goodMessageList();\n}\n```\n\nFrom our JavaScript
    we can simply use the `@js` directive to read this PHP variable: \n```diff\n<script>\n-
    let hints = JSON.parse('<?php echo json_encode($goodMsgs) ?>') \n+ let hints =
    @js($goodMsgs);\n```\nThe `@js` directive is one of the Livewire blade directives
    that Livewire class from the server scans the Livewire view for.\n\nThe server
    reads this and [renders](https://github.com/livewire/livewire/blob/master/src/LivewireBladeDirectives.php#L21)
    a proper JavaScript variable for us. This is pretty neat, because using it removes
    our need to manually add JSON parsing in JavaScript for reading objects or arrays.\n\n##
    Brighter Magic with Clarity!\n\nThe more we look under the [magic](https://youtu.be/x5qHj6bGvn0?t=1014)
    of Livewire, the more astonishing its workings become. \n\nBecause now, not only
    do we know the surface level of instantaneous, seamless interaction of wire elements
    with user interaction. \n  \nUnder the hood, we now know, that behind every PHP
    Livewire component, is a JavaScript [component](https://laravel-livewire.com/docs/2.x/inline-scripts#accessing-javascript-component-instance)
    coordinating interactions behind the scene.\n\nPHP and JavaScript communication
    is bridged, all under the [magic](https://calebporzio.com/how-livewire-works-a-deep-dive)
    of the wire that is `metadata`. \U0001FA84 \n\n\n"
- :id: phoenix-files-phx-gen-auth
  :date: '2022-12-13'
  :category: phoenix-files
  :title: Bringing Phoenix Authentication to Life
  :author: berenice
  :thumbnail: swirl-thumbnail.jpg
  :alt:
  :link: phoenix-files/phx-gen-auth
  :path: phoenix-files/2022-12-13
  :body: |2


    <p class="lead">With this post we reach the end a story we’ve [been](../phx-trigger-action/) [telling](../live-session/) [you](../forms-testing/) over the past few months. It’s the tale of how we developed a LiveView auth system that you can generate with a few keystrokes. Fly.io is a great place to run your Elixir applications! Check out how to [get started](/docs/elixir/)!</p>

    Many of us used the `phx.gen.auth` generator to quickly build out authentication systems in our Phoenix apps. It spits out well-designed auth logic so we can get on quickly to the more interesting parts of our app.

    But LiveView has increased in popularity, and in a LiveView app the generator’s “dead view” templates and views leave something to be desired. Wouldn’t it be great if those could be LiveView as well?

    The Phoenix team has developed a solution for that! We’ve been eagerly looking forward to the release of Phoenix 1.7 so we could announce some new and exciting functionality.

    We’ve created an auth system that uses only LiveView, and `phx.gen.auth` is ready to generate it for you. The command now takes a couple of options to let you choose whether to generate your auth system using Phoenix views &mdash;as before&mdash; or using LiveView:

    ```elixir
    mix phx.gen.auth Accounts User users --live
    mix phx.gen.auth Accounts User users --no-live
    ```

    A whole authentication system for LiveView, with a single command? Magic! But behind the magic, we can find some interesting implementation details in the generated code.

    The team took existing logic and adapted it to work with LiveView. Initially, each of the pages &mdash;Log in, Registration, Forgot Password, etc&mdash; was separated into different LiveViews. The functionality of the controllers was migrated to each of the respective LiveViews and the existing HTML was rendered using the [render/1](https://hexdocs.pm/phoenix_live_view/Phoenix.LiveView.html#c:render/1) callback. However, the process was not linear, there were a few issues to resolve before everything was working smoothly.

    ## Setting a Cookie from a LiveView Form

    The main issue was to write the logged-in user data in session when submitting a form within a LiveView.

    When we store data in cookies &mdash;such as the current user&#39;s identifier&mdash;, the following process is carried out through HTTP requests:

    [![](./cookies.png?centered&card&3/4&border)](./cookies.png)

    The browser identifies the `set-cookie` header and stores the cookie returned in the HTTP request response.  We can’t do this directly from a LiveView, because LiveView-server communication goes through a websocket connection &mdash;no HTTP requests&mdash;. So how can we store session data?

    The solution we settled on: go ahead and make an HTTP route call to a controller when the user submits their log-in or sign-up form &mdash;but validate the form data within the LiveView first, to make sure we don’t pop out of our LiveView unnecessarily.

    You can find more details on how we did this, with the `:action` attribute of Phoenix forms together with the `:phx-trigger-action` LiveView form attribute, in &quot;[Triggering a Phoenix controller action from a form in a LiveView](../phx-trigger-action/)&quot;.

    <aside class="right-sidenote">You can find some examples in login\_live.ex and \_registration\_live.ex files.</aside>
    [![](../2022-08-16/phx-trigger-action-cover.jpg?centered&card)](../phx-trigger-action/)

    ## Faster navigation

    We also took care to make navigation between related LiveViews (like the sign\_in and forgot\_password views) even more efficient and introduced the concept of LiveSessions. By grouping live routes into a LiveSession, we can navigate between them using the existing websocket connection, thus avoiding making extra HTTP requests. Let's see it in action!

    <%= video_tag "live_navigation.mp4?card&center&3/4&border", title: "User is navigating between pages and logs are printed in the iex console at the same time, showing that there are no extra HTTP requests" %>

    That&#39;s really cool but it was only half the job. We also used [live_redirect/2](https://hexdocs.pm/phoenix_live_view/0.10.0/Phoenix.LiveView.Helpers.html#live_redirect/2) from the `Phoenix.LiveView.Helpers` module to redirect between LiveViews in the same session &mdash;you won't find `live_redirect` in your generated code, though; more on that in a minute&mdash;.

    This is not all that LiveSessions can do, you can check how we used them together with Hooks to define different authorization strategies, and much more in &quot;[LiveSessions in action](../live-session/)&quot;.
    <aside class="right-sidenote">Take a look at the code in router.ex</aside>
    [![](../2022-09-05/live-session-cover.jpeg?centered&card)](../live-session/)


    ## Testing the auth system

    Once we had a working version, we also wanted to test that everything worked as expected. We used some functions of the `LiveViewTest` module to test, for example, that the LiveViews displayed the expected live errors and gave the user the necessary information, or test that the navigation between LiveViews was successful.

    You can check more examples in "[Testing LiveView forms](../forms-testing/)".

    <aside class="right-sidenote">You can find several examples in the test/your\_app\_web/live/ directory. </aside>
    [![](../2022-09-30/forms-testing-cover.jpg?centered&card)](../forms-testing/)


    ## The new LiveView and Phoenix features

    But this is not all you will find when using the `phx.gen.auth` generator, you will also find examples of new features introduced in LiveView 0.18!

    All LiveViews in the auth system use the UI components from the `CoreComponents` module (generated by creating a new project with `mix phx.new`). These components were designed by the Tailwind team exclusively for Phoenix and are defined using the new [`:attr`](https://hexdocs.pm/phoenix_live_view/Phoenix.Component.html#module-attributes) and [`:slots`](https://hexdocs.pm/phoenix_live_view/Phoenix.Component.html#module-slots) macros, taking a look at them is an excellent way to understand these new function component options.

    Remember the `live_redirect` function we used for faster navigation? Well, we won&#39;t actually find it in our auth system, since it was deprecated. Instead, we&#39;ll find the new [link/1](https://hexdocs.pm/phoenix_live_view/Phoenix.Component.html#link/1) component of the `Phoenix.Component` module, where the `:navigate` option was used as a substitute.

    You will also find examples of another of the Phoenix 1.7 features that the community is most excited about: [verified routes](https://www.phoenixframework.org/blog/phoenix-1.7-released#:~:text=1.7.0%2Drc.0-,Verified%20Routes,-Verified%20routes%20replace)!

    <%= partial "shared/posts/cta", locals: {
      title: "Fly.io ❤️ Elixir",
      text: "Fly.io is a great way to run your Phoenix LiveView app close to your users. It's really easy to get started. You can be running in minutes.",
      link_url: "https://fly.io/docs/elixir/",
      link_text: "Deploy a Phoenix app today!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>"
    } %>

    Tell us, have you already used the new generator? What was your favorite functionality?
- :id: phoenix-files-persistent-forms-with-your-url-on-liveview
  :date: '2022-12-12'
  :category: phoenix-files
  :title: Persistent forms with your URL on LiveView
  :author: lubien
  :thumbnail: toggle-thumbnail.jpg
  :alt: A mouse cursor near a search form, in a whimsical cartoon style.
  :link: phoenix-files/persistent-forms-with-your-url-on-liveview
  :path: phoenix-files/2022-12-12
  :body: |2-


    <p class="lead">In this article we talk about syncing LiveView state with URL. Fly.io is a great place to run your Phoenix LiveView applications! Check out how to [get started](/docs/elixir/)!</p>

    ## Problem

    Say you have a simple posts search form LiveView like this one:

    ![A form where you can search by post name and/or author](./initial-state.png)

    Whenever users change the title or author on the search form you perform the search just fine but whenever you hit the refresh button your filters are emptied, poof. We did not persist those in any way, that's why.

    ![When you refresh the page all your form input values are gone](./bad-case.gif)

    How can we persist our filters so that a page refresh would not make us have to do it all over again?

    ## Solution

    We will use a simple trick to sync your URL query string with you LiveView filters using nothing but [`push_patch/2`](https://hexdocs.pm/phoenix_live_view/Phoenix.LiveView.html#push_patch/2) and `handle_params/3`.

    <%= partial "shared/posts/cta", locals: {
      title: "Run your forms on Fly.io",
      text: "You can host your LiveView form apps here on Fly.io and get free SSL so users can see the lock icon on their browsers when they also see their URL change too.",
      link_url: "https://fly.io/docs/speedrun/",
      link_text: "Try Fly for free&nbsp;&nbsp;<span class='opacity-50'>&rarr;</span>"
    } %>

    ## The initial state

    It's very likely you used Phoenix's generators so your code must look almost like this on your `index.ex`:

    ```elixir
    defmodule FormUrlRecipeWeb.PostLive.Index do
      use FormUrlRecipeWeb, :live_view

      alias FormUrlRecipe.Blog
      alias FormUrlRecipe.Blog.Post

      @impl true
      def mount(_params, _session, socket) do
        {:ok, assign(socket, posts: [], authors: [])}
      end

      @impl true
      def handle_params(params, _url, socket) do
        {:noreply, apply_action(socket, socket.assigns.live_action, params)}
      end

      # Other actions omitted here, lets focus on the list

      defp apply_action(socket, :index, _params) do
        changeset = Blog.change_post(%Post{})

        socket
        |> assign(:page_title, "Listing Posts")
        |> assign(:post, nil)
        |> assign(:posts, Blog.search_posts(%{}))
        |> assign(:authors, Blog.list_authors())
        |> assign(:changeset, changeset)
      end

      @impl true
      def handle_event("search_posts", %{"post" => attrs}, socket) do
        changeset = Blog.change_post(%Post{}, attrs)

        {:noreply,
          socket
          |> assign(:changeset, changeset)
          |> assign(:posts, Blog.search_posts(attrs))
        }
      end

      # ...
    end
    ```

    As you can see there's two places who search for posts using `Blog.search_posts`. The first is inside `apply_action/3` which runs from `handle_params/3`, those happen when you open the page, and the other one is inside the `handle_event("search_posts", ...)` which is triggered from our form component.

    We can simplify this code and centralize where posts are loaded from on `apply_action/3` by simply making our `handle_event/3` just send you back to the same LiveView through `push_patch/3` like this:

    ```elixir
    @impl true
    def handle_event("search_posts", %{"post" => attrs}, socket) do
      {:noreply,
        socket
        |> push_patch(to: Routes.post_index_path(socket, :index, attrs))
      }
    end
    ```

    Assume we change the author to Michael, you will be sent to `/posts?author=Michael&title=`. Now we just need to start parsing the URL into a attributes we can fill our changeset and our search function:

    ```elixir
    defp apply_action(socket, :index, params) do
      # Changed these two lines below
      attrs = Map.take(params, ["title", "author"])
      changeset = Blog.change_post(%Post{}, attrs)

      socket
      |> assign(:page_title, "Listing Posts")
      |> assign(:post, nil)
      # Changed this line below
      |> assign(:posts, Blog.search_posts(attrs))
      |> assign(:authors, Blog.list_authors())
      |> assign(:changeset, changeset)
    end
    ```

    And that's it! We now sync URL with our forms plus we refactored our LiveView to deduplicate the search being run. Don't believe me? Here's the [commit URL](https://github.com/fly-apps/phoenix-recipe-sync-url-with-form/commit/70c163413017f89023977f4abe8935146a3bca9e).

    ## Notes

    It's worth mentioning Phoenix will understand any empty field as empty string so your params might look like `%{"title" => "", "author" => "Lubien"}`. For my search functionality to work the way I wanted [I've filtered any `nil` or `""` inside the Blog.search_posts function](https://github.com/fly-apps/phoenix-recipe-sync-url-with-form/blob/70c163413017f89023977f4abe8935146a3bca9e/lib/form_url_recipe/blog.ex#L43-L50).
- :id: ruby-dispatch-dockerfile-less-deploys
  :date: '2022-12-07'
  :category: ruby-dispatch
  :title: Dockerfile-less-deploys
  :author: rubys
  :thumbnail: gift-wrapping-thumbnail.jpg
  :alt: custom gift wrapping booth at a mall
  :link: ruby-dispatch/dockerfile-less-deploys
  :path: ruby-dispatch/2022-12-07
  :body: "\n\n<div class=\"lead\">\n  [Fly.io](http://fly.io/) is a great place to
    run Rails applications, especially if you plan on running them on multiple servers
    around the world so your users have a fast, snappy, low-latency experience.\n[Give
    us a whirl](https://fly.io/docs/rails/getting-started/) and get up and running
    quickly.\n</div>\n\n## Introduction\n\nFirst, let's talk about why we decided
    to use [OCI](https://opencontainers.org/) container images in the first place.
    In short, containers are freaking awesome. They allow us to package up our applications
    and all of their dependencies into a single, self-contained unit that can be easily
    extracted, deployed, and run on any machine. This makes it super easy for our
    users to get up and running with Fly.io, without having to worry about all the
    little details of setting up their environment.\n\nThis naturally leads to a baseline
    approach where every framework uses a [Dockerfile](https://docs.docker.com/engine/reference/builder/)
    and a [TOML](https://toml.io/en/) file.  This is great for system administrators,
    polyglots, and Rails developers who are comfortable with Dockerfiles. What that
    leaves behind is Rails developers who spend most of their time in an IDE on Macs
    or Windows; which frankly is most of them.  Many of which have been spoiled by
    Heroku for the past 10 years.\n\nA desire to avoid Dockerfiles has lead many to
    prefer to use buildpacks, nixpacks or other alternatives, and when they have problems
    with those approaches instead of reporting the problems to the maintainers of
    these alternatives they report the problem to us.\n\nBelow is a proof of concept
    of an alternative approach where from a Fly.io platform point of view everything
    is Dockerfiles and TOML files and from a developer point of view everything is
    Rails and Ruby, giving us the best of both worlds.\n\nThis is not a radical change.
    \ `flyctl` will already build you an initial Dockerfile that meets many needs.
    \ This merely takes that approach further by dynamically generating a custom tailored
    Dockerfile on *every* Deploy.\n\nWhat it does mean is that the Dockerfile needs
    to be correct and complete every time.  No more relying on webpages of instructions.
    \ Fortunately [Thor](http://whatisthor.com/) and [ERB](https://github.com/ruby/erb#erb)
    are good at this.\n\nIn order to run this make sure you have flyctl version v0.0.433
    or later as this is when support was added for dockerignore files to be provided
    at deploy time.\n\n<%= partial \"shared/posts/cta\", locals: {\n  title: \"You
    can play with this right now.\",\n  text: \"It'll take less than 10 minutes to
    get your Rails application running globally.\",\n  link_url: \"https://fly.io/docs/rails/\",\n
    \ link_text: \"Try Fly for free&nbsp;&nbsp;<span class='opacity-50'>&rarr;</span>\"\n}
    %>\n\n---\n\n## Part one, a simple visitor counter\n\nStart by creating a simple
    application and scaffold a visitor counter table:\n\n```bash\nrails new welcome
    --css tailwind\ncd welcome\ngit add .\ngit commit -a -m 'initial commit'\nbin/rails
    generate scaffold visitor counter:integer\nbin/rails db:migrate\n```\n\nModify
    the index method in the visitor controller to find the counter and increment it.\n\nEdit
    `app/controllers/visitors_controller.rb`:\n\n```ruby\n  # GET /visitors or /visitors.json\n
    \ def index\n    @visitor = Visitor.find_or_create_by(id: 1)\n\n    @visitor.update!(\n
    \     counter: (@visitor.counter || 0) + 1\n    )\n  end\n```\n\nChange the index
    view to show the fly.io balloon and the counter.\n\nReplace `app/views/visitors/index.html.erb`
    with:\n\n```erb\n<div class=\"absolute top-0 left-0 h-screen w-screen mx-auto
    mb-3 bg-navy px-20 py-14 rounded-[20vh] flex flex-row items-center justify-center\"
    style=\"background-color:rgb(36 24 91)\">\n  <img src=\"https://fly.io/static/images/brand/brandmark-light.svg\"
    class=\"h-[50vh]\" style=\"margin-top: -15px\" alt=\"The monochrome white Fly.io
    brandmark on a navy background\" srcset=\"\">\n\n  <div class=\"text-white\" style=\"font-size:
    40vh; padding: 10vh\" data-controller=\"counter\">\n    <%%= @visitor.counter.to_i
    %>\n  </div>\n</div>\n```\n\nDefine the root path to be the visitors index page:\n\nEdit
    `config/routes.rb`:\n\n```ruby\n  # Defines the root path route (\"/\")\n  root
    'visitors#index'\n```\n\nSave our work so we can see what changed later.\n\n```bash\ngit
    add .\ngit commit -m 'initial application'\n```\n\nNow let’s do our first deployment.
    \ If desired add `—name` and `—org` options to the `generate` command below, or
    let them default:\n\n```bash\nbundle add fly-rails\nbin/rails generate fly:app\nbin/rails
    fly:deploy\n```\n\nNote that a volume is created. That&#39;s to store the [sqlite3](https://www.sqlite.org/index.html)
    database. Making that work actually takes multiple steps: create a [volume](https://fly.io/docs/reference/volumes/),
    mount the volume, and set an environment variable to cause Rails to put the database
    on the mounted volume.\n\nAll of that is taken care of for you.\n\nTo see your
    app in production, run `fly open`.\n\n---\n\n## Part two: change the database
    to PostgreSQL\n\nWhile sqlite3 is more than adequate for this silly example, many
    applications require something more.  Let&#39;s switch to [postgresql](https://www.postgresql.org/).\n\nEdit
    `config/database.yml`:\n\n```yaml\nproduction:\n  adapter: postgresql\n```\n\nDeploy
    your change:\n\n```bash\nbin/rails fly:deploy\n```\n\nAt this point, a `pg` gem
    is installed, a `PostgreSQL` database is created, and a [secret](https://fly.io/docs/reference/secrets/)
    is set. Also, there now is a separate [release](https://fly.io/docs/reference/configuration/#the-deploy-section)
    step that will run your database migrations before restarting your server.\n\nAgain,
    all without you having to worry about anything.\n\n---\n\n## Part three: update
    the counter without requiring a refresh\n\nSending asynchronous updates requires
    a [WebSocket](https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API)
    as well as an ability to process updates in the background.  Rails makes this
    easy.\n\nStart by generating a new [Action Cable](https://guides.rubyonrails.org/action_cable_overview.html)
    channel:\n\n```bash\nbin/rails generate channel counter\n```\n\nMake a partial
    that puts the counter into a [Turbo Frame.](https://turbo.hotwired.dev/handbook/frames)\n\nCreate
    `app/views/visitors/_counter.html.erb`:\n\n```erb\n<%%= turbo_frame_tag(dom_id
    visitor) do %>\n  <%%= visitor.counter.to_i %>\n<%% end %>\n```\n\nUpdate the
    view to add `turbo_stream_from` and render the partial.\n\nUpdate `app/views/visitors/index.html.erb`:\n\n```erb\n<%%=
    turbo_stream_from 'counter' %>\n\n<div class=\"absolute top-0 left-0 h-screen
    w-screen mx-auto mb-3 bg-navy px-20 py-14 rounded-[20vh] flex flex-row items-center
    justify-center\" style=\"background-color:rgb(36 24 91)\">\n  <img src=\"https://fly.io/static/images/brand/brandmark-light.svg\"
    class=\"h-[50vh]\" style=\"margin-top: -15px\" alt=\"The monochrome white Fly.io
    brandmark on a navy background\" srcset=\"\">\n\n  <div class=\"text-white\" style=\"font-size:
    40vh; padding: 10vh\" data-controller=\"counter\">\n    <%%= render \"counter\",
    visitor: @visitor %>\n  </div>\n</div>\n```\n\nAdd `broadcast_replace_later` to
    the controller.\n\nEdit `app/controllers/visitors_controller.rb`:\n\n```ruby\n
    \ # GET /visitors or /visitors.json\n  def index\n    @visitor = Visitor.find_or_create_by(id:
    1) \n\n    @visitor.update!(\n      counter: (@visitor.counter || 0) + 1\n    )\n\n
    \   @visitor.broadcast_replace_later_to 'counter',\n      partial: 'visitors/counter'\n
    \ end\n```\n\nDeploy your change:\n\n```bash\nbin/rails fly:deploy\n```\n\nAt
    this point, a `redis` gem is installed (if it wasn&#39;t already), an [Upstash
    Redis](https://fly.io/docs/reference/redis/) cluster is created if your organization
    didn&#39;t already have one (otherwise that cluster is reused), and a secret is
    set.\n\nOnce again, all without you having to worry about anything.\n\n---\n\n##
    Part four: change your cable adapter to any_cable\n\nWe&#39;ve tried out two different
    databases. Let&#39;s try out [AnyCable](https://anycable.io/) as an alternate
    cable implementation.\n\nModify `config/cable.yml`:\n\n```yaml\nproduction:\n
    \ adapter: any_cable\n```\n\nDeploy your change:\n\n```bash\nbin/rails fly:deploy\n```\n\nNote
    that this time you are likely to see `502 Bad Gateway`. That&#39;s because [nginx](https://www.nginx.com/)
    typically starts faster than Rails and at this point this is just a demo. Don&#39;t
    worry, Rails will start in a few seconds and things will work once it starts.
    If you check the logs you will often see a similar problem where anycable go starts
    faster than anycable rpc, but that also corrects itself.\n\nOnce again, gems are
    installed and this time at runtime multiple processes are run, including one additional
    process (nginx) to transparently route the websocket to anycable. All on a single
    256MB fly machine. The details are messy, but you don&#39;t have to worry about
    them.\n\n---\n\n## Recap\n\nWe&#39;ve deployed four different configurations without
    having to touch\nanything but Rails files.\n\nRun the following command to see
    what files were modified:\n\n```bash\ngit status\n```\n\nIn addition to the `config`
    and `app` files that you modified you should see two files:\n\n- `config/fly.rb`\n-
    `fly.toml`\n\nBoth are relatively small, in fact `fly.toml` is only one line.
    The other file is likely to change dramatically so don&#39;t get too attached
    to it. What it is meant to describe is the deployment specific information that
    can&#39;t be gleaned from the configuration files alone, things like machine and
    volume sizes. The hope is that it will cover replication and geographic placement
    of machines; conceptually similar to what terraform provides today but expressed
    at a much higher level and in a familiar Ruby syntax.\n\nIf you want to see the
    configuration files that actually are used, run the following command:\n\n```\nbin/rails
    generate fly:app --eject\n```\n\nNote: this demo uses fly machines v2, and requires
    a script (`rails deploy`) to build a Dockerfile and run the underlying commands
    and APIs to create machines, set secrets, etc. It is possible to run with nomad
    (a.k.a. v1) by passing `--nomad` on the `bin/rails generate fly:app` command,
    and while this will allow you to run vanilla `fly deploy` the trade off is this
    is accomplished by creating a `Dockerfile` and various other artifacts.\n\n---\n\n##
    Futures\n\nSome examples of things worth exploring\n\n- Not implemented yet, but
    it should be possible to modify the size of a volume in `config/fly.rb` and deploy
    to make a change.\n- While the above demo made use of fly&#39;s Postgres offering,
    you may very well want a managed alternative.  Or go the other way and run Debian&#39;s
    Postgres within the same VM.  Or go with a different database entirely.  All should
    be easy as setting some [custom credentials](https://edgeguides.rubyonrails.org/security.html).\n-
    [Active  Storage](https://edgeguides.rubyonrails.org/active_storage_overview.html)
    supports a number of back-ends including Amazon&#39;s S3 and Google&#39;s Cloud.
    \ Let&#39;s make that easy too.\n- The example above ran AnyCable in the same
    VM which may not be optimal for scaling reasons.  Not to mention that taking AnyCable
    down every time you deploy a change to your application will drop sessions.  We
    should make it easy to say that I want two of this application running in this
    region and three of that application running in this other region.\n- Currently
    `fly deploy` can be run as a GitHub action.  Extend this work to cover `bin/rails
    fly:deploy`.\n- Consider adding other `bin/rails fly:` tasks that add value.  Perhaps
    one that directly runs `rails console` on the deployed machine.  Perhaps another
    that simply sets the working directory properly on ssh.\n\nThe common theme of
    all of the above is to look for a higher level abstraction than what is currently
    provided in Dockerfiles and fly.toml, while retaining the ability to drop down
    and say things like &quot;also install this Debian package&quot; or &quot;also
    expose this port&quot;.\n\nFeedback is welcome on [community.fly.io](https://community.fly.io/t/dockerfile-less-deploys-rails-demo/9234/1).\n\nOh,
    and it probably is worth mentioning that the source to the fly-ruby gem is [on
    GitHub](https://github.com/superfly/fly-rails) and written in Ruby.  Pull requests
    welcome!\n"
- :id: blog-logbook-november-14-to-december-5-2022
  :date: '2022-12-06'
  :category: blog
  :title: 'Logbook: November 14 to December 5, 2022'
  :author: brad
  :thumbnail: logbook-default2-thumbnail.jpg
  :alt:
  :link: blog/logbook-november-14-to-december-5-2022
  :path: blog/2022-12-06
  :body: "\n\n**Build real-time applications on _any_ backend with Replicache and
    Fly, run Cron on Fly, two LiveWire how-tos, and learn how Fly reluctantly built
    its Postgres database service.**\n\nOk, it's been longer than a week since the
    last update because a lot of us at Fly were enjoying some time with \U0001F983,
    \U0001F967, and \U0001F468‍\U0001F469‍\U0001F467‍\U0001F466. Let's get to it!\n\n##
    Real-Time Collaboration With Replicache and Fly-Replay\n\nDov Alperin wires up
    Replicache to WebSockets to show how any framework and Fly can be used to build
    realtime web applications. Check it out if you aspire to build the next Figma.\n\nRead
    [Real-Time Collaboration With Replicache and Fly-Replay](https://fly.io/blog/replicache-machines-demo/)\n\n##
    Laravel LiveWire\n\nThe Laravel team at Fly continues to crank out some pretty
    great tutorials.\n\n### Streaming to the Browser With LiveWire\n\nUse WebSockets
    to stream content from the server to a persons web browser. Chris Fidao walks
    through an example that shows how a log file could be streamed from a server to
    anybody watching it from a browser.\n\nRead [Streaming to the Browser with LiveWire](https://fly.io/laravel-bytes/streaming-to-the-browser-with-livewire/)\n\n###
    Offloading Data Baggage with LiveWire\n\nWhen paginating large datasets between
    the server and browser, you don't want to load so much data that the users browser
    slows down and becomes unresponsive. Kathryn Anne Tan shows how this data can
    be unloaded so that the people using your website don't have to deal with all
    your baggage.\n\nRead [Offloading Data Baggage with LiveWire](https://fly.io/laravel-bytes/offloading-data-baggage/)\n\n##
    Supercronic on Fly\n\nIn September, [Fly Machines got a way to run tasks periodically](https://community.fly.io/t/new-feature-scheduled-machines/7398)
    either \"monthly\", \"weekly\", \"daily\", and \"hourly; however, there is no
    way to control the precise time those jobs run because it's a hard problem to
    solve at scale.\n\nNow there's a guide for those who need more precise control
    over cron that runs on both versions of the Fly Apps platform. Brad Gessler runs
    through how to wire up Superchronic in your Fly Dockerfiles and deploy cron to
    production.\n\nRead [Supercronic on Fly](https://fly.io/docs/app-guides/supercronic/)\n\n##
    Postgres on Fly\n\nDid you know that Fly actually wants you to use off-platform
    database services, like RDS, CrunchyData, or PlanetScale? Well then how the heck
    did we end up building Fly Postgres!? Chris Nicoll and Shaun Davis walk down memory
    lane and chronicle how it all happened.\n\nRead [How We Built Fly Postgres](https://fly.io/blog/how-we-built-fly-postgres/)\n\nP.S.
    If you're CrunchyData, PlanetScale, or a cloud infrastructure provider you should
    run your infrastructure on Fly.\n\n---\n\nIf you're in the northern hemisphere,
    stay warm! See you on the next edition of The Logbook."
- :id: laravel-bytes-on-demand-compute
  :date: '2022-12-06'
  :category: laravel-bytes
  :title: On-Demand Compute
  :author: fideloper
  :thumbnail: on-demand-thumbnail.png
  :alt: Machines directing other machines tasks
  :link: laravel-bytes/on-demand-compute
  :path: laravel-bytes/2022-12-06
  :body: "\n\n<p class=\"lead\">Fly takes a Docker image, converts it to a VM, and
    runs that VM anywhere around the world. [Run a Laravel app](https://fly.io/docs/laravel/)
    in minutes!</a>\n\nWe're going to learn how to use [Fly Machines](https://fly.io/docs/machines/)
    to run short-lived tasks efficiently.\n\nThis boils down to having a process to
    run (an `artisan` command, in our case), and a VM ready to run that command. The
    VM will start, run the command, and stop when the command exits.\n\n## What's
    up with Machines?\n\nThe `fly` command helps you launch and run apps. Fly assumes
    your app is always running, 24/7.\n\nHowever, **Fly Machines** offer more control.
    They are a low-level building block that allows you to run \"stuff\" in interesting
    ways.\n\n**Here are a few fun details about Machines**:\n\n1. They can be managed
    by API\n1. They turn off automatically when a program exits\n1. Stopped machines
    can start in milliseconds\n1. Restarted machines are a blank slate - they are
    ephemeral\n1. Machines can be started manually, but can also wake on network access\n1.
    You can run multiple machines within an application\n\nIf you squint just a little,
    it looks a bit like serverless. This is useful!\n\nLet's write some code to run
    on-demand.\n\n## The Artisan Command\n\nHere's an `artisan` command that does
    some work.\nIn our case, we'll talk to the GitHub API to get the latest release
    for a given repository.\n\nHere is file `app/Console/Commands/GetLatestReleaseCommand.php`:\n\n```php\n<?php\n\nnamespace
    App\\Console\\Commands;\n\nuse Illuminate\\Console\\Command;\nuse Illuminate\\Support\\Facades\\Http;\nuse
    Illuminate\\Support\\Facades\\Log;\n\nclass GetLatestReleaseCommand extends Command\n{\n
    \   protected $signature = 'get-release {repo}';\n\n    protected $description
    = 'Get the latest release version';\n\n    public function handle()\n    {\n        //
    1️⃣ Build the URL\n        $url = sprintf(\n          \"https://api.github.com/repos/%s/releases/latest\",
    \n          $this->argument('repo')\n        );\n\n        // 2️⃣ Make the API
    call\n        $response = Http::get($url);\n\n        // 3️⃣ Test the results
    of the API call\n        if ($response->successful()) {\n            // We'll
    assume the tag_name is the release version\n            $tag = $response->json('tag_name');\n\n
    \           // Do something extremely useful with $tag\n            Log::info(\"The
    latest tag is $tag\");\n        } else {\n            Log::error(\"Could not retrieve
    repository release\", [\n                'url' => $url,\n                'repo'
    => $this->argument('repo'),\n                'status' => $response->status(),\n
    \               'body' =>  $response->body(),\n            ]);\n        }\n\n
    \       // Return success so the Machine VM\n        // doesn't attempt to restart
    the process\n        return Command::SUCCESS;\n    }\n}\n```\n\nThe command retrieves
    the the latest release from a public GitHub repository. We can run it like this:
    `php artisan get-release vessel-app/vessel-cli`.\n\nWith that out of the way,
    let's get to the interesting part! We'll next create a Machine and run that command.\n\n##
    Create a Machine App\n\nWe're going to setup an app in Fly.\n\n\n```bash\ncd path/to/laravel/project\nfly
    launch\n\n# Don't deploy the app!\n```\n\nYou can, of course, deploy and run this
    app if you want! **However**, for now we just want the `fly launch` command to
    generate a `Dockerfile` for us. From that, we can easily create a Docker image.
    This image will be used to run our code on-demand!\n\n**Let me be extra clear
    here**. The above command will create a regular old app within your Fly account.
    I'm not going to use that app. We're just using `fly launch` as a convenient way
    to setup a `Dockerfile`.\n\nSimilar to \"regular\" apps, Fly Machines are housed
    within a \"Machine App\". A machine app can have any number of machines running
    within it.\nHere's how to create one:\n\n```bash\n# Create a new machine app (with
    no running VM's)\nfly apps create --machines --name on-demand\n```\n\nThat makes
    an app named `on-demand`. \n\nRemember how I mentioned that Machines have an API?
    The API allows for more fine-grained controls over how you run things. We won't
    need it for this demonstration, but if you want to see what it looks like to create
    a machine app via API, it looks like this:\n\n```bash\n# Get your access token
    from ~/.fly/config.yml\n# Or via `fly auth token`\nexport FLY_API_TOKEN=\"$(fly
    auth token)\"\n\ncurl -X POST \\\n    -H \"Authorization: Bearer ${FLY_API_TOKEN}\"
    \\\n    -H \"Content-Type: application/json\" \\\n    \"https://api.machines.dev/v1/apps\"
    \\\n    -d '{\n      \"app_name\": \"on-demand\",\n      \"org_slug\": \"personal\"\n}'\n```\n\nThe
    app is basically just an empty shell - it has no running VMs. We can run some
    machines within the app.\n\n## Create a Machine VM\n\nTo run a machine, we need
    to supply it a Docker image (just like any Fly app). Fly machines, and regular
    apps, can pull Docker images either from a public repository or from their own
    private registry.\n\nNormally the `fly launch` and `fly deploy` commands build
    the Docker image for you. For Machines, we can do the same with a handy command:\n\n```bash\nfly
    m run -a on-demand \\\n    --env \"APP_ENV=production\" \\\n    --env \"LOG_CHANNEL=stderr\"
    \\\n    --env \"LOG_LEVEL=info\" \\\n    . \\\n    \"php\" \"artisan\" \"get-release\"
    \"vessel-app/vessel-cli\"\n```\n\nThe important part is the use of a period `.`
    for the image name: this forces the `fly m run...` command to build the Docker
    image used to create the VM.\nIt uses the `Dockerfile` in the current directory
    (or path provided by `--dockerfile`) to build the image.\n\nUsing any other image
    name results in an attempt to download a pre-built image from a public registry
    or Fly.io's registry.\n\nIf instead we wanted to run a machine via the API, we
    have to build a Docker image *ahead of time* and push it up to Fly's registry
    so it's available for use.\n\nThere's a good write up [on using Fly's registry
    here](https://til.simonwillison.net/fly/fly-docker-registry).\n\nHere's what it
    looks like to build and push a Docker image to Fly:\n\n```bash\n# Build the image
    locally. The image name must match\n# app name we used when creating the machine
    app\ndocker build -t registry.fly.io/on-demand:latest\n\n# Authenticate against
    Fly's registry\nfly auth docker\n\n# Push our newly tagged image\ndocker push
    registry.fly.io/on-demand:latest\n```\n\nThen we can run a machine using the API:\n\n```bash\nFLY_API_TOKEN=\"$(fly
    auth token)\"\nFLY_APP_NAME=\"on-demand\"\n\n# https://fly.io/docs/machines/working-with-machines/#create-a-machine\ncurl
    -X POST \\\n    -H \"Authorization: Bearer ${FLY_API_TOKEN}\" \\\n    -H \"Content-Type:
    application/json\" \\\n    \"https://api.machines.dev/v1/apps/${FLY_APP_NAME}/machines\"
    \\\n    -d '{\n  \"config\": {\n    \"image\": \"registry.fly.io/on-demand:latest\",\n
    \   \"processes\": [\n      {\n        \"name\": \"get-release\",\n        \"cmd\":
    [\"php\", \"artisan\", \"get-release\", \"vessel-app/vessel-cli\"],\n        \"env\":
    {\n          \"APP_ENV\": \"production\",\n          \"LOG_CHANNEL\": \"stderr\",\n
    \         \"LOG_LEVEL\": \"info\"\n        }\n      }\n    ]\n  }\n}'\n```\n\nNo
    matter which way you do it, creating a machine WILL also run the machine immediately!
    Expect whatever command you define to get run.\n\n## Run On-Demand\n\nNow that
    the machine exists, we can run it anytime we want our `php artisan` command to
    run! Here's how:\n\n```bash\n# First, fine our machine ID by listing\n# machines
    within our app\nfly m list -a on-demand\n\n# Start our machine by ID\nfly m start
    -a on-demand <machine-id-here>\n```\n\nWe used the `fly m start...` command to
    start and run our Machine, which in turn runs the defined `artisan` command.\n\nIf
    we want to programmatically start the machine, we can use the API.\n\nThe HTTP
    request we would make from our application is the equivalent of this `curl` command:\n\n```bash\nFLY_API_TOKEN=\"$(fly
    auth token)\"\nAPP=\"on-demand\"\nMACHINE=\"xyz133\"\n\ncurl -X POST \\\n    -H
    \"Authorization: Bearer ${FLY_API_TOKEN}\" \\\n    -H \"Content-Type: application/json\"
    \\\n    \"https://api.machines.dev/v1/apps/${APP}/machines/${MACHINE}/start\"\n```\n\nIn
    Laravel, that would look something like this:\n\n```php\n$token = \"foo\";\n$app
    = \"on-demand\";\n$machine = \"xyz133\";\n$url = \"https://api.machines.dev/v1/apps/${app}/machines/${machine}/start\"\n\n$result
    = Http::asJson()\n  ->withToken($token)\n  ->post($url); \n```\n\nAnd that's really
    all there is to it!\n\nOnce a machine exists, it can be started within milliseconds.
    Our example here will just run an `artisan` command whenever we'd like! Once the
    command finishes,\nit will exit and the VM will stop until called again. It's
    a bit like serverless that way!\n\n## Details\n\nThere's a few details we didn't
    cover here!\n\nIf you need to run multiple concurrent calls to the artisan command,
    you will need more machines. Starting a single machine multiple times won't create
    multiple instances. Instead, you will need to create additional machines.\n\nWe
    also haven't discussed managing a machine's lifecycle. In theory, you could create
    machines all day long (but we don't recommend it). In reality, it's better to
    manage a pool of machines.\n\nA good time to destroy and recreate a machine might
    be when you update the Docker image it was created from (although you can also
    update the machine itself to achieve the same result).\n\nKeep in mind that creating
    a machine is the slowest part of the process because Fly needs to convert the
    Docker image to a disk image before running a virtual machine.\n"
- :id: blog-how-we-built-fly-postgres
  :date: '2022-11-29'
  :category: blog
  :title: How We Built Fly Postgres
  :author:
  :thumbnail: keepers-proxies-thumbnail.jpg
  :alt: A robot emits an instruction, in the form of exclamation points, to a set
    of other robots holding stacks of cartoon disk platters. One replies with checkmarks.
    A figure on a hill in the background holds a sign with an arrow pointing right.
    Airships fill the sky.
  :link: blog/how-we-built-fly-postgres
  :path: blog/2022-11-29
  :body: "\n\n<div class=\"lead\">\nLike many public cloud platforms, Fly.io has a
    database offering. Where AWS has RDS, and Heroku has Heroku Postgres, Fly.io has
    Fly Postgres. You can spin up a Postgres database, or a whole cluster, with just
    a couple of commands. [Sign up for Fly.io](https://fly.io/docs/speedrun/) and
    launch a full-stack app in minutes!\n</div>\n\nFly.io is an ambivalent database
    provider&mdash;one might even use the word \"reluctant\". The reasons for that
    are interesting, as is the way Fly Postgres works. When we relate this in conversations
    online, people are often surprised. So we thought we'd take a few minutes to lay
    out where we're coming from with databases.\n\nWe started Fly.io without durable
    storage. We were a platform for \"edge apps\", which is the very 2019 notion of
    carving slices off of big applications, leaving the bulk running in Northern Virginia,
    and running the slices on small machines all around the world. In an \"edge app\"
    world, not having durable storage makes some sense: the real data store is in
    `us-east-1`, and the slices are chosen carefully to speed the whole app up (by
    caching, running an ML model, caching, serving images, and caching).\n\nOf course,
    people asked for databases from day one. But, on days one through three hundred
    thirty-one, we held the line.\n\nSomewhere around day fifteen, we grew out of
    the idea of building a platform exclusively for edge apps, and started looking
    for ways to get whole big crazy things running on Fly.io. We flirted with the
    idea of investing in a platform built-in database. We rolled out an (ultimately
    cursed) shared Redis. We even toyed with the idea of offering a managed [CockroachDB](https://github.com/cockroachdb/cockroach);
    like us, Cockroach is designed to run globally distributed. \n\nAnd then we snapped
    out of it. Databases! Feh!\n\nHere's our 2020 reasoning, for posterity: just because
    we didn't offer durable storage on the platform didn't mean that apps running
    on Fly.io needed to be stateless. Rather, they just needed to use off-platform
    database services, like RDS, CrunchyData, or PlanetScale. Hooking globally distributed
    applications up to RDS was (and remains) something ordinary teams do all the time.
    What did we want to spend our time building? Another RDS, or the best platform
    ever for you to run stuff close to your users?\n\nBy day two hundred and ninety
    or so, the appeal of articulating and re-articulating the logic of a stateless
    global platform for stateful global apps began to wear off. RDS! Feh! Somewhere
    around then, Jerome and Steve figured out LVM2, [gave all our apps attached disk
    storage](https://fly.io/blog/persistent-storage-and-fast-remote-builds/), and
    killed off the stateless platform talking point.\n\nNow, disk storage is just
    one of the puzzle pieces for giving apps a reliable backing store. Storage capabilities
    or not, we still didn't want to be in the business of replicating all of RDS.
    So we devised a cunning plan: Build the platform out so it can run a database
    app, build a friendly database app for customers to deploy on it, and add some
    convenience commands to deploy and manage the app.\n\nWe wouldn't have a managed
    database.\n\nNo, we have an automated database.\n\nPostgres is a good database
    for this. It's familiar and just works with the migration tools baked into full-stack
    frameworks.\n\nIn January 2021, we [soft-launched](https://community.fly.io/t/early-look-postgresql-on-fly-we-want-your-opinions/537/18)
    a `fly pg create` command that would deploy an automagically configured two-node
    Postgres cluster complete with metrics, health checks, and alerts. (The alerts
    were as cursed as our shared Redis.) This was a big-deal effort for us. Back in
    2020, we were really small. Almost everyone here had a hand in it. \n\nWhen Shaun
    arrived at Fly.io later that year, he took over the job of making Fly Postgres
    more reliable and more convenient to manage&mdash;still in hard mode: developing
    and shipping features that make the platform better for apps _like_ Fly Postgres,
    and making Fly Postgres plug into those.\n\nThis post is mostly ancient history!
    Shaun's no longer a team of one, and lots has happened since this post should
    have been written and shipped. Everything still holds; it's just more and better
    now.\n\n## Postgres is really cool all by itself\n\nHere's a way you can run Postgres
    on Fly.io: point `fly launch` at the latest official [Postgres Docker image](https://hub.docker.com/_/postgres).
    Remove the default services in `fly.toml`, since this isn't a public app. Provision
    and mount a volume. Store `POSTGRES_PASSWORD` as a Fly Secret. Deploy.\n\n(Then
    `fly ssh` in and create a database and user for your app.)\n\nIf you'll only ever
    want this one instance, this is pretty good. If anything happens to your lonely
    node, though, your Postgres service—and so, your app—is down (and you may have
    lost data).\n\nHere's a better setup: one primary, or leader, instance that deals
    with all the requests, and one replica instance nearby (but preferably on different
    hardware!) that stays quietly up to date with the latest transactions. And if
    the leader goes down, you want that replica to take over automatically. Then you
    have what you can call a high-availability (HA) cluster.\n\nPostgres has a lot
    of levers and buttons built right in. You can deploy two Postgres VMs configured
    so one's a writable leader and the other is a standby replica staying up to date
    by [asynchronous streaming replication](https://www.postgresql.org/docs/current/warm-standby.html).\n\nWhat
    Postgres itself doesn't have is a way to adapt cluster configuration on the fly.
    It can't notify a replica that the primary has failed and it should take over,
    and it certainly can't independently elect a new leader if there's more than one
    eligible replica that could take over. Something else has to manipulate the Postgres
    controls to get HA clustering behaviour.\n\nThat's where [Stolon](https://github.com/sorintlab/stolon)
    comes in.\n\n<div class=\"callout\">\n### Postgres, WAL, and Streaming Replication\n\n[Write-Ahead
    Logging](https://www.postgresql.org/docs/current/wal-intro.html) (WAL): Before
    a transaction is applied to tables and indexes on the primary (or only) instance,
    it's written to nonvolatile storage, in the Write-Ahead Log. This means you can
    afford not to write changes to every affected data file on disk after every single
    transaction; if data pages in memory are lost, they can be reconstructed by replaying
    transactions from the WAL.\n\n[Postgres streaming replication](https://www.postgresql.org/docs/current/warm-standby.html#STREAMING-REPLICATION)
    sends each WAL record along to the replica right after the transaction is committed
    on the leader. As the record is received, it's replayed to bring the replica up
    to date.\n\nWe have some heartier, SQLite-flavoured WAL content [around here somewhere](https://fly.io/blog/sqlite-internals-wal/).\n</div>\n\n##
    Clustering with Stolon\n\nStolon is a Golang Postgres manager. We chose it for
    a few reasons: it's open source, it's easy to build and embed in a Docker image,
    and it can use Consul as its backend KV store (we're good at Consul).\n\nWe spun
    up a Consul cluster for Fly Postgres to use, and since it was there, we also [made
    it available for any Fly app that wanted a locking service.](https://community.fly.io/t/sneak-peak-global-lock-service/554)\n\nStolon
    comes with three components that run alongside Postgres in each instance's VM:
    a sentinel, a keeper, and a proxy.\n\n- The lead sentinel keeps an eye on the
    cluster state as recorded by keepers in the Consul store, and decides if leadership
    needs to change. \n- Keepers each manage their local Postgres instance, making
    sure it behaves as a writable leader or a read-only replica (as dictated by the
    leader sentinel), and update Consul with their latest state. \n- Proxies are there
    to route all incoming client connections to the current leader, as recorded in
    the store (and only if it's healthy).\n\nIf the leader instance fails, the proxies
    start dropping all connections and Stolon elects a new leader, using Consul to
    lock the database in the meantime. If both (all) your instances fail, the database
    is unavailable until one or the other recovers.  New connections go to the new
    leader as soon as it's ready, without rebooting clients or changing their config.\n\nIf
    you’ve ever received a late-night email from Heroku saying your DB was replaced,
    you know why this is awesome.\n\n## Stolon + Consul Intensifies\n\nStolon is chatty
    as hell with Consul, and this can be a problem.\n\nKeepers, sentinels, and proxies
    do all their communication via the Consul leader.  If a Stolon component can't
    reach Consul, it repeats its request until it can. A single flapping Stolon cluster,
    early on, could saturate our Consul connections.\n\nMeanwhile, if a Stolon proxy
    can't reach Consul, it throws its hands in the air and drops all client connections
    until it can. We had several Postgres outages traceable to either Consul falling
    over or faraway Postgres nodes not being able to connect to it.\n\nThe more Postgres
    clusters people spun up, the more of a problem this was.\n\n## Less Consul With
    HAProxy\n\nThe Stolon proxy relies on Consul to know which instance to route connections
    to.\n\nBut Consul isn't the intrinsic authority on who the leader is: Postgres
    on every instance knows its own role. If we can replace the Stolon proxy with
    one that can just ask the nodes who's leader, that's less load on our shared Consul
    cluster, and if there's trouble with Consul there's one component fewer to freak
    out about it.\n\nIt's [not exactly supported](https://github.com/sorintlab/stolon/blob/master/doc/faq.md#why-didnt-you-use-an-already-existing-proxy-like-haproxy),
    but it's possible to use HAProxy with Stolon, and we did.\n\nHere's how we've
    got HAProxy set up:\n\n- HAProxy listens on port 5432 on all Fly Postgres instances
    for read or write requests.\n- When you create a Fly Postgres cluster using `fly
    postgres create`, it's configured with a `PRIMARY_REGION` environment variable.
    HAProxy gets the list of candidates from our internal DNS server using `$PRIMARY_REGION.$FLY_APP_NAME.internal`.\n-
    Then, every two seconds, it asks the HTTP health check server on each of these
    nodes for its role.\n- HAProxy marks the replicas as unhealthy and removes them
    from its list; it won't pass any requests to them.\n- If the incumbent leader
    fails its role check by returning &quot;replica&quot; or &quot;offline&quot;,
    or not responding at all, HAProxy drains connections from it while Stolon sorts
    out a new leader.\n- If there's a healthy leader, HAProxy routes all requests
    to it, on port 5433 (where the keeper has told actual Postgres to listen).\n\nWe
    also added Consul clusters in a couple more regions. This spreads the burden on
    Consul, but crucially, it puts Consul clusters close to people's primary Postgres
    VMs. Network flakiness between Stolon and Consul breaks Stolon. The internet is
    flaky. The less internet we can span, the happier Stolon is.\n\nStolon and Consul
    are still intense: we've been adding new Consul clusters ever since to keep up.\n\n##
    Here's the Fly Postgres App\n\nWe're running a few things on each Fly Postgres
    VM:\n\n- Stolon keeper\n- Stolon sentinel\n- HAProxy\n- Postgres\n- a cornucopia
    of internal commands and health checks\n- HTTP server to serve the command and
    health check endpoints\n- Golang supervisor code\n\nThis is a pretty deluxe Postgres
    cluster app. You can shell into a running instance and add a database, restart
    the PG process, trigger a failover, run stolonctl commands directly, and more.\n\nOur
    Golang supervisor, flypg, glues the other processes together and does nice things
    like try to recover from individual process crashes before giving up and letting
    the whole VM get rescheduled.\n\n[All](https://www.postgresql.org/)  [the](https://github.com/sorintlab/stolon)
    \ [parts](https://docs.haproxy.org/2.6/intro.html) are open source; you can [fork](https://github.com/fly-apps/postgres-ha)
    it and add [PgBouncer](https://www.pgbouncer.org/) or whatever.\n\n<aside class=\"right-sidenote\">
    You can enable extensions for [WAL-G](https://wal-g.readthedocs.io/PostgreSQL/),
    [TimescaleDB](https://github.com/timescale/timescaledb), and [PostGIS](https://postgis.net/)
    yourself, without forking. </aside>\n\nSo that's the Fly Postgres app. You can
    deploy it with `fly launch` like any Fly app, straight from a clone of the [postgres-ha
    repo](https://github.com/fly-apps/postgres-ha). It is faster to deploy the built
    [image](https://hub.docker.com/r/flyio/postgres/tags) straight from Docker Hub,
    and the image has version metadata you can use to upgrade later.\n\nThe following
    will create a 2-instance HA cluster that apps on your org's internal WireGuard
    network can connect to:\n\n1. Copy fly.toml from the postgres-ha repo\n1. Edit
    fly.toml to set the `PRIMARY_REGION` environment variable to match the region
    you're about to deploy to\n1. `fly apps create` a new app\n1. Create a volume\n1.
    Set passwords as secrets on the newly-created app: `SU_PASSWORD`, `REPL_PASSWORD`,
    and `OPERATOR_PASSWORD`\n1. `fly deploy --image=flyio/postgres:14`\n1. Create
    a second volume\n1. Add a replica by scaling up to 2 instances\n\nThen, to let
    an app use this Postgres:\n\n1. Use aforementioned in-VM commands on the Postgres
    leader to create a new user and database for the consuming app (you find the leader
    by running `fly ssh console -C \"pg-role\" -s` on each instance until you hit
    the one with the `\"leader\"` role)\n1. Then set a connection string, containing
    the new user and password, as a `DATABASE_URL` secret on the consuming app.\n\nNow
    I don't know if I made that look complicated or simple!\n\nIt's simple for what
    you get. Every instance of your postgres-ha  app is a magical cluster building
    block! Add an instance and it automatically becomes a member of the cluster and
    starts replicating from the leader. If it's in the `PRIMARY_REGION`, it's eligible
    to participate in leader elections. You can add nodes in other regions, too; they
    can't become leader, but you can read from them directly on port 5433. It's all
    inside the app. [Get a bit fancier with the Fly-Replay header](https://fly.io/blog/globally-distributed-postgres/)
    in your consuming app, and you can do your reads from the closest instance and
    send your writes to the primary region.\n\nBut yeah, this isn't _quite_ the Fly
    Postgres experience. Since we expect lots of people to deploy this exact app,
    it was reasonable to bundle up that mild cluster-creation rigamarole into a `fly
    pg create` command, which is much like `fly launch` with one of our more mature
    framework launchers. There are similar [nuggets of flyctl convenience](https://fly.io/docs/flyctl/postgres/)
    for managing your `fly pg create`d database cluster.\n\n\n<%= partial \"shared/posts/cta\",
    locals: {\n  title: \"Fly Postgres\",\n  text: \"Use it in something awesome!\",\n
    \ link_url: \"https://fly.io/docs/speedrun/\",\n  link_text: \"Launch a full-stack
    app now&nbsp;&nbsp;<span class='opacity-50'>&rarr;</span>\"\n} %>\n\n\n## An Observation\n\nWe've
    mentioned that continual reliance on Consul is something of an Achilles' heel
    for Stolon-managed clusters. It's not unique to Stolon and Consul, but a matter
    of needing a separate backend store for cluster state: in return for high availability
    and Borg-like assimilation of new instances, we accept an additional failure mode.\n\nIf
    you're running a single node, and you're never going to add another one to make
    a cluster, there's no upside to this high-availability machinery. A lone node
    is more reliable without any of it.\n\n<aside class=\"right-sidenote\"> We did
    briefly deploy a leaner, standalone Postgres app for the &quot;Development&quot;
    `fly pg create` configuration. This created a poor experience for users wanting
    to scale up to a HA cluster—the plumbing wasn't there to do it.</aside>\n\nBut
    quite a lot of people do run Fly Postgres on a single instance (just for development,
    right??). It's still automated, and you still get the knowledge that you're in
    good company and deploying a maintained app.\n\nThe great thing is: if you really
    want the simpler setup, you can just deploy your own Postgres app. It's all apps
    on Fly.io! \n\n\n## Snapshots and Restores\n\nYou can, and should, make your own
    backups of data that's important to you. That being said, a restore-your-database
    feature is guaranteed to make people's lives easier.\n\nIf you're shipping Postgres
    as a Service and don't care about the underlying infrastructure, you'll do Postgres
    native backups, copy data files and the WAL to object storage somewhere, then
    restore from those. Stolon will manage this for you.\n\nBut if you're building
    infrastructure that can run databases, this doesn't move you forward: every database
    has its own mechanism for backing up individual files. Some require data dumps
    using specific tools, some let you copy files out of the file system, etc.\n\nVolumes,
    which hold users' persistent data—for Postgres, SQLite, or whatever—are logical
    volumes on SSDs physically installed in our servers. We have low-level block device
    powers and the ability to take consistent, block-level snapshots of a disk.\n\nSo
    that's how we back up a Postgres database: by periodically grabbing a point-in-time
    version of the raw block device it's on. You recover a database by restoring this
    to an entirely new block device and deploying a Postgres instance to use it.\n\nConveniently,
    that approach works for pretty much anything that writes to a file system, solving
    backups for anything you want to run on Fly.io.\n\nOnce we got user-facing snapshot
    restores working for Postgres apps, we could generalize that to Volumes at large.
    Which is good, because people run every database you can think of on Fly.io.\n\nThis
    is a good example of &quot;Postgres&quot; work that was actually platform work
    with an elephant face taped on.  Like persistent storage itself, shared Consul,
    our crap health-check alerts, image version updates, and countless &quot;how should
    flyctl and the platform behave&quot; minutiae.\n\n## Back to Fly Postgres vs.
    Managed Databases\n\nSo Fly Postgres is an app, not a database service. This is
    not a bummer: it's fascinating, I tell you! Working on this one app helps us work
    through what we want the platform to offer to apps and how to implement that.
    It's an intrinsic part of the process of building a platform you could run _your_
    fully managed database service on.\n\nMeanwhile, we don't blame you if you'd actually
    prefer a boring managed database over our fascinating app. We love boring! Boring
    can be the best experience! We think the best solution to this is to partner with
    service providers to do integrations that really nail the Postgres, or MySQL,
    or Redis(!), or whatever, UX on Fly.io. After all, there's no single best database
    for everyone.\n\nAnd for all that, heading for 2023, Fly Postgres is doing the
    job for lots of apps! Automated Postgres turned out more useful than we'd have
    predicted."
- :id: blog-replicache-machines-demo
  :date: '2022-11-23'
  :category: blog
  :title: Real-Time Collaboration with Replicache and Fly-Replay
  :author: dov
  :thumbnail: network-thumbnail.jpg
  :alt: A cartoon of devices floating around and networked together with lines and
    nodes
  :link: blog/replicache-machines-demo
  :path: blog/2022-11-23
  :body: "\n\n<p class=\"lead\">We're Fly.io. React, Phoenix, Rails, whatever you
    use: we put your code into lightweight microVMs on our own hardware in 26 cities
    and counting. [Check us out](https://fly.io/docs/speedrun/)&mdash;your app can
    be running close to your users within minutes.</p>\n\nImagine this: you have invented
    the best design tool since Figma. But before you can compete with the design-industry
    heavyweight, you need to be able to compete on one of Figma&#39;s main propositions:
    real-time collaboration. You do some research and find that this is an upsettingly
    hard problem.\n\nYou start with a single server that coordinates users' changes
    for each document, using something like Replicache (which we'll come back to in
    a sec). Clients connect over WebSockets. But your business grows, and that one
    server gets overloaded with connections. \n\nYou'll have to split your documents
    among servers, but that means routing connections for each document to the server
    that has it. What now? Write complex networking logic to correctly route connections?
    &quot;There has to be a better way,&quot; you say to yourself.\n\n&quot;There
    is!&quot; I say, popping out of your computer!\n\nOK, I may have passed through
    \"conversational writing style\" right into \"witchcraft.\" Let's reel it back
    in.\n\nI'm going to demonstrate what I think is a good solution for the problem
    of a distributed real-time backend, using [Replicache](https://replicache.dev/),
    [Fly Machines](https://fly.io/blog/fly-machines/) and a Fly.io feature [we've
    talked about before](https://fly.io/blog/globally-distributed-postgres/): the
    [Fly-Replay header](https://fly.io/docs/reference/fly-replay/).\n\n Our demo app
    for today is&mdash;drum roll&mdash;a real-time collaborative todo app!\n\n## Replicache:
    an aside\n\nReplicache is a \"[JavaScript framework for building high-performance,
    offline-capable, collaborative web apps](https://replicache.dev/).\" It handles
    the hard parts of collaboration tech for us: conflicts, schema migrations, tabs,
    all the good (and bad) stuff. \n\nUnfortunately for us, for all that Replicache
    is super good at, it does not use WebSockets. But Websockets are perfect for frequent
    bidirectional updates to a document! Thankfully, using the Power of Software&#8482;
    we can just make Replicache bend to our WebSockety will.\n\nThe protocol that
    Replicache uses can be described in three parts: a `push` endpoint that clients
    can use to push changes to the server, a `pull` endpoint the client can use to
    pull changes made since the last time it pulled, and a persistent connection (usually
    either WebSockets or Server Sent Events) to `poke` clients and let them know something
    has changed so they should pull. If the protocol requires a persistent connection
    anyway, we should make everything work over that connection.\n\nDon&#39;t do what
    I am about to describe in production without some serious thought/tuning. While
    the way I do things totally _works_, I also have not tested it beyond the confines
    of this basic demo, so like, proceed with caution.\n\nBuilding on [replicache-express](https://github.com/rocicorp/replicache-express)
    which implements the `push`,  `pull`, and `poke` endpoints, I added a new endpoint
    for opening a WebSocket connection to a given &quot;room&quot;; in this case a
    room is just a given document.\n\nHere is the (slightly simplified) flow that
    happens when a WebSocket connection is opened via `/api/replicache/websocket/:spaceID`:\n\n1.
    The connection is opened.\n  * The server responds with `{\"data\":\"hello\"}`.\n
    \ * The WebSocket connection is stored in a map on the server with other WebSocket
    connections for that given `spaceID`.\n2. The client sends a message like `{messageType:
    \"hello\", clientID: \"4b9e0797-8321-4c03-ba82-01edafdeca0d\"}`.\n  * The server
    responds with the complete state it has stored to sync the local Replicache state
    on the client.\n3. When the client sends a change the server processes it, stores
    it, and then sends the updates to all clients subscribed to the `spaceID` where
    things were changed.\n\nThe actual process is slightly more nuanced to prevent
    sending more over the wire than we really have to (like updates we know clients
    have already seen). Every time we send a message to a given client we remember
    that we sent it. This way, the next time we need to update the client we can just
    send it what has changed and not the whole document. If you are curious how the
    server sync logic works, the code is available [here](https://github.com/fly-apps/replicache-websocket/blob/main/replicache-express/src/backend/ws.ts).\n\nThis
    whole process is implemented on the client basically in the inverse by overriding
    the Replicache client&#39;s `push` and `pull` logic to use our persistent WebSocket
    connection. When the frontend receives a message it stores the changes from the
    message in a temporary array, then instructs the Replicache client to \"`pull`\".
    The trick is that we override the `pull` logic from the Replicache client to just
    read straight from the array we've been storing websocket messages in.\n\n```ts\npuller:
    async () => {\n  const res: PullerResult = {\n    httpRequestInfo: {\n      httpStatusCode:
    200,\n      errorMessage: \"\"\n    },\n    response: {\n      cookie,\n      patch,\n
    \     lastMutationID\n    }\n  }\n  patch = []\n  return res\n}\n```\n\nWe similarly
    override the `push` functionality to write the message over our websocket instead
    of HTTP.\n\n```ts\npusher: async req => {\n  const res: HTTPRequestInfo = {\n
    \   httpStatusCode: 200,\n    errorMessage: \"\"\n  }\n  socket.send(JSON.stringify(await
    req.json()))\n  return res\n},\n```\n\n## Our architecture\n\nLet&#39;s take a
    look at the architecture diagram for this demo.\n\n![](https://slabstatic.com/prod/uploads/p1b436gf/posts/images/EDSQ6PEt1F70qRsOEttcAsi6.png)\n\nThe
    Replicache WebSocket server we just talked about is what we can see running on
    each Fly machine in the backend section. For the sake of the Figma-like app example,
    we can think of each machine running the backend for a given design document.
    You can see them named room(a-d).\n\nWe can see something cool happening, though:
    instead of our clients having to know where to find the specific backend server
    where the document they want lives, all the clients connect via the same router.
    The router is the super cool part of this app.\n\n## The router\n\nLet&#39;s think
    for a second about what our router has to do:\n\n1. Know the address of the server
    the user's stuff is on\n2. Somehow connect the user to that server\n\nFor the
    sake of this demo, we will consider requirement **1.** out of scope and have our
    router use hardcoded values.\n\nFor requirement **2.**, we just need to help the
    client find the right backend machine for its document when it makes a request
    for a WebSocket connection upgrade; the client and the backend can establish their
    connection and communicate directly from then on. \n\nWe'll be running the backend
    servers as Fly Machines on the same Fly.io private network as the router app,
    which means we can use a super-cool feature called [`fly-replay`](https://fly.io/docs/reference/fly-replay/)
    to send a connection request on to a given VM. \n\n`fly-replay` is a special header
    that fly-proxy, the aptly named Fly.io proxy, understands, that can be used to
    `replay` a request somewhere else inside your private network. For instance, to
    replay a request in region `sjc` your app&#39;s response could include the header
    `fly-replay: region=sjc`. You can also do `fly-replay: app=otherapp;instance=instanceid`,
    to target a specific VM. This is what we'll use in our router.\n\n## Putting it
    all together\n\nI unfortunately am not going to make the Next Best Thing Since
    Figma in this post for the sake of example, but I built something almost as good:
    a collaborative real-time todo list based on this [demo](https://github.com/rocicorp/replicache-quickstarts)
    from Replicache.\n\nOur todo list has three layers (directions to actually run
    these yourself can be found  [here](https://github.com/fly-apps/replicache-websocket/blob/main/README.md)):\n\n1.
    The frontend: a simple [React app](https://github.com/fly-apps/replicache-websocket/tree/main/client/react)
    that connects to…\n2. The router:  a simple [Express server](https://github.com/fly-apps/replicache-websocket/blob/main/replicache-express/src/index.ts#L117)
    that uses fly-replay to replay the WebSocket request against the right instance
    of…\n3. The backend: the Replicache-meets-WebSockets thing we talked about before.\n\nFor
    the backend we  just have a pool of Fly machines, each one responsible for a single
    todo list. Each todo list will be identified by the id of the machine it runs
    on.\n\nSo when a client connects to  `/api/replicache/websocket/machineID` on
    the router, the router responds with `fly-replay: app=replicache-backend;instance=machineID`.
    This response is handled by fly-proxy, who replays the request against the appropriate
    backend machine, and _voilà_: the client is now directly talking to the right
    backend machine. \n\nWe now have a way over-engineered todo list(s): each list
    runs on its own VM, and a simple router leveraging `fly-replay` sends WebSocket
    connections to the correct backend.\n\nYou can check out the demo for yourself
    at [https://replicache-frontend.fly.dev](https://replicache-frontend.fly.dev).
    You&#39;ll see a list of clickable IDs. Each ID is a machine running an instance
    of the backend which corresponds to an individual todo list. If you are so inclined
    to set this demo up for yourself, the code and instructions are [here](https://github.com/fly-apps/replicache-websocket).\n"
- :id: laravel-bytes-offloading-data-baggage
  :date: '2022-11-21'
  :category: laravel-bytes
  :title: Offloading Data Baggage with Livewire
  :author: kathryn
  :thumbnail: image-hoarder.png
  :alt: Photographs of buildings stacked up one after the other, forming a spiraling
    line of photographs fading away into the background.
  :link: laravel-bytes/offloading-data-baggage
  :path: laravel-bytes/2022-11-21
  :body: "\n\n<p class=\"lead\"> This article talks about properly handling data accumulation
    with Livewire. Livewire's faster close to your users. [Deploy your Laravel application](/docs/laravel/)
    globally with [Fly.io](/docs/)&mdash;you can be up and running in minutes.</p>
    \n\nIn [Hoarding Order with Livewire](/laravel-bytes/hoarding-order/) we implemented
    a client paginated table that relies on data accumulation. Instead of waiting
    for an entire dataset to load, the table periodically received and accumulated
    smaller portions of the dataset using `Livewire:poll`.\n\nData accumulation is
    the process of storing more data on top of an initial set of data retrieved. Our
    table's accumulated data will eventually grow to complete the entire dataset,
    and so will the space in memory it takes up.\n\nToday, let's dive headfirst into
    optimizing data accumulation with a focus on **\"Offloading data-accumulated baggage.\"**\n\nYou
    can check out the full repository [**here**](https://github.com/KTanAug21/hoard-table-data-using-livewire)**.**\n\n##
    What is data-accumulated baggage?\nThe table we've set up in [Hoarding Order with
    Livewire](/laravel-bytes/hoarding-order/) used client side pagination to easily
    paginate grouped rows of data. Instead of downloading an entire dataset, it fetched
    and accumulated parts of the entire dataset in the background using `Livewire:poll`.\n\nThis
    approach made pagination easy and lowered the bottleneck on processing large datasets.
    However, it also has a glaring drawback. \n\nAs the polling mechanism accumulates
    more data for the table component, the larger the memory sent back by the server
    and received by the client browser, and consequently:\n\n1. The longer the time
    it takes for the client to download data from the server\n1. The larger the memory
    space taken up by our users' devices\n\nWe are left with \"data baggage\" we must
    not allow to get out of hand!\n\n\n\n## Offloading the Baggage\n\nData accumulation
    can leave unnecessary, memory-heavy baggage. Today, we'll clean up both server
    and client baggage's we've accumulated so far:\n\n1. Offload data accumulation
    from server and assign the role to the client\n2. Reset accumulated data in client
    when it becomes irrelevant to the user\n\n\n\n## Offloading Data Accumulation
    to the Client\n[Previously](/laravel-bytes/hoarding-order/#controller), to accumulate
    data using Livewire, we periodically added data to a public Livewire array `$dataRows`.\nWe
    used a `Livewire:poll` element that periodically called <b>nextPageData</b>:\n```php\n//
    resources\\views\\livewire\\article-table.blade.php\n<div wire:poll.5s>\n  {{
    $this->nextPageData() }}\n  {{ $this->dispatchBrowserEvent('data-updated', ['newData'
    => $dataRows]); }}\n</div>\n```\n\n<b>nextPageData</b> eventually calls <b>addListToData</b>,
    a method that adds more data to `$dataRows`: \n```php\n// app\\http\\Livewire\\ArticleTable.php\npublic
    function addListToData($noneSubList)\n{\n    $subList = $this->getSubRows($noneSubList);\n
    \   foreach( $noneSubList as $item ){\n+        $this->dataRows[] = $item;\n        ...<redacted>...
    \ \n    }\n}\n```\n\nAs our server's `$dataRows` grows with accumulation, so does
    the time it takes for devices to download it. \n\nClient devices can download
    10 rows easily in milliseconds. However, it can take more than a second for devices
    to download rows counting to 1000 and above. Yikes!\n\n<%= partial \"shared/posts/cta\",
    locals: {\n  title: \"Fly.io ❤️ Laravel\",\n  text: \"Fly.io is a great way to
    run your Laravel Livewire app close to your users. Deploy globally on Fly in minutes!\",\n
    \ link_url: \"https://fly.io/docs/laravel\",\n  link_text: \"Deploy your Laravel
    app!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>\",\n} %>\n\nLuckily, this
    accumulation in the server is not necessary at all. \n\nOur client already holds
    data accumulated by the server in a separate, client-side attribute `myData`.
    The client updates `myData` by listening to the `data-updated` event which sends
    back `$dataRows` as `newData`.\n```javascript\n// \\resources\\views\\livewire\\article-table.blade.php\nwindow.addEventListener('data-updated',
    event => {\n   myData = event.detail.newData;\n   refreshPage();\n});\n```\n\nWe
    also don't want to be redundant by having the accumulated data in both server
    and client. So, let's keep accumulation strictly client side. \n\n\n1) From the
    server, clear  `$dataRows`&mdash;the attribute that accumulates data&mdash;for
    every new batch of data we're processing in `app/http/Livewire/ArticleTable.php`:\n\n```php\npublic
    function addListToData($noneSubList)\n{\n+   // Let's clear the data list we have\n+
    \  $this->dataRows = array();\n    $subList = $this->getSubRows($noneSubList);\n
    \   foreach( $noneSubList as $item ){\n            $this->dataRows[] = $item;\n
    \           ...<redacted>...  \n    }\n}\n```\nOur server can breathe easier with
    smaller datasets to store and send out.  And now, our client can more quickly
    download those datasets. \n\nAll that's left now is to move accumulation logic
    to our client:\n\n2) Revise [`/resources/views/livewire/article-table.blade.php`](https://github.com/KTanAug21/hoard-table-data-using-livewire/blob/master/resources/views/livewire/article-table.blade.php)
    to merge its accumulated data `myData` with new lists received from the server:\n\n```diff\nwindow.addEventListener('data-updated',
    event => {\n-   myData = event.detail.newData;\n\n+   // Append new dataset to
    myData list\n+   myData.push(...event.detail.newData)\n\n   // Reload the table
    display\n   refreshPage();\n});\n```\n\nThat's it for offloading server data baggage!
    When working with Livewire, keep your server responses lightweight. This means
    no data accumulation from the server when the client can do it instead.\n\n##
    Offloading Client Data-Baggage\nData accumulation was removed from the server
    above, but our client still holds accumulated data.\n\nTo alleviate this burden
    left on our client, let's keep only necessary data stored in memory.  We can do
    this by clearing the client's accumulated list every time our user requests to
    view a subset of our data.\n\nAs an example, if we have a table of  bookmarks&mdash;urls
    saved from different sources&mdash;when the user asks to view bookmarks from  &quot;www.pacocha.com&quot;,
    the only subset of data the client needs to store are those bookmarks containing
    that substring. The client wouldn't need other irrelevant data in the table, and
    so, we can safely clear the data it has accumulated so far, and start off new
    with relevant results.\n\nThis offloading allows our users' devices to breath
    easier from each reset. As an overview, we'll set up five things to offload our
    client data:\n\n1. A public Livewire attribute `$filters` which will receive parameters
    from our client using `Livewire:model`. \n2. A search bar mapped to `$filters[search]`
    that will let us know specific keywords our user wants to filter our data with.
    \n3. A `Livewire updated hook` that detects changes on `$filters` , triggering
    a call to our Livewire component's `initializeData` method that sends back filtered
    data and a `reset` flag to our client through the `data-updated` event.\n4. A
    revision on our client listener to `data-updated` that will clear its accumulated
    `myData` when it receives a `reset=true` flag.\n5. Finally, a static method `filterQuery`
    in our model which will receive `$filters` and apply client parameters to our
    database queries.\n\n\n### View\n\nLet's equip our client's view, [`resources/views/livewire/article-table.blade.php`](https://github.com/KTanAug21/hoard-table-data-using-livewire/blob/master/resources/views/livewire/article-table.blade.php),
    with a search bar for sending search queries, and a logic for clearing its accumulated
    data.\n\n1) From our view, we can bind a search bar with a public Livewire attribute
    using `wire:model``:`\n\n```diff\n#  resources/views/livewire/article-table.blade.php\n+
    <div> \n+   <input type=\"text\" wire:model.debounce.500ms=\"filters.search\"
    placeholder=\"Search\" class=\"bg-gray-50 border border-gray-300\">\n+ </div>\n```\n<small>\n<ul>\n<li>`wire:model`.debounce.500ms=`\"filters.search\"`
    maps to `$filter['search']`</li>\n<li>wire:model.`debounce.500ms`=&quot;…&quot;
    debounces request by 500ms after key up</li>\n</ul>\n</small>\n\n\n2) Next, let's
    revise our client listener on the `data-updated` event to clear its accumulated
    list `myData` whenever it receives a `reset=true` flag from the server:\n\n```php\n#
    \ resources/views/livewire/article-table.blade.php\n<script>\n...\n\nwindow.addEventListener('data-updated',
    event => {\n+ // Clear list when a reset is received and return to page 1\n+  if(
    event.detail.reset ){\n+    myData = [];\n+    page   = 1;\n+  }\n\n   // Append
    new dataset to myData list\n   myData.push(...event.detail.newData)\n   \n   //
    Reload the table display\n   refreshPage();\n});\n```\n\n\n\nThat's it for our
    view. Conditionally clearing `myData` is the core of offloading data baggage from
    the client!\n\nWhat's left below is to filter data based on user request. \nBelow,
    we'll update our controller to respond to changes on the public attribute mapped
    to our search bar, `$filters`.\n\n\n\n## Controller\n\nAfter revising our View
    above, we should now have a search bar to send `$filters[search]` requests to
    our server. \n\nIn our Controller, [`/app/http/Livewire/ArticleTable.php`](https://github.com/KTanAug21/hoard-table-data-using-livewire/blob/master/app/Http/Livewire/ArticleTable.php),
    let's detect changes on `$filters`, apply those filters to our queries, and send
    back a filtered batch of data along with a `reset` flag to the client. \n\n\n###
    Detecting Filters\n\nFrom our controller, we can add our public attribute `$filters`
    and listen to any changes on it through `Livewire's updated hook`.\n\n1) Add a
    public `$filters` attribute in `/app/http/Livewire/ArticleTable.php`. \n\n```php\n+
    public $filters;\n\n// Override this to initialize our table \npublic function
    mount()\n{\n+   // Let's use this for search, filter, and sort logic  \n+   $this->filters
    = [];\n    ...\n}\n```\n\n2) Detect changes on `$filters` through `Livewire's
    updated hook`. Any user input on `$filters` should trigger a call to our Livewire
    component's `initializeData` method. \n\n```php\n// Re-initialize our data for
    all changes made on $this->filters\n+ public function updatedFilters()\n+ {\n+
    \   $this->initializeData();\n+ }\n```\n\nThe `initializeData` method gets a new
    set of data for us. It eventually passes its new dataset to `addListToData,` which
    is in charge of sending a properly revised, ordered list of data to our client.\n\n3)
    Pass a `reset=true` flag between the `initializeData` to `addListToData` methods\n\n```php\npublic
    function initializeData()\n{\n    $noneSubList = $this->getBaseQuery()\n    ->limit($this->pagination*2)\n
    \   ->get();\n\n-   $this->addListToData( $noneSubList );\n+   $this->addListToData(
    $noneSubList, true );\n}\n```\n\n4) And finally, from our `addListToData` method
    pass the ordered dataset _and_ the `reset flag` to the client through a `dispatchBrowserEvent`
    on the `data-updated` event\n\n```php\n- public function addListToData($noneSubList)\n+
    public function addListToData($noneSubList, $resetClientList=false)\n{\n    ...<redacted
    logic here>...\n\n-   $this->dispatchBrowserEvent('data-updated', ['newData' =>
    $this->dataRows]); \n+   $this->dispatchBrowserEvent('data-updated', ['newData'
    => $this->dataRows, 'reset'=>$resetClientList]);  \n}\n\n```\n<small>\n<ul>\n<li>by
    default our `$resetClientList` is set to False, this is because we only want to
    reset during specific cases only and not all the time</li>\n<li>it is only from
    the `initializeData` method, which is triggered in mounting or with every **user-request**(search,filter,sort),
    does the `$resetClientList` receive a true flag</li>\n<li>`$resetClientList` is
    passed to our client as `event.detail.reset` dispatched in the `data-updated`
    event our client listens to. The client clears its accumulated data when it receives
    `event.detail.reset = true`</li>\n</ul>\n</small>\n\n\nCompleting Controller revisions
    1 to 4 above enabled detection of user-changes to `$filters` attribute. The changes
    above would clear the client's accumulated `myData` list with each user search,
    but it still does not process the filtering logic.\n\n\n### Applying Filters\n\nApplying
    filters to our data is very easy!\n\nWe'll later create a static function in our
    model to receive and process requests from `$filters`. For now, let's make our
    final changes to our controller to chain that static filter method in our existing
    model queries:\n\n\n\n1)  We have two methods in our `/app/http/livewire/ArticleTable.php`
    that's in charge of querying our Article model. So let's chain our filter query
    there:\n\n```php\n// Base Query for none-Sub data retrieval\npublic function getBaseQuery()\n{\n
    \    // Quickly refresh the totalRows every time we check with the db\n-    $this->totalRows
    = Article::query()->count();\n+    $this->totalRows = Article::filterQuery($this->filters)->count();\n\n
    \    // Return none-Sub rows to avoid duplicates in our $dataRows list\n-    return
    Article::query()->whereNull('lead_article_id');\n+    return Article::filterQuery($this->filters)->whereNull('lead_article_id');\n}\n\n...\n\n//
    Get's Sub data for a list of possible Lead rows\nprivate function getSubRows($noneSubList)\n{\n
    \   $idList = [];\n    foreach($noneSubList as $item){\n        $idList[] = $item->id;\n
    \   }\n\n-   return Article::query()\n+   return Article::filterQuery($this->filters)\n
    \   ->whereIn('lead_article_id', $idList)\n    ->get();\n}\n```\n\n\n##  Model\n\nFinally,
    let's add in a static method in our model to apply the `$filters` we receive.
    \n\nRevise [`/App/Models/Article.php`](https://github.com/KTanAug21/hoard-table-data-using-livewire/blob/master/app/Models/Article.php)
    and add a method where we can apply filters with.\n\n```php\n# /App/Models/Article.php\n\n+
    public static function filterQuery(array $filters)\n+ {\n+   $query = Article::query();\n\n+
    \  // Search\n+   if( isset($filters['search']) ){\n+       $query = $query->where(function($query)
    use($filters){\n+           $searchString = '%'.$filters['search'].'%';\n+           $query->where('url',
    'like', $searchString );\n+           $query->orWhere('source', 'like', $searchString
    );\n+       });\n+   }   \n \n+   return $query;\n+}\n```\n\nThat's it!\n\nIn
    one sitting we were able to create one method in our model to apply `$filters`,
    use that filter method throughout our model queries,  detect and respond to user
    input on `$filters` through `Livewire's updated hook`, bind user search on `$filters`
    through `Livewire:model`, and finally breathe easy with offloading (irrelevant)
    client data baggage!\n\n\n## Retrospect\n\n\nIn the [first part](/laravel-bytes/hoarding-order/)
    of this two-post series on paginating tables with grouped rows, we saw that we
    can do client-pagination without downloading entire datasets. With Livewire, we
    can now easily poll for more data to accumulate instead of waiting for an entire
    dataset to load. \n\nThis day, amidst the heavy baggage we've steadily accumulated,
    we saw that it's okay to offload data baggage. We can still have client-pagination
    on accumulated data. \n\nWe'll just remove accumulation from the server side to
    avoid sending large datasets for the client to download. And, with a little mix
    of reloading from the server when the time calls, our client can breathe easy
    with clearing its baggage as well.\n\nIt's okay to remove baggage&mdash;in the
    right place, time, and manner.\n\n\n\n"
- :id: laravel-bytes-streaming-to-the-browser-with-livewire
  :date: '2022-11-14'
  :category: laravel-bytes
  :title: Streaming to the Browser with Livewire
  :author: fideloper
  :thumbnail: browser-streamin-thumb.jpg
  :alt: streaming to the browser
  :link: laravel-bytes/streaming-to-the-browser-with-livewire
  :path: laravel-bytes/2022-11-14
  :body: "\n\n<p class=\"lead\">We're going to use Livewire to stream some content
    to the browser. Livewire works best with low latency! Luckily, Fly.io lets you
    deploy apps close to your users. [Try it out](https://fly.io/docs/laravel/), it
    takes just a minute.</p>\n\nWe're going to see how to stream content to the browser
    using Livewire.\n\n\U0001F914 What's that mean?\n\nHere's an example from [Chipper
    CI](https://chipperci.com \"laravel testing\"), which runs \"builds\" when you
    push code to git. When a command is run (perhaps `composer install`), the build
    output is streamed back, in chunks. \n\nChipper's code stores each chunk, and
    sends an event that tells your browser to retrieve the latest content.\n\n![streaming
    output to the browser](finished-product-md.webp)\n\nFrom your point of view, the
    content is streamed to the browser in real-time!\n\nLet's see how we can build
    the same thing.\n\n## How it Works\n\nThere's less magic than you may think (this
    is a good thing)!\n\n**Here's whats up:**\n\nImagine that some process is running
    a command. In our case, that command will be `composer install`.\n\nRunning `composer
    install` isn't instant - it takes around 10 seconds to finish, and outputs its
    work as it goes.\n\nTo stream that output, we take each \"chunk\" of output as
    it's given and \"flush\" each chunk (either periodically or when it gets large
    enough) into a queue job.\n\nLaravel reads those queue jobs, stores the output,
    and broadcasts an event.\n\nThat event is received by the user's browser, and
    informs the javascript to display the new content.\n\n### The Part That We Care
    About\n\nFor this article, we care about the part that streams the content to
    the browser.\n\nWe'll take some sample command output, split it into chunks, and
    dispatch an event for each chunk. The browser will respond to these events by
    displaying the content it has so far.\n\nTo do this, we need the server (PHP)
    to be able to dispatch an event that the client (the browser) will see. The common
    way to do this is to use websockets + Laravel [Echo](https://laravel.com/docs/9.x/broadcasting#client-side-installation)
    (via Laravel's [Broadcasting](https://laravel.com/docs/9.x/broadcasting) feature).\n\nLivewire
    has a great [integration with Echo](https://laravel-livewire.com/docs/2.x/laravel-echo)
    to make this super easy.\n\n**Here's what we're going to do:**\n\n1. Setup Laravel,
    and a websocket server\n2. Configure Laravel to broadcast events over websockets\n3.
    Create an event to send chunks of command output\n4. Use Livewire to handle those
    events and display the content\n\n## Setup Broadcasting\n\nSetting up the websocket
    server is actually the most work! To avoid setting up a websocket server yourself,
    I suggest using [Pusher](https://pusher.com/channels), which has a great free
    tier.\n\nHowever, if you want to just run something locally, let's see how to
    use the open source [Laravel Websockets](https://beyondco.de/docs/laravel-websockets/getting-started/installation)
    package instead.\n\nThat project is compatible with Pusher's API, so our code
    will think it's talking to Pusher.\n\nWe'll start by creating a Laravel project,
    and installing the [Laravel Websockets](https://beyondco.de/docs/laravel-websockets/getting-started/installation)
    package:\n\n```bash\n# 1. Create a Laravel project\ncomposer create-project laravel/laravel
    browser-stream\n\ncd browser-stream\n\n# 2. Install websockets\ncomposer require
    beyondcode/laravel-websockets\nphp artisan vendor:publish \\\n    --provider=\"BeyondCode\\LaravelWebSockets\\WebSocketsServiceProvider\"
    \\\n    --tag=\"migrations\" \\\n    --tag=\"config\"\n\n# 3. Get a specific version
    of the Pusher SDK\ncomposer require pusher/pusher-php-server:7.0.2\n\n## A bug
    in beyondcode/laravel-websockets prevents\n## us from using the latest pusher
    SDK\n# composer require pusher/pusher-php-server\n\n# 4. We also need Echo + Pusher
    client libraries\nnpm install --save-dev laravel-echo pusher-js\n```\n\nIn addition
    to Pusher, Laravel Echo, and Laravel-Websockets, we'll also install [Breeze](https://laravel.com/docs/9.x/starter-kits).
    \nBreeze gives us some view files and layout that are convenient for us to edit
    as a starting point.\n\n```bash\n# 1. Install Breeze\ncomposer require laravel/breeze
    --dev\nphp artisan breeze:install\n\n# 2. Migrate - Assuming your database is
    setup\nphp artisan migrate\n\n# 3. Install and build static assets\nnpm install\nnpm
    run build\n````\n\nThe Websocket library is [compatible with Pusher's API](https://beyondco.de/docs/laravel-websockets/basic-usage/pusher),
    so let's keep lying to our code and make it feel like Pusher is being used.\n\nWe'll
    therefore set Pusher's configuration in our `.env` file:\n\n```ini\n# Set to pusher\nBROADCAST_DRIVER=pusher\n\n#
    Farther down the default .env\n# are the Pusher variables:\n\n# Use any value
    for these 3\nPUSHER_APP_ID=123456\nPUSHER_APP_KEY=xxx\nPUSHER_APP_SECRET=yyy\n\n#
    Our websocket will run locally\nPUSHER_HOST=127.0.0.1\nPUSHER_PORT=6001\nPUSHER_SCHEME=http\n```\n\nYou
    can use **any value** you want for Pusher app ID, key, and secret. The host and
    port will be pointing to the Laravel Websocket server we'll run shortly, which
    defaults to `localhost` at port `6001`. We'll avoid TLS setup and use scheme `http`.\n\nWe'll
    use [Echo](https://github.com/laravel/echo) on the client-side. Echo is Laravel's
    library that integrates with the Broadcasting feature. This also is made to believe
    that we are using Pusher.\n\nEdit file `resources/js/bootstrap.js` file to uncomment
    the Pusher/Echo library stuff that's already there for you. **There's literally
    no other adjustment to make**:\n\n```javascript\nimport Echo from 'laravel-echo';\n\nimport
    Pusher from 'pusher-js';\nwindow.Pusher = Pusher;\n\nwindow.Echo = new Echo({\n
    \   broadcaster: 'pusher',\n    key: import.meta.env.VITE_PUSHER_APP_KEY,\n    wsHost:
    import.meta.env.VITE_PUSHER_HOST ? import.meta.env.VITE_PUSHER_HOST : `ws-${import.meta.env.VITE_PUSHER_APP_CLUSTER}.pusher.com`,\n
    \   wsPort: import.meta.env.VITE_PUSHER_PORT ?? 80,\n    wssPort: import.meta.env.VITE_PUSHER_PORT
    ?? 443,\n    forceTLS: (import.meta.env.VITE_PUSHER_SCHEME ?? 'https') === 'https',\n
    \   enabledTransports: ['ws', 'wss'],\n});\n```\n\nThen compile static assets
    again (via the `dev` or `build` command):\n\n```bash\n# This starts a dev server
    that\n# watches for file changes\n# Use \"npm run build\" for a one-shot build\nnpm
    run dev\n```\n\nWe're just about setup with broadcasting with Echo and \"Pusher\"
    (Laravel-Websockets). Let's test it out!\n\n## Create an Event\n\nWe're going
    to create a new event that is broadcast to the browser via the websocket server.
    \n\nThe Event is a PHP class. Laravel's Broadcasting library takes care of sending
    the event data to the websocket. The browser is also connected to the websocket,
    and thus our browser can see the event.\n\n![browser event diagram](browser-stream-diagram.png)\n\nLet's
    make that event - it's just a standard [Laravel event](https://laravel.com/docs/9.x/events).\n\nWe'll
    have it broadcast \"publicly\" (using a `Channel` instead of a `PrivateChannel`)
    so we don't need to mess with [channel authorization](https://laravel.com/docs/9.x/broadcasting#authorizing-channels).\n\n```bash\nphp
    artisan make:event ContentUpdated\n```\n\nThat generates file `app/Events/ContentUpdated.php`.
    We make a few adjustments to that file:\n\n```diff\n<?php\n\nnamespace App\\Events;\n\nuse
    Illuminate\\Broadcasting\\Channel;\nuse Illuminate\\Broadcasting\\InteractsWithSockets;\nuse
    Illuminate\\Broadcasting\\PresenceChannel;\nuse Illuminate\\Broadcasting\\PrivateChannel;\nuse
    Illuminate\\Contracts\\Broadcasting\\ShouldBroadcast;\nuse Illuminate\\Foundation\\Events\\Dispatchable;\nuse
    Illuminate\\Queue\\SerializesModels;\n\n- class ContentUpdated\n+ class ContentUpdated
    implements ShouldBroadcast\n{\n    use Dispatchable, InteractsWithSockets, SerializesModels;\n\n
    \   /**\n     * Create a new event instance.\n     *\n     * @return void\n     */\n-
    \   public function __construct() {}\n+    public function __construct(public
    string $content) {}\n\n    /**\n     * Get the channels the event should broadcast
    on.\n     *\n     * @return \\Illuminate\\Broadcasting\\Channel|array\n     */\n
    \   public function broadcastOn()\n    {\n-       return new PrivateChannel();\n+
    \      return new Channel('content');\n    }\n}\n```\n\nHere's what was added:\n\n1.
    The class implements `ShouldBroadcast` so Laravel knows to broadcast it\n2. We
    created attribute `public string $content`\n3. The class broadcasts on a public
    `Channel` named `'content'`\n\n**Now we just need the frontend to do its thing.**\n\nBreeze
    generated a layout file `resources/views/layouts/app.blade.php`. This file already
    has our static assets (CSS/JS) loading:\n\n```html\n@vite(['resources/css/app.css',
    'resources/js/app.js'])\n```\n\nSo we should have Echo available to us already!
    Let's use it.\n\n<%= partial \"shared/posts/cta\", locals: {\n  title: \"Fly.io
    ❤️ Laravel\",\n  text: \"Fly.io is a great way to run your Laravel Livewire app
    close to your users. Deploy globally on Fly in minutes!\",\n  link_url: \"https://fly.io/docs/laravel\",\n
    \ link_text: \"Deploy your Laravel app!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>\",\n}
    %>\n\n## Test Out Broadcasting\n\nLet's broadcast that event and see if the browser
    receives it.\n\nGo ahead and edit file `resources/views/layouts/app.blade.php`
    and add the following near the bottom, before the closing `</body>` tag:\n\n```html\n<!--
    type=\"module\" is important here -->\n<script type=\"module\">\n    Echo.channel('content')\n
    \       .listen('ContentUpdated', (e) => {\n            console.log(e);\n        });\n</script>\n```\n\nThis
    tells `Echo` to listen on channel `content` for an event named `ContentUpdated`.
    It will just `console.log()` the event data.\n\nWe'read ready to test that out!\n\nGo
    ahead and start the websocket server:\n\n```bash\n# This is a long-running process,
    \n# maybe run it in its own tab\nphp artisan websockets:serve\n```\n\nOne easy
    way to fire the `ContentUpdated` event is to use a route.\n\nCreate a route `/fire`
    to dispatch the event:\n\n```php\n# File routes/web.php\n\nRoute::get('/fire',
    function () {\n    \\App\\Events\\ContentUpdated::dispatch(\"more stuff\");\n
    \   return \"fired\";\n});\n```\n\nGoing to that route (`localhost:8888/fire`
    for me) will dispatch the event. If all goes well, you should see the event logged
    to the console in your browser.\n\n![logging to the console](console-log.png)\n\nIf
    all does not go well, be sure to check the contents of your `.env` and perhaps
    restart your services (re-run `npm run build`, restart your servers).\n\nSo, it
    works! Next we need to convert this to Livewire and then generate some content
    (perhaps in a console command) to pretend we have some data streaming in.\n\n\n##
    Using Livewire\n\nLet's add Livewire to our project. Setting it up is just 2 steps!\n\nFirst,
    get Livewire:\n\n```bash\ncomposer require livewire/livewire\n```\n\nThen add
    its static assets to `resources/views/layouts/app.blade.php`:\n\n```html\n...\n
    \   @vite(['resources/css/app.css', 'resources/js/app.js'])\n    @livewireStyles\n</head>\n<body>\n
    \   ...\n \n    @livewireScripts\n</body>\n</html>\n```\n\nLivewire is now setup.
    We can move onto making a Livewire component:\n\n```bash\nphp artisan livewire:make
    Content\n```\n\nThat command generates files:\n\n* `app/Http/Livewire/Content.php`\n*
    `resources/views/livewire/content.blade.php`\n\n\nThe Blade view file `content.blade.php`
    is pretty simple:\n\n```html\n<div \n  class=\"bg-gray-800 text-white p-6 rounded
    overflow-hidden\n         font-mono text-sm whitespace-pre-wrap\"\n>{{ $content
    }}</div>\n```\n\nWe added some Tailwind classes there to format the output to
    be a bit more like something you'd expect from running a command in a terminal.\n\n<div
    class=\"callout\">Since we use class `whitespace-pre-wrap`, removing whitespace
    between the `<div>` tags and the `$content` becomes important for formatting.</div>\n\nRemember,
    we're going to display the output from running command `composer install`. This
    output will have extra junk in it (ANSI color codes). Luckily there are packages
    we can use to help format and colorize the output:\n\n```bash\ncomposer require
    sensiolabs/ansi-to-html\n```\n\n\nWe'll use that package in the `Content.php`
    file to take our output and turn it into something that looks pretty:\n\n```php\n<?php\n\nnamespace
    App\\Http\\Livewire;\n\nuse App\\DraculaTheme;\nuse Livewire\\Component;\nuse
    SensioLabs\\AnsiConverter\\AnsiToHtmlConverter;\n\nclass Content extends Component\n{\n
    \   public $rawContent = '';\n\n    public $content = 'waiting...';\n\n    protected
    $listeners = [\n        'echo:content,ContentUpdated' => 'appendContent'\n    ];\n\n
    \   public function appendContent($event)\n    {\n        $this->rawContent .=
    $event['content'];\n        \n        $theme = new DraculaTheme;\n        $ansiConverter
    = new AnsiToHtmlConverter($theme);\n        $this->content = $ansiConverter->convert($this->rawContent);\n
    \   }\n\n    public function render()\n    {\n        return view('livewire.content');\n
    \   }\n}\n```\n\n**OK, so what's actually happening here!?**\n\n**First**, we
    have `$rawContent` and `$content`. The attribute `$content` is what the browser
    sees. Attribute `$rawContent` is the raw string. We transform the raw string into
    some HTML using the ANSI to HTML helper.\n\n**Second**, we have `$listeners`.
    This is a [built-in thing in Livewire](https://laravel-livewire.com/docs/2.x/laravel-echo)
    to integrate with events, including Echo. \n\nThe code above listens for Echo
    events on the `content` channel, and specifically reacts to event `ContentUpdated`.When
    `ContentUpdated` is received, it calls method `appendContent()`.\n\n**Third**,
    method `appendContent()` appends the latest content, and updates `$content` to
    valid/styled HTML content via `AnsiToHtmlConverter`.\n\nThat helper has themes!
    The theme file (`DraculaTheme`) is [this class](https://gist.github.com/fideloper/f36fc60f1370fdb82e29ae2d267395d5).\n\nNow
    that we have this, we can remove the `<script>` tag we added to the `resources/views/layouts/app.blade.php`
    file. Livewire will do the work for us!\n\n\n## Streaming Content\n\nWe're not
    going to get into streaming content from running a command (that'd be a smallish
    book instead of an article).\n\nInstead, what we'll do fire off the `ContentUpdated`
    event every half-second to simulate command output being streamed into our app.\n\nIn
    our case, we'll just append the content we get from the event to the browser output
    (we don't need to handle the case of persisting output to permanent storage).\n\nLet's
    create a command to help is with this:\n\n```bash\nphp artisan make:command --command
    content:run GenerateContentCommand\n```\n\nThat will generate file `app/Console/Commands/GenerateContentCommand.php`.
    We can add some functionality to it:\n\n```php\n<?php\n\nnamespace App\\Console\\Commands;\n\nuse
    App\\Events\\ContentUpdated;\nuse Illuminate\\Console\\Command;\n\nclass GenerateContentCommand
    extends Command\n{\n    protected $signature = 'content:run';\n\n    protected
    $description = 'Stream sample output to the browser';\n\n    protected $content
    = [\n        'chunks of string content',\n        'stored here within',\n        'this
    array'\n    ];\n\n    public function handle()\n    {\n        collect($this->content)->each(function($chunk)
    {\n            ContentUpdated::dispatch($chunk);\n            usleep(500000);
    // half second\n        });\n\n        return Command::SUCCESS;\n    }\n}\n\n```\n\nThe
    actual `$content` I used is sample command output that contains [ANSI color codes](https://gist.github.com/fnky/458719343aabd01cfb17a3a4f7296797#256-colors).
    You can see that [actual output here](https://gist.github.com/fideloper/fd2f09cdd550266d163649e2401eda41).\n\nThis
    takes the chunks of content (which I split up into chunks manually, roughly every
    16 lines of output) and dispatches our `ContentUpdated` event with a chunk of
    content every half second. This simulates our application receiving content from
    a running process.\n\n## Run the Command\n\nFinally we can run the command! If
    you run that and watch your browser, you should see the content streamed there,
    live and in color!\n\n```bash\nphp artisan content:run\n```\n\n![streaming output
    to the browser](finished-product-md.webp)\n\n## Review\n\nThe important thing
    to know here is that [Livewire has integrations with Laravel's broadcasting features](https://laravel-livewire.com/docs/2.x/laravel-echo).
    Livewire can listen for dispatched events!\n\nWe setup a Laravel application,
    installed [Breeze](https://laravel.com/docs/9.x/starter-kits) (so we got some
    view files to work with), and finally installed Livewire.\n\nWe've setup a command
    that broadcasts an event every half second. The event has some content in it.
    Livewire listens for that event, and appends the content within it to a variable,
    which results in the frontend being updated with that content.\n\nThere's not
    much to it, other than going through the motions of setting up Broadcasting via
    [Pusher](https://pusher.com/) or [Laravel-Websockets](https://beyondco.de/docs/laravel-websockets/getting-started/introduction).\n\n\n"
- :id: phoenix-files-opentelemetry-and-the-infamous-n-plus-1
  :date: '2022-11-14'
  :category: phoenix-files
  :title: Elixir, OpenTelemetry, and the Infamous N+1
  :author: akoutmos
  :thumbnail: pencil-thumbnail.jpg
  :alt:
  :link: phoenix-files/opentelemetry-and-the-infamous-n-plus-1
  :path: phoenix-files/2022-11-14
  :body: |2


    <p class="lead">This article explores observability in Elixir systems, specifically using OpenTelemetry. Fly.io is a great place to run your Elixir applications! Check out how to [get started](/docs/elixir/)!</p>

    In this article, we'll dive into the topic of observability and specifically the
    [OpenTelemetry project](https://opentelemetry.io/). We'll see how to set up the
    Elixir and [Erlang
    OpenTelemetry](https://github.com/open-telemetry/opentelemetry-erlang) libraries
    in a Phoenix LiveView application so you can debug troublesome database queries.
    We'll also see how this application can be deployed to Fly.io along with
    [Grafana](https://grafana.com/) and [Tempo](https://grafana.com/oss/tempo/) so
    we can store and query our sample traces. There is also a [demo repo](https://github.com/fly-apps/elixir_opentel_and_grafana)
    with all the referenced code. Be sure to check that out!

    Before we dive into the nitty-gritty, let's first get a sense
    for the application observability landscape.

    As software engineers, it is crucial that we have insight into our running applications. This is especially true when
    we have customers paying-for and depending-on our products. As a customer and user, there are few things more
    frustrating that trying to get your work done and some SaaS product that you pay for is currently unavailable or running
    extremely slow. As a business, we may be able to get away with some down-time and slow services here and there, but if
    it becomes the norm, the customer-experience deteriorates to the point where customers start looking elsewhere.

    But don't worry - it's not all doom and gloom! As software engineers, there are plenty of tools at our disposal that
    enable us to provide the best customer experience possible. For example, as we develop a piece of software, we can write unit, integration, end-to-end and stress tests to ensure that features are implemented properly and that they
    continue to work tomorrow even though the codebase is constantly changing. These tools work great for local development
    and CI/CD... but what do you do when the application is running in production and we encounter issues? To answer
    that question, we'll need to reach for tools that fall under the observability umbrella.

    ## The Three Pillars of Observability

    In the context of software, observability is the ability to inspect and understand a running application. In order to
    introspect an application we need to extract some data from while it is running without hindering its ability to
    service user requests. Observability tooling that interferes with an application's natural ability to service requests
    is not viable in production as it impacts our customers in undefined ways. So how do we extract data from a
    running application in such a way?

    The three most common ways to achieve this are logging, metrics and traces.
    Combined, they create the three pillars of observability and allow us to effectively analyze and debug our production
    applications and systems. Let's briefly look at what each of these pillars does for us to get a sense for how they work
    together to give us application observability:

    - **Logs:** Logs contain detailed information as to what events are taking place within an application. Logs can be either
      structured (JSON for example) or unstructured (free text).

    - **Metrics:** Metrics are measurements of your application over time and can contain a few bits of metadata to enrich the
      measurements and give them more context.

    - **Traces:** Traces are a collection of related events with each event containing metadata as to what happened and for how
      long. Traces can span across call stacks on a single machine and even across services via distributed tracing.

    In this article, we'll focus on the tracing observability pillar. Specifically, we'll see how we can leverage
    the OpenTelemetry tracing tooling to identify performance issues in an Elixir Phoenix LiveView application.

    Let's dive into OpenTelemetry. We'll start with what it is and how we can use it in our LiveView application.

    ## Configuring OpenTelemetry in Elixir

    OpenTelemetry (often abbreviated to OTel), is a collection of open source standards for how logs, metrics, and traces
    should be collected and exported from services. By design, it is a programming language agnostic standard and there are
    implementations of the [OTel standard in a lot of programming
    languages](https://opentelemetry.io/docs/getting-started/dev/) already including Erlang and Elixir!

    At a high level, the [OTel standard specifies a few different
    components](https://opentelemetry.io/docs/concepts/components/) that need to be available in order to ship logs, metrics
    and traces from your applications. The API and SDK components are what we lean on when we instrument our
    application with the tracing library for Ecto for example. The collector component, is what that Ecto tracing library
    ships telemetry data to, which, in turn, exports that telemetry data to Jaeger, Tempo, or whatever we choose to
    use to persist sample traces.

    Luckily, instrumenting your Elixir applications isn't too hard thanks to all of the hard work done by the contributors
    to the [Hex OpenTelemetry organization](https://hex.pm/orgs/opentelemetry). For this article, we'll look at a sample
    TODO list LiveView application that has two routes of interest. `/users-fast` and `/users-slow` both list all of the
    users of the application and also list how many TODO list items each of them have in their queue. As the names of the
    routes imply, one of the routes responds quickly, and the other not so much. The question that we need to answer is why
    is this occurring and how we can remedy the problem. The title of the article hints at the problem... but it'll really be clear once we see a trace from the application when the
    `/users-slow` route.

    All the code for the sample application can be found
    [here](https://github.com/fly-apps/elixir_opentel_and_grafana) but let's start
    by going through the application specific changes needed to instrument our
    application. As a note, this demo application was generated via the `mix phx.new
    APP_NAME --binary-id` command, with only a few changes made to support deploying
    to Fly.io. Let's first cover how to set up the OpenTelemetry libraries by
    opening up `mix.exs` and adding the following dependencies:

    ```elixir
    defp deps do
      [
        ...
        {:opentelemetry_exporter, "~> 1.0"},
        {:opentelemetry, "~> 1.0"},
        {:opentelemetry_api, "~> 1.0"},
        {:opentelemetry_ecto, "~> 1.0"},
        {:opentelemetry_liveview, "~> 1.0.0-rc.4"},
        {:opentelemetry_phoenix, "~> 1.0"},
        {:opentelemetry_cowboy, "~> 0.2"}
      ]
    end
    ```

    After that, we run `mix deps.get` to fetch the dependencies from Hex. Next, we'll open up `application.ex` and update the `start/1` callback as follows:

    ```elixir
    def start(_type, _args) do
      if System.get_env("ECTO_IPV6") do
        :httpc.set_option(:ipfamily, :inet6fb4)
      end

      :ok = :opentelemetry_cowboy.setup()
      :ok = OpentelemetryPhoenix.setup()
      :ok = OpentelemetryLiveView.setup()

      :ok =
        FlyOtel.Repo.config()
        |> Keyword.fetch!(:telemetry_prefix)
        |> OpentelemetryEcto.setup()

      ...
    end
    ```

    The `if-block` in the beginning checks for the presence of the `ECTO_IPV6` environment variable prior to setting an
    `:httpc` option. The reason for this being that when applications are deployed to Fly.io, they are interconnected
    by a mesh of [IPv6 Wireguard tunnels](https://fly.io/docs/reference/private-networking/), and by default
    the Erlang HTTP client `:httpc` is configured to use IPv4. In order for our OTel exporter to publish our traces to Tempo
    it needs the `:inet6fb4` option set so that it first attempts to connect to the remote host via IPv6, while falling
    back to IPv4 if needed. We lean on the `ECTO_IPV6` environment variable since Ecto is also configured to apply this
    `socket_options` if the environment variable is present (look at the `config/runtime.exs` if you are interested in
    seeing how this is set up).

    Next we have a few Opentelemetry library calls that configure the trace collectors. The first three `setup/0` calls set
    up the Cowboy, Phoenix, and LiveView tracing libraries. These calls instruct the OTel libraries to attach handlers to
    the telemetry events that are emitted by each of the underlying libraries. The Ecto tracing library requires a little
    more work to set up as we need to fetch the configured telemetry prefix so the OTel library can attach the handler
    to the correct Repo event.

    With that in place, all that needs to be done now is update some configuration in `runtime.exs` so the
    telemetry exporter knows where to send trace data. Add the following inside of the `config_env() == :prod` if-block:

    ```elixir
    if config_env() == :prod do
      config :opentelemetry_exporter,
        otlp_protocol: :http_protobuf,
        otlp_endpoint: System.fetch_env!("OTLP_ENDPOINT")

      ...
    end
    ```

    With this in place, we can configure our application at runtime to send traces to the correct
    service. In this example, we will lean on Tempo to capture and persist traces. Once the traces are in Tempo, we
    can use Grafana to explore the persisted traces and see why our endpoints differ in performance.
    Hopefully, it's clear that there is not a lot of ceremony or effort needed on our part to start collecting traces from our
    application.

    Next, we'll deploy our application to Fly.io so we can capture and view some real traces!

    ## Deploying and Observing Your Application on Fly.io

    Let's start by installing the `flyctl` CLI utility and authenticating with Fly.io so we can start deploying our services
    using the following guide: https://fly.io/docs/hands-on/install-flyctl/.

    With that done, you are ready to start deploying all of the necessary services including our trace enabled Phoenix
    LiveView application. Let's begin by deploying our Phoenix application.

    ### Phoenix App + Postgres

    To deploy the instrumented Phoenix LiveView application, we first need to update the `fly.toml` file to reflect the name
    of the application. Specifically, we need to update the following fields:

    - Update the `OTEL_RESOURCE_ATTRIBUTES` environment variable to have the correct `service.name` value for your service.
    - Update `OTLP_ENDPOINT` environment variable to have the correct URL for the Tempo service. The URL will have the
      following format: `http://REGION.YOUR-APP-tempo.internal:4318` where `REGION` is one of the [Fly.io datacenter regions](https://fly.io/docs/reference/regions/).
    - Update the `PHX_HOST` environment variable to reflect the URL for the application based on the application name.

    With those fields updated, you can run `fly launch` and let the Fly.io CLI tool do the heavy lifting. Be sure to create
    a Postgres database when prompted so your application has something to communicate with. After the application is up and
    running, we'll want to hydrate the database with some data. Connect to the running instance by running
    `flyctl ssh console` in the CLI. After you connect to the Phoenix LiveView application, go ahead and run
    `/app/bin/YOUR_APP REMOTE` to attach an IEx session to the live application. After connecting to the application, run the following snippet of Elixir code to populate the database with some initial data:

    ```elixir
    alias Faker.Person
    alias Faker.Lorem

    alias FlyOtel.Accounts
    alias FlyOtel.Accounts.TodoListItem
    alias FlyOtel.Accounts.User

    1..20
    |> Enum.each(fn _ ->
      {:ok, %User{} = user} =
        Accounts.create_user(%{
          age: Enum.random(18..65),
          name: "#{Person.first_name()} #{Person.last_name()}"
        })

      1..Enum.random(5..50)
      |> Enum.each(fn _ ->
        {:ok, %TodoListItem{}} = Accounts.create_todo_list_item(%{task: Lorem.sentence()}, user)
      end)
    end)
    ```

    With that done, we can vist the application in a browser (`https://YOUR-APP.fly.dev/users-fast`) to see it in
    action!

    Next we'll deploy Tempo to store all of the traces that our collector exports.

    ### Tempo

    To run Tempo in Fly.io, we need to create our own Docker container (built upon the Grafana Tempo image) that includes our necessary configuration file.
    The following Dockerfile is the bare minimum required to deploy Tempo to Fly.io and will probably need
    some more work and configuration if you want to set this up for production:

    ```dockerfile
    FROM grafana/tempo:1.5.0

    COPY ./tempo-config.yaml /etc/tempo.yaml

    CMD ["/tempo", "-config.file=/etc/tempo.yaml"]
    ```

    In that short Dockerfile, it mentioned `tempo-config.yaml`. We'll create the file to copy into the Dockerfile and configure how Tempo listens for trace data and how to store the data. Similar to the Dockerfile, this is the minimum requirement to get Tempo up and running and for a production application some additional configuration might be needed. The contents of the YAML file are:

    File: `temp-config.yaml`

    ```yaml
    server:
      http_listen_port: 3200

    search_enabled: true

    distributor:
      receivers:
        otlp:
          protocols:
            http:
              endpoint: "0.0.0.0:4318"

    storage:
      trace:
        backend: local
        block:
          bloom_filter_false_positive: .05
          index_downsample_bytes: 1000
          encoding: zstd
        wal:
          path: /tmp/tempo/wal
          encoding: snappy
        local:
          path: /tmp/tempo/blocks
        pool:
          max_workers: 100
          queue_depth: 10000
    ```

    With the Dockerfile and configuration located in the same directory, all we need is a `fly.toml` file in
    the same directory so we can deploy Tempo. The contents of the `fly.toml` file should contain the following:

    ```toml
    app = "YOUR-APP-tempo"

    [build]
    dockerfile = "./Dockerfile"
    ```

    For those familiar with deploying services on Fly.io, they may wonder why there is no `[[services]]` section. The reason
    is, we don't want this service accessible from the public internet. We only want it available internally on our network for Grafana and our Phoenix
    application to communicate with.

    With that all in place, all that is left is to run `flyctl apps create YOUR-APP-tempo && flyctl deploy` in the directory
    with all of the files and Tempo should be deployed!

    With that going, let's deploy Grafana next.

    ### Grafana

    Like how we deployed Tempo, we'll create a new directory to house the Dockerfile, Grafana config, and Fly.io
    deployment manifest. Again, the Dockerfile is simple and copies in a configuration file and starts
    Grafana:

    ```dockerfile
    FROM grafana/grafana:9.2.3

    COPY ./grafana.ini /etc/grafana/grafana.ini

    CMD ["/run.sh"]
    ```

    Our configuration file contains a flag to enable the experiment Tempo search functionality. We use this in Grafana
    to help easily find relevant traces for our application:

    File: `grafana.ini`

    ```ini
    [feature_toggles]
    enable = tempoSearch tempoBackendSearch
    ```

    With that ready, we just need the `fly.toml` file:

    File: `fly.toml`

    ```toml
    app = "YOUR-APP-grafana"

    [build]
    dockerfile = "./Dockerfile"
    ```

    Once again, we run `flyctl apps create YOUR-APP-grafana && flyctl deploy` to get
    Grafana up and running on Fly.io. Also, note the file exposes no services on the public internet. Basically, it's fairly simple (and secure) to connect to internal applications running on Fly.io via Wireguard
    using `flyctl`. This limits our exposed surface area on the public internet which is always a good things from a security
    standpoint.

    Let's connect to Grafana after it is deployed and configure our Tempo data source so we can visualize
    application traces.

    #### Configuring Tempo Datasource in Grafana

    From inside the directory containing the Grafana `fly.toml` manifest, run the following command:

    ```session
    $ flyctl proxy 3000:3000
    ```

    Now, open a browser and navigate to `http://localhost:3000` to access your Grafana instance (by
    default, the username and password are both `admin`)! After logging in, go to the data source configuration page
    (`http://localhost:3000/datasources`) and click the `Add data source` button. After that, look for Tempo in the list of
    available data sources (under `Distributed tracing`) and select it. On the next page, the only field we need
    to fill out is the `URL` field. We want to fill it out as shown below (substituting `REGION` with the region
    where the Tempo instance is running and substituting `YOUR-APP` with the name given to Tempo when it was created):

    [![Configure Tempo data source](./config_data_source.png?centered&card)](./config_data_source.png)

    If all goes well, after clicking `Save & test`, we should see a success message:

    ![Configured Tempo data source displaying success message](./data_source_success.png?centered&card)

    With Grafana running and connected to Tempo, all that is left is to exercise the Phoenix application a little and
    compare the trace results! Navigate a few times to `https://YOUR-APP.fly.dev/users-fast` and
    `https://YOUR-APP.fly.dev/users-slow` to ensure that there is trace information in Tempo and then click on the `Explore`
    button in the side nav in Grafana.

    ## Comparing the Application Traces

    By default, the selected data source should be Tempo, but if it is not, be sure to select it. Then, in the row of filters
    labeled `Query type`, select `Search`. After that, select `/users-fast` from the `Span Name` drop down and click on one
    of the returned `Trace ID`s. After clicking on a trace sample, we should see something like this:

    [![Fast users page trace](./fast_live_view_trace.png?centered&card)](./fast_live_view_trace.png)

    We can even click on individual trace segments to drill down and see what metadata is associated with the event:

    [![Trace metadata](./trace_metadata.png?centered&card)](./trace_metadata.png)

    As a comparison, now select `/users-slow` from the `Span Name` drop down and see how the trace compares. It should appear
    something like this:

    [![Slow N+1 trace](./slow_trace.png?centered&card)](./slow_trace.png)

    As we can see, not only is this trace visually busier than the previous trace but it is also much slower (22.47ms
    duration versus 4.08ms duration from the previous trace). When we expand the trace segments, we can see that the same Ecto
    queries keep getting executed over and over again inside of the LiveView `mount/3` callback whereas the previous trace
    only has a single Ecto query executed. This right here is a visual representation of the infamous N+1 query in action!

    Looking closely at the trace metadata from the slow trace, we can see that it starts off by making one call to
    `fly_otel.repo.query:users` and then makes numerous repeated calls to `fly_otel.repo.query:todo_list_items`. In this
    case the one call to the users is the `1` in `N+1` and the `N` is the 20 other calls that the backend had to make to the
    database to get the TODO list for each and every user (which is supported by the `Child Count` value of `21`):

    [![N+1 metadata](./metadata_zoom_in.png?centered&card)](./metadata_zoom_in.png)

    And just like that, we can now visualize and diagnose less than ideal database interactions all the way down the call
    stack.

    ## Conclusion

    Well done and thanks for sticking through to the end! We covered quite a lot of ground and hopefully you picked up a
    couple of cool tips and tricks along the way.

    To recap, we learned about the various pillars of observability and took a
    deep dive into the tracing pillar. We learned about setting up an Elixir application with the OpenTelemetry tooling and
    even deployed our application along with some supporting monitoring tools to Fly.io. We then compared trace results
    between two different LiveView pages and were able to see the effects that an N+1 query would have on our application's
    performance.


    <%= partial "shared/posts/cta", locals: {
      title: "Fly.io ❤️ Elixir",
      text: "Fly.io is a great way to run your Phoenix LiveView app close to your users. It's really easy to get started. You can be running in minutes.",
      link_url: "https://fly.io/docs/elixir/",
      link_text: "Deploy a Phoenix app today!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>"
    } %>
- :id: blog-logbook-november-7-to-november-14-2022
  :date: '2022-11-14'
  :category: blog
  :title: 'Logbook: November 7 to November 14, 2022'
  :author: brad
  :thumbnail: logbook-thumbnail.jpg
  :alt: A 1950s style cartoon lady pointing at a vintage flight logbook
  :link: blog/logbook-november-7-to-november-14-2022
  :path: blog/2022-11-14
  :body: |2-


    **Automatically deploy Elixir apps to production with Github Actions CI and troubleshoot performance issues with OpenTelemetry. Help the Python community draft Django docs. Deploy S3-compatible object storage close to your Fly apps.**

    [Last week](http://studio.local:4567/blog/logbook-october-29-to-november-6-2022/) we got improved [Elixir on Fly docs](https://fly.io/docs/elixir/getting-started/)—this week we get even more goodies that make it easier to deploy and monitor Elixir apps in production.

    ## Github Actions for Elixir CI

    Mark rolls up his sleeves, grabs a shovel, and digs into getting Github Actions working for Elixir CI. The best part? You don't have to get as dirty to get it setup because Mark did all the hard work for you.

    Read [Github Actions for Elixir CI](https://fly.io/phoenix-files/github-actions-for-elixir-ci/)

    ## Elixir, OpenTelemetry, and the Infamous N+1

    After your [Elixir CI Github Action](https://fly.io/phoenix-files/github-actions-for-elixir-ci/) automatically deploys your app to production, learn how to use OpenTelemetry to monitor and track down performance issues in your production deployments. Alexander Koutmos shows us how by tracking down an N+1 query issue.

    Read [Elixir, OpenTelemetry, and the Infamous N+1](https://fly.io/phoenix-files/opentelemetry-and-the-infamous-n-plus-1/)

    ## Django on Fly

    Amazing things happen on the Internet, like the Python community getting together and documenting how to deploying Django apps to Fly. While we don't have an official 1.0 set of docs yet, it's getting mighty close thanks to community contributors.

    [Will Vincent](https://github.com/wsvincent) has been pushing forward a more comprehensive guide for Django that's way better than our current ["Run a Python App" guide](https://fly.io/docs/languages-and-frameworks/python/) in this [Github Pull Request](https://github.com/superfly/docs/pull/369/commits). There's also a thread in the [Community Forum](https://community.fly.io/t/updated-blog-django-hello-world-fly-io-deployment/8565) about an updated blogpost for deploying a "Hello World" Django app.

    If you're a Django or Python developer, check out the [PR](https://github.com/superfly/docs/pull/369/commits) or read the [Community post](https://community.fly.io/t/updated-blog-django-hello-world-fly-io-deployment/8565).

    ## Deploy your own MinIO S3-compatible object storage to Fly

    Chris updates our docs on how to deploy your own self-hosted S3-compatible object storage server to Fly. We still think its easier to use an S3 host, like AWS S3, but sometimes that doesn't make sense when you need object storage in the same datacenter as your application.

    Read the [MinIO deployment guide](https://fly.io/docs/app-guides/minio/)

    ---

    I'll see you next week!
- :id: jobs-infrastructure-ops-engineering
  :date: '2022-11-09'
  :category: jobs
  :title: Infrastructure Ops Engineering
  :author: michael
  :thumbnail:
  :alt:
  :link: jobs/infrastructure-ops-engineering
  :path: jobs/2022-11-09
  :body: "\n\nInfrastructure Ops engineers are our human to metal interface. They
    manage the foundation that everything else is built on, and provide the tools
    to support our Platform team and Fly.io end customers. This is one of the most
    important roles in the company.\n\n## Here's What This Role Has To Offer:\n\n-
    A fleet of servers that will outnumber your team 100-1000:1.\n- All the weird
    Linux things, from eBPF to LVM2 to WireGuard to policy routing. \n- Green-field
    automation, infrastructure-as-code, observability and alerting work; we're up
    to our eyeballs in fun infra projects.\n- An infra team that's fully engaged and
    deadly serious about stalking issues and taking them down before they grow into
    problems, and that works well together when problems occur.\n- The most lovable
    user base in the whole technology industry, and a company culture that will keep
    you in touch with them directly.\n- An opportunity to work with Python, Ruby,
    Rust, Elixir, and Go, and to sharpen your Go programming ability (you don't need
    to know how to write Go to apply for this role!).\n\n## You're Likely To Succeed
    In This Role If:\n\n- You get that code doesn't matter unless the rest of the
    team knows how to use it. You have a habit of writing guides, docs, and frameworks
    so the wheels keep turning when you're not online.\n- You never waste a crisis.
    You treat each failure as a chance to learn and improve so it doesn't happen again.
    You consider it a crisis when users notice a problem before we do.\n- You know
    the helpless feeling when us-east-1 is on fire while the status page stays green.
    You communicate with users clearly and transparently, the way you'd want as a
    user. \n- You know how to prioritize when everything is vying for your attention.
    Even if it means letting some fires burn while you put out more important ones.
    We're building something big and ambitious with a very tight team; small fires
    are common!\n\n## More Details\n\nThis is a senior level fully-remote full-time
    position. You can live anywhere in the world; your work hours and holidays observed
    are up to you. The salary ranges from $165k to $200k USD. We offer competitive
    equity grants with a long exercise window. US employees get health care, everyone
    gets flexible vacation time (with a minimum), hardware/phone allowances, the standard
    stuff.\n\n## How We Hire People\n\nWe're weird about hiring. We're skeptical of
    resumes and we don't trust traditional interviews. We respect career experience
    but we're more excited about potential.\n\nThe premise of our hiring process is
    that we're going to give you a series of challenges that each simulate the kind
    of work you'll actually be doing here. Unlike a lot of places that assign “take-home
    problems”, our challenges are the backbone of our whole process; they're not pre-screeners
    for an interview gauntlet. Checkout [our hiring documentation](https://fly.io/docs/hiring/)
    to learn more than you probably ever wanted to know.\n\nIf you're interested,
    send a message to [jobs+infra@fly.io](mailto:jobs+infra@fly.io). You can tell
    us a bit about yourself, if you like. Please also include 1. your GitHub username
    (so we can create a private work sample repo for you) 2. your location (so we
    know what timezone you're in for scheduling) and 3. a sentence about your favorite
    food (so we know you're not a bot.)"
- :id: blog-logbook-october-29-to-november-6-2022
  :date: '2022-11-08'
  :category: blog
  :title: 'Logbook: October 29 to November 6, 2022'
  :author: brad
  :thumbnail: logbook-default3-thumbnail.jpg
  :alt: A 1950s style cartoon lady pointing at a vintage flight logbook
  :link: blog/logbook-october-29-to-november-6-2022
  :path: blog/2022-11-08
  :body: "\n\n**This week Redis gets some power-ups, Elixir confesses its love for
    React, Fly gets real about Postgres, Livewire gifts us with a few tutorials, and
    a many of us in North America didn't have to change the clocks around our house
    because we convinced our families last year to make the switch to UTC.**\n\n##
    React inspires Elixir LiveView ❤️ \U0001F917\n\nChris McCord writes about how
    React inspired LiveView reminding us that seemingly very different frameworks
    actually have a lot to learn from each other.\n\nIt's worth taking this moment
    to think about what you could learn from other frameworks that evoke strong emotions—there's
    lots to learn about what they did well, or even some of the mistakes they may
    have made.\n\nRead [A Love Letter to React](https://fly.io/blog/love-letter-react/)\n\n##
    Elixir docs improvements\n\nWell well well, our documentation just keeps getting
    better around here. [Last week](https://fly.io/blog/logbook-october-21-to-28-2022/#postgres-docs-improvements)
    we saw improvements to [Postgres docs](https://fly.io/docs/postgres/). This week
    Elixir is taking a turn.\n\nRead the revamped [Elixir Docs](https://fly.io/docs/elixir/getting-started/)\n\n##
    Postgres: What Fly manages and what you manage\n\nFly tries to make it very clear
    that we provide tools that make it easy to provision and manage a Postgres database
    cluster, but the tool is so good at quickly getting Postgres databases up and
    running that it's easy to forget that its _not_ managed.\n\nThe Postgres docs
    were updated to lay out, in more detail, exactly what Fly manages and what we
    expect customers to manage.\n\nRead about [what Fly manages, and what you manage
    with Postgres](https://fly.io/docs/postgres/)\n\nIf managing a Postgres database
    isn't your thing, we even threw in a few links to some popular managed Postgres
    services.\n\n## Much to learn about Laravel Livewire\n\nThere's lots to learn
    this week about Laravel Livewire. Our first teacher, Kathryn Anne, makes complex
    client-side pagination, grouping, and sorting in a table look easy with [this
    tutorial](https://fly.io/laravel-bytes/hoarding-order/).\n\nRead [Hoarding Order
    with Livewire](https://fly.io/laravel-bytes/hoarding-order/)\n\nThen Chris walks
    us through how to send server-side notifications from a Livewire app to peoples'
    browsers who are currently using your app.\n\nRead [Global Notifications with
    Livewire](https://fly.io/laravel-bytes/global-notifications-with-livewire/)\n\n##
    Fly Redis gets a dashboard\n\nWhen you run `fly redis dashboard <org>` you'll
    be whisked into the Upstash Redis console where you'll see stats on your Redis
    instances and instructions on how to connect it to your application. Don't forget
    to run `fly version update` to get the latest CLI before you run this spiffy new
    command.\n\n## Fly Machines multiple processes preview\n\nIt's pretty slick how
    they work: define your processes and many of them will run inside of one container.
    Shhhh, don't tell the container police that we're running multiple processes inside
    of one container.\n\nRead the [thread about multiple process in Fly Machines](https://community.fly.io/t/multi-process-machines/8375)\n\n---\n\nSee
    you next week!"
- :id: jobs-platform-engineering-proxy
  :date: '2022-11-02'
  :category: jobs
  :title: 'Platform Engineering: Proxy'
  :author: jerome
  :thumbnail:
  :alt:
  :link: jobs/platform-engineering-proxy
  :path: jobs/2022-11-02
  :body: |2+


    Fly.io's platform code, the engine that makes all our stuff work, is written in two different systems languages: Rust and Go. Go code powers our orchestration; it’s what [converts Docker images and provisions VMs](https://fly.io/blog/docker-without-docker/). Rust code drives `fly-proxy`, our Anycast network.

    We’re looking for people who want to work on things like `fly-proxy`.

    `fly-proxy` is an interesting piece of code. A request for some Fly.io app running in Dallas and Sydney arrives at our edge in, say, Toronto. We need to get it to the closest VM (yes, in this case, Dallas). So `fly-proxy` needs a picture of where all the VMs are running, so it can make quick decisions about where to bounce the request. This happens billions of times a day.

    We don’t simply forward those requests, though. `fly-proxy` runs on both our (external-facing) edge hosts and our (internal) workers, where the VMs are. It builds on-the-fly, multiplexed HTTP2 transports (running over our [internal WireGuard mesh](https://fly.io/blog/incoming-6pn-private-networks/)) to move requests over. The “backhaul” configuration running on the worker demultiplexes and routes the request over a local virtual interface to the VM.

    It gets more interesting in every direction you look. For instance: we don’t just route simple HTTP requests; we also do raw TCP (no HTTP2 for that forwarding path). And WebSockets. All these requests get balanced (most of our users run a bunch of instances, not just one). And in the HTTP case, we automatically configure HTTPS, and [get certificates issued with the LetsEncrypt ALPN challenge](https://fly.io/blog/how-cdns-generate-certificates/).

    Zoom in on the raw request routing and there’s still more stuff going on. `fly-proxy` is build on [Tokio](https://fly.io/blog/the-tokio-1-x-upgrade/), [Hyper](https://github.com/hyperium/hyper), and [Tower](https://docs.rs/tower/0.4.10/tower/). A single `fly-proxy` is managing connectivity for lots and lots of Fly.io apps, and isolates the concurrency budget for each of those apps, so a busy app can’t starve the other apps. We’re [tracking metrics for each of those apps](https://fly.io/blog/measuring-fly/) and making them accessible to users.

    `fly-proxy` also has some fun distsys problems. That global picture of where the VMs are is updating all the time. So too are the load stats for all those VMs, which impact how we balance requests. Requests can fail and get retried automatically elsewhere; in fact, that’s the core of [how we do distributed Postgres](https://fly.io/blog/globally-distributed-postgres/).

    ## What We're Up To

    As an engineering team, we've barely scratched surface of what we can do for our users with this code. Now we're getting our pickaxes out:

    - We might allow users to configure custom routing or middleware processors per-app or per-machine.
    - We'll be adding new handlers to the proxy, beyond HTTP, HTTP2, and TCP.
    - We're working on a source-to-edge static asset caching system, so that our Anycast proxy network behaves more like a CDN for the kinds of content that benefit from CDN behavior.
    - We allocate routable IPv4 apps to every app we run, which puts a cost floor on new apps on Fly.io; we'll build a system to do shared IPs soon.
    - We do internal (OpenTelemetry-style) tracing today, but we'll ultimately want to make this end-to-end, linking browsers and customer VMs into request traces.

    And, of course, whatever you come up with, too. Fly.io runs on small, single-pizza teams that largely set their own direction. If that sounds fun to you, we should talk.

    ## More About Us And How We Hire

    There's more about us than you probably want to know [at our hiring documentation](https://fly.io/docs/hiring/).

    Make sure you read the part about our hiring process! We do hiring differently here.

    The salary range for this role is $120k-$200k, plus equity.

    Interested? Have questions? Hit us up at `jobs+platform@fly.io`.

- :id: phoenix-files-github-actions-for-elixir-ci
  :date: '2022-11-02'
  :category: phoenix-files
  :title: Github Actions for Elixir CI
  :author: mark
  :thumbnail: elixir-workflow-thumbnail.jpg
  :alt:
  :link: phoenix-files/github-actions-for-elixir-ci
  :path: phoenix-files/2022-11-02
  :body: |2


    <p class="lead">This is about setting up Github Actions to run CI checks for your Elixir and Phoenix projects on Github. Fly.io is a great place to run those Elixir applications! Check out how to [get started](/docs/elixir/)!</p>

    A critical ingredient for modern development teams is a regularly run set of
    code checks. If it's up to every developer to run code tests and checks locally
    before pushing code, you know it will be forgotten at some point. This leaves it
    as a problem for someone else to cleanup later. Uncool!

    We want the benefits of modern Continuous Integration (CI) workflows for our
    Elixir projects. This lays out a good starting point that teams can customize to
    suit their needs.

    ## What is Continuous Integration (CI)?

    Continuous integration is a software development practice where developers
    frequently merge code changes into a central repository. Automated builds and
    tests are run to assert the new code's correctness before integrating the
    changes into the main development branch.

    The goal with CI is to find and correct bugs faster, improve software quality
    and enable software releases to happen faster.

    CI is a critical ingredient for modern development teams.

    ## Getting Started

    To get started with Github Actions in your project, let's create a "test"
    workflow. To do this, create this path and file in the root of your project:

    `.github/workflows/elixir.yaml`

    Let's look at a sample file. Comments are included to explain and document what
    we're doing and why.

    <aside class="right-sidenote">
    This information has been added to the [Elixir documentation guides](/docs/elixir/advanced-guides/github-actions-elixir-ci-cd/) for easy reference.
    </aside>

    ```yaml
    name: Elixir CI

    # Define workflow that runs when changes are pushed to the
    # `main` branch or pushed to a PR branch that targets the `main`
    # branch. Change the branch name if your project uses a
    # different name for the main branch like "master" or "production".
    on:
      push:
        branches: [ "main" ]  # adapt branch for project
      pull_request:
        branches: [ "main" ]  # adapt branch for project

    # Sets the ENV `MIX_ENV` to `test` for running tests
    env:
      MIX_ENV: test

    permissions:
      contents: read

    jobs:
      test:
        # Set up a Postgres DB service. By default, Phoenix applications
        # use Postgres. This creates a database for running tests.
        # Additional services can be defined here if required.
        services:
          db:
            image: postgres:12
            ports: ['5432:5432']
            env:
              POSTGRES_PASSWORD: postgres
            options: >-
              --health-cmd pg_isready
              --health-interval 10s
              --health-timeout 5s
              --health-retries 5

        runs-on: ubuntu-latest
        name: Test on OTP ${{matrix.otp}} / Elixir ${{matrix.elixir}}
        strategy:
          # Specify the OTP and Elixir versions to use when building
          # and running the workflow steps.
          matrix:
            otp: ['25.0.4']       # Define the OTP version [required]
            elixir: ['1.14.1']    # Define the elixir version [required]
        steps:
        # Step: Setup Elixir + Erlang image as the base.
        - name: Set up Elixir
          uses: erlef/setup-beam@v1
          with:
            otp-version: ${{matrix.otp}}
            elixir-version: ${{matrix.elixir}}

        # Step: Check out the code.
        - name: Checkout code
          uses: actions/checkout@v3

        # Step: Define how to cache deps. Restores existing cache if present.
        - name: Cache deps
          id: cache-deps
          uses: actions/cache@v3
          env:
            cache-name: cache-elixir-deps
          with:
            path: deps
            key: ${{ runner.os }}-mix-${{ env.cache-name }}-${{ hashFiles('**/mix.lock') }}
            restore-keys: |
              ${{ runner.os }}-mix-${{ env.cache-name }}-

        # Step: Define how to cache the `_build` directory. After the first run,
        # this speeds up tests runs a lot. This includes not re-compiling our
        # project's downloaded deps every run.
        - name: Cache compiled build
          id: cache-build
          uses: actions/cache@v3
          env:
            cache-name: cache-compiled-build
          with:
            path: _build
            key: ${{ runner.os }}-mix-${{ env.cache-name }}-${{ hashFiles('**/mix.lock') }}
            restore-keys: |
              ${{ runner.os }}-mix-${{ env.cache-name }}-
              ${{ runner.os }}-mix-

        # Step: Download project dependencies. If unchanged, uses
        # the cached version.
        - name: Install dependencies
          run: mix deps.get

        # Step: Compile the project treating any warnings as errors.
        # Customize this step if a different behavior is desired.
        - name: Compiles without warnings
          run: mix compile --warnings-as-errors

        # Step: Check that the checked in code has already been formatted.
        # This step fails if something was found unformatted.
        # Customize this step as desired.
        - name: Check Formatting
          run: mix format --check-formatted

        # Step: Execute the tests.
        - name: Run tests
          run: mix test
    ```

    ## When the Workflow Runs...

    When code is pushed to the `main` branch, this workflow is run. This happens
    either from directly pushing to the `main` branch or after merging a PR into the
    `main` branch.

    This workflow is also configured to run checks on PR branches that target the
    `main` branch. This is where it's most helpful. We can work on a fix or a new
    feature in a branch and as we work and push our code up, it automatically runs
    the full gamut of checks we want.

    When all steps in the workflow succeed, the workflow "passes" and the automated
    checks say it can be merged. When a step fails, the workflow halts at that point
    and "fails", potentially blocking a merge.

    ## Customizing the Workflow Steps

    This workflow is a starting point for a team. Every project is unique and every
    team values different things. Is this missing something your team wants? Check
    out some additional steps that can be added to your workflow.

    - Add a step to run [Credo](https://github.com/rrrene/credo) checks.
    - Add a step to run [Sobelow](https://github.com/nccgroup/sobelow) for security
      focused static analysis.
    - Add a step to run [dialyxir](https://github.com/jeremyjh/dialyxir). This runs
      [Dialyzer](https://www.erlang.org/doc/man/dialyzer.html) static code analysis
      on the project. Refer to the project for tips on caching the PLT.
    - Customize the `mix test` command to include [code coverage](https://hexdocs.pm/mix/main/Mix.Tasks.Test.html#module-coverage) checks.
    - Add Node setup and caching if `npm` assets are part of the project's test suite.
    - Add a step to run [mix_audit](https://github.com/mirego/mix_audit). This provides
      a `mix deps.audit` task to scan a project's Mix dependencies for known Elixir
      security vulnerabilities
    - Add a step to run [`mix hex.audit`](https://hexdocs.pm/hex/Mix.Tasks.Hex.Audit.html).
      This shows all Hex dependencies that have been marked as retired, which the
      package maintainers no longer recommended using.
    - Add a set to run [`mix deps.unlock --check-unused`](https://hexdocs.pm/mix/Mix.Tasks.Deps.Unlock.html).
      This checks that the `mix.lock` file has no unused dependencies. This is useful
      if you want to reject contributions with extra dependencies.

    ## Benefits of Caching

    It's worth spending time tweaking your caches. Why? A lot of effort has been put
    into speeding up Elixir build times. If we don't cache the build artifacts, then
    we don't reap any of those benefits!

    Of course, faster build times means you spend less money running your CI
    workflow. But that's not the reason to do it! Better caches mean the checks are
    performed faster and that means faster feedback. Faster feedback means that, as
    a team, you save time and can move faster. No waiting 20 minutes for the checks
    to complete so a PR can be merged. (Yes, I have felt that pain!)

    This starting template builds in two caching steps. If `node_modules` factor
    into your project's tests, then caching there makes a lot of sense too.

    Just keep in mind that the reason we cache is to reduce drag on the speed of our
    team.

    If our caches ever cause a problem, and sometimes they can, it's good to know
    how to clear them. In our project, go to Actions > (Sidebar) Management >
    Caches. This is the list of caches saved for the project. We can use our naming
    format to identify which cache file is for what.

    ![Annotated screenshot of Github cache management interface](github-actions-cache-management.png?centered&card&border)

    ## What about Continuous Delivery (CD)?

    With CI working, the next obvious question is, "Can I auto-deploy my application
    on Fly.io?" The answer is an emphatic, "Yes!"

    It's actually pretty straightforward and there's nothing specific to Elixir
    about it. The process is documented well in the [Continuous Deployment with Fly and GitHub Actions](/docs/app-guides/continuous-deployment-with-github-actions/) guide.

    ## Discussion

    In very short order we've got a slick Continuous Integration (CI) workflow
    running for our Elixir project! Similar approaches can be used for projects
    hosted on [Gitlab](https://gitlab.com/) and elsewhere.

    The goal is to keep the code quality of our application high without slowing
    down or overly burdening the development process. With a customized CI workflow
    based on something like this, we get the benefits of Elixir's improved compile
    times while enforcing security checks, coding practices and more.

    Hopefully this improves your code checks while reducing the friction for your
    team at the same time!

    NOTE: This guide can also be found in the growing section of [Elixir documentation](/docs/elixir/advanced-guides/github-actions-elixir-ci-cd/).

    <%= partial "shared/posts/cta", locals: {
      title: "Fly.io ❤️ Elixir",
      text: "Fly.io is a great way to run your Phoenix LiveView app close to your users. It's really easy to get started. You can be running in minutes.",
      link_url: "https://fly.io/docs/elixir/",
      link_text: "Deploy a Phoenix app today!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>"
    } %>
- :id: jobs-platform-engineering
  :date: '2022-11-02'
  :category: jobs
  :title: Platform Engineering
  :author: michael
  :thumbnail:
  :alt:
  :link: jobs/platform-engineering
  :path: jobs/2022-11-02
  :body: |2


    Platform Engineers are building the kernel of Fly.io platform. This job is all about sculpting infrastructure into a product customers want. It&#39;s deeply technical, but developer experience is our forcing function all the way down to the metal.

    Fly.io launched, years ago, on HashiCorp&#39;s Nomad orchestrator. Nomad is an excellent system, and we&#39;d recommend it. But we&#39;ve outgrown it. For the past year, we&#39;ve been building something new to replace it, a new engine for the platform called `flyd`. And with `flyd` comes a whole new way of thinking about how apps deploy on Fly.io.

    This summer, we [announced the Fly Machines API](https://fly.io/blog/fly-machines/). Fly Machines allow you to think about VMs the same way you&#39;d think about forking off a process: just something your app can do, whenever it needs to. Our Machines API gives developers fine-grained control over where and when their code runs on our platform: unlike Fly Apps, which take a set of constraints (&quot;10 instances, 2 CPUs each, split between Sydney and Frankfurt&quot;), Fly Machines allow you to reserve capacity precisely where you need it. The Machines API is built on `flyd`.

    ## What We&#39;re Up To

    The next step is to get the rest of our platforms, including Apps, running on `flyd` as well. Do you have a love-hate relationship with Kubernetes? We&#39;re building an alternative; we&#39;re the anti-k8s, and we&#39;re here to convert you.

    We&#39;ve got lots of fun problems to work on:

    - Distributed systems
    - Low-level systems development
    - Serious performance work and lots of opportunities for more gains
    - Exciting deployments! Just what you want in a deployment system!

    Most of the work we do on this stuff is in Go, but the bridge between our orchestration and customer VMs is a Rust `init` we own as well. Everyone here does both kinds of coding.

    ## More About Us And How We Hire

    There&#39;s more about us than you probably want to know [at our hiring documentation](https://fly.io/docs/hiring/).

    Make sure you read the part about our hiring process! We do hiring differently here.

    The salary range for this role is $120k-$200k, plus equity.

    Interested? Have questions? Hit us up at `jobs+platform@fly.io`.
- :id: laravel-bytes-global-notifications-with-livewire
  :date: '2022-11-02'
  :category: laravel-bytes
  :title: Global Notifications with Livewire
  :author: fideloper
  :thumbnail: livewire-notifications-thumbnail.png
  :alt: A web page showing an application notification as the Livewire squid thingy
    watches
  :link: laravel-bytes/global-notifications-with-livewire
  :path: laravel-bytes/2022-11-02
  :body: "\n\n<p class=\"lead\">We're going to create dynamic app notifications. We'll
    use Livewire, which is like an SPA but without all the JavaScript.\nLivewire works
    best with low latency! Luckily, Fly.io lets you deploy apps close to your users.
    [Try it out](https://fly.io/docs/laravel/), it takes just a minute.</p>\n\nI've
    never met an app that didn't make me wish for a global notification system.\n\nWhat's
    that mean exactly? I always want an easy way to display notifications on any page
    of my application. It's \"global\" because I can show it anywhere.\n\nLivewire
    (and AlpineJS) gives us a way to make dynamic interfaces without writing our own
    javascript. This sounds like an ideal pairing for a notification system!\n\n![the
    finished product](finished-product.gif)\n\nLet's see how that might work.\n\n##
    Basic Setup\n\nI landed on a setup that's pretty simple. To see it in action,
    we'll need to do some basic setup. Let's create a fresh Laravel application!\n\nWe'll
    use Laravel Breeze to get some basic scaffolding up. This gives us Tailwind CSS
    along with a pre-built layout, so we don't need to start from zero.\n\nHere's
    how I created a new project:\n\n```bash\n# New Laravel installation\ncomposer
    create-project laravel/laravel livewire-global-notifications\ncd livewire-global-notifications\n\n#
    Scaffold authentication with Breeze\ncomposer require laravel/breeze --dev\nphp
    artisan breeze:install blade\n\n# Add in Livewire\ncomposer require livewire/livewire\n\n#
    Install frontend utilities\nnpm install\n```\n\nFor the database, I typically
    start with SQLite:\n\n```ini\n# Use sqlite, comment out the database\n# name so
    sqlite's default is used\nDB_CONNECTION=sqlite\n#DB_DATABASE=laravel\n```\n\nThen
    we can run the migrations generated by Breeze. This command will ask us to create
    the sqlite DB file if it does not exist (yes, please!):\n\n```cmd\nphp artisan
    migrate\n```\n\nTo run the application locally, I use the 2 commands (run in separate
    terminal windows):\n\n```bash\n# These are both long running processes\n# Use
    separate tabs, tmux or similar\nphp artisan serve\nnpm run dev\n```\n\n## Setting
    Up Livewire\n\nThe first thing to do when setting up Livewire is adding its styles
    and scripts.\n\nAfter we installed Breeze, we'll have a convenient layout file
    to add those. That file is `resources/views/layouts/app.blade.php`:\n\n```diff\n<!--
    I omitted some stuff for brevity -->\n  <head>\n      <title>{{ config('app.name',
    'Laravel') }}</title>\n\n      <!-- Fonts -->\n      <link rel=\"stylesheet\"
    href=\"...\">\n+     @livewireStyles\n      @vite(['resources/css/app.css', 'resources/js/app.js'])\n
    \ </head>\n  <body class=\"font-sans antialiased\">\n      <div class=\"min-h-screen
    bg-gray-100\">\n          @include('layouts.navigation')\n\n          <!-- Page
    Content -->\n          <main>\n              {{ $slot }}\n          </main>\n
    \     </div>\n+     @livewireScripts\n  </body>\n```\n\n## A Livewire Component\n\nLet's
    create a Livewire component. This component is just going to be a button. The
    button will dispatch an event that tells the global notification to display a
    notification!\n\nRun the following to create a component:\n\n```cmd\nphp artisan
    livewire:make ClickyButton\n```\n\nThis will generate the following files:\n\n*
    `resources/views/livewire/clicky-button.blade.php`\n* `app/Http/Livewire/ClickyButton.php`\n\nLet's
    see the `clicky-button.blade.php` file first, it's simple:\n\n```php\n<div>\n
    \   <button\n        wire:click=\"tellme\"\n        class=\"rounded border px-4
    py-2 bg-indigo-500 text-white\"\n    >\n        Tell me something\n    </button>\n</div>\n```\n\nJust
    a button! Clicking the button calls function `tellme`. We'll have a corresponding
    function `tellme` in our PHP Livewire component `ClickyButton.php`:\n\n```php\n<?php\n\nnamespace
    App\\Http\\Livewire;\n\nuse Livewire\\Component;\n\nclass ClickyButton extends
    Component\n{\n    public function tellme()\n    {\n        $messages = [\n            'A
    blessing in disguise',\n            'Bite the bullet',\n            'Call it a
    day',\n            'Easy does it',\n            'Make a long story short',\n            'Miss
    the boat',\n            'To get bent out of shape',\n            'Birds of a feather
    flock together',\n            \"Don't cry over spilt milk\",\n            'Good
    things come',\n            'Live and learn',\n            'Once in a blue moon',\n
    \           'Spill the beans',\n        ];\n        \n\n        $this->dispatchBrowserEvent(\n
    \           'notify', \n            $messages[array_rand($messages)]\n        );\n
    \   }\n\n    public function render()\n    {\n        return view('livewire.clicky-button');\n
    \   }\n}\n```\n\nClicking the button tells PHP to run `tellme()`. The `tellme()`
    function picks a message at random and returns it. In reality you'd have a set
    message to use in response to a user action, but our example here is a bit contrived.\n\n\n###
    Sending Notifications\n\nLivewire does a lot of magic where JavaScript appears
    to call server-side code, and visa-versa.\n\nOne neat bit of magic: We can [tell
    PHP to fire off a client-side event](https://laravel-livewire.com/docs/2.x/events#browser)!
    Livewire will fire that event, and regular old JavaScript can listen for it.\n\nWe
    did just that in our component:\n\n```php\n$this->dispatchBrowserEvent('notify',
    $message);\n```\n\nThis fires an event `notify`, with data `$message`.\n\n<div
    class=\"callout\">\nSince Livewire is doing some HTTP round trips already, that
    event data piggybacks on the response. Livewire's client-side code sees that the
    response contains and event, and dispatches it.\n</div>\n\n\nWe can then write
    some JavaScript to listen for that event. We won't use this exact thing, but this
    would work:\n\n```javascript\nwindow.addEventListener('notify', event => {\n    alert('The
    message: ' + event.detail);\n})\n```\n\n### Macros\n\nTaking this a bit farther,
    Livewire supports the use of [macros](https://tighten.com/blog/the-magic-of-laravel-macros/).
    It's not directly documented, but you can source-dive to find the [Component classes
    are macroable](https://github.com/livewire/livewire/blob/78b7e23226ab1cd40b577fc9d3c50903adc2b160/src/Component.php#L20).\n\nA
    macro lets us create a function that any Livewire component can call.\n\nLet's
    create one in `app/Providers/AppServiceProvider.php`. Add this to the `boot()`
    method:\n\n```php\nuse Livewire\\Component;\n\nComponent::macro('notify', function
    ($message) {\n    // $this will refer to the component class\n    // not to the
    AppServiceProvider\n    $this->dispatchBrowserEvent('notify', $message);\n});\n```\n\nThen
    we can update our `ClickyButton` component to use that macro instead:\n\n```php\n<?php\n\nnamespace
    App\\Http\\Livewire;\n\nuse Livewire\\Component;\n\nclass ClickyButton extends
    Component\n{\n    public function tellme()\n    {\n        $messages = [\n            //
    snip\n        ];\n        \n\n        // Update this to use the macro:\n        $this->notify($messages[array_rand($messages)]);\n
    \   }\n\n    // snip\n}\n```\n\n### Using ClickyButton\n\nThe next step is adding
    the `ClickyButton` component into our application so we can click on it.\n\nSince
    we used Breeze to scaffold our site, we have a convenient place to put our button.
    We'll add it to the Dashboard that users see after logging in.\n\nEdit file `resources/views/dashboard.blade.php`:\n\n```diff\n<x-app-layout>\n
    \   <x-slot name=\"header\">\n        <h2 class=\"font-semibold ...\">\n            {{
    __('Dashboard') }}\n        </h2>\n    </x-slot>\n\n    <div class=\"py-12\">\n
    \       <div class=\"max-w-7xl mx-auto sm:px-6 lg:px-8\">\n            <div class=\"bg-white
    ...\">\n                <div class=\"p-6 ...\">\n                    You're logged
    in!\n                    <br><br>\n+                   <livewire:clicky-button
    />\n                </div>\n            </div>\n        </div>\n    </div>\n</x-app-layout>\n```\n\nWe
    took the Dashboard view (generated by Breeze), and added in `<livewire:clicky-button
    />`. Now when you log in, we have a button we can click!\n\nClicking it talks
    to our PHP code, which in turn fires an event. Normally clicking a button would
    perform some business logic first, but we're over here in contrived-example land.\n\nThat's
    great so far, but...**nothing is listening to that event**!\n\n![zee clicks, they
    do nothing!](clicks-do-nothing.gif)\n\nLet's set up our application to listen
    to that event and display a notification.\n\n\n## Displaying Notifications\n\nWe
    need to listen for the event sent above, and then display a notification.\n\nTo
    accomplish that, we'll make a component. It's not a Livewire component! Instead,
    it's a [Laravel component](https://laravel.com/docs/9.x/blade#components) - basically
    just another view file.\n\nLet's create file `resources/views/components/notifications.blade.php`,
    and reference it in our application template.\n\nThe application template is the
    very same file we added the `@livewire` directives into - file `resources/views/layouts/app.blade.php`:\n\n```diff\n<!--
    I omitted some stuff for brevity -->\n  <head>\n      <title>{{ config('app.name',
    'Laravel') }}</title>\n\n      <!-- Fonts -->\n      <link rel=\"stylesheet\"
    href=\"...\">\n      @livewireStyles\n      @vite(['resources/css/app.css', 'resources/js/app.js'])\n
    \ </head>\n  <body class=\"font-sans antialiased\">\n      <div class=\"min-h-screen
    bg-gray-100\">\n          @include('layouts.navigation')\n\n          <!-- Page
    Content -->\n          <main>\n+             <x-notifications />\n              {{
    $slot }}\n          </main>\n      </div>\n      @livewireScripts\n  </body>\n```\n\nOur
    notification system is now \"global\" - it's on every page within the application
    (what users see when logged in).\n\nWe can (finally!) create our notification
    component in file `resources/views/components/notifications.blade.php`:\n\n```javascript\n//
    full markup here:\n// https://gist.github.com/fideloper/d6133aea37ce8924543a2b96058f2a86\n\n<div\n
    \   x-data=\"{\n        messages: [],\n        remove(message) {\n            this.messages.splice(this.messages.indexOf(message),
    1)\n        },\n    }\"\n    @notify.window=\"\n      let message = $event.detail;\n
    \     messages.push(message);\n      setTimeout(() => { remove(message) }, 2500)\n
    \   \"\n    class=\"z-50 fixed inset-0 ...\"\n>\n    <template \n      x-for=\"(message,
    messageIndex) in messages\" \n      :key=\"messageIndex\" \n      hidden\n      >\n
    \       <div class=\"...\">\n            <div class=\"...\">\n                <p
    x-text=\"message\" class=\"text-sm ...\"></p>\n            </div>\n            <div
    class=\"...\">\n                <button @click=\"remove(message)\" class=\"...\">\n
    \                   <svg>...</svg>\n                </button>\n            </div>\n
    \       </div>\n    </template>\n</div>\n```\n\nI omitted a bunch of things to
    make it more clear. [Here's the full file](https://gist.github.com/fideloper/d6133aea37ce8924543a2b96058f2a86).\n\nThere's
    a bunch of AlpineJS syntax there! Let's explore that.\n\n### AlpineJS\n\nA few
    things to point out about the above component:\n\n1. Here we're floating the notifications
    on the top right of the browser (we use Tailwind CSS as Laravel Breeze sets it
    up for us. Also Tailwind CSS is the *literal* best)\n2. We're using [AlpineJS](https://alpinejs.dev/)
    to display messages, and hide them after 2.5 seconds\n3. Alpine has [`<template>`'s](https://alpinejs.dev/essentials/templating)
    and [`for` loops](https://alpinejs.dev/directives/for). We use these to dynamically
    show messages\n\nLet's talk about the Alpine specific things!\n\n**First**, `x-data`
    allows us to add properties and methods onto a DOM element. We created an array
    `messages` and a method `remove()`.\n\n**Second**, `@notify.window` is an event
    *listener*. It listens for event `notify` on the top-level object `window`. It
    is the equivalent of this:\n\n```javascript\nwindow.addEventListener('notify',
    event => {...});\n```\n\nThis is the listener for the event we fire server-side
    via:\n\n```php\n$this->dispatchBrowserEvent('notify', $message);\n```\n\nWe react
    to the event by adding a message to the `messages` array, and then set a timeout
    of 2.5 seconds. When that timeout elapses, we remove the message.\n\n**Third**,
    is the `<template>`. As the name suggests, this lets us template some DOM elements
    dynamically.\n\nIn our case, we create a notification `<div>` for each message
    in the `messages` array. It's reactive, so adding/removing messages updates the
    DOM in real-time.\n\n<%= partial \"shared/posts/cta\", locals: {\n  title: \"Fly.io
    ❤️ Laravel\",\n  text: \"Fly.io is a great way to run your Laravel Livewire app
    close to your users. Deploy globally on Fly in minutes!\",\n  link_url: \"https://fly.io/docs/laravel\",\n
    \ link_text: \"Deploy your Laravel app!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>\",\n}
    %>\n\n## Improving our Notifications\n\nOur notifications work, which is great!
    One annoying thing is that they just pop up and then disappear in a way that's
    a bit jarring. Maybe some transitions would be nice?\n\n[Alpine supports transitions](https://alpinejs.dev/directives/transition),
    but if we try them, we'll see they don't work in conjunction with the reactive
    `<template>`. They just pop in and out.\n\nLet's see how to make them work!\n\nFirst
    we'll add in the transitions. This is basically straight from the [Alpine docs](https://alpinejs.dev/directives/transition).
    We use [Tailwind](https://tailwindcss.com/) classes to complete the effect.\n\n```diff\n-
    full markup here:\n- https://gist.github.com/fideloper/e862beae4db2586b8a5244fc8a89255f\n\n<template
    \n  x-for=\"(message, messageIndex) in messages\"\n  :key=\"messageIndex\"\n  hidden\n
    \ >\n  <div\n+   x-transition:enter=\"transform ease-out duration-300 transition\"\n+
    \  x-transition:enter-start=\"translate-y-2 opacity-0 sm:translate-y-0 sm:translate-x-2\"\n+
    \  x-transition:enter-end=\"translate-y-0 opacity-100 sm:translate-x-0\"\n+   x-transition:leave=\"transition
    ease-in duration-100\"\n+   x-transition:leave-start=\"opacity-100\"\n+   x-transition:leave-end=\"opacity-0\"\n
    \   class=\"...\"\n  >\n    <div class=\"...\">\n      <div class=\"...\">\n        <p
    x-text=\"message\" class=\"text-sm ...\"></p>\n      </div>\n      <div class=\"...\">\n
    \       <button @click=\"remove(message)\" class=\"...\">\n          <svg>...</svg>\n
    \       </button>\n      </div>\n    </div>\n  </div>\n</template>\n```\n\nAs
    mentioned, this doesn't (yet) work due to how Alpine is showing/hiding the message
    elements. See the [full markup here](https://gist.github.com/fideloper/e862beae4db2586b8a5244fc8a89255f).\n\n###
    $nextTick\n\nTo get the \"enter\" transitions to work, we need to delay showing
    notifications until they exist as DOM elements. We can do using [`$nextTick`](https://alpinejs.dev/magics/nextTick)!\n\nFrom
    the docs:\n\n> $nextTick is a magic property that allows you to only execute a
    given expression AFTER Alpine has made its reactive DOM updates.\n\nThis is a
    tricky way to get Alpine to delay applying the transition until *after* the message
    `<div>` element has been created.\n\n```diff\n- full markup here:\n- https://gist.github.com/fideloper/542608f327ee30043aeb35f0b04ab60f\n\n<template
    \n  x-for=\"(message, messageIndex) in messages\"\n  :key=\"messageIndex\"\n  hidden\n
    \ >\n  <div\n+   x-data=\"{ show: false }\"\n+   x-init=\"$nextTick(() => { show
    = true })\"\n+   x-show=\"show\"\n    x-transition:enter=\"transform ease-out
    duration-300 transition\"\n    x-transition:enter-start=\"translate-y-2 opacity-0
    sm:translate-y-0 sm:translate-x-2\"\n    x-transition:enter-end=\"translate-y-0
    opacity-100 sm:translate-x-0\"\n    x-transition:leave=\"transition ease-in duration-100\"\n
    \   x-transition:leave-start=\"opacity-100\"\n    x-transition:leave-end=\"opacity-0\"\n
    \   class=\"...\"\n  >\n    <div class=\"...\">\n      <div class=\"...\">\n        <p
    x-text=\"message\" class=\"text-sm ...\"></p>\n      </div>\n      <div class=\"...\">\n
    \       <button @click=\"remove(message)\" class=\"...\">\n          <svg>...</svg>\n
    \       </button>\n      </div>\n    </div>\n  </div>\n</template>\n```\n\nWe
    added a property `show` and only show the element when `show=true`. Then `$nextTick()`
    sets that to `true` after the element exists. This allows the transition to work.
    See the [full markup here](https://gist.github.com/fideloper/542608f327ee30043aeb35f0b04ab60f).\n\nHowever,
    this trick **only works for the \"enter\" transition**! The \"leave\" transition
    is not run, as we completely remove the element once that timeout of 2.5 seconds
    has elapsed. \n\n### Leave Transition\n\nWe need more trickery, and it's kinda-sorta
    the inverse of the trick above.\n\nThe goal is to give each message `<div>` element
    time to transition out by setting `show=false` *before* truly deleting the DOM
    element.\n\nLet's see the solution!\n\n```diff\n- full markup here:\n- https://gist.github.com/fideloper/aee47e08569c23aba2a80157735d7053\n\n<div\n
    \ x-data=\"{\n    messages: [],\n-   remove(message) {\n+   remove(mid) {\n-     this.messages.splice(this.messages.indexOf(message),
    1)\n+     $dispatch('close-me', {id: mid})\n+\n+     let m = this.messages.filter((m)
    => { return m.id == mid })\n+     if (m.length) {\n+       setTimeout(() => {\n+
    \        this.messages.splice(this.messages.indexOf(m[0]), 1)\n+       }, 2000)\n+
    \    }\n    },\n  }\"\n- @notify.window=\"\n-   let message = $event.detail;\n-
    \  messages.push(message); \n-   setTimeout(() => { remove(message) }, 2500)\n-
    \"    \n+ @notify.window=\"\n+   let mid = Date.now();\n+   messages.push({id:
    mid, msg: $event.detail});\n+   setTimeout(() => { remove(mid) }, 2500)\n+ \"\n
    \ class=\"z-50 ...\"\n>\n  <template \n    x-for=\"(message, messageIndex) in
    messages\"\n    :key=\"messageIndex\"\n    hidden\n  >\n    <div\n-     x-data=\"{
    show: false }\"\n+     x-data=\"{ id: message.id, show: false }\"\n      x-init=\"$nextTick(()
    => { show = true })\"\n      x-show=\"show\"\n+     @close-me.window=\"if ($event.detail.id
    == id) {show=false}\"\n      x-transition:enter=\"...\"\n      x-transition:enter-start=\"...\"\n
    \     x-transition:enter-end=\"...\"\n      x-transition:leave=\"...\"\n      x-transition:leave-start=\"...\"\n
    \     x-transition:leave-end=\"...\"\n      class=\"max-w-sm ...\"\n    >\n      <div
    class=\"rounded-lg ...\">\n        <div class=\"p-4\">\n          <div class=\"flex
    items-start\">\n            <div class=\"...\">\n-             <p x-text=\"message\"
    class=\"...\"></p>\n+             <p x-text=\"message.msg\" class=\"...\"></p>\n
    \           </div>\n            <div class=\"...\">\n-             <button @click=\"remove(message)\"
    class=\"...\">\n+             <button @click=\"remove(message.id)\" class=\"...\">\n
    \             </button>\n            </div>\n              </div>\n          </div>\n
    \       </div>\n      </div>\n  </template>\n</div>\n```\n\n**Two big changes
    here!**\n\n1. The `messages` array now holds objects in format `{id: Int, msg:
    String}`\n2. We dispatch and listen for a new event `close-me`, which uses the
    new `id` property to close a specific message\n\nSee the [full markup here](https://gist.github.com/fideloper/aee47e08569c23aba2a80157735d7053).\n\nThe
    main \"thing\" here is the `close-me` event. When our 2.5 second timeout elapses,
    we call `remove()` and pass it a generated message ID. The ID is just a timestamp,
    with milliseconds.\n\nThe `remove()` method dispatches the `close-me` event. Each
    message is listening for that event. If it detects that its ID matches the ID
    in the event, then it sets `show=false`, which kicks off the hide transition.\n\nThe
    `remove()` method also searches for the specific message by the ID. If it finds
    it, it sets another timeout to actually *delete* the message (vs just hiding it)
    after another 2 seconds. This ensures the message has transitioned out before
    sending it on its way to garbage collection.\n\nWe also have some minor changes
    to deal with the fact that each `message` is an object rather than a string. We
    need to reference `message.msg` and `message.id` where appropriate.\n\n## We did
    it!\n\n![the finished product](finished-product.gif)\n\nWe did it! Livewire with
    a sprinkle of AlpineJS is really magical. In fact, you may have noticed we didn't
    explicitly add Alpine. It's just there for us via Livewire.\n\nThe \"hard\" part
    wasn't even setting up a notification system. Instead it was trying to make a
    bunch of fancy transitions! Livewire and Alpine made the hard part easy.\n\n1.
    We were able to easily send client-side events from the server-side\n2. We used
    Alpine to show messages based on a user action\n3. We spent a bunch of superfluous
    time making the transitions look nice\n\nYou can [see the whole project here](https://github.com/fideloper/livewire-global-notifications).\n\nThe
    way Livewire lets us \"Do JavaScript™\" without actually writing much JavaScript
    at all is amazing."
- :id: blog-love-letter-react
  :date: '2022-11-02'
  :category: blog
  :title: A love letter to React
  :author: chris
  :thumbnail: desktop-thumbnail.jpg
  :alt: Overhead view of a keyboard, monitor, mouse, cup of coffee, and small potted
    plant.
  :link: blog/love-letter-react
  :path: blog/2022-11-02
  :body: |2



    <p class="lead">We're Fly.io. React, Phoenix, Rails, whatever you use: we put your code into lightweight microVMs on our own hardware in 26 cities and counting. [Check us out](https://fly.io/docs/speedrun/)&mdash;your app can be running close to your users within minutes.</p>


    It&#39;s hard to overstate the impact React has had since its release in 2013. For me, React came with a few revelations. First was a reactive HTML-aware component model for building UIs. Second was colocated markup _directly in the app code_. Third, it focused on efficiency in a world where SPAs were increasingly heavy-handed.

    It was also something that I could grok in a weekend.

    My previous attempts at drive-by learning other reactive frameworks of the day were not so successful. Phoenix borrowed a lot from React when we first shipped LiveView in 2018, but only recently have we gone all-in with an HTML-aware component system in Phoenix 1.7.

    It&#39;s hard to imagine building applications any other way. What follows is a heartfelt homage to React&#39;s impact on the frontend and backend world, and how Phoenix landed where it did thanks to the revelations React brought almost ten years ago.

    ## Reactive component system

    With LiveView, I was inspired by React components and their beautifully simple programming model. A component is an object that defines a render function, and returns some HTML (or in later versions, a function that renders HTML). That function makes use of component state, and whenever a state change occurs the render function is called again.

    ```elixir
    class Example extends React.Component {
      render(){
        return (
          <div>
            <p>You clicked {this.state.count} times</p>
            <button onClick={() => this.setState({count: this.state.count + 1})}>
              Click me
            </button>
          </div>
        )
      }

      constructor(props){
        super(props)
        this.state = {count: 0}
      }
    }
    ```

    This was such a simple model to understand when coming into React for the first time. Here we have a React component with some state, and a `render` function. The function returns some HTML, and calls `setState` when a button is clicked. Any time `setState` is called, React will call `render` again, and the UI updates. Easy peasy.

    We borrowed from this with LiveView by taking that model and slapping it on the server in a stateful process:

    ```elixir
    defmodule DemoWeb.CounterLive do
      use Phoenix.LiveView

      def render(assigns) do
        ~H"""
        <div>
          <p>You clicked <%%= @count %> times</p>
          <button phx-click="inc">Click me</button>
        </div>
        """
      end

      def mount(_, _, socket), do: {:ok, assign(socket, count: 0)}

      def handle_event("inc"), do: {:noreply, update(socket, :count, &(&1 + 1))}
    end
    ```

    I&#39;ve talked previously about [what this kind of programming model on the server enables](https://fly.io/blog/liveview-its-alive/), like the fact we don&#39;t write bespoke routes, controllers, and serializers, or JSON APIs or GraphQL endpoints. But here we're just appreciating how easy this model is to understand. In a world of ever-increasing framework complexity, React&#39;s take on interactive applications was a breath of fresh air that we were quick to borrow.

    Another choice React made was also extremely contentious at the time: putting HTML right in with your app code. People hated it. But React was right.

    ## JSX: A Colocation Revelation

    Like many folks ten years ago, you might still be thinking &quot;HTML in your app code?! Are we back to the 2000&#39;s PHP days of mixing code and markup together in a file? And we call this progress?&quot;

    These kind of takes were common. They also missed the point. Unlike the PHP days of yore, React applications weren&#39;t a string concatenation of app code, HTML, and business logic masquerading as a web application.

    React&#39;s JSX templates place the most coupled pieces of UI together: the markup and stateful code supporting that markup. Think about it: you have a bunch of variables (state) in your app code that are also needed in your template code for UI rendering or behavior. You also have a bunch of UI interactions in your templates that make it back into app code&mdash;like button clicks. These two things are necessarily tightly coupled. Change either side of the contract and the other side breaks. So React made the wise step to put those tightly coupled things together.

    This brings us to a lesson React taught me that I later carried over to Phoenix: if two pieces of code can only exist together, they should live together. Or to think about it another way, if two pieces of code must _change_ together, they must live together.

    There&#39;s no guesswork on what happens if I change some LiveView state or LiveView template variables because they live in the same file. I also don&#39;t have to search throughout the codebase to find which coupled-but-distant template file needs to be added or changed to accommodate the code I&#39;m writing.

    Now, there are times where it&#39;s not practical to write app code and markup in a single file. Sometimes template reuse or a large document means it makes more sense to have a separate template. In these cases, you want the next best thing: colocated files. In general, the tightly coupled parts of your application should be as close as practically possible. If not the same file, then the same directory, and so on.

    ## HTML-aware components as extensible building blocks

    React also popularized HTML-aware components with their JSX template system. On top of writing HTML in your component&#39;s app code, you _call_ components from markup in an HTML tag-like way.

    This is more than a cute way to make function calls. It&#39;s also not something I appreciated right away. The advantage of this approach is a natural composition of static HTML tags alongside dynamic components and logic. Large HTML structures quickly lose their shape when mixing dynamic code and reusable UI with tags—an issue with Ruby or Elixir-like templating engines.

    For example, imagine you need to render a collection of items, then within that collection, conditionally call some other template code. With Rails or older Phoenix style `<%%= %>` templates, the markup structure almost entirely gets lost in the mess of branches:

    ```erb
    <h1><%%= @title %></h1>
    <table class="border border-gray-100 rounded-lg">
      <thead>
        <%%= for {_field, label} <- @fields do %>
          <th><%%= label %></th>
        <%% end %>
      </thead>
      <tbody>
        <%%= for row <- @rows do %>
          <tr>
            <%%= for {{field, label}, i} <- Enum.with_index(@fields) do %>
              <td>
                <%%= if i == 0 do %>
                  <div class="text-bold">
                    <%%= row[field] %>
                  </div>
                <%% else %>
                  <div class="text-center p-4">
                    <%%= row[field] %>
                  </div>
                <%% end %>
              </td>
            <%% end %>
          </tr>
        <%% end %>
      </tbody>
    </table>
    ```

    This has a few problems. First, the markup structure is completely lost when mixing code branches and comprehensions.

    ```erb
                  </div>
                <%% end %>
              </td>
            <%% end %>
          </tr>
        <%% end %>
      </tbody>
    ```

    This makes template editing a brittle and frustrating experience. If our goal is to dynamically build markup, why does the markup structure get lost in the mix? It gets worse when we try to encapsulate this table into a reusable piece of UI. The best we could do prior to adopting React&#39;s approach is bespoke functions or templates that hide the entire table from the caller:

    ```js
    def table(assigns) do
      ~H"""
      <h1><%%= @title %></h1>
      <table class="border border-gray-100 rounded-lg">
        ...
      </table>
      """
    end
    ```

    Then the caller can render the component:

    ```erb
    <%%= table(title: "Users", rows: @users, fields: [name: "Name", bio: "Bio"]) %>
    ```

    This works, but extensible UI components are all but impossible. The moment we want to customize one aspect of the table, we need to write another template like `user_table` which slightly alters the cells or adds more actionable links to another cell, and so on. If we tried to make it extensible without an HTML-aware component primitive, we&#39;d end up with something like:

    ```erb
    <%%= table(title: "Users", rows: @users, fields: [name: "Name, bio: "Bio"]) do %>
      <%%= for row <- @rows do %>
        <tr>
          <%%= for {field, label} <- @fields do %>
            <%%= user_cell(user: row, field: field, label: label) %>
          <%% end %>
          <td class="actions">
            <a href="..." data-method="post">Confirm User</a>
            <a href="..." data-method="post">Ban User</a>
          </td>
        </tr>
      <%% end %>
    <%% end %>
    ```

    Our bespoke functions now mask the HTML structure, which makes it difficult to figure out what&#39;s happening. We also can&#39;t easily encapsulate table row and cell styling.

    Worse, we prevent the caller from passing their own arbitrary block content to our components.

    For example, imagine instead of a string &quot;Users&quot; as the table title, the caller wanted to render HTML within the `<h1>`, such as a subtitle, icon, or even another component? With template engines that only do string concatenation, passing strings around prohibits all of this. A caller may try passing a string of HTML instead, but it&#39;s a nonstarter:

    ```erb
    <%%= table(title: """
      Listing <em>Users</em>
      #{icon(name: "avatar")}
      """,
      rows: @users, fields: [name: "Name, bio: "Bio"])
    do %>
      <%%= for row <- @rows do %>
        <tr>
          <%%= for {field, label} <- @fields do %>
            <%%= user_cell(user: row, field: field, label: label) %>
          <%% end %>
          <td class="actions">
            <a href="..." data-method="post">Confirm User</a>
            <a href="..." data-method="post">Ban User</a>
          </td>
        </tr>
      <%% end %>
    <%% end %>
    ```

    Passing strings around for arbitrary content quickly breaks down. It&#39;s not only terrible to write, but the user would have to forgo HTML escaping and carefully inject user-input into their dynamic strings. That&#39;s a no-go.

    React&#39;s JSX showed us a better way. If we make our templating engine HTML-aware and component calls become tag-like expressions, we  solve the readability issues. Next, we can allow the caller to provide their own arbitrary markup as arguments.

    React allows passing markup as an inner component block, or as a regular argument (&quot;prop&quot; in React parlance) to the component. For example, in React, one could write:

    ```react
    <Table
      rows={users}
      title={
        <h1>Listing <em>Users</em></h1>
      }
    />
    ```

    Later frameworks like Vue, and the [Web Component spec itself](https://developer.mozilla.org/en-US/docs/Web/Web_Components/Using_templates_and_slots) standardized and expanded this concept with the &quot;slot&quot; terminology.

    In Phoenix, HTML syntax for components along with slots turns our mess of mixed HTML tags and strings into this beautifully extensible UI component:

    ```elixir
    <div>
      <.table rows={@users}>
        <:title>Listing <em>Users</em></:title>
        <:col :let={user} label="Name"><%%= user.name %></:col>
        <:col :let={user} label="Bio"><%%= user.bio %></:col>
      </.table>
    </div>
    ```

    The Phoenix HEEx template engine supports calling components external to the current scope in a similar React style, such as `<Table.simple>...</Table.simple>`. Phoenix also allows calling imported function components directly with the `<.component_name />` notation.

    In the table example above, we call the `table` function with arguments passed in a tag-like attribute syntax, just like in React props. Next, the table accepts an internal block of arbitrary markup, and we here we can make use of slots to pass `title` and `col` information.

    The neat thing about slots in Phoenix is the fact that they are collection based. The caller can provide an arbitrary number of entries, such as in our `<:col>` example. To render a table, internally the `table` component can simply iterate over the `col`s we passed for each `row`, and &quot;yield&quot; back to us the individual user resources. You can see this in action via the `:let={user}` syntax in the col entries.

    The internal table can also iterate over the `col`s to build the `<th>`s for the table head. What results is far more pleasant to write than pure HTML and can be extended by the caller without bespoke functions. The function component and slot primitives allow us to encapsulate everything about building tables in our UI in a single place.

    Like React, you&#39;ll find that your Phoenix applications establish a surprisingly small set of core UI building blocks that you can use throughout your application.


    <%= partial "shared/posts/cta", locals: {
      title: "Phoenix screams on Fly.io.",
      text: "Fly.io was practically born to run Phoenix. With super-clean built-in private networking for clustering and global edge deployment, LiveView apps feel like native apps anywhere in the world.",
      link_url: "https://fly.io/docs/elixir/",
      link_text: "Deploy your Phoenix app in minutes.&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>"
    } %>

    ## Efficient at its core

    My SPA trials and tribulations began before React entered this world. I&#39;ve gone from jQuery spaghetti, Backbone.js, Angular 1, Angular 2, Ember, and finally React. React provided just the right amount of structure, while being quick to pick up and get going with. It was also super fast.

    React really pushed the industry forward with their virtual DOM features. Instead of replacing large parts of the browser&#39;s DOM with a freshly rendered template on any little change, React kept a &quot;virtual&quot; DOM as a datastructure that it was able to compute diffs against. This allowed React to compute the minimal set of concrete DOM operations required to update the browser when state changes occur.

    This was groundbreaking at the time.

    Other SPA frameworks quickly followed suit with their own optimizations. Server-side frameworks are a different paradigm entirely, but they can learn a lot from React&#39;s innovation. Phoenix certainly did.

    For Phoenix, we borrowed these ideas, but we have this pesky layer between the app and the client, known as the network. Our problem set is quite different from React, but if you squint, you can see all the same inspirations and approaches we took in Phoenix LiveView&#39;s optimizations.

    For example, on the server we only want to execute the parts of the template that changed rather than the entire template. Otherwise we&#39;re wasting CPU cycles. Likewise, we only want to send the dynamic parts of the template that changed down the wire instead of the entire thing to limit latency and bandwidth. While we don&#39;t keep a virtual DOM on the server, we do keep track of the static and dynamic parts of the HEEx templates. This allows us to do efficient diff-based rendering on the server and send down minimal diffs to the client. Meanwhile, the client uses [morphdom](https://github.com/patrick-steele-idem/morphdom) to apply only the minimal patches necessary on the client.

    The end result is this: a state change occurs in the LiveView component tree, a diff of changes is computed on the server with noops where possible, and the minimal diff of changes is sent down the wire. On the client, we take those changes and apply them via a minimal set of DOM operations to efficiently update the UI. Sound familiar?


    ## React's influence on the backend

    React changed the front-end game when it was released, and its ideas have trickled up to the backend world. And no, I don&#39;t mean React Server Components (but React is also trickling up to the server too!). Outside of Phoenix, you&#39;ll find other backend frameworks now ship with their own HTML-aware component system, such as Laravel&#39;s Blade templates in the PHP space.

    If you&#39;re a backend framework in 2022 and not shipping an HTML-aware engine,  it&#39;s time to follow React&#39;s lead. I can&#39;t imagine Phoenix not landing where we did, and my only regret is we didn&#39;t follow React sooner. Thank you React for paving the way! ❤️
- :id: jobs-backend-engineer
  :date: '2022-11-01'
  :category: jobs
  :title: Backend Engineer
  :author: nina
  :thumbnail:
  :alt:
  :link: jobs/backend-engineer
  :path: jobs/2022-11-01
  :body: |2


    The fullstack team is the front door to the Fly.io platform. Our goal is to surface the platform to developers in a simple and straightforward way.

    In this role, you'll get up close and personal with our users. You'll develop the APIs that power the web and command line experience, and build the plumbing developers rely on to track their Fly.io usage. These are things our users directly interact with, and we think that's pretty fun.

    We want you to help us solve some big problems:

    * Give developers insight into their infrastructure usage. When a dev runs 10,000 VMs across six continents, they need a special UX to understand exactly what they're paying for.
    * Help developers bring our tools to their big organizations. This means investing in features that make it ok for devs to use us at work. Stuff their bosses need. All without detracting from the developer experience.

    Like most roles at Fly.io, your day-to-day work will touch a lot of other important things, from UX to security and even the low-level platform. You'll also help encode how we think about concepts like organizations, memberships, and billing at Fly.io.

    ## This Role Is a Good Fit For You If:

    * You're comfortable with Rails (our core API is written in Rails) and you're not scared to dive into some big, hairy database projects (we use Postgres, but we may need to expand beyond that and you'll help make those decisions). You're energized by the opportunity to help Fly.io's API scale and grow.
    * Speaking of databases, you know how to model real life problems in Postgres, and you know when Postgres may not be the right fit. And you somehow keep things simple along the way.
    * You care about users. You can empathize with developers and make decisions to improve their lives.
    * You love jumping into something different every day! Sometimes that will be a web service in Rails, Elixir, and Go. Sometimes you'll wind up digging into our CLI or interfacing with our lower-level platform. Sometimes you'll be working through weird infrastructure problems that no one's solved before to make a feature possible.
    * You're comfortable with uncertainty, maybe even thrive in it; you can take big ideas and break them down into small, concrete features and then build those incrementally.
    * You like to move quickly to get your work into people's hands, even if it means pushing your messy drafts and improving them later. We're OK with imperfect solutions that have a really good UX and solve a real customer problem.
    * You view writing code as part of, but not the whole, of your role. You're down to answer questions, help someone get unstuck, and mentor and develop the people around you. You are the younger version of you's favorite engineer.

    ## You'll Know You're Succeeding in This Job If:

    * You feel comfortable making Fly.io better for developers.
    * You're an expert in our APIs, and you are constantly making it easier for other people at Fly to work on them.
    * Our API is more stable and reliable because of your contributions.
    * You've built features with flexibility in mind. We can experiment on the business side of things without needing to undertake big infrastructure projects.
    * You know when to optimize for now versus later. You can make quick progress while keeping your eye on the end-goal.
    * You're actively contributing to the growth of the people around you.

    ## More Details

    This is a mid to senior level fully-remote full-time position. You can live anywhere in the world; your work hours and holidays observed are up to you. The salary ranges from $120k to $200k USD. We offer competitive equity grants with a long exercise window. Hopefully that's enough to keep you intrigued; here's what you should _really_ care about:

    * We're a small team, almost entirely technical.
    * We are active in developer communities, including our own at [community.fly.io](https://community.fly.io).
    * Virtually all customer communication, documentation and blog posts are in writing. We are a global company, but most of our communication is in English. Clear writing in English is essential.
    * We are remote, with team members in Colorado, Quebec, Chicago, London, Mexico, Spain, Virginia, Brazil, and Utah. Most internal communication is written, and often asynchronous. You'll want to be comfortable with not getting an immediate response for everything, but also know when you need to get an immediate response for something.
    * We are an unusually public team; you'd want to be comfortable working in open channels rather than secretively over in a dark corner.
    * We're a real company - hopefully that goes without saying - and this is a real, according-to-Hoyle full-time job with health care for US employees, flexible vacation time (with a minimum), hardware/phone allowances, the standard stuff.

    ## How We Hire People

    We're weird about hiring. We're skeptical of resumes and we don't trust traditional interviews. We respect career experience but we're more excited about potential.

    The premise of our hiring process is that we're going to give you two challenges, a "work sample" and a "work day", that simulate the kind of work you'll actually be doing here. Unlike a lot of places that assign “take-home problems”, our challenges are the backbone of our whole process; they're not pre-screeners for an interview gauntlet. For the "work sample", you'll build an invoices model and migration so we can properly bill users (we'll tell you more). You can do this challenge at your own pace, and it should take about 2 hours. For the "work day" you'll join two of us on Slack to work through a problem together.

    If you're interested, mail ~~jobs+backend&#64;@fly.io~~. You can tell us a bit about yourself if you like. Please also include 1. your GitHub username (so we can create a private work sample repo for you) 2. your location (so we know what timezone you're in for scheduling) and 3. a sentence about your favorite food (so we know you're not a bot.)
- :id: laravel-bytes-hoarding-order
  :date: '2022-10-31'
  :category: laravel-bytes
  :title: Hoarding Order With Livewire
  :author: kathryn
  :thumbnail: image-hoarder.png
  :alt: Photographs of buildings stacked up one after the other, forming a spiraling
    line of photographs fading away into the background.
  :link: laravel-bytes/hoarding-order
  :path: laravel-bytes/2022-10-31
  :body: "\n\n<p class=\"lead\">This post explores the use of Livewire's polling and
    event mechanisms to accumulate and maintain ordering of groupable data across
    table pages. If you want to keep your polling up to speed, [deploy your Laravel
    application](/docs/laravel/) close to your users, across regions, with [Fly.io](/docs/)&mdash;you
    can be up and running in a jiffy!</p> \n\nIn this post we'll craft ourselves a
    hassle-free ordering of groupable data across table pages, and land our users
    a lag-free pagination experience.\n\nIn order to do so, we'll use [Livewire's
    polling feature](https://laravel-livewire.com/docs/2.x/polling) to accumulate
    ordered data, and keep a client-side-paginated table up to date with polling results
    through [Livewire's event mechanism.](https://laravel-livewire.com/docs/2.x/events#browser)\n\nDoes
    that sound exciting, horrifying, or both? \n\nAny of the above's a great reason
    to read on below! \n\n### Following Along\nYou can check out our [full repository
    here](https://github.com/KTanAug21/hoard-table-data-using-livewire), clone it,
    and make some pull requests while you're at it!\nAfterwards, connect to your preferred
    database, and execute a quick `php artisan migrate:fresh --seed` to get set up
    with data.\n\nIf you're opting out of the repository route, you'll need a [Laravel
    project](https://laravel.com/docs/9.x#your-first-laravel-project) that has both
    [Livewire](https://laravel-livewire.com/docs/2.x/quickstart#install-livewire)
    and [Tailwind](https://tailwindcss.com/docs/guides/laravel) configured. \nMake
    sure you have a groupable dataset with you so you can see the magic of our approach
    in maintaining grouped data order across pages. \nRun `php artisan make:livewire
    article-table` to create your Livewire component, and you're free to dive in below.
    \n\n\n\n## Paginating carefully arranged data\n\nWhat's a good approach in displaying
    a meticulously arranged list of data? \n\n\nLet's say we receive and record \"bookmarks\"—article
    links—from news feeds we follow. These bookmarks can be grouped together based
    on some logic manually or automatically imposed. As an example, we may encounter
    exactly the same bookmark from two or more different feeds. If we're allowing
    duplicate bookmarks from different feeds, then it makes sense to group these similar
    bookmarks together, like so:\n\n| ![An illustration of a table containing three
    column headers: Url, Feed Name, and Date. It contains six rows of article links
    received from different news feed sources that got recorded from the dates September
    10, 11, 12, and 13. The third, fourth, and fifth rows have the same article links,
    but were retrieved from different sources. These rows are grouped together due
    their reference to the exact same article link. The illustration refers to the
    third row which came in on September 11 as the Lead of the group, and refers to
    the fourth and fifth rows that both came in on September 13 as its Sub. Other
    rows not belonging to a group are referred to as Normal.](hoard-table.png) |\n|:--:|
    \n| *A table containing records of bookmarks received from various feed sources.
    These bookmarks are groupable together, containing a LEAD row and its Sub rows.*
    |\n\nGrouping table rows that don't sequentially come together is tricky. As seen
    from the image above, rows in a group don't necessarily come in the same day,
    nor do they get saved one after the other. Amidst this absence of natural ordering
    in our database, we'll have to carefully arrange our data every time we  want
    to display them.\n\nFurthermore, we'll need to paginate our data to avoid bottlenecks
    from database retrieval, processing, and client data download when working with
    humongous data sets. \n\n| ![A screen recording of a browser window showing a
    page containing a table, a mouse pointer highlights the ID, LEAD ID, and URL of
    a group of related rows spanning from the current page to the next. The table
    contains four column headers: Url, Source, ID, and Lead ID. There are ten rows
    shown in the current page. The nineth and tenth row have the same url, because
    of this they are grouped together. The screen recording starts with a mouse pointer
    highlighting the nineth row. Next it highlights the nineth row's ID which has
    a value 21. The mouse pointer goes to the tenth row and highlights the row's LEAD
    ID which is also 21, indicating that the tenth row is a Sub row of the nineth
    row. The mouse pointer clicks on the Next button of the table. This interaction
    renders the next set of ten rows. The first row contains the Lead ID 21, indicating
    that this is also a Sub of the previous Lead row. And it is! The mouse pointer
    highlights its Lead ID, 21, then highlights its URL. The mouse pointer navigates
    to and clicks the Prev button. This interaction renders the rows of the previous
    page. The mouse pointer highlights the tenth row's URL, then the nineth row. They
    are exactly the same URL as the first row found in the Next page.](data-across-pages.gif)
    |\n|:--:| \n| A group of bookmark rows spans from the current page( nineth and
    tenth rows ) to the next page( first row ). |\n\n\nPagination while maintaining
    order of data in different pages of a table can become complicated only too quickly.
    We'll have to make sure that what we've shown in previous pages are still there
    when users return, and especially maintain ordering of groups that encompasses
    more than one page.\n\n\n## Client Side Pagination\n\n&mdash;with a side dish
    of data accumulation.\n\n| ![A screen recording of a browser window showing a
    page visibly open to the left and a network calls view visibly open to its right,
    a mouse pointer moves between the two sections of the browser window, guiding
    viewers on the silent accumulation of data through continuous network calls. Inside
    the page is a table with four column headers: Url, Source, ID, and Lead ID. Above
    the table comments are indicated. From left to right these comments are: \"Current
    Rows:51\", \"Max Rows:457\", and \"Loading more data...\". The moving image starts
    with the mouse pointer highlighting the comment \"Loading more data...\". The
    pointer next highlights the comment \"Current Rows:51\", indicating to the viewers
    \"we initially had 51 rows in our page\". Next, A new network call identified
    by the name \"article-table\" suddenly gets listed in the network calls view.
    This listing means that the page made a request to the server to get more data.
    The mouse pointer circles around this name four times before traveling back to
    the \"Current Rows:51\" comment. The network call identified as article-table
    completes after 2.461 seconds. This completion updates the comment \"Current Rows:51\"
    to \"Current Rows:289\". The mouse pointer highlights this change in the comment,
    and travels back to the network calls view. The mouse pointer circles around the
    second \"article-table\" network call that popped up. This network call completes
    in just milliseconds. After it completes, the mouse pointer again highlights the
    change in the comment \"Current Rows:289\" to \"Current Rows:457\". The last comment
    above the table \"Loading more data...\" disappears.](data-accumulation.gif) |\n|:--:|
    \n| This table initially held 51 bookmark rows. As background processes poll for
    more data, the number of rows it contains eventually reach the total record of
    457 rows. |\n\n\n\nWith Livewire, we can easily set up a table component with
    public attributes keeping track of the list of items, accumulating data over time
    with their ordering intact. \n\nAccumulating data means we'll have an evolving
    list of properly ordered data. Therefore, we can simply rely on displaying indices
    specific to a page and not worry about grouping logic during pagination. \n\nAnother
    added benefit of our approach is client-side pagination. Livewire smoothly integrates
    JavaScript listeners with server-side events, allowing our rendered page's JavaScript
    to easily update our client's accumulated data. \n\nAs a result, our users' interaction
    can rely solely on the data accumulated, without any calls to our server for data—all
    thanks to the silent accumulation provided by `Livewire:poll`.\n\n| ![An illustration
    of the flow of events between the server Livewire component and the client Livewire
    component, with an inclusion of functionalities used for user interaction.](hoard-diagram.png)\n|:--:|\n|
    This diagram separates the flow of initialization, data accumulation, and user
    interaction between the Livewire component's controller and view |\n\nBelow, we'll
    set up our Livewire controller's public attributes and functionalities to retrieve
    and accumulate ordered data. Then on the last step, we'll update our Livewire
    view to display and refresh our accumulated data with events and polling.\n\n\n###
    Controller\nLet's make some changes to our `/app/http/Livewire/ArticleTable.php`:\n\n```php\n#
    /App/Http/Livewire/ArticleTable.php\n\nclass ArticleTable extends Component\n{\n\n
    \   // List that accumulates data over time\n    public $dataRows;\n    \n    //
    This is total number of data rows for reference, \n    // Also used as a reference
    to stop polling once reached\n    public $totalRows;\n\n    // Used for querying
    next batch of data to retrieve\n    public $pagination;\n    public $lastNsId;\n\n
    \   // Override this to initialize our table \n    public function mount()\n    {\n
    \       $this->pagination = 10;\n        $this->initializeData();\n    }\n```\n\n**Polling
    Accumulation**\n\nFor smooth sailing for our users, we'll set up a client-side
    paginated table that allows table interaction to be lag-free. \n\nWe'll initially
    query the necessary rows to fit the first page in the client's table&mdash;with
    a pinch of allowance. This should be a fast enough query that takes less than
    a second to retrieve from the database, and the size of the data returned shouldn't
    be big enough to cause a bottleneck in the page loading. \n\nWhat makes this initial
    data retrieval especial is the extra number of rows it provides from what is initially
    displayed. We get double the first page display, so there is an allowance of available
    data for next page intearction.\n\n```php\n/**\n * Initially let's get double
    the first page data \n * to have a smooth next page feel \n * while we wait for
    the first poll result\n */\npublic function initializeData()\n{\n    $noneSubList
    = $this->getBaseQuery()\n    ->limit($this->pagination*2)\n    ->get();\n\n    $this->addListToData(
    $noneSubList );\n}\n\n/**\n * Gets the base query\n */\npublic function getBaseQuery()\n{\n
    \   // Quickly refresh the $totalRows every time we check with the db\n    $this->totalRows
    = Article::count();\n\n    // Return none-Sub rows to avoid duplicates in our
    $dataRows list\n    return Article::whereNull('lead_article_id');\n}\n\n\n```\n\nThen
    in order to get more data into the table, Livewire from the frontend can quietly
    keep adding items to the table through polling one of the controller's public
    functions: `nextPageData`.\n\n```php\n/**\n * For every next page, \n * we'll
    get data after our last reference\n */\npublic function nextPageData()\n{\n  $noneSubList
    = $this->getBaseQuery()\n    ->where('id','>',$this->lastNsId)\n    ->limit($this->pagination*10)\n
    \   ->get();\n  \n  $this->addListToData( $noneSubList );\n}\n```\n\nAdd in our
    core functionality for ordering our data retrieved: get possible sub rows for
    the data result, merge data inclusive of their sub rows in proper ordering to
    our `$dataRows`.\n\n```php\n /**\n * 1. Get possible Sub rows for the list of
    data retrieved in nextPageData or initializeData\n * 2. Merge list of data inclusive
    of their possible Sub rows, in proper ordering, to the accumulated $dataRows \n
    * 3. Update the $lastNsId reference for our nextPage functionality \n */\npublic
    function addListToData($noneSubList)\n{\n    $subList = $this->getSubRows($noneSubList);\n
    \   foreach( $noneSubList as $item ){\n        $this->dataRows[] = $item;\n        $this->lastNsId
    \  = $item->id;\n        foreach( $subList as $subItem){\n            if( $subItem->lead_article_id
    == $item->id ){\n                $this->dataRows[] = $subItem;\n            }\n
    \       }\n    }\n}\n\n\n/**\n * Get the Sub rows for the given none-Sub list\n
    */\nprivate function getSubRows($noneSubList)\n{\n    $idList = [];\n    foreach($noneSubList
    as $item){\n        $idList[] = $item->id;\n    }\n\n    return Article::whereIn('lead_article_id',
    $idList)->get();\n}\n```\n\n\n\n### View\n\nOnce we have our Livewire controller
    set up, let's bring in some color to our Livewire-component view, [`/app/resources/views/livewire/article-table.blade.php`](https://github.com/KTanAug21/hoard-table-data-using-livewire/blob/master/resources/views/livewire/article-table.blade.php):\n\n```php\n<table>\n
    \ <thead>...</thead> \n  {{-- wire:ignore helps to not reload this tbody for every
    update done on our $dataRows --}}\n  <tbody id=\"tbody\" wire:ignore></tbody>\n</table>\n<nav
    role=\"navigation\" aria-label=\"Pagination Navigation\" class=\"flex justify-between\"
    >\n    <button onclick=\"prevPage()\">Prev</button>\n    <button onclick=\"nextPage()\">Next</button>\n</nav>\n```\n\nWhat
    makes our setup pretty cool is that pagination will be done strictly client-side.
    This means user interaction is available only on data the UI has access to&mdash;meaning
    from the JavaScript side of things&mdash;and consequently, a lag-free pagination
    experience for our users!\n\n**Client-Side Pagination**\n\n\nTo display our data,
    we initialize all variables we need to keep track of from the JavaScript side.
    This includes a reference to our table element, some default pagination details,
    and finally a `myData` variable to easily access the data we received from `$dataRows`.\n\nGo
    ahead and add in a `<script>` section to our Livewire-component view in `/app/resources/views/livewire/article-table.blade.php`:\n```javascript\n<script>\n
    \ // Reference to table element\n  var mTable   = document.getElementById(\"myTable\");\n
    \ // Transfer $dataRows to a JavaScript variable for easy use\n  var myData   =
    JSON.parse('<?php echo json_encode($dataRows) ?>');\n  // Default page for our
    users\n  var page     = 1;\n  var startRow = 0;\n  // Let's update our table element
    with data\n  refreshPage();\n```\n\n\nThen set up a quick JavaScript function
    that will display `myData` rows in our tbody based on the current `page`.\n\n```javascript\nfunction
    refreshPage()\n{\n    // Let's clear some air \n    document.getElementById(\"tbody\").innerHTML
    = '';\n    \n    // Determine which index/row to start the page with\n    startRow
    = calculatePageStartRow(page);\n   \n    // Add rows to the tbody\n    for(let
    row=startRow; row<myData.length && row<startRow+10; row++){\n        let item
    = myData[row];\n        var rowTable = mTable.getElementsByTagName('tbody')[0].insertRow(-1);\n
    \       \n        // Coloring scheme to differentiate Sub rows\n        if(item['lead_article_id']!=null){\n
    \           rowTable.className = \"pl-10 bg-gray-200\";\n            var className
    = \"pl-10\";   \n        }else\n            var className = \"\"; \n\n        var
    cell1 = rowTable.insertCell(0);\n        var cell2 = rowTable.insertCell(1);\n
    \       var cell3 = rowTable.insertCell(2);\n        var cell4 = rowTable.insertCell(3);\n\n
    \       cell1.innerHTML = '<div class=\"py-3 '+className+' px-6 flex items-center\">'
    + item['url'] + '</div>';\n        cell2.innerHTML = '<div class=\"py-3 '+className+'
    px-6 flex items-center\">' + item['source'] + '</div>';\n        cell3.innerHTML
    = '<div class=\"py-3 '+className+' px-6 flex items-center\">' + item['id'] + '</div>';\n
    \       cell4.innerHTML = '<div class=\"py-3 '+className+' px-6 flex items-center\">'
    + item['lead_article_id'] + '</div>';\n    }\n}\n\n```\n\nAlong with functionality
    to allow movement from one page to the next and back:\n\n```javascript\nfunction
    nextPage()\n{   \n    if( calculatePageStartRow( page+1 ) < myData.length ){\n
    \       page = page+1;\n        refreshPage();\n    }\n}\n\nfunction prevPage()\n{\n
    \   if( page > 1 ){\n        page = page-1;\n        refreshPage();\n    }\n}\n\nfunction
    calculatePageStartRow( mPage )\n{\n    return (mPage*10)-10;\n}\n```\n\nDiscreetly
    add in our accumulation of remaining data with the help of Livewire's magical
    polling feature that can eventually stop once we've reached maximum rows:\n\n```php\n@if(
    count($dataRows) < $totalRows )\n    <div wire:poll.5s>\n        Loading more
    data... \n        {{ $this->nextPageData() }}\n        {{ $this->dispatchBrowserEvent('data-updated',
    ['newData' => $dataRows]); }}\n    </div>\n@endif\n```\n\nAnd finally, in response
    to the `dispatchBrowserEvent` above, let's create a JavaScript listener to refresh
    our `myData` list and re-render our table rows&mdash;just in case the current
    page still has available slots for rows to show.\n\n```javascript\nwindow.addEventListener('data-updated',
    event => {\n   myData = event.detail.newData;\n   refreshPage();\n});\n  \n```\nAnd
    that's it! We're done, in less than 300 lines of logic! \n\nWe now have our easy-to-maintain,
    ordered listing of groupable data, paginated in a table. We boast a client-side-only
    pagination that promotes a lag-free experience for our users' navigation interaction.
    \n\n<%= partial \"shared/posts/cta\", locals: {\n  title: \"Fly.io ❤️ Laravel\",\n
    \ text: \"Fly.io is a great way to run your Laravel Livewire app close to your
    users. Deploy globally on Fly in minutes!\",\n  link_url: \"https://fly.io/docs/laravel\",\n
    \ link_text: \"Deploy your Laravel app!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>\",\n}
    %>\n\n## Retrospect\n\n\n<div class=\"callout\">\nIf there's one treasure that
    should stay with us from our journey today, it's this: \n\nLearning comes from
    retrospect.\n\n&mdash;Don't be afraid to look up the past: it's where we've all
    been through, it's how we learn from our actions, and it's what drives us forward
    to new, exciting roads&mdash;heading towards our next destination.\n</div>\n\nTake
    a deep breath. Let's recap what we've been through today:\n\n### Problem:\n\nGrouping
    rows and displaying them across pages in a table can get complicated thanks to
    the obligation of maintaining order of groupings across pages.\n\n### Solution:\n\nLivewire
    provides its polling mechanism to call server functions periodically. It also
    gives out this marvelous integration of JavaScript listeners with PHP Livewire
    events.\n\nTo solve the complexity of grouping rows in a paginated table, we learned
    that we can initially have a short list of ordered data&mdash;hopefully with some
    allowance. \nAfterwards we just needed to keep the table's data bustling and updated
    with [`Livewire's polling mechanism`](https://laravel-livewire.com/docs/2.x/polling)
    to keep user-interaction strictly in the client side of things. \n\nFor the last
    puzzle piece of our setup, we used [`Livewire's JavaScript event listener`](https://laravel-livewire.com/docs/2.x/events#browser)
    to update data in our client page's table after every completed poll done from
    the background.\n\nWith data accumulation happening in the background, we were
    able to implement our user-table interaction purely on client-side JavaScript,
    eliminating any server-induced next-page data-request lag.\n\nThe approach we've
    implemented is not perfect by any means. It has benefits and drawbacks:\n\n<b>Benefits:</b>\n\n1.
    Data accumulation reduces complexity while maintaining the order of data *with
    pagination*.\n2. User interaction is sublimely performant because it only deals
    with local data.\n\n<b>Drawbacks:</b>\n\n1. Because polling happens automagically
    in the background until we complete our entire dataset, we risk unnecessary queries
    to the server for the duration our user has the page actively opened.\n2. Livewire
    sends back all public attributes with every call. The closer our dataset gets
    to completion, the larger the data returned by the server.\n\n<b>Experiment!</b>\n\nMaybe
    the polling bit until all data was consumed was a bit overwhelming&mdash;might
    have given some of us a spike to the good-old blood pressure, it really depends
    on the circumstances of our application, and our personal health.\nBut, see&mdash;and
    I hope you see this&mdash;with Livewire, there's just <i>tons</i> of ways that
    our user-table interaction can evolve!\n\n1. Want to remove the polling part but
    keep the client-side pagination? Keep the data accumulation, but replace polling
    with extra allowance from our next page&mdash;show the next 10 rows, but append
    20 rows to our accumulated list!\n2. Want to add search and sorting? You can opt
    with working on the current accumulated data as usual, if that's a bit too naivet,
    how about a refresh of the list?\n\nThe fun part in the two experiments you can
    try out above is you can still keep the pagination client-side. \n\n### Let's
    wrap up our journey here\n\nI hope you learned a thing or two from this blog post.
    If you have any questions, concerns, or <i>constructively kind criticism</i>,
    be sure to let me know! \n\nI'm always up for conversation and just a [tweet](https://twitter.com/KTan360)
    post away. "
- :id: blog-logbook-october-21-to-28-2022
  :date: '2022-10-31'
  :category: blog
  :title: 'Logbook: October 21 to 28, 2022'
  :author: brad
  :thumbnail: logbook-default2-thumbnail.jpg
  :alt:
  :link: blog/logbook-october-21-to-28-2022
  :path: blog/2022-10-31
  :body: |2-


    <p class=lead>It's been a minute since the last [Logbook](https://fly.io/blog/logbook-2022-07-18/). It turn's out tracking every single change monthly gets to be a lot, so we're going to try something a little different—an over-generalized weekly tl;dr of changes at Fly.io that you'll hopefully find helpful for deploying or managing your Fly.io apps.</p>

    ## Postgres docs improvements

    First up, some new documentation was created to run people through how to fail over a Postgres database.

    - [Performing a Failover](https://fly.io/docs/postgres/advanced-guides/high-availability-and-global-replication/#performing-a-failover)
    - [Performing a Regional Failover](https://fly.io/docs/postgres/advanced-guides/high-availability-and-global-replication/#performing-a-regional-failover)

    "Getting Started" was updated to show how to setup a Postgres database and attach it to an application. There's also docs on how to bring over your Postgres database from Heroku if you're moving your apps over from there.

    - [Getting Started](https://fly.io/docs/postgres/getting-started/)
    - [Migrate from Heroku](https://fly.io/docs/postgres/getting-started/migrate-from-heroku/)

    If you have ideas for improving these docs, open the "Edit on Github" link at the bottom of each page to propose changes. Expect more improvements over the next few weeks.

    ## PSA: Fly Postgres is not fully managed Postgres

    Want to know a secret that's not a secret? Fly's Postgres database _is not a fully managed database._ Fly does give you great tools to provision and upgrade Postgres instances, but if they run out of disk space and you don't have monitoring hooked up, your customers will be telling you about it.

    Read the [Anatomy of a Postgres Outage](https://community.fly.io/t/anatomy-of-a-postgres-outage/8167).

    This isn't to single anybody out—outages happen to the best of us. I once `sudo rm -rf /` a production sever before containers made for easy recoveries, which is why I'm on a Frameworks team and don't let myself near the Fly.io production servers. We just want folks to know what they're getting into when they deploy their apps on Fly.io.

    ## Integrating the Elastic Stack (ELK) Into a Laravel App on Fly.io

    Not to be confused with [Cervus canadensis](https://en.wikipedia.org/wiki/Elk), ELK is a way to process streams of data and store them in a way that can be quickly retrieved later. In this example you'll learn how ELK can be used in a Laravel application to track user analytics and generate reports with them in a fancy pants dashboard.

    Read [Integrating the Elastic Stack (ELK) Into a Laravel App on Fly.io](https://fly.io/laravel-bytes/integrating-the-elastic-stack-elk-into-a-laravel-app-on-fly/).

    ## Shut down an idle Phoenix app

    Why pay for something you're not using? Chris McCord shows how a Phoenix app can shut itself down on [Fly Machines](https://fly.io/docs/reference/machines/) if it hasn't received quests for a configurable period of time.

    Read [Shut down an idle Phoenix app](https://fly.io/phoenix-files/shut-down-idle-phoenix-app/).

    ## What's up with Fly Apps v1 and v2?

    You may have heard of Fly Machines, but did you know when you `fly launch` an app today, it doesn't deploy to a Machine?

    Chris F lays out the differences you can expect between the way Fly apps currently behave today, and how they'll behave when they're deployed to Fly Machines.

    Read the [Advantages/Disadvantages of Machine Apps](https://community.fly.io/t/advantages-disadvantages-of-machine-apps/8147) post.

    `fly pg create` does use Fly Machines, which you can read more about at [Fly's Postgres Docs](https://fly.io/docs/postgres/) and the [Postgres on Machines](https://community.fly.io/t/preview-postgres-on-machines-fly-apps-v2/7483/3) post.

    ## Colorblind friendly dashboard improvements

    A little change in the Fly dashboard goes a long way!

    Read the [Colorblind friendly dashboard improvements](https://community.fly.io/t/color-blind-friendly-status-indicators/7768) post.

    ---

    That's it for this week. Happy Halloween, stay safe out there trick-or-treating, and I'll see you next week!
- :id: phoenix-files-shut-down-idle-phoenix-app
  :date: '2022-10-26'
  :category: phoenix-files
  :title: Shutting down a Phoenix app when idle
  :author:
  :thumbnail: shutting-down-thumbnail.jpg
  :alt: A neon sign of a man unplugging a cord from a giant electrical outlet
  :link: phoenix-files/shut-down-idle-phoenix-app
  :path: phoenix-files/2022-10-26
  :body: "\n<p class=\"lead\">This is a quick post about how you can get a Phoenix
    app to shut itself down from within if nobody's using it. On Fly.io, this makes
    the most sense if the app is running on [Fly Machine VMs](https://fly.io/docs/reference/machines/).
    Check them out!</p>\n\nIf you&#39;ve ever wanted to shut down an application when
    no one&#39;s connected to it, say for demand-driven horizontal scaling using [Fly
    Machines](https://fly.io/blog/fly-machines/), then we have the perfect little
    recipe for you. At the time of writing, [\"regular\" Fly apps](https://fly.io/docs/reference/apps/#apps-v1)
    don't run on Machines, so you can't use this to scale.\n\nWhen a Fly Machine is
    in a stopped state, you're not using RAM or CPU, so you pay only for storage.
    It becomes entirely reasonable to boot a Phoenix app *per user* to serve their
    requests. If you're wondering why you might do such a thing, stay tuned ;) \n\nTo
    unlock this feature in an Elixir app, simply add a task to your supervision tree
    that checks periodically for active connections and shuts down the Erlang runtime
    if it finds there are none:\n\n```elixir\n# lib/my_app/application.ex\n\n  def
    start(_type, _args) do \n    children = [\n      ...,\n      AppWeb.Endpoint,\n
    \     {Task, fn -> shutdown_when_inactive(:timer.minutes(10)) end},\n    ]\n    opts
    = [strategy: :one_for_one, name: App.Supervisor]\n    Supervisor.start_link(children,
    opts)\n  end\n\n  defp shutdown_when_inactive(every_ms) do\n    Process.sleep(every_ms)\n
    \   if :ranch.procs(AppWeb.Endpoint.HTTP, :connections) == [] do\n      System.stop(0)\n
    \   else\n      shutdown_when_inactive(every_ms)\n    end\n  end\n```\n\nWithin
    an app's `start/2` function, `Supervisor.start_link(children, opts)` spawns the
    top-level supervisor and all the child processes in the list of `children`.\n\nWe
    can pass a function directly into our supervision tree using the [`Task`](https://hexdocs.pm/elixir/1.13/Task.html)
    module, without having to wrap things in an extra GenServer process. Here, we
    use it to pass an anonymous function that runs our custom shutdown code: \n\n```elixir\n{Task,
    fn -> shutdown_when_inactive(:timer.minutes(10)) end}\n```\n\nWe implement a private
    function, `shutdown_when_inactive/1`, that sleeps for a given interval before
    checking the web server's connection pool library, [Ranch](https://ninenines.eu/docs/en/ranch/2.1/guide/),
    for any active HTTP connections. If there are none, it calls `System.stop(0)`
    to gracefully shut down the VM. Otherwise, it calls itself, starting the next
    sleep interval before it checks again. Lather, rinse, repeat.\n\nWhen a Fly Machine
    VM's main process exits, that VM enters a `stopped` state, but it isn't destroyed.
    The Fly.io platform proxy will try to wake up a `stopped` machine automatically
    in response to incoming connection requests, so with this recipe we can build
    a scale-on-demand Phoenix app!\n\nSince HTTPS for our application on Fly.io is
    terminated at the load balancer, we don&#39;t need to worry about HTTPS connections.
    For those applications that accept both HTTP and HTTPS, you&#39;ll need to check
    the `:ranch.procs(AppWeb.Endpoint.HTTPS, :connections)` for active HTTPS processes
    as well.\n\n<%= partial \"shared/posts/cta\", locals: {\n  title: \"Fly.io ❤️
    Elixir\",\n  text: \"Fly.io is a great way to run your Phoenix LiveView app close
    to your users. It's really easy to get started. You can be running in minutes.\",\n
    \ link_url: \"https://fly.io/docs/elixir/\",\n  link_text: \"Deploy a Phoenix
    app today!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>\"\n} %>\n\n"
- :id: jobs-platform-engineering-private-networking
  :date: '2022-10-25'
  :category: jobs
  :title: 'Platform Engineering: Private Networking'
  :author: thomas
  :thumbnail:
  :alt:
  :link: jobs/platform-engineering-private-networking
  :path: jobs/2022-10-25
  :body: "\n\n**Fly.io runs apps close to their users around the world, by converting
    Docker containers into VMs running on our hardware around the world.**\n\nNetworking
    at Fly.io is pretty funky.\n\nOur hosts are racked hardware that we manage, and
    they&#39;re all linked together in a WireGuard mesh. Once traffic hits our network,
    everything is WireGuard, until we spit the response back out.\n\nEvery app running
    on Fly.io is linked to a customer-specific IPv6 private network (we call them
    `6PNs`). That&#39;s how apps talk to each other. Fly Postgres, for instance, just
    boots up a Postgres VM that only knows how to talk on its attached `6PN`.\n\nCustomers
    can talk directly to their apps&#39; `6PN` services. They do that by bringing
    up a WireGuard tunnel to one of our gateways, where they&#39;re bridged into their
    `6PN`. We use WireGuard for everything, so much so that we built the WireGuard
    client and a full TCP/IP stack into our `flyctl` CLI so that we could bring up
    on-demand tunnels without asking the OS for permission.\n\nUnderneath all of this
    is a bit of eBPF code, which makes the routing and access control simple and easy
    to reason about.\n\nIf this stuff sounds interesting, we&#39;re building a new
    team that will own all of it, and take it in new directions. Here&#39;s some of
    what we want to do:\n\n- On-demand load WireGuard peers when connections for them
    come in, rather than keeping everything installed in the kernel all the time.\n-
    Allow WireGuard peers to &quot;float&quot; across multiple gateways, rather than
    having them locked to particular regions, which is how it works today.\n- Figuring
    out what it means to Anycast WireGuard, and then doing that.\n- Building a state-sharing
    scheme (or something else!) that will allow us to run failover pairs for particular
    gateways. This is tricky! We don&#39;t run a routing protocol for this stuff.\n-
    Integrating our private networks with our new scoped Macaroon tokens.\n- Improving
    the UX for `flyctl`, the only developer CLI in the industry that runs its own
    TCP/IP stack. \n\nThis work is pretty low level. There&#39;s packet parsing involved.
    We&#39;re not afraid to crack open WireGuard packets if we have to. There&#39;s
    some routing involved. Lots of distributed state.\n\nThe codebase you&#39;d be
    dropping into is Go, BPF-flavored C, and Rust, in that order. The C will become
    Rust, and we&#39;re not religious about languages, so new serverside code can
    be Rust as well. You&#39;ll need to be pragmatic and open-minded about languages
    though: you can&#39;t hate Rust or Go and be comfortable in this role. We have
    all-Rust and all-Go roles if that&#39;s what you&#39;re looking for; this won&#39;t
    be one of those.\n\nSome things you should know about us:\n\n- We&#39;re ruthless
    about working on stuff that our users will see and care about, to the exclusion
    of a lot of engineering formalism. &quot;How will this immediately help users?&quot;
    is a standard we hold ourselves to, even when it makes us uncomfortable.\n- We&#39;re
    on call, 24/7. Everyone shares a rotation (a couple days every 6 weeks or so,
    right now). We&#39;ve chosen a cortisol-intensive domain to work in: when our
    stuff breaks, our users notice, and because we&#39;re global, they notice in every
    time zone.\n- We don&#39;t care what the cool kids are using. We&#39;re addicted
    to code that works, right away, with minimal ceremony. We like SQLite, and we
    get nervous when people talk about Raft. The engineering culture here is pragmatic
    to what Hacker News would consider a fault.\n\nThis is a mid to senior level job.
    The salary ranges from $120k to $200k USD. We also offer competitive equity grants.\n\nWe&#39;re
    remote-first, with team members in Colorado, Quebec, Chicago, London, Mexico,
    Spain, Virginia, Brazil, and Utah. Most internal communication is written, and
    often asynchronous. You&#39;ll want to be comfortable with not getting an immediate
    response for everything.\n\n## How We Hire\n\nWe&#39;re weird about hiring. We&#39;re
    skeptical of resumes and we don&#39;t trust interviews (we&#39;re happy to talk,
    though). We respect career experience but we aren&#39;t hypnotized by it, and
    we&#39;re thrilled at the prospect of discovering new talent.\n\nThe premise of
    our hiring process is that we&#39;re going to show you the kind of work we&#39;re
    doing and then see if you enjoy actually doing it; “work-sample challenges”. Unlike
    a lot of places that assign “take-home problems”, our challenges are the backbone
    of our whole process; they&#39;re not pre-screeners for an interview gauntlet.\n\nFor
    this role, we&#39;re asking people to write us a small proxy in Go or Rust that
    does just a couple of interesting things (we&#39;ll tell you more). We&#39;re
    looking for people who are super-comfortable with Go or Rust and network programming
    in general, but we&#39;re happy to bring people up to speed with the domain-specific
    stuff in Fly.io.\n\nIf you're interested, mail jobs+6pn@fly.io. Tell us a bit
    about yourself, if you like, and also tell us your least favorite IPv6 feature,
    just so we know you're not a bot.\n\n  \n"
- :id: laravel-bytes-integrating-the-elastic-stack-elk-into-a-laravel-app-on-fly
  :date: '2022-10-24'
  :category: laravel-bytes
  :title: Integrating the Elastic Stack (ELK) into a Laravel app on Fly.io
  :author: johannes
  :thumbnail: elastic-thumbnail.jpg
  :alt:
  :link: laravel-bytes/integrating-the-elastic-stack-elk-into-a-laravel-app-on-fly
  :path: laravel-bytes/2022-10-24
  :body: |2-


    <div class="lead">"How many new users have we got yesterday?" "Is that more than what we had two days ago?" If you've ever encountered these questions, you're in luck! Today, we'll be taking a look into the Elastic Stack, and we'll see how we can get it running alongside a Laravel application. I'll then show you how to log every new user that registers, so you can keep track of your app's exponential rise to success!</div>

    The Elastic stack is hugely powerful, and a great tool in any developer's arsenal. Since this is an intro I'll keep it simple but please let me know if you're interested in seeing more of this! You can find me on [twitter](https://twitter.com/JohannesWerbrou) or in the Laravel category of our [Fly.io community](https://community.fly.io).

    ## So, what is the Elastic stack?

    Simply put, the Elastic stack (formerly known as the ELK stack) is a group of applications for processing data, handling and indexing it, and presenting that data. It is widely used for logging, searching, analytics, and many more. We will be focusing on the logging part today, which is a common use case for application developers like yourself.

    If you want more info about the Elastic stack, the people that built it explain everything clearer than I ever could right here: [What is the elk stack?](https://www.elastic.co/what-is/elk-stack).

    ## Setting up the applications

    Let's kick things off with setting up our applications. After that, we'll add the Laravel logic to write the logs and then I'll show you how to set up Kibana.

    We will be creating four applications on Fly.io: a Laravel app with a MySQL database app, and then an Elasticsearch and a Kibana app. We will not be using Logstash or Beats, because we can log directly from Laravel to Elasticsearch. Also, this will shorten this article a bit so your brain won't be cooking at the end.

    ### A humble suggestion

    You are free to set up the applications however you'd like, but here's how I set them up: I created a **logging-app** directory and in there, I made a directory for every application, like this:

    - logging-app
        - laravel
        - mysql
        - elasticsearch
        - kibana

    ### Making a new Laravel app

    For this example application, we will be using Breeze and Livewire, along with a mySQL database. You can find all the information you need right here: [https://fly.io/laravel-bytes/full-stack-laravel/](https://fly.io/laravel-bytes/full-stack-laravel/)

    In short, we need to create a new Laravel application, install Breeze, and then connect it to a newly set up mySQL database. Don't forget to run the migrations after setting everything up! Pro tip: The full-stack Laravel article talks about automating this in the **migrate on deploy** section.

    Once we can log in and register users, we can get the logging side of things running!

    ### Setting up Elasticsearch

    As explained in the intro, we will be setting up Elasticsearch and Kibana. To begin, go to the official Elastic dockerfile repository [https://github.com/elastic/dockerfiles](https://github.com/elastic/dockerfiles) and download the Elastic and Kibana repositories. We will need the entire directories, not just the dockerfiles! We will also need to make some changes to get everything running correctly. Worry not, I'll guide you through it!

    Let's take a look at the customized Elastic dockerfile first:

    ```diff
    ################################################################################
    # This Dockerfile was generated from the template at distribution/src/docker/Dockerfile
    #
    # Beginning of multi stage Dockerfile
    ################################################################################

    ################################################################################
    # Build stage 1 `builder`:
    # Extract Elasticsearch artifact
    ################################################################################

    FROM ubuntu:20.04 AS builder

    # Install required packages to extract the Elasticsearch distribution

    RUN for iter in 1 2 3 4 5 6 7 8 9 10; do \
          apt-get update && DEBIAN_FRONTEND=noninteractive apt-get install -y curl && \
          exit_code=0 && break || \
            exit_code=$? && echo "apt-get error: retry $iter in 10s" && sleep 10; \
        done; \
        exit $exit_code

    # `tini` is a tiny but valid init for containers. This is used to cleanly
    # control how ES and any child processes are shut down.
    #
    # The tini GitHub page gives instructions for verifying the binary using
    # gpg, but the keyservers are slow to return the key and this can fail the
    # build. Instead, we check the binary against the published checksum.
    RUN set -eux ; \
        tini_bin="" ; \
        case "$(arch)" in \
            aarch64) tini_bin='tini-arm64' ;; \
            x86_64)  tini_bin='tini-amd64' ;; \
            *) echo >&2 ; echo >&2 "Unsupported architecture $(arch)" ; echo >&2 ; exit 1 ;; \
        esac ; \
        curl --retry 10 -S -L -O https://github.com/krallin/tini/releases/download/v0.19.0/${tini_bin} ; \
        curl --retry 10 -S -L -O https://github.com/krallin/tini/releases/download/v0.19.0/${tini_bin}.sha256sum ; \
        sha256sum -c ${tini_bin}.sha256sum ; \
        rm ${tini_bin}.sha256sum ; \
        mv ${tini_bin} /bin/tini ; \
        chmod 0555 /bin/tini

    RUN mkdir /usr/share/elasticsearch
    WORKDIR /usr/share/elasticsearch

    RUN curl --retry 10 -S -L --output /tmp/elasticsearch.tar.gz https://artifacts-no-kpi.elastic.co/downloads/elasticsearch/elasticsearch-8.4.3-linux-$(arch).tar.gz

    RUN tar -zxf /tmp/elasticsearch.tar.gz --strip-components=1

    # The distribution includes a `config` directory, no need to create it
    COPY config/elasticsearch.yml config/
    COPY config/log4j2.properties config/log4j2.docker.properties

    #  1. Configure the distribution for Docker
    #  2. Create required directory
    #  3. Move the distribution's default logging config aside
    #  4. Move the generated docker logging config so that it is the default
    #  5. Reset permissions on all directories
    #  6. Reset permissions on all files
    #  7. Make CLI tools executable
    #  8. Make some directories writable. `bin` must be writable because
    #     plugins can install their own CLI utilities.
    #  9. Make some files writable
    RUN sed -i -e 's/ES_DISTRIBUTION_TYPE=tar/ES_DISTRIBUTION_TYPE=docker/' bin/elasticsearch-env && \
        mkdir data && \
        mv config/log4j2.properties config/log4j2.file.properties && \
        mv config/log4j2.docker.properties config/log4j2.properties && \
        find . -type d -exec chmod 0555 {} + && \
        find . -type f -exec chmod 0444 {} + && \
        chmod 0555 bin/* jdk/bin/* jdk/lib/jspawnhelper modules/x-pack-ml/platform/linux-*/bin/* && \
        chmod 0775 bin config config/jvm.options.d data logs plugins && \
        find config -type f -exec chmod 0664 {} +

    ################################################################################
    # Build stage 2 (the actual Elasticsearch image):
    #
    # Copy elasticsearch from stage 1
    # Add entrypoint
    ################################################################################

    FROM ubuntu:20.04

    # Change default shell to bash, then install required packages with retries.
    RUN yes no | dpkg-reconfigure dash && \
        for iter in 1 2 3 4 5 6 7 8 9 10; do \
          export DEBIAN_FRONTEND=noninteractive && \
          apt-get update && \
          apt-get upgrade -y && \
    -     apt-get install -y --no-install-recommends \
    -       ca-certificates curl netcat p11-kit unzip zip && \
    +     # CUSTOM install gosu as well
    +     apt-get install -y --no-install-recommends \
    +       ca-certificates curl netcat p11-kit unzip zip gosu && \
          apt-get clean && \
          rm -rf /var/lib/apt/lists/* && \
          exit_code=0 && break || \
            exit_code=$? && echo "apt-get error: retry $iter in 10s" && sleep 10; \
        done; \
        exit $exit_code

    RUN groupadd -g 1000 elasticsearch && \
        adduser --uid 1000 --gid 1000 --home /usr/share/elasticsearch elasticsearch && \
        adduser elasticsearch root && \
        chown -R 0:0 /usr/share/elasticsearch

    ENV ELASTIC_CONTAINER true

    WORKDIR /usr/share/elasticsearch

    COPY --from=builder --chown=0:0 /usr/share/elasticsearch /usr/share/elasticsearch
    COPY --from=builder --chown=0:0 /bin/tini /bin/tini

    ENV PATH /usr/share/elasticsearch/bin:$PATH

    COPY bin/docker-entrypoint.sh /usr/local/bin/docker-entrypoint.sh

    + #CUSTOM copy over my own script
    +COPY bin/setup.sh /usr/local/bin/setup.sh

    # 1. Sync the user and group permissions of /etc/passwd
    # 2. Set correct permissions of the entrypoint
    # 3. Ensure that there are no files with setuid or setgid, in order to mitigate "stackclash" attacks.
    #    We've already run this in previous layers so it ought to be a no-op.
    # 4. Replace OpenJDK's built-in CA certificate keystore with the one from the OS
    #    vendor. The latter is superior in several ways.
    #    REF: https://github.com/elastic/elasticsearch-docker/issues/171
    # 5. Tighten up permissions on the ES home dir (the permissions of the contents are handled earlier)
    # 6. You can't install plugins that include configuration when running as `elasticsearch` and the `config`
    #    dir is owned by `root`, because the installed tries to manipulate the permissions on the plugin's
    #    config directory.
    RUN chmod g=u /etc/passwd && \
        chmod 0555 /usr/local/bin/docker-entrypoint.sh && \
    +   #CUSTOM: set correct permissions for our own setup.sh script
    +   chmod 0555 /usr/local/bin/setup.sh && \
        find / -xdev -perm -4000 -exec chmod ug-s {} + && \
        chmod 0775 /usr/share/elasticsearch && \
        chown elasticsearch bin config config/jvm.options.d data logs plugins

    # Update "cacerts" bundle to use Ubuntu's CA certificates (and make sure it
    # stays up-to-date with changes to Ubuntu's store)
    COPY bin/docker-openjdk /etc/ca-certificates/update.d/docker-openjdk
    RUN /etc/ca-certificates/update.d/docker-openjdk

    EXPOSE 9200 9300

    LABEL org.label-schema.build-date="2022-10-04T10:35:41.162162476Z" \
      org.label-schema.license="Elastic-License-2.0" \
      org.label-schema.name="Elasticsearch" \
      org.label-schema.schema-version="1.0" \
      org.label-schema.url="https://www.elastic.co/products/elasticsearch" \
      org.label-schema.usage="https://www.elastic.co/guide/en/elasticsearch/reference/index.html" \
      org.label-schema.vcs-ref="42f05b9372a9a4a470db3b52817899b99a76ee73" \
      org.label-schema.vcs-url="https://github.com/elastic/elasticsearch" \
      org.label-schema.vendor="Elastic" \
      org.label-schema.version="8.4.3" \
      org.opencontainers.image.created="2022-10-04T10:35:41.162162476Z" \
      org.opencontainers.image.documentation="https://www.elastic.co/guide/en/elasticsearch/reference/index.html" \
      org.opencontainers.image.licenses="Elastic-License-2.0" \
      org.opencontainers.image.revision="42f05b9372a9a4a470db3b52817899b99a76ee73" \
      org.opencontainers.image.source="https://github.com/elastic/elasticsearch" \
      org.opencontainers.image.title="Elasticsearch" \
      org.opencontainers.image.url="https://www.elastic.co/products/elasticsearch" \
      org.opencontainers.image.vendor="Elastic" \
      org.opencontainers.image.version="8.4.3"

    + #CUSTOM entrypoint
    +ENTRYPOINT ["/usr/local/bin/setup.sh"]
    # Dummy overridable parameter parsed by entrypoint
    CMD ["eswrapper"]

    -USER elasticsearch:root
    +#CUSTOM do not run as elasticsearch but as root
    +#USER elasticsearch:root

    ################################################################################
    # End of multi-stage Dockerfile
    ################################################################################
    ```

    The main change over the original dockerfile is that we will be using our own script as the entrypoint, which will then run the *'official'* script later on. This way, we can run our own script as root and run the *'official'* script as the Elasticsearch user. Running as root means we can set up things up properly. Not to worry, I'll explain further once we get there. Let's first take a look at every change over the original file:

    - Line 89-91: In build stage 2 we also install gosu, so we can run commands as a different user.
    - Line 115-116: we copy over the custom `setup.sh` script.
    - Line 131-132: we set the permissions for our custom script, so it can be executed properly.
    - Line 164-165: here we use our custom `setup.sh` as the entrypoint.
    - Line 169-170: here we comment out the USER command, so the entrypoint is not run as the elasticsearch user but as root.



    After that, we need to create the custom `setup.sh` script. All it does is set up some configuration on the actual VM where the application runs, and then fires the original entrypoint script as the elasticsearch user. Make sure you create this script in the `bin/` directory, the dockerfile will be looking to copy it over from there! Here's how it looks:

    ```bash
    #!/bin/bash

    # set some parameters that will be checked when ElasticSearch bootstraps
    ulimit -n 65535
    ulimit -u 4096
    sysctl -w vm.max_map_count=262144

    gosu elasticsearch:root /usr/local/bin/docker-entrypoint.sh
    ```

    What this does is set up some configuration of the VM where it'll be running, and then it uses `gosu` to run the original entrypoint as the `elasticsearch` user. The reason we're doing it this weird way is because Elasticsearch does some bootstrap checks which will fail without this script. By default, Fly.io will have set these parameters a lot tighter and the checks would fail. If for some reason you can't fall asleep, you could read up on those checks here: [Elasticsearch bootstrap checks](https://www.elastic.co/guide/en/elasticsearch/reference/current/bootstrap-checks.html).

    Finally, we also need to make some small changes in the `elasticsearch.yml` configuration file. Here's how it should look:

    ```yaml
    cluster.name: "docker-cluster"
    network.host: 0.0.0.0
    # add these:
    xpack.security.enabled: false
    discovery.type: single-node
    ```
    <div class="callout">Setting `xpack.security.enabled` to false will turn off a lot of security, which will enable us to use http instead of https. This will simplify our setup a lot since we don't have to deal with certificates and whatnot, but it's still a security concern. Since the application will be running on a private network that isn't publicly available, I'm not too worried about it. You can read more about private networking on Fly [here](https://fly.io/docs/reference/private-networking).
    </div>

    That's all our config done, so we are ready for ~~lunch~~ launch! Launch the app using `fly launch` but don't deploy it yet because we've got one last change to make. Gotcha!
    Launching the app will have generated a new fly.toml file for you automagically. Open it up and under the `[[services]]` section change the `internal_port` to **9200**, like this:

    ```toml
    # ...
    [[services]]
      # ...
      internal_port = 9200
      # ...
    ```

    This is the port that will be used to check if the application is actually running. It's seen as healthy if the application replies on requests on this port, otherwise it's seen as unhealthy (you can find more info [here](https://fly.io/docs/reference/configuration/#services-ports). And why exactly do we need this to be 9200? Well, young padawan, because that's the port Elasticsearch uses by default.

    The only thing left to do is to give the app a bit more memory using `fly scale memory 1024`, otherwise it'll crash and burn when it's deployed. Now, you can deploy the app, and get a coffee or some tea brewing while it's setting up. Once you're all caffeinated again and your app is running, we can set up the final piece of the puzzle: the Kibana frontend.

    ### Setting up Kibana

    Now we're cooking! Just one more application to set up, and it's probably  the easiest. All we have to do is open up the Kibana directory (you know, the one I told you to download from github) and locate the `kibana.yml` file. In there, you should find a variable that's named `elasticsearch.hosts`. Right now, it's set to `http://elasticsearch:9200` but that won't work. Change the `elasticsearch` part of the url to the name of your elasticsearch application, and add `.internal` after that:

    ```yaml
    # Default Kibana configuration for docker target
    server.host: "0.0.0.0"
    server.shutdownTimeout: "5s"
    # add your elasticsearch app's name here:
    elasticsearch.hosts: [ "http://*YOUR_ELASTICSEARCH_APP_NAME_HERE*.internal:9200" ]
    monitoring.ui.container.elasticsearch.enabled: true
    ```

    Remember that I said using http is not that big of a deal since our apps are in a private network? Well that's why we add `.internal`. In my case, my Elasticsearch app is called `logging-elasticsearch` so my hosts url is `http://logging-elasticsearch.internal:9200` .

    After that we can launch the app but just like Elasticsearch we can't deploy it yet. In the generated fly.toml file change the `internal_port` parameters to **5601**, this is the default kibana port. This way, Fly will be able to report that the application is running successfully!

    Also, just like with Elasticsearch, scale the memory to 1024MB and deploy!

    We'll quickly check if the app is doing OK by running the `fly open` command, this will open the public url of the app so we can see if everything has started up all right.

    Whew, that was quite a lot. What follows now is the part where we actually code some things instead of setting up applications and wiring up scripts, configs and terminal commands. Let's go!

    ## Logging when a new user registers

    It's been a while, so here's a refresher on what we actually set out to do: We want to log every new user that registers on your app to Elasticsearch so you can keep tabs on how many new users have signed up.

    ### One (1) environment change

    I'm assuming you've already got the database config down, but there's just one extra change we need to do to get our logging running nicely. If you open the `fly.toml` of the Laravel application, you'll see some variables in the `[env]` section. Comment out the `LOG_CHANNEL` and the `LOG_LEVEL`:

    ```diff
        ...
    - LOG_CHANNEL = "stderr"
    - LOG_LEVEL = "info"
    + #remove these from the env so they won't override the default .env file.
    + #LOG_CHANNEL = "stack"
    + #LOG_LEVEL = "info"
        ...
    ```

    Here's why: the environment variables in the fly.toml will override those in the .env file. So, the `LOG_CHANNEL` variable will be set to `stderr` by default, which is just the way that Fly can capture and display the logs of a running application. We'll comment it out and set up the `stderr` channel later. Don't worry, I'll remind you. By commenting these variables out, our app will take them from the .env file instead of fly.toml, which makes more sense in my opinion.

    ### Setting up the logging channel

    Now, let's connect our Laravel application to our ELK logging.

    There is some good news and some bad news here. Let me hit you with the good news first: Laravel provides some logging channels out of the box. The bad news is, there is no default channel that will work with Elasticsearch. Not to worry though, we'll just build our own!

    To do this, first install the elasticsearch php SDK:

    ```bash
    composer require elasticsearch/elasticsearch
    ```

    For us to build a logging channel, we will need some information about how these channels work and how they are set up. Under the hood, Laravel uses the [Monolog](https://github.com/Seldaek/monolog) library for logging, which makes it possible to log to all kinds of endpoints. For that, Monolog uses handlers. Luckily for us, there is an `ElasticsearchHandler` available. You can find the documentation here, under **log to databases**: [documentation](https://github.com/Seldaek/monolog/blob/main/doc/02-handlers-formatters-processors.md).

    We can find all the configured logging channels in `config/logging.php`, all we need to do to add our own logging channel to the `channels` array and we are good to go. We will use `custom` as the driver, this way we can use a factory to create our logging channel. Add the following to the channels array in `config/logging.php`:

    ```php
    'elasticsearch' => [
                'driver' => 'custom',
                'via' => \App\Logging\CreateElasticsearchLogger::class,
                ],
    ```

    This will create a logging channel called elasticsearch. The handler for this channel will be created using the `CreateElasticsearchLogger` class. Let's build that out now:

    Create a new class in `app/Logging` called `CreateElasticsearchLogger`. In it, we only need one method: **__invoke()** . This class takes an array and should return a Monolog instance. Here's how it should look:

    ```php
    namespace App\Logging;

    use Elastic\Elasticsearch\ClientBuilder;
    use Monolog\Handler\ElasticsearchHandler;
    use Monolog\Logger;

    class CreateElasticsearchLogger
    {
        /**
         * Create a custom Monolog instance.
         *
         * @param  array  $config
         * @return \Monolog\Logger
         */
        public function __invoke(array $config)
        {
            $logger = new Logger('elasticsearch');

            //create the client
            $client = ClientBuilder::create()
                        ->setHosts(['http://logging-elasticsearch.internal:9200'])
                        ->build();

            //create the handler
            $options = [
                'index' => 'user_logs',
                'type' => '_doc'
            ];
            $handler = new ElasticsearchHandler($client, $options, Logger::INFO, true);

            $logger->setHandlers(array($handler));

            return $logger;
        }
    }
    ```

    One thing worth pointing out is the **index** in the `$options` array. In Elasticsearch, an index is like a database for mySQL. When we configure our views, we will be able to set an index pattern that specifies what indexes we want it to match. There is a LOT of flexibility here: You could create an index per day, so you can filter out the logging per day. Or perhaps an index per user, so you can see exactly how every user is using your app. Right now we'll keep it simple and set the index to `user_logs` .

    <div class="callout">If you want to use multiple indexes, I'd suggest looking into [on-demand channels](https://laravel.com/docs/9.x/logging#on-demand-channels). They are channels that are built at the moment the app needs it, and would be a good way to use multiple indexes.</div>

    ### Logging a message when a new user subscribes

    You'll remember that we set this whole thing up so you can keep track of how many new users you gained every day, week or month. So, we'll add a log every time a user registers. Events and Listeners are perfectly suited for this and lucky for us, Breeze already has a `Registered` event set up! You can find it here: `Laravel/vendor/laravel/framework/src/Illuminate/Auth/Events/Registered.php`. This event is fired every time a new user registers and is wired up to a Listener that sends out an email to verify the user's email address. We'll just add our own Listener to the same event, easy! Here's how:

    ```cmd
    php artisan make:listener LogRegisteredUser --event=Registered
    ```

    the `--event` flag will set up our listener for use with the `Registered` event. Do make sure that all references of `Registered` point to `Illuminate/Auth/Events/Registered`.

    To see what Listeners are connected to this Registered event, head on over to the `EventServiceProvider`. In the $listen array, the Listeners are connected to their Events. Add our newborn Listener there:

    ```diff
    protected $listen = [
        Registered::class => [
            SendEmailVerificationNotification::class,
            # Add this line
    +       LogRegisteredUser::class,
        ],
    ];
    ```

    All we have to do now is to specify what the LogRegisteredUser listener should actually do. Navigate to LogRegisteredUser and add the following in the preconfigured handle method:

    ```diff
    public function handle(Registered $event)
    {
    +   // add an 'info' log with our new user:
    +   Log::info("New user '{$event->user->name}' registered", $event->user->toArray());
    }
    ```

    The `Log::info` will use the default logging channel that's specified in `logging.php`. If we check there we can see it uses the environment variable `LOG_CHANNEL`, with `stack` as a fallback. Let's set it up in our .env-file: find the `LOG_CHANNEL` variable and set it to `stack`. It makes the most sense to put this kind of config in the .env file, if you ask me.

    One other thing to note: the `Log::...` methods can take one or two parameters with the second one being the context. We pass along our User object, which will be really useful in Kibana. Don't worry, it'll make sense when we get there.

    Finally, go to `logging.php` and find the `stack` channel. This is the channel that consists of multiple channels, so logs can be sent to different locations based on the level of the log. In the `channels` array, add `stderr` and `elasticsearch`. Here's how the stack channel should look like now:

    ```diff
    'stack' => [
        'driver' => 'stack',
    -   'channels' => ['single'],
    +   'channels' => ['single','elasticsearch','stderr'], # also log to elasticsearch and fly logs
        'ignore_exceptions' => false,
    ],
    ```

    If you recall, the `stderr` channel is the one that makes the Fly logs work, and we removed it from the env-section in the fly.toml file so don't forget to add it here!

    Now, redeploy the changes to Fly to update the running application using `fly deploy`. Then, register a new user to trigger our Listener. After that, head on over to Kibana to take a look at our logs! Remember, you can do this by going to your app's URL found in the fly dashboard, or by using the `fly open` command in the directory where Kibana's fly.toml file is located. See you on the flip side!

    ## Configuring Kibana

    This is it. The moment you've been waiting for. The final chapter. This is the moment where we… will display logs in Kibana! Okay, I may have overdone the drama a little bit. Anyhow, let's see how we can show our logs:

    In Kibana, open up the menu on the left, scroll down and the `stack management` tab. Then, select `data views` on the left. You should see Kibana getting snarky and telling us we first need data. Who does that guy think he is? It has a point though, we will need some data and we'll get some right now!

    Open up your laravel app in a new tab and register a new user. This should send out the `Register` event which will in turn fire our `LogRegisteredUser` listener. After that, hit reload in the Kibana tab and if everything is set up correctly, we should see Kibana has calmed down and tells us we hava data in Elasticsearch. All we need to do now is create a data view. Click the very clickable blue button that says `create data view` in the middle of the page. Here we can see how a data view relates to the indexes in Elasticsearch: a pattern matches one or more indexes. Using the right pattern we could get all the logs for a month, for a specific user, or for a specific feature in your app. The possibilities are basically endless, you just set up the indexes and data views the way you want!

    For now, just use a pattern like `user*` . As the timestamp field we can use `context.created_at`. Remember when I was rattling about the context and how we should add it to our logs? This is why: Since we added the User as context, Elasticsearch can see all the fields of the User object and display them in tables, graphs, charts and well, basically everywhere. Neat!

    Once the data view is created, hop on over to `Analytics - discover` in the sidebar on the left. Here we can see all the logs in a specific time range.

    ![](kibana_screenshot.png)

    On the left, we can see all the fields Elasticsearch has found. We logged the complete user object in the context field so we can display everything we want here, even if we make changes to the user model later on!

    There is a whole lot more to explore here, this has just been us dipping our little toe in the deep waters of the Elastic stack. It has been enough for a simple use case, so I'll wrap it up here. Maybe you could set up a dashboard that shows how much new users have registered for each day of the past week? I'll let you figure that one out on your own.

    Thanks a lot for reading, and don't be afraid to let me know if something doesn't work for you, if there's a weird typo or if you'd like me to continue writing about the Elastic stack!

    <%= partial "shared/posts/cta", locals: {
      title: "Fly.io ❤️ Laravel",
      text: "Fly.io is a great way to run your Laravel Livewire app close to your users. Deploy globally on Fly in minutes!",
      link_url: "https://fly.io/docs/laravel",
      link_text: "Deploy your Laravel app!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>",
    } %>
- :id: laravel-bytes-deus-ex-machina
  :date: '2022-10-20'
  :category: laravel-bytes
  :title: Deus Ex Machina
  :author: fideloper
  :thumbnail: deus-ex-machina-thumbnail.png
  :alt:
  :link: laravel-bytes/deus-ex-machina
  :path: laravel-bytes/2022-10-20
  :body: "\n\n<p class=\"lead\">Fly.io empowers you to run apps globally, close to
    your users. If you want to ship a Laravel app, [try it out on Fly.io](/docs/laravel/).
    It takes just a couple of minutes.</p>\n\n\nI've been on the search for a **remote
    development setup** for the better part of 11 years.\n\nOriginally this was due
    to falling in love with an underpowered Macbook Air. Now I'm older and impatient
    - I'm opting out of dependency hell.\n\nRecently, I landed on something that I
    really enjoy. It's fast, and the trade-offs (there are always trade offs!) are
    ones I don't mind.\n\n![fideloper's remote dev setup in a neat little diagram](diagrams-are-neat.png)\n\nThe
    main \"thing\" that I like about this is that I sync local files into the remote
    machine. This way my IDE, git config, code (etc) lives on my machine, but I can
    use a fast, ephemeral VM for compute.\n\nThe reason this all works is because
    Fly Machines (and a guy named Amos) are rad as heck. I wrapped up the Machines
    API and a few other technical details into [Vessel, a CLI tool to make dev environments](https://github.com/Vessel-App/vessel-cli)
    for me.\n\nTo see how this is all put together, read on here! You can also check
    out my [Laracon lightning talk on Vessel](https://www.youtube.com/watch?v=f4QShF42c6E&t=30221s).\n\n\n##
    The State of Things\n\nThe remote dev dream is often paired with the word \"just\".
    Just spin up a VM somewhere! Just automate it! [What could a banana cost? $10?](https://www.youtube.com/watch?v=Nl_Qyk9DSUw&ab_channel=heyitsadriann)\n\nRemote
    dev via some VM in the cloud is death by a million paper cuts. The VM is often
    the source of truth, containing your codebase, git repository, SSH access to git
    repos, etc. They easily grow into a trash fire of sloppily installed and configured
    software.\n\nThere's just...friction when you're not developing locally.\n\n###
    There's hope, though\n\nI actually think we're early in a growing remote development
    \"movement\".\n\nWithin the last few years, more modern solutions have come about!
    \n\nA well known solution right now is [Gitpod](https://www.gitpod.io/). In their
    model, your code is on their machines alongside their compute \"stuff\" (the programming
    language and version you need, etc). \n\nGitpod, however, doesn't gel with me.
    I have to use VSCode in the browser, or setup its SSH agent. If you prefer Jetbrains
    IDE's, you need to install and configure their Gateway.\n\nAdditionally, your
    code (the source of truth™) lives in their servers. You \"git push…\" from within
    their servers. This isn't necessarily an issue (presumably they have permission
    to push to your repository via their oAuth applications, alleviating worry about
    SSH access management), but 3rd party push access to my code is not my favorite.\n\nWhat
    I do like about GitPod is that the environments are ephemeral. This is painful
    at first, but having a forcing function to make you automate the creation of an
    environment to be productive quickly ends up being a huge win.\nIt's harder for
    you to muck up the place with ill-conceived stack-overflow-sourced `apt-get` commands!\n\n##
    What I Want\n\nIdeally my development environment feels local, but the compute
    is remote.\n\nI already have my **stuff** setup and configured locally, just as
    I like it. Git (and git aliases), SSH keys, IDE's, code - it's all local and configured
    just-so for me. I just don't want to handle my projects dependencies - flavors
    and versions of various programming languages, required server dependencies, and
    so on.\n\nIn 2022, there should be technical solutions to make this happen. The
    closest I've come so far is with Fly.io's Machines API.\n\n## What We Need\n\nAfter
    a bit of thought (and nixing some totally dumb ideas), a basic setup that still
    feels satisfying only has 3 requirements:\n\n1. Fast(ish) internet and geographically
    close compute - to keep latency low\n1. Fast file syncing - since I want my source
    of truth to be local\n1. Security - this shouldn't be publicy accessible\n\nSeems
    simple enough! I decided to see if I could find solutions to all of these without
    too much headache.\n\n### Close to Me (and You)\n\nHere's the simplest requirement
    met: Fly.io's tagline is \"serve apps close to your users\". I can throw some
    compute into a LOT of regions!\n\nThis was the first thing that made me want to
    try a remote-dev experiment out. Lots of cloud providers have plenty of regions,
    but Fly.io has a *lot* of them. Launching a VM into any given region is, intentionally,
    very low friction.\n\nGeographically close compute means reduced latency for everything
    else we need to do.\n\n### Fast File Syncing\n\nWe have the compute, but we still
    need file syncing and some network forwarding. [Mutagen](https://github.com/mutagen-io/mutagen)
    handles both of these nicely. It uses SSH to do the exact 2 things we need:\n\n1.
    File synchronization\n1. Network forwarding\n\nThese are Mutagen's 2 main features.\n\nFile
    syncing is pretty obvious - it'll sync files over SSH. I'll get into what that
    looks like in a bit.\n\n### Security via Network Forwarding\n\nI'm using \"security\"
    with heavy quotes around it. Basically I just don't want the dev environment to
    be publicly reachable.\n\nMutagen handles network forwarding over SSH. This provides
    the \"security\"!\n\nNetwork forwarding lets me load `localhost:8000` in my browser
    to reach a web app I'm hacking on. No one else can use their browser to reach
    the dev environment this way.\n\nI'm using a web application as an example, but
    you're not limited to HTTP. You can forward any ports you want! I think the only
    limitation is that it needs to be a TCP connection instead of UDP.\n\n<div class=\"callout\">Machines,
    and all apps in Fly.io, can be accessed over their private network via a Wireguard
    VPN. If you need that level of security, you could have absolutely nothing listening
    for public connections, and connect yourself via VPN.</div>\n\n## The \"Machina\"\n\nSo
    what's the deal with Machines?\n\nMachines are basically version 2 of Fly.io's
    app platform. Eventually everything will be Machines and we can maybe stop calling
    them Machines and \"v2\".\n\nMachines have some *interesting* behaviors.\n\n**First**,
    the mundane (but important): We can [manage Machines via an API](https://fly.io/docs/reference/machines/)
    (there's even a [Terraform provider](https://fly.io/docs/app-guides/terraform-iac-getting-started/)).
    This means we can automate the lifecycle of a development environment, which is
    great although still a bit more work than I want to do.\n\nHere's something more
    interesting: **Machines can be stopped**. Not only that, but they'll stop **automatically**
    if a program exits with status code of zero. I don't need to send any API calls
    to stop a machine. \n\nSince you're not billed for compute time you don't use,
    this is great. \n\n**Another interesting thing**: Machines can start back up in
    just a few milliseconds. They run on Firecracker, just like AWS's Lambda. Stopped
    machines are essentially \"paused\" and startup time is quick.\n\n**On top of
    that**, Fly's proxy layer will **start a machine on network access**. Just make
    a request to the machine! Yet another API call I don't need to make.\n\nSo we
    have some really useful building blocks on which to make a development environment,
    with minimal effort.\n\n\n### Automatically Stopping Machines\n\nAmos, who works
    on the Fly Proxy (the routing layer of Fly.io) wrote an article on [remote development
    on Machines](https://fasterthanli.me/articles/remote-development-with-rust-on-fly-io).
    He's a vim user, which is a bit grim to me for development, but the article goes
    into detail on some really cool stuff.\n\nMost notably, he figured out how to
    determine when a server isn't in use, and shut it down. The TL;DR is that if there's
    no SSH connections, the machine is considered idle and will shutdown (`exit 0`)
    after a timeout.\n\nWe can totally steal that idea and do the same.\n\n\n### What's
    Running\n\nWe need to run stuff in our Machine that lets us track if it's idle
    (listen for SSH connections) and contain our development dependencies.\n\nIf you're
    not familiar with Fly.io, all you need to know is that it takes a Docker image,
    and [converts it to a for-real virtual machine](https://fly.io/blog/docker-without-docker/).\n\nTo
    get setup with a functional Docker image, there's a few things to consider for
    this use case.\n\nThe first thing is that Fly.io uses port 22 (on the private
    network interface) by the stuff that makes `fly ssh console` work. To avoid conflicts,
    I had an SSH daemon listen port `2222`. \n\nYou can see the base Docker image
    I used, and it's SSH configuration, [here](https://github.com/Vessel-App/vessel-run/blob/0f24823a0117305b360433b6137c4d181a1e7fa8/Dockerfile#L84-L90).
    The `ENTRYPOINT` script takes care of finding a public key and adding it to `authorized_keys`
    ([here](https://github.com/Vessel-App/vessel-run/blob/0f24823a0117305b360433b6137c4d181a1e7fa8/docker/entrypoint#L5)).\n\nThe
    Docker configuration linked above is used as a base image. For my own PHP projects,
    I made a [PHP-specific Docker image on top of that base image](https://github.com/Vessel-App/vessel-run-php).\n\n\n###
    Counting Idle Time\n\nLike mentioned, Machines will stop when your application
    exits with a success status code `0` . An error code will result in a restart.\n\nAdditionally,
    machines work just like Fly.io apps - you feed it a Docker image, and Fly.io converts
    it to a real VM.\n\nWe (ab)use this by having our Docker image run the (\"borrowed\")
    Rust program that exits when it detects the machine is idle.\n\nAmos' article
    covers [how to do this ad nauseam](https://fasterthanli.me/articles/remote-development-with-rust-on-fly-io),
    and I 100% stole his idea like it was a hot Stack Overflow answer. Here is my
    almost-exact-copy [of it in the base Docker image](https://github.com/Vessel-App/vessel-run/blob/main/src/main.rs).\n\nThe
    main trick here is that the Docker image is configured to run that Rust program
    as its \"main\" process (the `ENTRYPOINT` or `CMD`).\n\nSince that program is
    the main process running, it's a convenient place to take care of a few other
    things. One task is starting the SSH daemon. The other is running...whatever else
    we need.\n\nI chose to use Supervisord to help run anything else. The Rust program
    starts SSH, and then Supervisord. Supervisord will start whatever it's configured
    to monitor.\n\nNow, while Supervisord is installed in the base image, it actually
    has no configuration. However, for my PHP image, the Dockerfile uses the base
    image and adds Supervisord configuration to run Nginx and PHP-FPM!\n\n<%= partial
    \"shared/posts/cta\", locals: {\n  title: \"Run your apps on Fly\",\n  text: \"Don't
    stop at remote development. Deploy Laravel globally on Fly in minutes!\",\n  link_url:
    \"https://fly.io/docs/laravel\",\n  link_text: \"Deploy your Laravel app!&nbsp;&nbsp;<span
    class='opacity:50'>&rarr;</span>\",\n} %>\n\n## Managing Machines\n\nMachines
    conveniently let us skip some lifecycle management, but we can't avoid writing
    at least some code against Fly.io's APIs.\n\nLet's see what it looks like to create
    a Machine. The basic steps are:\n\n1. Register an App\n1. Run a Machine within
    the app\n\nOf course, there are details!\n\n```bash\n# For the time being, we
    need to proxy into our \n# Fly.io org private network to make API calls.\nfly
    machine api-proxy\n\n# Create an app named \"some-app\"\ncurl -i -X POST \\\n
    \   -H \"Authorization: Bearer ${FLY_API_TOKEN}\" \\\n    -H \"Content-Type: application/json\"
    \\\n    \"http://${FLY_API_HOSTNAME}/v1/apps\" \\\n    -d '{\"app_name\": \"some-app\",
    \"org_slug\": \"personal\"}'\n\n# Run a machine in that app\ncurl -i -X POST \\\n
    \   -H \"Authorization: Bearer ${FLY_API_TOKEN}\" \\\n    -H \"Content-Type: application/json\"
    \\\n    \"http://${FLY_API_HOSTNAME}/v1/apps/some-app/machines\" \\\n    -d '{bunch
    of json}'\n```\n\n\nYour `FLY_API_HOSTNAME` value will depend on if you're using
    the api-proxy, VPNed into your private network, or if you're reading this article
    way later and the API is public.\n\n\nThere's no surprises so far! I did cheat
    a bit with the `{bunch of json}` thing though. Here's what that JSON looks like:\n\n```javascript\n{\n
    \ \"name\": \"vessel-php\",\n  \"region\": \"dfw\",\n  \"config\": {\n    \"image\":
    \"vesselphp/php:8.1\",\n    \"env\": {\n      \"VESSEL_PUBLIC_KEY\": \"public-key-string\"\n
    \   },\n    \"services\": [\n      {\n        \"internal_port\": 2222,\n        \"protocol\":
    \"tcp\",\n        \"ports\": [\n          {\n            \"port\": 22\n          }\n
    \       ]\n      }\n    ]\n  }\n}\n\n```\n\nThere's a few things worth pointing
    out here.\n\nFirst, the `image` used is a public Docker image, available on Docker
    Hub. Fly.io doesn't currently support privately hosted Docker images, so the 2
    places they can live are:\n\n1. Somewhere public (usually, but not exclusively,
    Docker Hub)\n1. Fly.io's own registry\n\nThe Fly.io registry is used behind the
    scenes when deploying via `fly deploy`. But you can actually push to that registry
    yourself after creating an app. [This article](https://til.simonwillison.net/fly/fly-docker-registry)
    has a great write-up of what that looks like.\n\nThe next interesting thing is
    the environment variable `VESSEL_PUBLIC_KEY`. I configured the Docker container's
    `ENTRYPOINT` script to add the public key to the relevant `authorized_keys` file,
    allowing SSH access.\n\nFinally, the `services` block is telling Fly to expose
    port 22 publicly, and route it to internal port `2222`, where the SSH daemon is
    listening for connections.\n\n## Reaching the Machine\n\nThere's one more wrinkle
    there. The Machine is created within an App, but there's no IP address assigned
    to it. So, while we're telling Fly to expose port `22` for connections, we don't
    have a way to reach the Machine.\n\nAdding an IP address to an app can take one
    of 2 forms:\n\n```bash\n# A. Use the `fly` command\nfly ips allocate-v6 -a some-app\n\n#
    B. Make a call to the semi-hidden GraphQL API\n#    See https://api.fly.io/graphql\ncurl
    -X POST \\\n    https://api.fly.io/graphql \\\n    -H \"Authorization: Bearer
    ${FLY_API_TOKEN}\" \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n
    \   \"query\": \"mutation($input: AllocateIPAddressInput!) { allocateIpAddress(input:
    $input) { ipAddress { id address type region createdAt } } }\",\n    \"variables\":
    { \"input\": { \"appId\": \"some-app\", \"type\": \"v6\" } }\n}'\n```\n\nMy ISP
    supports using IPv6, but not everyone is so lucky. You can assign IPv4 addresses
    if needed.\n\n### A Region Near You\n\nWhen I created the Machine, I defined what
    region to create it in. You can use the GraphQL API to get your closest region.
    What's neat is that requests to Fly.io's infrastructure goes to the nearest \"edge\"
    location. From that, the region closest to is calculated. So, we just need to
    make an API call:\n\n```bash\ncurl -X POST \\\n    https://api.fly.io/graphql
    \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\"query\": \"query
    { nearestRegion { code name gatewayAvailable } }\"}'\n```\n\nAfter that, we should
    be able to connect to the machine over SSH!\n\nThere's a few tidbits to know about
    networking with a Machine:\n\n1. You can run multiple Machines within an App\n1.
    IP addresses are assigned at the App-level, NOT to a specific Machine\n1. The
    Fly Proxy will load balance requests across Machines running within an App. It
    takes the port into account, so multiple machines listening on Port 22 means you
    can't be sure which will be connected to. To ensure you reach a specific machine,
    you'd need to connect via VPN and use hostname `<machine-id>.vm.<app-name>.internal`.\n\n\n##
    Syncing and Forwarding\n\nThe last-ish piece of the puzzle is integrating Mutagen
    into the workflow.\n\nMy first stab at this was to pull Mutagen into my own Golang
    project and programmatically manage it.\n\nThe structure of Mutagen's code base
    made it awkward to manage its background agent, so I copied what other OS projects
    do and shell out to Mutagen.\n\nThere are 2 commands used to start a development
    session - one to start file syncing, and one to do network forwarding.\n\nThey
    look a bit like this:\n\n```bash\n# Sync current directory \".\" to /var/www/html
    in the server\n# An entry in ~/.ssh/config named \"ssh-alias\" helps us with SSH
    details\nmutagen sync create \\\n    --ignore-vcs \\\n    -i node_modules \\\n
    \   --name \"my-session\" \\\n    --sync-mode two-way-resolved \\\n    . ssh-alias:/var/www/html\n\n#
    Forward localhost:8000 to localhost:80 in the Machine\nmutagen forward create
    \\\n    --name \"my-session\" \\\n    tcp:127.0.0.1:8000 ssh-alias:127.0.0.1:80\n```\n\nCombined,
    you get file syncing, and the ability to open your browser to `localhost:8000`
    and see your app being served (assuming an HTTP server is listening on port `80`
    in the Machine).\n\nSyncing is fast too! This is due to both decent quality internet
    and the proximity to a region. From San Antonio (where I am) to Dallas (the nearest
    Fly.io region), I was getting application responses back in just ~30-40ms.\n\n\n\n##
    Vessel\n\nI Made a Thing™ to wrap up all this functionality so it's just a few
    commands to create and run a remote development environment.\n\nIt will grab your
    Fly API token, and create an App/Machine for you, and then start syncing/forwarding
    for you as needed.\n\nI did a [quick lightning talk](https://www.youtube.com/watch?v=f4QShF42c6E&t=30221s)
    at Laracon Online showing this off if you want to check it out.\n\nIt looks a
    bit like this:\n\n```bash\n# Install Vessel\ncurl https://vessel.fly.dev/install.sh
    | sh\n\n# Add it to your $PATH as instructed, or use ~/.vessel/bin/vessel\n\n#
    Grab or provide your Fly API token\nvessel auth\n\n# Head to your project and
    init\ncd ~/code/my-project\nvessel init\n# Run through the init steps...it will
    create a dev environment\n\n# Start a development session. This is a long-lived
    session.\n# ctrl+c will clean up and stop the dev session\nvessel start\n```\n\nThe
    project has a few repos:\n\n- [`vessel-cli`](https://github.com/Vessel-App/vessel-cli)
    is the main thing\n- [`vessel-run`](https://github.com/Vessel-App/vessel-run)
    is the base Docker image with SSH and the Rust program\n- [`vessel-run-php`](https://github.com/Vessel-App/vessel-run-php)
    is the Laravel-ready Docker images for PHP 7.4/8.0/8.1\n- [`vessel-installer`](https://github.com/Vessel-App/vessel-installer)
    is a little web app that provides the installer shell script\n\nYou can use the
    `vessel-run` base image ([`vesselapp/base:latest`](https://hub.docker.com/r/vesselapp/base))
    and add whatever you want to your own Docker images. If you push an image of your
    own up to Docker Hub, you can use it in your own Vessel projects by defining it
    as part of the `vessel init` command (it has prompts).\n\nI've been really enjoying
    it. There's still some sharp edges, but give it a spin!\n"
- :id: phoenix-files-migrating-to-lv-0-18
  :date: '2022-10-20'
  :category: phoenix-files
  :title: Migrating to LiveView v0.18
  :author:
  :thumbnail: voyage-thumbnail.jpg
  :alt:
  :link: phoenix-files/migrating-to-lv-0-18
  :path: phoenix-files/2022-10-20
  :body: |2

    <p class="lead">This post talks about errors that may appear when migrating our projects to LiveView v0.18 and how to deal with them. If you want to deploy your Phoenix LiveView app right now, then check out how to [get started](/docs/elixir/). You could be up and running in minutes.</p>

    [LiveView 0.18](https://hexdocs.pm/phoenix_live_view/changelog.html#0-18-0-2022-09-20) is here,  bringing new and exciting features to try! In this post we&#39;ll talk about errors that may appear when migrating your project from LiveView v0.17 to v0.18, and how to deal with them.

    ## Migrated Functions

    Some of the errors that you will probably find are of the type:

    ```elixir
    ** (CompileError) ... undefined function assign/3
    ```



    ```elixir
    ** (CompileError) ... undefined function sigil_H/2
    ```

    The above messages refer to functions that have been moved into the [Phoenix.Component](https://hexdocs.pm/phoenix_live_view/Phoenix.Component.html) module. This includes some functions previously found in the `Phoenix.LiveView` module (including `assign/3` and `assign_new/3`), and all non-deprecated functions (like `sigil_H/2`) from the `Phoenix.LiveView.Helpers` module, which has been removed.

    ### Solution

    The solution in both cases is simple. Import the `Phoenix.Component` module and remove the `Phoenix.LiveView.Helpers` import. You may be able to remove the `Phoenix.LiveView` import as well.

    ```elixir
     #add
    import Phoenix.Component

    #remove
    import Phoenix.LiveView
    import Phoenix.LiveView.Helpers

    ```

    ## Deprecated functions

    You may get other errors related to undefined functions:

    ```elixir
    ** (CompileError) ... undefined function live_patch/2
    ```



    ```elixir
    ** (CompileError) ... undefined function live_redirect/2
    ```

    These errors appear since the `live_patch/2` and `live_redirect/2` functions have been deprecated in favor of the new `:link` function component.

    ### Solution

    Change the `live_patch/2`  calls and adjust them to use the new function component, for example:

    ```elixir
    #from
    <%%= live_patch [to: @patch, class: "..."] do %>
      Link Text
    <%% end %>

    #to
    <.link patch={@patch} class="...">
      Link Text
    </.link>
    ```

    Change `live_redirect/2` calls too:

    ```elixir
    #from
    <%%= live_redirect(to: @target, class: "...") do %>
      Link Text
    <%% end %>

    #to
    <.link navigate={@target} class="...">
      Link Text
    </.link>
    ```

    ## Closing

    With these few changes, we&#39;ve cleaned up our warnings and now we&#39;re enjoying the new LiveView v0.18 goodness! We&#39;ll have more to talk about as we take advantage of some of the new LiveView features.

    <%= partial "shared/posts/cta", locals: {
      title: "Fly.io ❤️ Elixir",
      text: "Fly.io is a great way to run your Phoenix LiveView app close to your users. It's really easy to get started. You can be running in minutes.",
      link_url: "https://fly.io/docs/elixir/",
      link_text: "Deploy a Phoenix app today!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>"
    } %>
- :id: jobs-engineering-manager
  :date: '2022-10-18'
  :category: jobs
  :title: Engineering Manager
  :author: michael
  :thumbnail:
  :alt:
  :link: jobs/engineering-manager
  :path: jobs/2022-10-18
  :body: |2


    We're hiring engineering managers to help Fly.io engineers do their best work.

    We're fully remote with people contributing from all over the world. We work asynchronously as much as possible to be inclusive of people across timezones. We're also a team of developers who are building a product for developers. Everyone here uses the product and talks to users. Engineers are grouped into small teams that are each responsible for a portion of the product. They work across the entire stack — from the UX to the metal.

    Roadmap decisions are heavily influenced (and sometimes decided autonomously) by the teams doing the actual work. Company priorities "come from the top", and teams have broad discretion to build what users need within the larger framework. The most successful Fly.io engineers understand company goals, empathize with users, and optimize for iteration speed.

    We believe we're a stronger company when we distribute decision making power to people doing the work. Engineers do better work when we give them decision making support, and they develop skills faster by making decisions and learning from the result. This is especially true for early to mid career engineers.

    When we hire engineers, we select for people who want to make important decisions on behalf of users. When we hire engineering managers, we select for people who want to help engineers get _really_ good at making important decisions on behalf of users.

    As an EM, you'll be responsible for building teams of all skill levels that "own" product development. As _one of the first_ EMs, you'll be doing foundational work that shapes Fly.io into a company 5-years-ago-you would love to work at, and 10-years-from-now-you would be proud of.

    ## This role would be a good fit for you if:

    - You know how to define, communicate, and execute despite uncertainty. Our team, customer base, and product are growing at a dizzying rate. Every day it seems like something we've outgrown is on fire. But these are "good" fires — the kind that mean we're doing something right. Help us build the systems to do "right" better.
    - You want experience building an organization. We don't have the structure of a big company (yet); if the idea of building that structure together gives you goosebumps, this is your chance. As we grow, your career opportunities are practically endless.
    - You know what makes a great developer UX. For the vast majority of our work, we find a typical triad-based approach is less effective than giving our engineers autonomy to run with good product and good developer UX intuition.
    - You have enough experience with dev tools to know why some are loved and others are despised. You bring that knowledge to chaperone engineering decisions, making a dev tool people love.
    - You want to learn and want to help others learn too.
    - You, like us, have a lot in common with our users. We're all developers and we all speak the same language. Our users are eager to talk to us, and you eager to talk with them and learn.
    - You love building things. You know how to build and ship software, and you revel in the chance to write code. We believe the best engineering managers are engineers at heart. We don't expect you to ship product code, but we encourage EMs to block off time to keep their maker skills sharp. There's a plethora of things to scratch your coding itch on that also help the team, product, and company work better.
    - You have a knack for finding multipliers that help your team do far more than you ever could. You get satisfaction when your team solves a problem on their own.
    - You have experience managing engineers and understand the importance of glue work. You don't shy away from performance conversations and you recognize the relationship between objective feedback and career growth.
    - You love helping people develop new skills and advance in their careers. You know what everyone on your team wants to learn, and are always looking for opportunities to help them grow. We are growing the next generation of engineers and engineering leaders.
    - You understand the value of a diverse, equitable, and inclusive team. Fly is used by people of all skill levels, all over the world, from many different backgrounds and with many different goals. We believe will build a better company and product with a team just as diverse our users.
    - You're more excited by potential than you are hypnotized by experience. You know that the best hires in the long run are the ones who have potential but need an opportunity.
    - You're a genuinely good human, who wants to work with and hire other good humans. You communicate with kindness and lead with empathy.

    ## You know you'll be succeeding in this job if:

    - Your team is confident, happy, having fun, and doing valuable work.
    - You are hiring people at all levels from diverse backgrounds, and providing enough onboarding and ongoing support for them to be successful.
    - You've helped your team establish habits so the above to continue to happen when you aren't online.
    - People on your team are reaching their goals and eligible for promotion.
    - The above continue to happen as your team scales, and can be replicated to other teams as the company scales.

    ## More Details

    This is a senior level fully-remote full-time position. You can live anywhere in the world; your work hours and holidays observed are up to you. The salary ranges from $165k to $200k USD. We  offer competitive equity grants with a long exercise window. US employees get health care, everyone gets flexible vacation time (with a minimum), hardware/phone allowances, the standard stuff.

    ## How We Hire People

    We're weird about hiring. We're skeptical of resumes and we don't trust traditional interviews. We respect career experience but we're more excited about potential.

    The premise of our hiring process is that we're going to give you two challenges, a "work sample" and a "work day", that each simulate the kind of work you'll actually be doing here. Unlike a lot of places that assign “take-home problems”, our challenges are the backbone of our whole process; they're not pre-screeners for an interview gauntlet.

    For the "work sample", we're asking people to write a few mock posts for our internal forum that address questions from your team. Managers here actually do this often, and you'd be doing it as well. You do this challenge at your own pace, and it should take about 2 hours. For the "work day" you'll join two of us on Slack to work through a problem together.

    If you're interested, mail jobs+em@fly.io. You can tell us a bit about yourself, if you like. Please also include 1. your GitHub username (so we can create a private work sample repo for you) 2. your location (so we know what timezone you're in for scheduling) and 3. a sentence about your favorite food (so we know you're not a bot.)
- :id: phoenix-files-plucking-the-a-from-petal
  :date: '2022-10-06'
  :category: phoenix-files
  :title: Plucking the 'A' from PETAL
  :author: mark
  :thumbnail: petal-thumbnail.png
  :alt:
  :link: phoenix-files/plucking-the-a-from-petal
  :path: phoenix-files/2022-10-06
  :body: |2-


    <p class="lead">This post is asking if LiveView is able to remove Alpine, the "A" in PETAL. If you want to deploy your LiveView app right now, then check out how to [get started](/docs/elixir/). You could be up and running in minutes.</p>

    Recently, [LiveView](https://github.com/phoenixframework/phoenix_live_view) has matured greatly. A recent feature in v0.18, covered by [Chris McCord in his ElixirConf 2022 keynote](https://www.youtube.com/watch?v=9-rqBLjr5Eo), is the addition to  declarative assigns (think the old React/Vue props). This feature improves the developer experience when creating lots of small, reusable components.

    The previous [Phoenix.LiveView.JS](https://hexdocs.pm/phoenix_live_view/Phoenix.LiveView.JS.html) feature added in v0.17, made it easier to execute JavaScript commands integrated with LiveView.

    With all that LiveView goodness, can we drop [Alpine.js](https://alpinejs.dev/) from our default application stack? What happens when we pluck the "A" from PETAL? What are we left with? PETL? The real question is, can we accomplish everything we need?

    ## What is PETAL?

    Quick review here. The **PETAL** stack uses [Alpine.js](https://alpinejs.dev/) for client-side JavaScript interactions. Here's a list of the different parts of PETAL:

    - [**P**hoenix](https://www.phoenixframework.org/)
    - [**E**lixir](https://elixir-lang.org/)
    - [**T**ailwind](https://tailwindcss.com/)
    - [**A**lpine.js](https://alpinejs.dev/)
    - [**L**iveView](https://github.com/phoenixframework/phoenix_live_view)

    The PETAL acronym was coined by [Patrick Thompson](https://twitter.com/pthompson) and popularized on the [Thinking Elixir Podcast back in Nov 2020](https://podcast.thinkingelixir.com/21) and a series of articles.

    ## Are we ready to drop Alpine?

    This is really a question of, "Can I do everything I need with just LiveView?"

    In my previous post, [Making Tabs Mobile Friendly](/phoenix-files/making-tabs-mobile-friendly/), I tried to do exactly that. Could I do something with front-end JavaScript behavior using only JS commands? For reference, the tab behavior looks like this:

    ![Animated GIF showing 3 tab pages that display different content when selected. When the viewport is resized smaller, the tabs change to a select input for choosing the active tab.](/phoenix-files/2022-08-08/finished-working-tab-behavior.gif?centered&card&border)

    The end result? I got really far! I ended up using a little JavaScript to bring it across the finish line.

    From our recent Phoenix Files posts, I think we've demonstrated that, in most cases, we can provide a great user experience using just LiveView.

    However, there was one aspect that I could not cleanly solve and that's the problem we'll explore here.

    ## The Problem: Duplicated Logic

    The compromise made was to duplicate some logic in two places.

    - JS command logic is executed when clicking/changing the tab selection.
    - Same logic is duplicated for the initial rendering of the component.

    The component would be more streamlined if the initial render styled everything as "unselected". For the tabs example, an un-styled initial render would look like this:

    ![Example of tabs with no "active" tab styled as highlighted.](./tab-no-selection-displayed.png?centered&card)

    Then, immediately after rendering in the DOM, it would execute the JS commands to activate and style the selected tab. That means we would have only one implementation of "How do I make this look active?"

    Unfortunately, that approach isn't quite available yet.

    ### Illustrating the Problem

    In an effort to keep people from getting dizzy and falling out of chairs, I turned the following block of code into a screenshot. The code itself isn't as important as recognizing a pattern. First, let's see the image then follow it up with discussion about what's important.

    ![Image shows a HEEx template where two sections are highlighted.](./duplicated_code_blocks.png?centered&card&border)

    There are two highlighted blocks of HEEx code with sections that both use an `if tab[:current]` statement. Inside, the `if/else` statements are nearly identical lines of markup.  This was the compromise made in [Making Tabs Mobile Friendly](/phoenix-files/making-tabs-mobile-friendly/) that I didn't like making.

    These blocks of nearly duplicated lines of code only exist to handle the initial render. Separately, JS commands duplicate the logic to change the styles at runtime in response to user actions.

    Let's look at one example of the duplicated lines in greater detail. This markup sets the initial state for the `select` input displayed on mobile devices. The initial render is handled with this code.

    ```elixir
    <%%= if tab[:current] do %>
      <option value={"#{@id}-#{i}"} selected><%%= tab.title %></option>
    <%% else %>
      <option value={"#{@id}-#{i}"}><%%= tab.title %></option>
    <%% end %>
    ```

    What's the difference between the two lines? If the option represents the "current" tab, then the `selected` attribute is included.

    Here are the JS commands that perform the same logic when a user selects a different tab.

    ```elixir
    js
    #... skipped
    |> JS.remove_attribute("selected", to: "##{id}-mobile option")
    |> JS.set_attribute({"selected", ""}, to: "##{id}-mobile option[value='#{tab_id}'")
    ```

    Both blocks of code have the same effect but go about it differently. They manage the "selected" value on the `option` tag to match the "current" tab.

    A similar situation exists for the tab display. It sets the colors for the current tab along with adding or removing an `aria-current="page"` attribute.

    That's two different ways of doing the same thing. The template version handles the initial render and the JS version is for when the user makes a change. The JS version is always required. It must be there to make it work at runtime in the browser.

    Can we somehow avoid duplicating the logic for the initial render?

    ## Mind the Gap

    I tried to execute the JS commands in `phx-mounted` or with a hook using the `mounted` callback. What happens when we execute the JS commands using the `phx-mounted` callback?

    ![Tab display where, when reloaded, the active styles flicker to appear after a brief delay.](./mounted-callback-reloads.gif?centered&card&border)

    In this animation, I'm repeatedly hitting the browser refresh. The `phx-mounted` callback doesn't fire until **after** the websocket upgrade completes. The longer it takes for the websocket to connect (think longer distances), the more delayed the effect.

    **We are missing an earlier callback when the DOM is ready but before the websocket is mounted.**

    If we try adding an event listener to the browser's `DOMContentLoaded` event and execute the code there, it works fine on an the initial load. However, when the component is _dynamically_ displayed after the initial page load, then the event does not fire.

    Hooking into the browser's `DOMContentLoaded` event isn't a general solution.

    ## Possible Solutions?

    We need a new callback. Not knowing what to call it, we'll just refer to it as "domReady". It would fire after the component is loaded to the DOM either on an initial page load or after being patched to the DOM via LiveView and [morphdom](https://github.com/patrick-steele-idem/morphdom).

    Another option is for the existing `mounted` callback to be executed earlier. However, there are valid JS commands like `JS.push` that assumes a connected websocket is present.

    Perhaps there are other possible solutions?

    ## Time to Pluck the A from PETAL

    LiveView + JS commands and the possible sprinkling of a hook here and there is great. We can do everything we need with LiveView!

    Currently, that may mean our components duplicate some styling logic for the initial render. To improve that situation, and remove the compromise, a new callback earlier in the process that executes our JS commands is needed.

    Even without an improved callback, we've seen that we _can_ solve our front-end UI needs without bringing in extra front-end focused frameworks.

    Yup. It's time to pluck the "A" from PETAL.

    That means we need a new acronym to describe our default stack! What should our new, further simplified stack be called?

    <%= partial "shared/posts/cta", locals: {
      title: "Fly.io ❤️ Elixir",
      text: "Need a place to deploy your Phoenix LiveView app? Fly.io is a great place to deploy your applications. You can be running in minutes.",
      link_url: "https://fly.io/docs/elixir/",
      link_text: "Deploy your Phoenix app today!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>"
    } %>
- :id: ruby-dispatch-rails-background-jobs-with-fly-machines
  :date: '2022-10-05'
  :category: ruby-dispatch
  :title: Rails Background Jobs with Fly Machines
  :author: brad
  :thumbnail: taffy-robots-thumbnail.jpg
  :alt: DALL-E-generated image of 'whimsical painting of robots stretching taffy in
    a factory'
  :link: ruby-dispatch/rails-background-jobs-with-fly-machines
  :path: ruby-dispatch/2022-10-05
  :body: "\n\n**Fly Machines can boot a VM in 500ms, run a Rails background job, then
    turn off when it's done. That means you don't have to pay for a server to sit
    idle if there's no jobs to process _and_ you can have a much more scalable pool
    of on-demand workers when your application starts to get busy.**\n\nWhen a Rails
    application needs to do some heavy lifting like processing lots of data or making
    calculation that takes a long time, a common approach is to spin up an ActiveJob
    and asynchronously run it in a background worker.\n\nAccording to [The Ruby Toolbox](https://www.ruby-toolbox.com/categories/Background_Jobs),
    the [most popular background job framework](https://www.ruby-toolbox.com/categories/Background_Jobs)
    for Rails is [Sidekiq](https://www.ruby-toolbox.com/projects/sidekiq). Sidekiq
    is a great framework for creating background jobs, managing retries when they
    fail and queue priorities/topologies, and monitoring their status from the application
    or admin panel.\n\nLike many background workers, Sidekiq runs in a separate process
    from the Rails application, which requires additional CPU and memory resources.
    That's all great when there's jobs to process, but it's not fun paying for a server
    that sits around doing nothing when there's no jobs on the queue to process.\n\nAdditionally,
    when a bunch of jobs start rolling in that need to be processed, it would be great
    if the pool of workers could scale up temporarily to handle the increased load.
    Then scale back down when things are less busy.\n\nFly Machines could solve that
    problem by having zero background workers or processes running if there's no jobs.
    When a job comes in, the Rails ActiveJob processor spins up a Fly Machine to process
    it. Here's what that looks like as a really basic ActiveJob adapter.\n\n```ruby\nmodule
    ActiveJob\n  module QueueAdapters\n    # == Fly Machine adapter for Active Job\n
    \   #\n    # Boot VMs in 500ms, run a Rails background job, and shuts it down
    when it's all done.\n    #\n    #   Rails.application.config.active_job.queue_adapter
    = :fly_machine\n    class FlyMachineAdapter\n      def enqueue(job) # :nodoc:\n
    \       Fly.app.machine.fork init: {\n          cmd: [\n            \"/app/bin/rails\",\n
    \           \"runner\",\n            \"ActiveJob::Base.deserialize(#{job.serialize}).run\"\n
    \         ]\n        }\n      end\n\n      def enqueue_at(*) # :nodoc:\n        raise
    NotImplementedError, \"Does not yet support queueing background jobs in the future\"\n
    \     end\n    end\n  end\nend\n```\n\nWhen the job is done and exits, the Fly
    Machine also shuts down. If the machine took 3 seconds to run, you pay for 3 seconds
    of Fly Machine time. The important part is that you do _not_ have to keep paying
    for resources when there's no jobs on the queue.\n\nThe [`Fly.app.machine.fork`](https://github.com//fly-apps/rails-machine-workers/blob/main/app/models/fly/machine.rb#L32-L42)
    looks like magic, but it's not. All it does is gets the image name and `ENV` of
    the currently running Rails application and stuffs it into a [Fly Machines API](https://fly.io/docs/reference/machines/)
    call that boots a Firecracker VM. It's _almost_ like forking, close enough to
    call it a fork for purposes of this demo.\n\nYou can see the source code at [https://github.com/fly-apps/rails-machine-workers](https://github.com/fly-apps/rails-machine-workers)
    and the running application at [https://rails-machine-workers.fly.dev](https://rails-machine-workers.fly.dev),
    which includes a really big security warning you should know about if you try
    doing this today.\n\nBut wait, there's more!\n\n## Embracing the monolith\n\nIt's
    tempting to reach for edge function services from `$BIG_CLOUD_COMPANY`, but those
    usually require managing functions as a \"separate thing\" from the Rails app,
    which means as a developer you have to build it differently and manage it differently,
    like deploying the functions separately from the application code. There's tooling
    that makes this a little easier, but it's still another thing you'd have to worry
    about.\n\nWhen using  Fly Machines for background workers, you don't have to do
    anything special—you just treat your background jobs as background jobs and there's
    nothing extra to manage once you get it all setup. This makes the `fly deploy`
    command _even more powerful_, which is insane because the thing can already deploy
    your single application to a fleet of servers around the world. Now it can also
    deploy your background workers. \U0001F92F\n\n## Limitations to this proof-of-concept\n\nThis
    Fly Machines background worker proof-of-concept is still very basic and comes
    with a few issues depending on the needs of your application.\n\n### Lower processing
    latency\n\nThe basic concept of \"fire-and-forget-a-background-job-into-a-machine\"
    comes with latency that may or may not be acceptable for your application. Here's
    what the back-of-napkin math looks like:\n\n```\n  Fly Machine Boot Time     500ms\n+
    Rails Boot Time          1500ms..5000ms+\n--------------------------------------\n
    \ Total latency            2000ms..5500ms+\n```\n\nThis latency could be negligible
    if the background job takes a few minutes to complete, but if the background job
    should only take a half a second, it doesn't make sense to spend five seconds
    booting up to do it.\n\nThis problem is very solvable. The most straight forward
    way would be dumping the background job into a queue, boot the machine, the machine
    phones back home to the Rails app \"I'm alive and processing jobs\". The machine
    then proceeds to process all jobs on the queue until its empty. When the queue
    is empty the worker could either be configured to shutdown right away or wait
    for a configurable amount of time for another job before shutting down.\n\nHere's
    what that might look like:\n\n```\n./bin/fly-machine-worker --wait 600\n```\n\nIf
    the value of `600` is passed, the worker would wait for 600 seconds after the
    last job was processed before terminating. The first job would have to boot the
    Rails application, but subsequent jobs would get picked up by the already booted
    VM. When things slow down for 10 minutes, the work would terminate.\n\nTo guarantee
    one worker is always running so there's no wait time, `forever` could be passed
    to the worker.\n\n```\n./bin/fly-machine-worker --wait forever\n```\n\n### Handle
    retries for jobs that fail\n\nIf a Fly Machine jobs fails, the work wouldn't retry
    because the state of the job isn't being persisted anywhere and updated. It would
    be great to track the state of a job and the current attempts so they could be
    retried.\n\n### Too many workers\n\nThe funny thing about this approach is the
    problem of _too many workers_ spinning up and potentially bringing down your application.
    Let's say somebody loaded up your application with a gazillion requests that spin
    up a gazillion background jobs. If those jobs are competing for the same resource
    in your application, like trying to write to the same row in a database, your
    application could start to have some problems.\n\nThere's lots of approaches to
    solving these problems. One way is to name queues and set limits to the number
    of workers that can be spun up per queue. This approach has many precedents in
    a lot of Rails existing job queue frameworks.\n\n## ActiveJob might not be the
    right place to solve these problems\n\nThe list of limitations above have mostly
    been solved by all the great [background job](https://www.ruby-toolbox.com/categories/Background_Jobs)
    frameworks in Ruby. It might make more sense to perform the \"Fly Machine Fork\"
    from within a worker process itself.\n\nDoes this mean you'd then be back to paying
    for a worker that sits mostly idle monitoring a queue? Not necessarily. [Sidekiq
    7.0 introduces a concept called \"embedding\"](https://github.com/mperham/sidekiq/wiki/Embedding),
    which is another way of saying Puma creates a Sidekiq worker process from within
    the Rails server. If an embedded work is monitoring a queue and sitting mostly
    idle, you don't care as much because you're not paying for a dedicated server.
    When a job comes rolling in, Sidekiq would kick off a job in a new Fly Machine,
    getting back to the world where you only have to pay for what you use.\n\n## \"Compress
    the complexity of modern web apps\"\n\nThe ability to \"fork\" a Rails app on
    an entirely different machine without working about managing \"cloud functions\"
    is valuable to developers because they can spend more time worrying about their
    application and less time worrying about \"managing stuff\".\n\nIt makes the monolith
    even more majestic since you don't have to worry as much about capacity planning
    for your background jobs and can think about doing some really hefty work with
    the full might of Fly Machines at your disposal.\n\nThe approach still needs a
    lot of work before its production ready, but the basic building blocks are there
    to make Rails applications do even more incredible things.\n\n## Additional resources\n\n-
    [https://github.com/fly-apps/rails-machine-workers](https://github.com/fly-apps/rails-machine-workers)
    -Source code of Rails Background workers\n- [https://rails-machine-workers.fly.dev](https://rails-machine-workers.fly.dev)
    - Website that goes through the step-by-step of how to setup Fly background workers\n-
    [https://fly.io/docs/rails/advanced-guides/machine/](https://fly.io/docs/rails/advanced-guides/machine/)
    - API docs for Fly Machines\n- [https://fly.io/docs/reference/machines/](https://fly.io/docs/reference/machines/)
    - `flyctl machine` docs.\n"
- :id: phoenix-files-infinitely-scroll-images-in-liveview
  :date: '2022-10-03'
  :category: phoenix-files
  :title: Infinitely Scroll Images in LiveView
  :author: jaeyson
  :thumbnail: infinite-scroll-thumbnail.jpg
  :alt:
  :link: phoenix-files/infinitely-scroll-images-in-liveview
  :path: phoenix-files/2022-10-03
  :body: |2


    <p class="lead">This is a post about how to build an infinite scrolling LiveView page that shows a list of photos. It can be used for lots more than photos, but pretty pictures are more fun to look at than a list of products. If you want to deploy your Phoenix LiveView app right now, then check out how to [get started](/docs/elixir/). You could be up and running in minutes.</p>

    We see plenty of examples around the web of infinite scrolling content. Phoenix LiveView gives us some nifty abilities to do this elegantly and smoothly without needing any frontend frameworks.

    In this post we're going to build a very simple infinite scroll page with LiveView, using [TailwindCSS](https://tailwindcss.com) for the grid layout. Check out the [Demo app](https://infinite-scroll.fly.dev) and the final code named `complete branch`. The repo's `main` branch is the starting point for this post if you want to follow along.

    _Special thanks to [fly.io](https://fly.io/) for the [demo app](https://infinite-scroll.fly.dev) as well as [Pexels](https://pexels.com/) for our test photos._

    ## Problem

    You have a project that needs to show a grid layout of images. It should allow continuous scrolling and fetch more data as the user scrolls down.

    It should look something like this:

    <%= video_tag "infinite-scroll-images-animated.mp4?center", title: "Images being continuously loaded when scrolling down the page" %>

    Browsing around the web, you've seen lots of sites that use infinite scrolling. You know it can be done. Someone suggested using a JavaScript frontend library to build this feature. While that would work, you'd have to build out a paginating REST API, define how to serialize the data, and more to just to support the component.

    You're already using Phoenix and you think, "I'll bet LiveView could do this well. I might even get it done faster while still delivering a smooth experience."

    Okay, you're up for it. But now the question is…

    How do we create an infinite scrolling page of images in a grid layout using LiveView?

    ## Solution

    There are at least 2 important parts in our solution:

    - We'll use the browser's  [Intersection Observer API](https://developer.mozilla.org/en-US/docs/Web/API/Intersection_Observer_API) for observing scroll events and `phx-hook` to link it up. We put an element that acts as a target at the bottom of the page, then it'll trigger events.
    - DOM patches are done using  `phx-update``="append"`. This **adds** images to the end instead of replacing items.

    ### Following Along

    If you'd like to follow along, [clone the starter files](https://github.com/fly-apps/phoenix_infinite_image_scroll) and follow the instructions in the README and join me back here.

    ### Route for our page

    Using the starter project, we'll replace the default route that's being used by `PageController`:

    ```elixir
    # router.ex
    ...
      scope "/", InfiniteScrollWeb do
        pipe_through :browser

        live "/", HomeLive.Index, :index
        # get "/", PageController, :index
      end
    ...
    ```

    ### Create files for our LiveView and LiveComponent

    Here's what we'll add inside `infinite_scroll_web/` folder:

    ```
    infinite_scroll_web/
    ├── ...
    ├── live/
    │   ├── components/
    │   │   └── gallery_component.ex
    │   └── home_live/
    │       ├── index.ex
    │       └── index.html.heex
    └── ...
    ```

    That's 3 folders and 3 files. Based on what we see here:

    - `live/` is where we save our LiveView/LiveComponent files. We instructed our router to point us to a LiveView page instead of a regular template.
    - `components/` is where we save stackable elements, e.g. reusable modal.
    - `home_live/` is where where we save our parent layout for the homepage (`/`), logic for loading the images as well as our state.

    #### Code to add

    Parent layout where our component lives:

    ```xml
    <!-- infinite_scroll_web/live/home_live/index.html.heex -->
    <section class="my-4">
      <.live_component
        module={GalleryComponent}
        id="infinite-gallery-home"
        images={@images}
        page={@page}
      />
    </section>
    ```

    Our data comes through the  `images={@images}` attribute, while the `@page` assigns serves as our page number after every page load. That's 2 states that our component needs.

    #### Gallery Component

    This is a [Live component](https://hexdocs.pm/phoenix_live_view/Phoenix.Component.html). That means it's a reusable function that contains our  HEEx (html) template.

    ```elixir
    # infinite_scroll_web/live/components/gallery_component.ex
    defmodule InfiniteScrollWeb.Components.GalleryComponent do
      use InfiniteScrollWeb, :live_component

      defp random_id, do: Enum.random(1..1_000_000)

      def render(assigns) do
        ~H"""
        <div>
          <div
            id="infinite-scroll-body"
            phx-update="append"
            class="grid grid-cols-3 gap-2"
          >
            <%%= for image <- @images do %>
              <img id={"image-#{random_id()}"} src={image} />
            <%% end %>
          </div>
          <div id="infinite-scroll-marker" phx-hook="InfiniteScroll" data-page={@page}></div>
        </div>
        """
      end
    end
    ```

    I want to draw special attention to the following parts:

    - `phx-update="append"` used for adding new images
    - `phx-hook="InfiniteScroll"`   for detecting when we are at the bottom of the page and loading another set of images.
    - Notice that the div `phx-hook="InfiniteScroll"` is at the bottom, it acts as a target when we scroll at the bottom.
    - `phx-update` and `phx-hook` attributes needs an `id` attribute as well as each child elements that's using state (e.g.  `@title, @image, etc.`).

    #### Initial state and event handle

    The code below is where we have our initial state (since we don't use Ecto) and a function (`load-more`) getting the next set of images. We'll explain what they do below:

    ```elixir
    # infinite_scroll_web/live/home_live/index.ex
    defmodule InfiniteScrollWeb.HomeLive.Index do
      use InfiniteScrollWeb, :live_view

      alias InfiniteScrollWeb.Components.GalleryComponent

      @impl true
      def mount(_params, _session, socket) do
        {:ok,
          socket
          |> assign(page: 1),
          temporary_assigns: [images: []]
        }
      end

      @impl true
      def handle_event("load-more", _, %{assigns: assigns} = socket) do
        {:noreply, assign(socket, page: assigns.page + 1) |> get_images()}
      end

      defp get_images(%{assigns: %{page: page}} = socket) do
        socket
        |> assign(page: page)
        |> assign(images: images())
      end

      defp images do
        url = "https://images.pexels.com/photos/"
        query = "?auto=compress&cs=tinysrgbg&w=600"
        ~W(
          2880507 13046522 13076228 13350109 13302244 12883181
          12977343 13180599 12059441 6431576 10651558 5507243
          13386712 13290875 13392891 13156418 8581056 13330222
          10060916 8064098
        )
        |> Enum.map(&("#{url}#{&1}/pexels-photo-#{&1}.jpeg#{query}"))
        |> Enum.shuffle()
      end
    end
    ```

    There are a few important points we should pay attention to here:

    - The `mount/3` function is called for initial page load and to establish the live socket. The `temporary_assigns``: [images: []]` tells us that the value will reset after every render.
    - `handle_event/3` function is called from the JS file (hooks, client-side) that we will create later. The purpose of this function is to load our images.
    - Our list of images (data) lives in `images/0` function.

    Both `mount/3` and `handle_event/3` are [callback functions](https://hexdocs.pm/phoenix_live_view/Phoenix.LiveView.html#callbacks) that's needed for our LiveView (and hook) to work.

    If you wondered by our `temporary_assigns` didn't call `get_images/1`, there's a reason for that:

    - Assigns are stateful by default. Having to resend full list on every update is expensive!
    - Images that are saved in assigns are stored in memory (in our server) and holds it in the entire session. As your state grows, the performance of your app might be of concern here.
    - Also note that using temporary assigns reverts to the default value every update.
    - Based in the docs, `mount/3` is invoked twice: once to do the initial page load and again to establish the live socket. If we add `get_images/1` inside mount, you'll notice it'll render twice (for lack of a better term: `double mounting`). One way to mitigate this is to use `connected?/1` to check initial socket load, something like this:

    ```elixir
    def mount(_params, _session, socket) do
      socket = assign(socket, page: 1)

      # on initial load it'll return false,
      # then true on the next.
      if connected?(socket) do
        get_images(socket)
      else
        socket
      end

      {:ok, socket, temporary_assigns: [images: []]}
    end
    ```

    ### Prepare the hook

    In order to load images, we need client hooks. Let's add our `infinite_scroll.js` file:

    ```javascript
    // assets/js/infinite_scroll.js
    export default InfiniteScroll = {
      page() {return this.el.dataset.page;},
      loadMore(entries) {
        const target = entries[0];
        if (target.isIntersecting && this.pending == this.page()) {
          this.pending = this.page() + 1;
          this.pushEvent("load-more", {});
        }
      },
      mounted() {
        this.pending = this.page();
        this.observer = new IntersectionObserver(
          (entries) => this.loadMore(entries),
          {
            root: null, // window by default
            rootMargin: "400px",
            threshold: 0.1,
          }
        );
        this.observer.observe(this.el);
      },
      destroyed() {
        this.observer.unobserve(this.el);
      },
      updated() {
        this.pending = this.page();
      },
    };
    ```

    There are at least 5 interesting things to note:

    - `page()` function gets the value from `data-page={@page}` attribute from `gallery_component.ex`.
    - `this.pushEvent("load-more", {});` calls our `handle_event/3` function in `index.ex`.
    - The `root`, `rootMargin`, `threshold` options inside `mounted()` function tells us where to find the target viewport, margin and the percentage of the target's visibility before we load the next set of images.
    - `destroyed()` detaches from the observed element and move on to the next one.
    - `updated()` updates the new value of the page number.

    ### And now we’re [hook]ing

    Finally, let's import `infinite_scroll.js` file in `assets/js/app.js` and attach it in our live socket.

    ```javascript
    //  assets/js/app.js

    ...
    import topbar from "../vendor/topbar"
    import InfiniteScroll from "./infinite_scroll" // <-- import

    let csrfToken = document.querySelector("meta[name='csrf-token']").getAttribute("content")
    let liveSocket = new LiveSocket("/live", Socket, {
      hooks: {InfiniteScroll}, // <-- add the hook!
      params: {_csrf_token: csrfToken}
    })
    ...
    ```

    We did it! We created an infinite scrolling LiveView! The JavaScript hook linked a browser feature to our LiveView in fewer than 30 lines of JS code.

    Our page should now look something like this:

    ![Animated gif showing images being continuously loaded when scrolling down the page](./infinite-scroll-images-animated.gif?centered&card&2/3&border)

    Be sure to check out the  [demo](https://infinite-scroll.fly.dev/) and the [complete source code](https://github.com/fly-apps/phoenix_infinite_image_scroll/tree/complete)!

    ## Optionally Deploy it to Fly.io

    You can deploy it yourself by  [following this guide](https://fly.io/docs/elixir/getting-started/), with the exception of adding Postgres (assuming you're using CLI with `fly launch`).

    ## "To Infinity and Beyond!"

    This solution works well for rendering uniformly shaped content. Aside from displaying  images, this same feature could be used to show things like:

    - Contacts
    - Products
    - Product reviews
    - Client testimonials
    - …and more!

    What will you build with your infinitely scrolling LiveView?

    <%= partial "shared/posts/cta", locals: {
      title: "Fly ❤️ Elixir",
      text: "Fly is an awesome place to run your Elixir apps. Deploying, clustering, and more is supported!",
      link_url: "https://fly.io/docs/elixir/",
      link_text: "Deploy your Elixir app today!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>"
    } %>
- :id: jobs-security-engineer
  :date: '2022-10-03'
  :category: jobs
  :title: Security Engineer
  :author: thomas
  :thumbnail:
  :alt:
  :link: jobs/security-engineer
  :path: jobs/2022-10-03
  :body: "\n  \nWe're doing something ambitious at Fly.io: a new public cloud,\nrunning
    on our own hardware around the world, designed to make it easy\nto run distributed
    and real-time apps close to users anywhere in the\nworld.\n\nSecurity is part
    of what we promise our users. We run containers by\nconverting them into micro-VMs
    running under Firecracker, a\nmemory-safe, stripped-down Rust hypervisor. Apps
    on Fly.io are\nisolated from other customers with their own kernel, private networks,\nand
    WireGuard, Jason Donenfeld's next-gen VPN, which provides the\nnetworking dial-tone
    for our entire fleet.\n\nSecurity at Fly.io is part of the engineering team, and
    we're\nlooking for security engineers to join it.\n\nSecurity engineering goes
    beyond the code we write and the production\nservers we manage; it's responsible
    for our corporate security and all\nour endpoints as well. We're looking for engineers
    who are interested\nin working on the full gamut of security challenges facing
    an\nintensely technical, relatively small team competing with the largest\ncompanies
    in the industry.\n\nThis is sort of a golden moment to come work with us at Fly.io.
    We've\nhammered out our basic service and have a base of enthusiastic users\n[shipping](https://community.fly.io/t/git-limo-a-git-source-code-management-tool-powered-by-elixir/1420)\n[super](https://messwithdns.net/)
    [cool](https://multiplayer.dev/)\n[stuff](https://minesweeper.fly.dev/sessions/new)\n[on](https://twitter.com/benbjohnson/status/1494800752333647872)\n[it](https://semantleless.fly.dev/).
    Our team is growing but still at\na point where everyone knows everyone and what
    they're working\non. Nobody's off in a corner on a solitary death march. It's
    still\neasy to have a good idea here, float it to the team, and have it take\noff.
    We're having fun. For some of us, this kind of environment is why\nwe work in
    startups.\n\nIt's not all sparkles and lollipops. Security Engineering is a\nno-fooling
    serious role. We don't want to sell anyone a bill of\ngoods. Here are some messy
    things we want you to know:\n\n- We're smaller than you'd guess given our competitors.
    There is some\n  chaos. We've held the line on a lot of nuts-and-bolts security\n
    \ stuff, but there's lots left to do.\n\n- We're ruthless about working on stuff
    that our users will see and\n  care about, to the exclusion of a lot of engineering\n
    \ formalism. \"How will this immediately help users?\" is a\n  standard we hold
    ourselves to, even when it makes us uncomfortable.\n\n- We're on call, 24/7. Everyone
    shares a rotation. We've chosen a\n  cortisol-intensive domain to work in: when
    our stuff breaks, our\n  users notice, and because we're global, they notice in
    every time\n  zone.\n\n- We're a helpful bunch, but all of us are learning stuff
    as we go\n  along and we expect you to do the same.\n\n- We don't care what the
    cool kids are using. We're addicted to code\n  that works, right away, with minimal
    ceremony. We like SQLite, and\n  we get nervous when people talk about Raft. The
    engineering culture\n  here is pragmatic to what Hacker News would consider a
    fault.\n\nExtrapolate all the bad implications you can from that list. Then ask\nus
    about them, and we'll be candid.\n\n## The Role\n\nThis is a mid to senior level
    job. The salary ranges from $120k to\n$200k USD. We also offer competitive equity
    grants.\n\nWe're remote-first, with team members in Colorado, Quebec, Chicago,\nLondon,
    Mexico, Spain, Virginia, Brazil, and Utah. Most internal\ncommunication is written,
    and often asynchronous. You'll want to be\ncomfortable with not getting an immediate
    response for everything.\n\nThe role reports to our security practice lead, and
    up to engineering management.\n\nHere's some of what you'll be working on in this
    role:\n\n- CorpSec/endpoint security for everyone in the company. Which in our\n
    \ case means designing a security CorpSec strategy from the ground up:\n  our
    processes and controls are OK for the size we're at now, but\n  won't be a year
    from now.\n\n- InfraSec for our production fleet: visibility and monitoring, and\n
    \ taking advantage of the platform capabilities we've spent the last\n  several
    years investing in. Our prod fleet is all Linux, we nerd out\n  about BPF, and
    we're happy to sink resources into interesting\n  security projects.\n\n- Software
    security (and software supply chain security). You'll want\n  to be comfortable
    with the idea of picking up a PR in a language you\n  don't write all the time,
    understanding it, and spotting flaws. This\n  could be the year we end up getting
    `semgrep` religion!\n\n- Designing and building security features for our users.
    We get to\n  write the rulebook for this platform as we go, and there are a\n
    \ zillion opportunities to improve the security status quo for apps\n  that deploy
    here.\n\nDoes that sound like \"all of security\", like we're looking\nfor people
    to work in multiple different security subspecialties?\nProbably, we are. If that's
    appealing to you, we'd like to talk.\n\n## You'll Dig This Role If You:\n\n- Are
    looking for a technical firehose for your next gig, and you're happiest when there's
    always more important stuff to learn.\n- Want to work in a diverse and respectful
    team that values communication, glue work, and small, autonomous teams making
    decisions for themselves.\n- Are deeply comfortable with software development
    and with building solutions to challenges when existing tools and libraries invariably
    prove to be insufficient. Our problems are idiosyncratic!\n- Look forward to working
    on problems with immense scope and a million degrees of freedom, while relentlessly
    focusing on impact and building incrementally. Managing scope is here is even
    harder than it looks.\n- Like the idea of a high-profile role working in public
    communicating directly with our users. We value prose writing more than most tech
    companies.\n- Naturally ask lots of questions and can function effectively in
    situations where you don't immediately have the right answer to every question.
    \n- Believe in what we're doing. Every company says this. We're a startup where
    it's unusually easy to explain how big the project can get to, and the risks are
    easy to see too.\n\n## How We Hire\n\nWe are weird about hiring. We’re skeptical
    of resumes and we don’t\ntrust interviews (we’re happy to talk, though). We respect
    career\nexperience but we aren’t hypnotized by it, and we’re thrilled at the\nprospect
    of discovering new talent.\n\nWe hire with work-sample testing. That means we
    give candidates\nchallenges that model the work we actually do. We've taken the
    time to\nbuild scoring rubrics for those challenges in advance. The challenges\nare
    the whole process; they're not a hoop you jump through before we\nhaze you with
    interviews.\n\nFor this role, we've got 3 at-home challenges: a relatively light\nsoftware
    security assessment (we'll give you working code, you give us\nthe dumb bugs),
    an `osquery` exercise, and a quick thought exercise\nabout the security of our
    platform. We think these challenges should\ntake substantially less time than
    a series of phone screens, but we're\nnot timing you; knock them out in your spare
    time.\n\nIf the results of those challenges suggest you'll be happy in the\nrole,
    we'll invite you onto our Slack for an hour or two to talk\nthrough a design exercise:
    we'll be designing new security monitoring\ncapabilities for our production hosts.\n\nThere's
    nothing up our sleeves with these challenges. If you're\ninterested, we'll tell
    you much more about them, including all the\nresources we can think of to bone
    up on technical subjects. We want to\nsee you in your best light, not surprise
    you with tricky questions.\n\nIf you're interested, mail ~~jobs+security&#64;fly.io~~.
    Tell us a bit about yourself, if you like. We're happy to chat, online or voice.\n
    \ \n"
- :id: phoenix-files-forms-testing
  :date: '2022-09-30'
  :category: phoenix-files
  :title: Testing LiveView forms
  :author:
  :thumbnail: forms-testing-thumbnail.jpg
  :alt:
  :link: phoenix-files/forms-testing
  :path: phoenix-files/2022-09-30
  :body: "\n<p class=\"lead\">This is a post about a few functions to test that our
    app does what we think it does when a form is submitted. If you want to deploy
    your Phoenix LiveView app right now, then check out how to [get started](/docs/elixir/).
    You could be up and running in minutes.</p>\n\n[In](https://fly.io/phoenix-files/sdeb-toggling-element/)
    \ [previous](https://fly.io/phoenix-files/liveview-multi-select/)  [posts](https://fly.io/phoenix-files/phx-trigger-action/)
    we've used forms for different purposes, but we've never talked about how to test
    that our app does the right thing when a form is submitted. In this post we'll
    take a walk around some functions of the [LiveViewTest](https://hexdocs.pm/phoenix_live_view/0.18.0/Phoenix.LiveViewTest.html)
    module that come in handy for testing forms.\n\n## Testing the phx-submit result\n\nWe
    can test the behavior of our LiveView when the event specified with the `phx-submit`
    option is handled. The [render_submit/2](https://hexdocs.pm/phoenix_live_view/0.18.0/Phoenix.LiveViewTest.html#render_submit/1)
    function sends a form submit event and returns the rendered result. It is useful
    if we want to test the  contents of our LiveView right after handling the submit
    event.\n\nFor example, this form checks what happens if we submit a password-update
    form with a too-short password and a non-matching password confirmation:\n\n<aside
    class=\"right-sidenote\"> [Verified routes](https://twitter.com/chris_mccord/status/1554478915477028864)
    can be used in tests too, Phoenix 1.7 will bring really cool stuff!</aside> \n\n```elixir\n
    \ test \"Update password: renders errors with invalid data\", %{conn: conn} do\n
    \   {:ok, lv, _html} = live(conn, ~p\"/users/settings\")\n\n    result =\n      lv\n
    \     |> form(\"#password_update_form\", %{\n        \"current_password\" => \"invalid\",\n
    \       \"user\" => %{\n          \"password\" => \"too short\",\n          \"password_confirmation\"
    => \"does not match\"\n        }\n      })\n      |> render_submit()\n\n    assert
    result =~ \"<h1>Settings</h1>\"\n    assert result =~ \"should be at least 12
    character(s)\"\n    assert result =~ \"does not match password\"\n    assert result
    =~ \"is not valid\"\n  end \n```\n\nWe use the `render_submit` function to try
    to update the user's password. After handling the submit event, we can verify
    that the LiveView's content contains the expected error messages by asserting
    the resulting content.\n\n## Testing a form submission with a redirect\n\nThe
    `render_submit` function has two return types. We already saw the first one &mdash;the
    rendered content of the LiveView&mdash; but we haven't mentioned the second one;
    it returns a tuple of type `{:error, reason}` when the LiveView redirects after
    handling the `phx-submit` event.\n\nWe can use the [follow_redirect/3](https://hexdocs.pm/phoenix_live_view/0.18.0/Phoenix.LiveViewTest.html#follow_redirect/3)
    function to verify that the LiveView redirects as expected. When there is a redirect
    it returns a `{:ok, conn}` or  `{:ok, live_view, html}` tuple depending on the
    redirect type we use: a regular redirect or a live redirect respectively. If there
    is no redirect or if it redirects to the wrong place, it raises an error.\n\nWe
    can also use the resulting `conn` to verify the new rendered page content. For
    example, we have a LiveView to reset the user's password after receiving a reset
    link. If the password is successfully reset, an `:info` message is added and the
    user is redirected to the login page as follows:\n\n```elixir\n{:noreply,\n   socket\n
    \  |> put_flash(:info, \"Password reset successfully.\")\n   |> redirect(to: ~p\"/users/log_in\")}\n```\n\nWe
    can verify that the LiveView redirects to the correct place and it's setting the
    expected `:info` message:\n\n```elixir\ntest \"resets password once\", %{conn:
    conn, token: token, user: user} do\n  {:ok, lv, _html} = live(conn, ~p\"/users/reset_password/#{token}\")\n\n
    \ {:ok, conn} =\n    lv\n    |> form(\"#reset_password_form\",\n      user: %{\n
    \       \"password\" => \"new valid password\",\n        \"password_confirmation\"
    => \"new valid password\"\n      }\n    )\n    |> render_submit() # {:error, {:redirect,
    %{to: \"/users/log_in\", flash: \"SFMyNTY...\"}}}\n    |> follow_redirect(conn,
    ~p\"/users/log_in\")\n\n  refute get_session(conn, :user_token)\n  assert get_flash(conn,
    :info) =~ \"Password reset successfully\"\n  assert Accounts.get_user_by_email_and_password(user.email,
    \"new valid password\")\nend\n```\n\nThe `render_submit` function is called and
    it returns an  `{:error, redirect}` tuple. We pass the redirect tuple and the
    `conn` to `follow_redirect/3`  to perform the underlying request, and the route
    `~p\"/users/log_in\"` to verify that it redirects to the correct place.\n\nFinally
    we use the resulting `conn` to verify that:\n\n1) The user wasn't logged in.\n\n2)
    The expected flash message was added.\n\n3) The user's password was reset.\n\n##
    Testing an HTTP form submission\n\nIn a [previous post](https://fly.io/phoenix-files/phx-trigger-action/),
    we used a form's `:action` attribute to execute a controller action from a LiveView.\n\nFor
    example, we have a form to send a username and password to the `~p/users/log_in`
    route and save the user's data in session if the authentication is successful:\n\n```elixir\n
    <.form\n  id=\"login_form\"\n  :let={f}\n  for={:user}\n  action={~p\"/users/log_in\"}\n
    \ as={:user}\n  >\n  <%%= label f, :email %>\n  <%%= email_input f, :email, required:
    true, value: @email %>\n\n  <%%= label f, :password %>\n  <%%= password_input
    f, :password, required: true %>\n\n  <div>\n    <%%= submit \"Log in\" %>\n  </div>\n</.form>\n```\n\nIn
    this case, we don't have the `phx-submit` option since we don't do any validation/process
    inside the LiveView, but we delegate the whole login process to the `~p\"/users/log_in\"`
    controller action.\n\nIn LiveView 0.18, the function [submit_form/2](https://hexdocs.pm/phoenix_live_view/0.18.0/Phoenix.LiveViewTest.html#submit_form/2)
    was added to test the results of sending an HTTP request through the plug pipeline.
    Let's use it to test a form submission with valid login params:\n\n```elixir\ntest
    \"Login user with valid credentials\", %{conn: conn} do\n  password = \"valid_password\"\n
    \ user = user_fixture(%{password: password})\n\n  {:ok, lv, _html} = live(conn,
    ~p\"/users/log_in\")\n\n  form =\n    form(lv, \"#login_form\", user: %{\n      email:
    user.email, \n      password: password, \n      remember_me: true})\n\n  conn
    = submit_form(form, conn)\n\n  assert redirected_to(conn) == ~p\"/\"\n  assert
    get_session(conn, :user_token)\n  assert get_flash(conn, :info) =~ \"Welcome back!\"\nend\n```\n\nThe
    `submit_form/2` function receives a form and executes the HTTP request specified
    in the form's `:action` attribute. Then we use the resulting `conn` to check that:\n\n1)
    This results in the right redirect.\n\n2) The logged user's token was added to
    the session.\n\n3) The expected flash message was returned.\n\n## Triggering the
    submit form action and testing the results\n\nIn the same [post](https://fly.io/phoenix-files/phx-trigger-action/),
    we used the `phx-trigger-action` option to trigger a form `:action` only once
    a condition had been met. Now we'll cover how to test that the `phx-trigger-action`
    attribute has been set to `true`, and we'll follow the underlying HTTP request
    to test the action call results.\n\nIn this test we submit the same password-update
    form we used at the beginning of the post, but this time we send valid params.
    We expect the form `:action` — in charge of updating the user's token in session
    — is triggered after validating that the user's password was successfully updated
    and setting the `phx-trigger-action` to the form:\n\n```elixir\ntest \"updates
    the user password\", %{conn: conn, user: user} do\n  new_password = valid_user_password()\n
    \ \n  {:ok, lv, _html} = live(conn, ~p\"/users/settings\")\n  \n  form =\n    form(lv,
    \"#password_update_form\", %{\n      \"current_password\" => user.password,\n
    \     \"user\" => %{\n        \"email\" => user.email,\n        \"password\" =>
    new_password,\n        \"password_confirmation\" => new_password\n      }\n    })\n
    \ \n  assert render_submit(form) =~ ~r/phx-trigger-action/\n\n  new_password_conn
    = follow_trigger_action(form, conn)\n  \n  assert get_session(new_password_conn,
    :user_token) != \n           get_session(conn, :user_token)\n  assert get_flash(new_password_conn,
    :info) =~ \n           \"Password updated successfully\"\n  assert Accounts.get_user_by_email_and_password(user.email,
    new_password)\nend\n```\n\nWe build a form and pass it to the `render_submit/1`
    function, which returns the new content of our LiveView after handling the `phx-submit`
    event  so that we can assert that the `phx-trigger-action` option is added to
    the form.\n\nThen, we execute the controller action specified in the `:action`
    attribute using the [follow\\_trigger\\_action/2](https://hexdocs.pm/phoenix_live_view/0.18.0/Phoenix.LiveViewTest.html#follow_trigger_action/2)
    function, and we use `new_password_conn` to check that:\n\n1) The user's token
    was renewed with the new password.\n\n2) The expected flash message was added
    to the connection.\n\n3) The user's password was updated.\n\n<%= partial \"shared/posts/cta\",
    locals: {\n  title: \"Fly.io ❤️ Elixir\",\n  text: \"Fly.io is a great way to
    run your Phoenix LiveView app close to your users. It's really easy to get started.
    You can be running in minutes.\",\n  link_url: \"https://fly.io/docs/elixir/\",\n
    \ link_text: \"Deploy a Phoenix app today!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>\"\n}
    %>\n\n## Wrap-up\n\nIn this post we used a few functions of the `LiveViewTest`
    module to test form submission results when we are using LiveView.\n\nWe can use
    `render_submit`  to test that our LiveView executes the correct logic and renders
    the expected results after handling the submit event that we specify with the
    `phx_submit` attribute. Additionally, we can use  `follow_redirect` together with
    it for those times when we redirect to another LiveView after handling the submit
    event.\n\nWe can also test the results of executing an HTTP form submission using
    the `:action` attribute. We use `submit_form` to test the results of sending an
    HTTP request through the plug pipeline, and `follow_trigger_action` to test the
    resulting content of the action that is executed once the `phx-trigger-action`
    attribute is set to true.\n"
- :id: blog-introducing-litefs
  :date: '2022-09-21'
  :category: blog
  :title: Introducing LiteFS
  :author: ben
  :thumbnail: litefs-thumbnail.png
  :alt: A bird is packing a box full of logs to be shipped.
  :link: blog/introducing-litefs
  :path: blog/2022-09-21
  :body: |2


    <div class="lead">
      [Fly.io](http://fly.io/) runs apps close to users by taking containers and upgrading them to full-fledged virtual machines running on our own hardware around the world. We're also building an open-source distributed file system for SQLite called LiteFS which is pretty cool too. [Give us a whirl](https://fly.io/docs/speedrun/) and get up and running quickly.
    </div>

    Full-stack developers are sleeping on SQLite, a database most devs think more suited to unit tests than production loads. That's true enough for some apps. Most web apps are read-heavy, though, and we can use that to our advantage. With the right tooling, SQLite makes for faster, simpler web apps.

    To understand why we won't shut up about SQLite, think about latency. You have a budget of around 100ms to make an app feel snappy. Individual Postgres queries add milliseconds of latency. Apps often run multiple queries before responding to users. Database round trips can take a big bite out of a latency budget.

    The same problem infects your full-stack code. Developing against a relational database requires devs to watch out for "N+1" query patterns, where a query leads to a loop that leads to more queries. N+1 queries against Postgres and MySQL can be lethal to performance. [Not so much for SQLite](https://www.sqlite.org/np1queryprob.html).

    The challenge of building full-stack on SQLite is that it isn't client-server: it's a library that runs inside of your app. In the past, that's made it hard to get durability and replication. Most devs aren't comfortable with [a "single-server" architecture](https://crawshaw.io/blog/one-process-programming-notes), where any downtime in your app server takes your whole app down.

    But you don't need to make peace with single-server designs to take advantage of SQLite. Earlier this year, we wrote about why we're [all in on Litestream](https://fly.io/blog/all-in-on-sqlite-litestream/). Litestream is SQLite's missing disaster recovery system: it's a sidecar process that hooks into SQLite's journaling and copies database pages to object stores such as S3. Like SQLite itself, it has the virtue of being easy to get your head around; we explained most of the design [in a single blog post](https://fly.io/blog/all-in-on-sqlite-litestream/), and using it just takes a couple commands.

    We want to see how far we can push this model, and so we've been working on something new.

    ## LiteFS: Where We're Going We Don't Need Database Servers

    At least, not as such.

    LiteFS extends the idea of Litestream with fine-grained transactional control. Where Litestream simply copies the raw SQLite WAL file, LiteFS can inspect and ship individual transactions, which span pages, and are the true unit of change in a SQL database.

    SQLite imposes on us a constraint that makes this transactional control harder: SQLite is baked into the apps that use it. If you build something that changes the SQLite library itself, you're not building tooling; you're building a new database. And we're not interested in getting people to switch to a new flavor of SQLite.

    There's two options for intercepting the file system API in SQLite:

    1. Use the [Virtual File System (VFS)](https://www.sqlite.org/vfs.html) abstraction in SQLite.
    1. Build a [FUSE file system](https://www.kernel.org/doc/html/latest/filesystems/fuse.html).

    The VFS option is easier so, naturally, we chose to build a FUSE file system. That's how you're supposed to do it, right?

    LiteFS works by interposing a very thin virtual filesystem between your app and your on-disk database file. It's not a file system like ext4, but rather a pass-through. Think of it as a file system proxy. What that proxy does is track SQLite databases to spot transactions and then LiteFS copies out those transactions to be shipped to replicas.

    In the default journaling mode, transactions are easy to identify: a write transaction starts when the `-journal` file is created, and ends when it's deleted. The journal stores the page numbers and old page data and we can look up the new page data from the main database file.

    You see where this is going. SQLite's exquisitely documented file format makes it easy for LiteFS to replicate whole databases. Now we've got transaction boundaries. So we roll those transactions up into a simple file format we call [LTX](https://github.com/superfly/ltx). LiteFS replicas can replay those transactions back to recreate the current (or any previous) transaction state of a LiteFS-tracked SQLite database — without touching app code. It seems like magic, but it's a natural consequence of SQLite's strong design.

    ### Ok, so why didn't you write a VFS instead?

    First off, we have nothing against the SQLite VFS system—it's great! We're planning on also releasing LiteFS as a VFS with a super catchy name like… LiteVFS.

    If you're unfamiliar with VFSes, they serve as an abstracted file system API. In fact, you use them all the time since SQLite ships with two built-in VFS modules: one for Unix & one for Windows. You can also load a third-party VFS as an extension, however, therein lies the first problem. There's an extra step to use it. Every time someone needs to use the database, they have to remember to load the VFS. That includes when your application runs but also when you just load up the `sqlite3` CLI.

    LiteFS also needs to run an API server to replicate data between nodes. This gets complicated if you have multiple processes on a single machine trying to access the same local database. Which one runs the API server?

    The FUSE file system solves many of these usability issues by being a single point that all database calls go through. Once you mount it, there's no additional steps to remember and any number of processes can use it just like a regular file system.

    ## What LiteFS Can Do Today

    LiteFS' roots are in Litestream which was built with a simple purpose: keep your data safe on S3. However, it still ran with a single-server architecture which poses two important limitations.

    First, if your one server goes down during a deploy, your application stops. That sucks.

    Second, your application can only serve requests from that one server. If you fired up your server in Dallas then that'll be snappy for Texans. But your users in Chennai will be cursing your sluggish response times since there's a 250ms ping time between Texas & India.

    LiteFS aims to fix these limitations.

    To improve availability, it uses leases to determine the primary node in your cluster. By default, it uses Hashicorp's [Consul](https://www.consul.io/).

    With Consul, any node marked as a candidate can become the primary node by obtaining a time-based lease and is the sole node that can write to the database during that time. This fits well in SQLite's single-writer paradigm. When you deploy your application and need to shut down the primary, that node can release its lease and the "primary" status will instantly move to another node.

    To improve latency, we're aiming at a scale-out model that works [similarly to Fly Postgres](https://fly.io/blog/globally-distributed-postgres/). That's to say: writes get forwarded to the primary and all read requests get served from their local copies. Most app requests are reads, and those reads can be served lightning fast from in-core SQLite replicas anywhere in your deployment.

    But wait, that's not all! There are many ways to do replication and each application has its own needs around data access. LiteFS also lets you use a static primary node if you don't want to use Consul.

    We even have more topologies in the works. We've had suggestions from the community to support other approaches like [primary-initiated replication](https://github.com/superfly/litefs/issues/24). That would allow folks to stream real-time database updates to customers outside their network instead of customers connecting in. Kinda niche, but cool.

    ## Split Brain Detection

    LiteFS uses asynchronous replication between a loose membership of ephemeral nodes. It trades some durability guarantees for performance and operational simplicity that can make sense for many applications.

    It's able to do this because the primary election through Consul is dynamic and self-healing, which is again both the good and the bad news. Because dynamic topologies can have weird failure modes, LiteFS is designed defensively: we maintain a checksum for the entire state of the database and include it in each LTX file. This sounds expensive, but we can maintain it incrementally.

    We're able to maintain this checksum by calculating the checksum for each page and XOR'ing the results together:

    ```
    chksum = 0
    FOREACH page
        chksum = chksum XOR crc64(page.number, page.data)
    END
    ```

    When a transaction changes pages in the database, we'll start with the checksum of the previous LTX file, remove the old checksums for the changed pages, and add in the new checksums for the changed pages:

    ```
    chksum = prev_chksum
    FOREACH page
        chksum = chksum XOR crc64(page.number, page.old_data)
        chksum = chksum XOR crc64(page.number, page.new_data)
    END
    ```

    Since XOR operations are commutative, we can even checksum across compacted LTX files or checksum the current state of the database. We can do this in LiteFS because we have fine-grained control over the file system writes.

    These database checksums ensure that an LTX file cannot be applied out of order and corrupt your database: they ensure byte-for-byte consistency for all the underlying data. We verify these on startup so that every database must be in a consistent state relative to its LTX checksum.

    ## Where We're Heading With This

    We think LiteFS has a good shot at offering the best of both n-tier database designs like Postgres and in-core databases like SQLite. In a LiteFS deployment, the parts of your database that really want to be networked are networked, but heavy lifting of the data itself isn't.

    It's not just about performance. If you're using a database server like Postgres or MySQL today, chances are you're using a "managed" database service, where some other team is making sure your database is up and running. Everybody uses managed services because keeping database servers happy is annoying. With SQLite, there's not as much stuff that can break.

    And we'll keep saying this: the reason we think LiteFS and full-stack SQLite is a good bet is that the design is simple. You can [read a summary of the LiteFS design](https://github.com/superfly/litefs/blob/main/docs/ARCHITECTURE.md) and understand what each of these components is doing. SQLite is one of of the most trusted libraries in the world; most of our job is just letting SQLite be SQLite. Your app doesn't even need to know LiteFS is there.

    We're plowing ahead on LiteFS features. Here are a few big ones to look out for:

    **WAL-mode Support:** today, LiteFS works with SQLite's default rollback journal mode. But WAL mode is where it's at with modern SQLite. The FUSE proxy model works fine here too: transactions start with a write to the `-wal` file, and end with another write that marks a header with the _commit_ field set.

    **Write Forwarding:** SQLite works with a single-writer, multiple-reader model and our primary/replica replication mimics that. However, it adds friction to require developers to forward writes to the primary node. Instead, we're making it so any node can perform a write and then forward that transaction data to the primary. The primary can then replicate it out to the rest of the cluster.

    **S3 Replication:** running a cluster of LiteFS nodes significantly improves your durability  over a single-server deployment. However, nothing gives quite the same warm fuzzy feeling as tucking away a copy of your database in object storage. This will work similarly to Litestream, however, LiteFS' LTX files are built to be efficiently compacted so restoring a point-in-time copy of your database will be nearly instant.

    **Encryption:** we want developers to feel safe keeping SQLite replicas on services like S3. So we've designed an AEAD encryption scheme that fits into LiteFS naturally and ensures that even if you manage to expose your LTX files to the Internet, you won't have exposed any plaintext.

    ## Try It Out

    After several months of work, we're comfortable calling LiteFS beta-ready. We'd be happy if you played around with it.

    We've set up a [documentation site for LiteFS](https://fly.io/docs/litefs/) so you can get going with it and understand how it works. The easiest way to get up and running is to walk through our [_Getting Started with LiteFS_](https://fly.io/docs/litefs/getting-started/) guide. It only takes about 10 minutes and you'll have a globally-distributed SQLite application running. Crazy, right!?

    LiteFS is completely open source, developed in the open, and in no way locked into Fly.io, which invests resources in this solely because we are nerds about SQLite and not in any way because LiteFS is part of a secret plan to take over the world. Pinky-swear!
- :id: laravel-bytes-reusable-dynamic-tables-with-laravel-livewire
  :date: '2022-09-21'
  :category: laravel-bytes
  :title: Reusable, Dynamic Tables with Laravel Livewire
  :author: aprod
  :thumbnail: livewire-organized-thumbnail.png
  :alt:
  :link: laravel-bytes/reusable-dynamic-tables-with-laravel-livewire
  :path: laravel-bytes/2022-09-21
  :body: "\n\nTables have always played an essential role in the history of web development.
    Decades ago, tables lived their golden age; they were used not only to structure
    data but also for the layout of web pages. It was easy to arrange things with
    tables, a `<tr>` here, a `<td>` there, and everything was in the proper position.\n\nWith
    the desire to develop responsive and accessible websites and because of CSS' growing
    feature set, tables are finally used for the purpose that they were originally
    created: **displaying structured data.**\n\nTables can be far more complex than
    we might think. To break down the complexity, we can make a component that can
    be reused throughout a project and also help create a unified look and user experience.\n\nThese
    are the goals with our table component:\n\n- columns are customizable for different
    kinds of data\n- columns can be custom styled\n- can be paginated\n- can be sorted
    by columns\n\nHere's a quick look at what we want to build by the end of this:\n\n![dynamic
    sortable table](a-dynamic-table.gif)\n\n\n\nThe component is meant to be re-usable.
    To accomplish this, we'll create an abstract Table component, and a helper Column
    class. Combined, these will help us quickly scaffold useful tables in our app.\n\n##
    Prepare some example data\n\nFirst, let's  [create a new Laravel project](https://laravel.com/docs/9.x#your-first-laravel-project),
    [install Livewire using composer](https://laravel-livewire.com/docs/2.x/quickstart#install-livewire),
    and [install Tailwind using npm](https://tailwindcss.com/docs/guides/laravel).\n\nFor
    this example, we will create a user table. To store the data we want, let's edit
    the `create_users_table` migration and add these lines:\n\n```php\nSchema::create('users',
    function (Blueprint $table) {\n    ...\n    $table->string('avatar');\n    $table->string('status');\n
    \   $table->string('country');\n    ...\n}); \n```\n\nLet's also edit the `UserFactory`
    class to generate some nice fake data for us.\n\n```php\npublic function definition()\n{\n
    \   return [\n        ...\n        'avatar' => $this->faker->imageUrl(40,40),\n
    \       'status' => $this->faker->randomElement(['active','inactive','deleted']),\n
    \       'country' => $this->faker->country,\n    ];\n}\n```\n\nNext, let's edit
    the `DatabaseSeeder` class to create 55 users.\n\n```php\npublic function run()\n{\n
    \ \\App\\Models\\User::factory(55)->create();\n}\n```\n\nWhen we run the command
    `php artisan migrate:refresh --seed`, it will refresh and seed our database.\n\nGreat!
    We've got our database structure ready and our dev database is populated with
    realistic looking fake user data. Time to start on the component!\n\n## Creating
    the table component\n\nFirst, let's create an `abstract` component, called Table.
    That will handle the basic functionality of the table. We  can extend different
    kinds of tables from that.\n\nLet's start by running:  `php artisan make:livewire
    table`\n\nWe change the `Table` class to `abstract` and add two abstract functions
    called `query()`  and  `columns()`. Also, we'll add a non-abstract function called
    `data()` that returns the query results.\n\n```php\nabstract class Table extends
    Component\n{\n  ...\n\n  public abstract function query() : \\Illuminate\\Database\\Eloquent\\Builder;\n\n
    \ public abstract function columns() : array;\n\n  public function data()\n  {\n
    \   return $this\n      ->query()\n      ->get();\n  }\n}\n```\n\n## Column class\n\nThe
    column class is the heart of our table component. We can create a `Table` folder
    in our `app` directory, and create the `Column` class here. (It can go anywhere
    in your projects, it depends on how you structure your code.)\n\nLet's add a `$component`
    property to the class. It describes the component that should be rendered to display
    column data. We can override this to customize it.\n\nWe'll also add a `__construct($key,
    $label)` function that sets two properties. By creating a static `make()` function,
    we can use our class as a [fluent API](https://en.wikipedia.org/wiki/Fluent_interface)
    that enables method chaining.\n\nAfter those changes, this is how our class looks:\n\n```php\nclass
    Column\n{\n  public string $component = 'columns.column';\n  \n  public string
    $key;\n  \n  public string $label;\n  \n  public function __construct($key, $label)\n
    \ {\n      $this->key = $key;\n      $this->label = $label;\n  }\n\n  public static
    function make($key, $label)\n  {\n      return new static($key, $label);\n  }\n}\n```\n\nAs
    you can see, we set `$component` to `'columns.column'` . To make this work, we
    need to create this component as  `resources/views/components/columns/column.blade.php`\n\nThe
    `column` component is responsible for displaying the data passed as a prop named
    \ `value`. We can make that happen in our `column.blade.php` component as shown
    below.\n\n```php\n@props([\n    'value',\n])\n\n<div>\n    {{ $value }}\n</div>\n```\n\n##
    Making the layout of the table\n\nLet's focus on the `table.blade.php` file now.\n\nFirst,
    we start with a simple table layout; then, we render the heading by iterating
    over the `$this->column()` array.\n\nAfter that, we iterate over the `$this->data()`
    array, which currently contains our users. Using the `<x-dynamic-component>` helper
    dynamically renders the component set in the `$column->component` property.\n\n```html\n<div>\n
    \ <div class=\"relative overflow-x-auto shadow-md rounded-lg\">\n    <table class=\"w-full
    text-sm text-left text-gray-500\">\n      <thead class=\"text-xs text-gray-700
    uppercase bg-gray-50\">\n        <tr>\n          @foreach($this->columns() as
    $column)\n            <th>\n              <div class=\"py-3 px-6 flex items-center\"\n
    \               {{ $column->label }}\n              </div>\n            </th>\n
    \         @endforeach\n        </tr>\n      </thead>\n      <tbody>\n        @foreach($this->data()
    as $row)\n          <tr class=\"bg-white border-b hover:bg-gray-50\">\n            @foreach($this->columns()
    as $column)\n              <td>\n                <div class=\"py-3 px-6 flex items-center
    cursor-pointer\">\n                  <x-dynamic-component\n                      :component=\"$column->component\"\n
    \                     :value=\"$row[$column->key]\"\n                  >\n                  </x-dynamic-component>\n
    \               </div>\n              </td>\n            @endforeach\n          </tr>\n
    \       @endforeach\n      </tbody>\n    </table>\n  </div>\n</div>\n```\n\n##
    Initializing our `UsersTable`\n\nWe made all the preparations needed to render
    our first table. Let's make it happen.\n\nLet's create a `UsersTable`  Livewire
    component using this command  `php artisan make:livewire UsersTable`.\n\nThen
    we'll edit the `UsersTable`class to extend from our abstract `Table` class. Also,
    let's remove the `render()` function because we don't need to override it.\n\n```php\nclass
    UsersTable extends Table\n{\n}\n```\n\nWe created two abstract functions in our
    base `Table` class, let's make their implementation here.\n\nIn the `query()`
    function we need to return an `Eloquent\\Builder` that describes our data model.\n\n```php\npublic
    function query() : Builder\n{\n    return User::query();\n}\n```\n\nIn the `columns()`
    function, we need to return an array of `Column` instances. We'll set the keys
    to match our database and the labels however we want.\n\n```php\npublic function
    columns() : array\n{\n  return [\n       Column::make('name', 'Name'),\n       Column::make('email',
    'Email'),\n       Column::make('status', 'Status'),\n       Column::make('created_at',
    'Created At'),\n   ];\n}\n```\n\nThe last step is to render our table. For simplicity,
    \ we'll just include it in `welcome.blade.php`.\n\n```html\n<html>\n    <head>\n
    \       <meta charset=\"utf-8\">\n        <meta name=\"viewport\" content=\"width=device-width,
    initial-scale=1\">\n\n        <title>Livewire Table</title>\n\n        <link href=\"{{
    mix('/css/app.css') }}\" rel=\"stylesheet\">\n        @livewireStyles\n    </head>\n
    \   <body class=\"flex items-center justify-center min-h-screen\">\n\n    <div
    class=\"w-full max-w-6xl\">\n        <livewire:users-table></livewire:users-table>\n
    \   </div>\n\n    @livewireScripts\n    </body>\n</html>\n```\n\nWhen we run it,
    we should see something like this. We have a working reusable table component!\n\n![a
    static table](b-static-table.png)\n\n## Customizing column styling and data formatting\n\nIt's
    a frequent requirement to apply custom styling to our columns. How would we do
    that?\n\nWe can create a `component($component)` function in our `Column` class,
    that sets our `$component` property.\n\n```php\npublic function component($component)\n{\n
    \   $this->component = $component;\n\n    return $this;\n}\n```\n\nReturning with
    `$this` is a common pattern when you want to make a fluent API. Using this pattern,
    we can call multiple functions after each other on the same instance.\n\nWe will
    put the commonly used components into a folder called `common` in `resources/views/components/columns`.
    Create a file there called `human-diff.blade.php` with the following content:\n\n```php\n@props([\n
    \   'value'\n])\n\n<div>\n    {{ \\Carbon\\Carbon::make($value)->diffForHumans()
    }}\n</div>\n```\n\nColumns that are very specific to our table can go in  `resources/views/components/columns/users`
    folder. Let's create a status column file  `status.blade.php` that formats our
    status data more elegantly.\n\n```php\n@props([\n    'value'\n])\n\n<div class=\"flex\">\n
    \   <div @class([\n        'text-white rounded-xl px-2 uppercase font-bold text-xs',\n
    \       'bg-red-500' => $value === 'deleted',\n        'bg-green-500' => $value
    === 'active',\n        'bg-gray-500' => $value === 'inactive',\n])>\n        {{
    $value }}\n    </div>\n</div>\n```\n\nNow, we'll edit the `columns()` function
    in our `UsersTable` class to call the previously created `component($component)`
    function.\n\n```php\npublic function columns() : array\n{\n  return [\n    Column::make('name',
    'Name'),\n    Column::make('email', 'Email'),\n    Column::make('status', 'Status')->component('columns.users.status'),\n
    \   Column::make('created_at', 'Created At')->component('columns.common.human-diff'),\n
    \ ];\n}\n```\n\nOur  `status` column is now custom-styled, and the `created_at`
    column is showing the formatted data.\n\n![a table with customized column output](c-customized-columns.png)\n\n<%=
    partial \"shared/posts/cta\", locals: {\n  title: \"Livewire is better on Fly\",\n
    \ text: \"Livewire apps run even smoother when you move them closer to your users.
    Deploy your app on Fly.io in minutes!\",\n  link_url: \"https://fly.io/docs/laravel/\",\n
    \ link_text: \"Deploy your Laravel app!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>\"\n}
    %>\n\n## Adding pagination\n\nLivewire has a built-in pagination component that
    uses Tailwind CSS by default; let's use it now.\n\nWe need to use Livewire's `WithPagination`
    trait and add two properties to the `Table` class.\n\n```php\nabstract class Table
    extends Component\n{\n  use WithPagination;\n\n  public $perPage = 10;\n  \n  public
    $page = 1;\n\n  ...\n}\n```\n\nLet's edit the `data()` function to paginate results.\n\n```php\npublic
    function data()\n{\n    return $this\n        ->query()\n        ->paginate($this->perPage);\n}\n```\n\nWe
    can include the pagination component in our `table.blade.php` file using `$this->data()->links()`.
    For those curious about how does this work under the hood, you can learn more
    in the [Livewire docs.](https://laravel-livewire.com/docs/2.x/pagination)\n\n```html\n<div
    class=\"flex flex-col gap-5\">\n    <div class=\"relative overflow-x-auto shadow-md
    rounded-lg\">\n        <table class=\"w-full text-sm text-left text-gray-500\">\n
    \         ...\n        </table>\n    </div>\n    {{ $this->data()->links() }}\n</div>\n```\n\nWe
    have working pagination!\n\n![paginated table](d-paginated-table.gif)\n\n## Sort
    the columns\n\nIn order to sort the columns, we need to make a few modifications.\n\nFirst,
    we'll add a `$sortBy` and a `$sortDirection` property to our `Table` class.\n\n```php\npublic
    $sortBy = '';\n\npublic $sortDirection = 'asc';\n```\n\nWe'll edit the `data()`
    function to add the sorting functionality with `$query->orderBy()`.\n\n```php\npublic
    function data()\n{\n    return $this\n        ->query()\n        ->when($this->sortBy
    !== '', function ($query) {\n            $query->orderBy($this->sortBy, $this->sortDirection);\n
    \       })\n        ->paginate($this->perPage);\n}\n```\n\nWe need an action in
    our `Table` class that gets called when a column's title is clicked. If the selected
    key matches the clicked label's key, it reverses the direction. Otherwise, it
    sets the `$sortBy` to the clicked key, and the `$sortDirection` to `'asc'`. It
    also handles page reset to operate well with pagination.\n\n```php\npublic function
    sort($key) {\n  $this->resetPage();\n\n  if ($this->sortBy === $key) {\n      $direction
    = $this->sortDirection === 'asc' ? 'desc' : 'asc';\n      $this->sortDirection
    = $direction;\n\n      return;\n  }\n\n  $this->sortBy = $key;\n  $this->sortDirection
    = 'asc';\n}\n```\n\nLet's modify the `table.blade.php`   to add our click listener
    to the labels. We'll also add icons to represent the current sorting state. We
    will use the icons from [heroicons](https://heroicons.com/) for that.\n\n```html\n<th
    wire:click=\"sort('{{ $column->key }}')\">\n  <div class=\"py-3 px-6 flex items-center
    cursor-pointer\">\n    {{ $column->label }}\n    @if($sortBy === $column->key)\n
    \     @if ($sortDirection === 'asc')\n        <svg xmlns=\"http://www.w3.org/2000/svg\"
    class=\"h-5 w-5\" viewBox=\"0 0 20 20\"\n             fill=\"currentColor\">\n
    \           <path fill-rule=\"evenodd\"\n                  d=\"M14.707 10.293a1
    1 0 010 1.414l-4 4a1 1 0 01-1.414 0l-4-4a1 1 0 111.414-1.414L9 12.586V5a1 1 0
    012 0v7.586l2.293-2.293a1 1 0 011.414 0z\"\n                  clip-rule=\"evenodd\"/>\n
    \       </svg>\n      @else\n        <svg xmlns=\"http://www.w3.org/2000/svg\"
    class=\"h-5 w-5\" viewBox=\"0 0 20 20\"\n             fill=\"currentColor\">\n
    \           <path fill-rule=\"evenodd\"\n                  d=\"M5.293 9.707a1
    1 0 010-1.414l4-4a1 1 0 011.414 0l4 4a1 1 0 01-1.414 1.414L11 7.414V15a1 1 0 11-2
    0V7.414L6.707 9.707a1 1 0 01-1.414 0z\"\n                  clip-rule=\"evenodd\"/>\n
    \       </svg>\n      @endif\n    @endif\n  </div>\n</th>\n```\n\nLet's sort!\n\n![a
    sortable, paginated, dynamic table](e-fully-dynamic-table.gif)\n\n## Conclusion\n\nCreating
    tables can be a monotonous task; using libraries like Livewire and Tailwind can
    speed up the process and make the development joyful. Creating a reusable component
    makes adding additional tables super easy.\n\nAdding dynamic behavior without
    writing Javascript is where Livewire shines. With built-in helpers like pagination,
    we can implement solutions for complex problems with just a few lines of code.
    With the growing popularity of Livewire, more and more open-source packages are
    available daily.\n\nWith how Livewire enables dynamic behaviors without full page
    loads or even having to write a JSON API, you can create fast, responsive apps
    for your users more rapidly than before.\n"
- :id: blog-scale-to-zero-minecraft
  :date: '2022-09-14'
  :category: blog
  :title: Scale-to-Zero Minecraft server with Terraform and Fly Machines
  :author: dov
  :thumbnail: mc-thumbnail.jpg
  :alt: Minecraft Steve in a hot air balloon that has the face of a creeper
  :link: blog/scale-to-zero-minecraft
  :path: blog/2022-09-14
  :body: "\n\n<p class=\"lead\">\nI'm Dov Alperin. I wrote, and currently maintain,
    the official [Fly.io Terraform provider](https://registry.terraform.io/providers/fly-apps/fly).
    You don't need Terraform to run scale-to-zero Minecraft on Fly Machines, but it
    makes configuration and resource provisioning a breeze. I admit I'm totally biased.
    (It's true, though.) [Start at the beginning with Fly.io](https://fly.io/docs/speedrun/),
    or jump right in here.\n</p>\n\nRunning a Minecraft server for friends has become
    an archetypal first foray into the workings of the Internet. For some it's learning
    to expose the tender underbelly of a home network to outside connections. For
    others it's exploring the world of VMs, SSH, and infinite VPS options.\n\nFor
    me, as for so many, a Minecraft server was an early experience of running a \"production\"
    web service—one that others consumed and \"depended\" on. Mine was a DigitalOcean
    droplet held together with glue and duct tape.\n\nJust a few years of experience
    (and a gig at a cloud-compute company) later, here's my new take on this: an over-engineered,
    scale-to-zero Minecraft server running on a [Fly Machine](/blog/fly-machines/).\n\n##
    Scale-to-Zero?\n\nImagine this: you're middle-school me and your Minecraft server
    has picked up a few more Daily Active Users than you'd expected. RAM is running
    low and the mortification of disappointing your peers is fast approaching as VPS
    resource utilization creeps up.\n\nYou scale the VPS up: more vCPUs, more RAM,
    smoother gameplay. You are now munching hungrily through the free tier of your
    hosting provider, or worse, _paying money_ to keep your friends in enchanted boots
    and rotten flesh. Wouldn't it be awesome if the munching could stop when nobody's
    actually playing? Even better if the VM could start up again and carry on automatically
    when someone attempts to connect again.\n\nThis is the fundamental idea of scale-to-zero
    on Fly Machines: shut them down when no one is using them, but start them back
    up again when the user needs it, fast enough that no one is ever the wiser.\n\n##
    The plan\n\nOur magic scale-to-zero Minecraft server takes a few ingredients:\n-
    [Terraform](https://www.terraform.io/docs), to abstract the Fly.io API into a
    declarative configuration file. We'll configure the app and specify the resources
    to provision within `main.tf`. Then `terraform apply` will make it all happen.
    You can deploy the same app with `flyctl`. But Terraform is what my whole gig
    at Fly.io is about! Naturally I'm going to take advantage of it here. \n- [Fly
    Machines](https://fly.io/docs/reference/machines/): Firecracker VMs you can start
    and stop fast with a REST API.\n- Geoff Bourne's ([itzg](https://github.com/itzg))
    [minecraft-server](https://github.com/itzg/docker-minecraft-server) Docker [image](https://hub.docker.com/r/itzg/minecraft-server),
    for a well-tested and flexible way to run a Java Minecraft server, featuring a
    configurable Autostop feature that will automatically shut down the server if
    people haven't used it in a given amount of time. This is what allows for our
    server to scale to zero. Thanks to itzg for working with me to get it working
    smoothly on Fly.io!\n\n## Getting started\n\nIf you don't have Terraform yet,
    now's a good time to [install it](https://learn.hashicorp.com/tutorials/terraform/install-cli).\n\nSet
    the FLY\\_API\\_TOKEN environment variable. The Terraform provider uses this to
    authenticate us to the Fly.io API:\n\n```cmd\nexport FLY_API_TOKEN=$(flyctl auth
    token)\n```\n\nIn a new terminal, open a proxy to give Terraform access to the
    internal APIs we'll be using. Leave it open:\n\n```cmd\nflyctl machine api-proxy\n```\n\nCreate
    a new directory to work in:\n\n```cmd\nmkdir tf-fly-minecraft && cd tf-fly-minecraft\n```\n\nThen
    let's start our Terraform prep by creating a file called `main.tf` where we can
    import the Fly.io provider:\n\n```hcl\nterraform {\n  required_providers {\n    fly
    = {\n      source = \"fly-apps/fly\"\n      version = \"0.0.16\"\n    }\n  }\n}\n\nprovider
    \"fly\" {}\n```\n\nWith this in place, run `terraform init` to set up your workspace.\n\n<%=
    partial \"shared/posts/cta\", locals: {\n  title: \"\",\n  text: \"It's just a
    few steps to get started with Terraform and Fly Machines\",\n  link_url: \"/docs/app-guides/terraform-iac-getting-started/\",\n
    \ link_text: \"Learn more&nbsp;&nbsp;<span class='opacity-50'>&rarr;</span>\"\n}
    %>\n\n## Let's Build!\n\nWe are going to create four different resources:\n\n1.
    A Fly.io app, which is a sort of administrative umbrella that the VM will belong
    to;\n1. a Fly Machine that runs our server inside the app;\n1. a [Fly Volume](/docs/reference/volumes/)
    that our Minecraft world persists to; and\n1. a [public IP address](/docs/reference/services/)
    so that the player's Minecraft client actually has something to connect to!\n\nWe
    will use the following assumptions for now:\n1. The app name is `flymcapp` (replace
    this with a name of your own), and\n2. we're deploying in region `yyz` (replace
    this with somewhere near you).\n\nAdd the following blocks to the `main.tf`  we
    created earlier:\n\n```hcl\nresource \"fly_app\" \"minecraft\" {\n  name = \"flymcapp\"\n
    \ org  = \"personal\"\n}\n\nresource \"fly_volume\" \"mcVolume\" {\n  app    =
    \"flymcapp\"\n  name   = \"mcVolume\"\n  size   = 15\n  region = \"yyz\"\n\n  depends_on
    = [fly_app.minecraft]\n}\n\nresource \"fly_ip\" \"mcIP\" {\n  app  = \"flymcapp\"\n
    \ type = \"v4\"\n\n  depends_on = [fly_app.minecraft]\n}\n\nresource \"fly_machine\"
    \"mcServer\" {\n  name   = \"mc-server\"\n  region = \"yyz\"\n  app    = \"flymcapp\"\n
    \ image  = \"itzg/minecraft-server:latest\"\n\n  env = {\n    EULA                    =
    \"TRUE\"\n    ENABLE_AUTOSTOP         = \"TRUE\"\n    AUTOSTOP_TIMEOUT_EST    =
    120\n    AUTOSTOP_TIMEOUT_INIT   = 120\n    MEMORY                  = \"7G\"\n
    \   AUTOSTOP_PKILL_USE_SUDO = \"TRUE\"\n  }\n\n  services = [\n    {\n      ports
    = [\n        {\n          port = 25565\n        }\n      ]\n      protocol      =
    \"tcp\"\n      internal_port = 25565\n    }\n  ]\n\n  mounts = [\n    { path   =
    \"/data\"\n      volume = fly_volume.mcVolume.id\n    }\n  ]\n\n  cpus     = 4\n
    \ memorymb = 8192\n\n  depends_on = [fly_volume.mcVolume, fly_app.minecraft]\n}\n```\n\nThe
    first block creates the Fly.io app, as you might guess. From there we have blocks
    that create a 15GB persistent storage volume and an IPv4 address.\n\nNow we get
    to the meat of it: the `fly_machine` block. We start off by defining some basics:
    the machine name, what app it belongs to, what region it should run in, and what
    image it should run. In this case we use the super awesome `minecraft-server`
    Docker image from itzg.\n\nThe `env` block sets environment variables used by
    `minecraft-server` for configuration; for example, we're setting the Autostop
    feature to shut down the VM when no one's been connected for 120 seconds.\n\nThe
    `services` block exposes port 25565 to the outside world via the IP we defined
    earlier, and the `mounts` block connects the previously defined volume to our
    machine.\n\nYou may have noticed the MEMORY environment variable that we set to
    `\"7G\"`. A Minecraft server wants a fair amount of memory, and some CPU oomph
    to match. So we specify vCPUs and 8G of RAM for this VM.\n\nFinally, with `depends_on`
    we tell Terraform to make sure the app and the volume are in place before trying
    to start a VM.\n\n\n<div class=\"callout\">\n\n## Some Warnings\n\n### Access\n\nAs
    we configured it here, anyone can join the server. That's probably not what you
    want! Check out the [documentation](https://github.com/itzg/docker-minecraft-server#whitelist-players)
    to find out how to set up an allowlist using environment variables.\n\n### Costs\n\nMinecraft
    servers aren't exactly lightweight. Or rather: Java isn't exactly lightweight.
    The example code creates a machine with 4 shared vCPUs and 8GB of RAM, and a 15GB
    storage volume. Vanilla Minecraft should still work fine, if a bit slower, if
    you tweak down the resources a bit. But we're well outside of \"free allowances\"
    territory.\n\nThis is why the \"scale to zero\" aspect of this project is so useful,
    of course! However, **any** TCP traffic will wake up the machine, including things
    like port scans. It'll go back to sleep, but the surefire way to prevent it incurring
    any further costs is to destroy the app.  If you're not going to use the server
    again, you'll want to do this anyway, so you don't have to pay for the storage
    volume.\n\n</div>\n\n\n## Let's Play!\n\nOnce you have finished tweaking anything
    you want to tweak in the Terraform file, go ahead and run `terraform apply` (and
    confirm when it prompts you) to create all the resources.\n\nOnce the command
    stops running, open up your Minecraft Java Edition installation, head to the multiplayer
    screen and connect to `flymcapp.fly.dev` (once again replacing it with the app
    name you chose earlier) and find a tree to cut down with your fist!\n\n<aside
    class=\"right-sidenote\">While it defaults to the latest \"Vanilla\" server, docker-minecraft-server
    can be configured to run any number of modded servers. Check out the [README](https://github.com/itzg/docker-minecraft-server#readme)
    for configuration options.</aside>\n\nOnce you have played around for a few minutes,
    try quitting out of Minecraft and watch the logs on the Monitoring tab of the
    Fly.io dashboard. You'll see that once the configured timeout is hit, it will
    shut itself down. Try connecting again, you'll see the machine automatically start
    itself back up. Cool huh? :)\n\nIf you are done with this guide and don't intend
    to use the server again, go ahead and destroy the app. We have a cornucopia of
    tools for destruction! Since you created this app with Terraform, you can use
    `terraform destroy`; for any Fly.io app, including this one, there's also `fly
    apps destroy`; or you can hit the red \"Delete app\" button under your app's Settings
    tab in the Fly.io dashboard. Check in your dashboard, or use `fly apps list`,
    to check that it's gone.\n\n![A rudimentary version of the Fly.io logo built in
    Minecraft](fly-in-mc-day.jpg)\n"
- :id: ruby-dispatch-turbostream-fetch
  :date: '2022-09-08'
  :category: ruby-dispatch
  :title: Using TurboStream with the Fetch API
  :author: rubys
  :thumbnail: ballroom-thumbnail.jpg
  :alt: Ballroom dancers observing a competition
  :link: ruby-dispatch/turbostream-fetch
  :path: ruby-dispatch/2022-09-08
  :body: "\n\n<div class=\"lead\">\n  [Fly.io](http://fly.io/) runs apps close to
    users around the world. This enables highly dynamic interactive forms, even in
    cases where a server interaction is required. [Give us a whirl](https://fly.io/docs/speedrun/)
    and get up and running quickly.\n</div>\n\nMany people see Rails as a framework
    that will get you to IPO and beyond. I, personally, I&#39;m more interested in
    the long tail of applications that are used by only a small group of people.\n\nI
    wrote one such application to schedule heats for ballroom competitions, which
    I named [Showcase](https://github.com/rubys/showcase#showcase). That&#39;s heats,
    like in a swim match, but where the women&#39;s outfits are sequined and men may
    be wearing tails. One key difference: ballroom dances are performed by pairs of
    participants, which complicates scheduling as one person cannot be  in two places
    at the same time.\n\nThroughput isn&#39;t a concern for the Showcase application,
    but latency is. If you are entering a large amount of data (in human terms, not
    in &quot;big data&quot; terms), you don&#39;t want to wait for requests to be
    processed. Therefore server responses for trivial requests need to be seen as
    instantaneous, which is anything under 100ms.\n\nThe remainder of this post focuses
    on how forms in the Showcase application are made dynamic using [Hotwire](https://hotwired.dev/),
    combining a number of techniques that may not be obvious.\n\n## Forms That Change\n\nForms
    can change based on user input for any number of reasons: drag and drop; selecting
    values from a drop-down menu; even submitting the form can cause new form fields
    to appear, disappear, or change. I&#39;ll use a New Participants form to demonstrate:\n\n![New
    Participant/Guest form with inputs for Name, Studio, Type, and Package. Type is
    set to Guest. There are two buttons: Create Person, and Back to People.](new-guest-form.png)\n\nNot
    shown above are a number of hidden fields.  If the participant&#39;s type is Professional,
    additional fields will be added to the form to indicate whether the participant
    is a Leader or Follower (or both!). Students have additional fields including
    Level and Age categories, and even have a way to request that they not be scheduled
    at the same time as a friend or spouse so that they can video each other. Leaders
    have one additional form field where a back number can be entered.\n\nA form with
    all of the input fields revealed looks like the following:\n\n![New Participant
    form as above, but with Type set to Student and additional fields for Level, Age,
    Role, Back Number, and Avoid scheduling the same time as.](new-student-form.png)\n\nHiding
    and revealing portions of the form is relatively straightforward, oftentimes it
    is as simple as placing lines of code like the the following in a [Stimulus](https://stimulus.hotwired.dev/)
    controller:\n\n```javascript\nthis.levelTarget.classList.remove('hidden');\n```\n\nNow
    I&#39;d like to draw your attention to the the Package field in this form. It
    turns out that the list of packages to choose from depends on both the Studio
    and Type of the person being added or edited. The  server knows  how to construct
    the Package selection from that information.\n\nA  common approach to solving
    problems such as these is a [single page application](https://en.wikipedia.org/wiki/Single-page_application)
    \ which would involve client-side rendering of this part of the page, initially
    using either hydration on the client, possibly with server-side rendering (SSR).\n\nThe
    Hotwire approach is different. The content is placed into a [Turbo Frame](https://turbo.hotwired.dev/handbook/frames)
    and rendered on the server. The only role the client has in the process is to
    replace an element in the DOM with the new content.\n\nNow lets look at how this
    is put together, starting with the HTML...\n\n## Initial Render of the Form\n\nThe
    Showcase application makes use of both Stimulus and Turbo Frames.\n\nStimulus
    enables you to attach JavaScript controllers and actions to your HTML via  `data-`
    attributes. Turbo Frames encourages you to split out content that may be replaced
    later into a [partial](https://guides.rubyonrails.org/layouts_and_rendering.html#using-partials).
    Both approaches are employed here.\n\nFocusing on the portions of HTML that are
    related to the need to show the correct list of packages given the selected Studio
    and Type leaves the following portions of the form template,  `_form.html.erb`:\n\n```erb\n<%%=
    form_with(model: person, data: {\n      controller: \"person\",\n      id: person.id\n
    \   }) do |form| %>\n\n...\n\n  <div>\n    <%%= form.label :type %>\n    <%%=
    form.select :type, @types, {},\n      'data-person-target' => 'type',\n      'data-action'
    => 'person#setType',\n      'data-url' => type_people_path %>\n  </div>\n\n...\n\n
    \ <%%= render partial: 'package', locals: { person: person } %>\n<%% end %>\n```\n\nNote
    the `data-url` attribute which identifies the route where POST requests will be
    sent.\n\nAnd now, `_package.html.erb` which is a reusable partial enabling it
    to be both rendered within the initial form and later rendered separately:\n\n```erb\n<%%=
    turbo_frame_tag('package-select') do %>\n<%% unless @packages.empty? %>\n  <%%=
    label_tag :person_package_id, 'Package' %>\n  <%%= select_tag 'person[package_id]',\n
    \    options_for_select(@packages, @person.package_id || '') %>\n<%% end %>\n<%%
    end %>\n```\n\nNow that you have seen how the initial content is rendered, lets
    move on to how the frame gets replaced as the user makes their selections.\n\n##
    Fetching New Content on Demand\n\nThis is where the Showcase application  combines
    two techniques that may be a bit obscure.\n\nThe HTML form referenced a `person`
    stimulus controller. This controller has a number of responsibilities in the Showcase
    application. Here are the parts we’re interested in at the moment:\n\n```javascript\nimport
    { Controller } from \"@hotwired/stimulus\"\n\n// Connects to data-controller=\"person\"\nexport
    default class extends Controller {\n  static targets = ['studio'];\n\n  connect()
    {\n    this.id = JSON.parse(this.element.dataset.id);\n    this.token = document.querySelector(\n
    \     'meta[name=\"csrf-token\"]'\n    ).content;\n  }\n\n  setType(event) {\n
    \   fetch(event.target.getAttribute('data-url'), {\n      method: 'POST',\n      headers:
    {\n        'X-CSRF-Token': this.token,\n        'Content-Type': 'application/json'\n
    \     },\n      credentials: 'same-origin',\n      body: JSON.stringify({\n       id:
    this.id,\n       type: event.target.value,\n       studio_id: this.studioTarget.value\n
    \     })\n    }).then (response => response.text())\n    .then(html => Turbo.renderStreamMessage(html));\n
    \ }\n}\n``` \n\nThe `setType` method is invoked whenever the level selection changes.
    Looking again at part of the `form.select` input in `_form.html.erb`:\n\n```erb\n<%%=
    form.select :type, 'data-action' => 'person#setType' %>\n```\n\nThe `setType`
    method itself uses the [Fetch API](https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API),
    passing in the body the `params` that will be made available to the controller.
    In this case, the `params` include the person&#39;s `id`, `type`, and `studio.id`.
    The text of the response to this fetch request is extracted and then rendered
    as if it had been delivered via [TurboStream](https://turbo.hotwired.dev/handbook/streams).\n\nThere
    are two keys to making this work. Both are documented, albeit a bit obtusely.\n\nFirst
    we need to deal with `X-CSRF-Token`. You will find it mentioned in the [Securing
    Rails Applications](https://guides.rubyonrails.org/security.html), as follows:\n\n>
    By default, Rails includes an unobtrusive scripting adapter, which adds a header
    called X-CSRF-Token with the security token on every non-GET Ajax call. Without
    this header, non-GET Ajax requests won&#39;t be accepted by Rails. When using
    another library to make Ajax calls, it is necessary to add the security token
    as a default header for Ajax calls in your library. To get the token, have a look
    at `<meta name='csrf-token' content='THE-TOKEN'>` tag printed by `<%%= csrf_meta_tags
    %>` in your application view.\n\nThe reference to an _unobtrusive scripting adapter_
    and _AJAX_ are both anachronisms here, but the important essence here is that
    if you want non-GET HTTP requests to work you will need to extract the content
    from te `csrf-token` meta tag and place it in an `X-CSRF-Token` HTTP header.\n\nTo
    be fair, there are two alternatives to getting the CSRF token as was done here.\n\nFirst,
    in some cases is possible to rewrite this particular request to use `encodeURIComponent`
    and HTTP GET, but in other cases (such as recording scores in the Showcase application),
    HTTP GET would not be appropriate. Passing JSON in request bodies is not only
    more convenient, but also can contain more complex nested data payloads than URL
    query strings.\n\nSecond, [requestSubmit](https://developer.mozilla.org/en-US/docs/Web/API/HTMLFormElement/requestSubmit)
    can be used to submit an entire HTML form. HTML doesn&#39;t support nested forms,
    but you can add additional, completely hidden, forms as needed; copy the relevant
    values to the hidden form&#39;s input fields; and submit them.\n\nIf either of
    these alternatives work for you, use what you feel most comfortable with. Oftentimes
    it is useful to have a third alternative, especially when that alternative requires
    less code. Drag and drop is an clear example where HTTP GET is not appropriate,
    and where it generally is more straightforward to issue a fetch call than it is
    to construct and update a hidden form and submit it.\n\n---\n\nThe other important
    part is the method used to render the response. You will find this method mentioned
    in [Processing Stream Elements](https://turbo.hotwired.dev/reference/streams#processing-stream-elements):\n\n>
    If you need to process stream actions from different source than something producing
    MessageEvents, you can use Turbo.renderStreamMessage(streamActionHTML) to do so.\n\nTo
    have found this you would have needed to look at the documentation for Processing
    Stream Messages to find the description of how to handle _non_-Stream Messages.\n\nWhile
    the documentation is a bit obscure, the resulting code is fairly straightforward:
    extract the body of the fetch response as HTML, and pass that HTML to `Turbo.renderStreamMessage`.\n\nNow
    that the  Studio and Type have been submitted to the server, the server has to
    render the Package input part of the form with the right stuff.\n\n## Rendering
    New Content\n\nExcerpt of the method in `people_controller.rb` that generates
    responses to `post_type` requests:\n\n```ruby\n  def post_type\n    @person =
    Person.find_by_id(params[:id]) || Person.new\n    @person.studio = Studio.find(params[:studio_id])\n
    \   @person.type = params[:type]\n\n    selections\n\n    respond_to do |format|\n
    \     format.turbo_stream { \n        render turbo_stream: turbo_stream.replace('package-select',\n
    \         render_to_string(partial: 'package'))\n      }\n\n      format.html
    { redirect_to people_url }\n    end\n  end\n\nprivate\n\n  def selections\n    .
    . . \n  end\n```\n\nThis is straightforward. An `@person` object is constructed
    based on the `params` in the request. A private method is called to populate the
    `@packages` instance variable. Finally a `turbo_stream.replace` response is produced
    including the rendered package partial. This is the same partial that was originally
    rendered in the form.\n\n<%= partial \"shared/posts/cta\", locals: {\n  title:
    \"You can play with this right now.\",\n  text: \"It'll take less than 10 minutes
    to get your Rails application running globally.\",\n  link_url: \"https://fly.io/docs/rails/\",\n
    \ link_text: \"Try Fly for free&nbsp;&nbsp;<span class='opacity-50'>&rarr;</span>\"\n}
    %>\n\n## Summary\n\nOverall, using `fetch` in response to DOM events, and processing
    the responses as if they were turbo stream messages, is a useful design pattern.
    Making it work required solving two problems: obtaining a CRSF token, and rendering
    a fetch response as a turbo stream messages.\n\nOn my development laptop, such
    requests can be processed quickly:\n\n```\nCompleted 200 OK in 5ms (Views: 0.0ms
    | ActiveRecord: 0.4ms | Allocations: 7213)\n```\n\nThis means that if you can
    deploy your app close to your users so that the network latency is minimal, you
    can actually produce results comparable to single page applications in terms of
    form changes being perceived as instantaneous by users.\n\nI hope you found this
    post useful and can find interesting ways to apply this technique in your Rails
    applications!\n"
- :id: blog-sqlite-virtual-machine
  :date: '2022-09-07'
  :category: blog
  :title: How the SQLite Virtual Machine Works
  :author: ben
  :thumbnail: vm-thumbnail.jpg
  :alt: A robot furiously making sandwiches.
  :link: blog/sqlite-virtual-machine
  :path: blog/2022-09-07
  :body: "\n\n<div class=\"lead\">\n  [Fly.io](http://fly.io/) runs apps close to
    users around the world, by taking containers and upgrading them to full-fledged
    virtual machines running on our own hardware around the world. Sometimes those
    containers run SQLite and we make that easy too. [Give us a whirl](https://fly.io/docs/speedrun/)
    and get up and running quickly.\n</div>\n\nSQL is a weird concept. You write your
    application in one language, say JavaScript, and then send commands in a completely
    different language, called SQL, to the database. The database then compiles and
    optimizes that SQL command, runs it, and returns your data. It seems terribly
    inefficient and, yet, your application might do this hundreds of times per second.
    It's madness!\n\nBut it gets weirder.\n\nSQL was [originally designed for non-technical
    users](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6359709) to interact
    with the database, however, it's used almost exclusively by software developers
    peppering it throughout their applications.\n\nWhy would this language made for
    \"business folks\"  become the industry standard for how applications are built?\n\nOne
    of the key benefits of SQL is that it is declarative. That means you tell the
    database what you want but not how to do it. Your database knows _WAY_ more about
    your data than you do so it should be able to make better decisions about how
    to fetch it and update it. This lets you improve your data layer by adding indexes
    or even restructuring tables with minimal effects on your application code.\n\nSQLite
    is unique among embedded databases in that it not only has a transactional, b-tree
    storage layer but it also includes a robust SQL execution engine. Today, we're
    diving into how SQLite parses, optimizes, & executes your SQL queries.\n\n## A
    sandwich-making machine\n\nIf you've read our previous sandwich-themed SQLite
    blog posts on the [SQLite file format](/blog/sqlite-internals-btree/), the [rollback
    journal](/blog/sqlite-internals-rollback-journal/), & the [WAL](/blog/sqlite-internals-wal/),
    then you're probably feeling pretty hungry by now. You're also probably tired
    of the tedium of making sandwiches by hand, so we'll use a sandwich-making machine
    as our analogy in this blog post.\n\nThis machine will do a few tasks:\n\n1. Take
    an order for sandwiches.\n1. Determine the most efficient way to build the sandwiches.\n1.
    Build the sandwiches and hand them to you.\n\nThe process for building and executing
    SQL queries is similar to this sandwich-building process, albeit less delicious.
    Let's dive in.\n\n## Teaching our machine to read\n\nThe first step is to give
    our machine an order. We hand it an order slip that says:\n\n```\nMake 3 BLT sandwiches
    hold the mayo, 1 grilled cheese\n```\n\nTo our computer, this order is just a
    string of individual characters: `M`, `a`, `k`, `e`, etc… If we want to make sense
    of it, we first need to group these letters together into words, or more specifically,
    \"tokens\". This process is called \"tokenizing\" or \"lexing\".\n\nAfter tokenizing,
    we see this list of tokens:\n\n```\n\"MAKE\", \"3\", \"BLT\", \"SANDWICHES\",
    \"HOLD\", \"THE\", \"MAYO\", \",\", \"1\", \"GRILLED\", \"CHEESE\"\n```\n\nFrom
    there, we start the parsing stage. The parser takes in a stream of tokens and
    tries to structure it some way that makes sense to a computer. This structure
    is called an _Abstract Syntax Tree,_ or _AST._\n\nThis AST for our sandwich command
    might look like this:\n\n```\n{\n  \"command\": \"MAKE\",\n  \"sandwiches\": [\n
    \   {\n      \"type\":\"BLT\",\n      \"count\": 3,\n      \"remove\": [\"MAYO\"]\n
    \   },\n    {\n      \"type\": \"GRILLED CHEESE\",\n      \"count\": 1\n    }\n
    \ ]\n}\n```\n\nFrom here, we can start to see how we might take this definition
    and begin building sandwiches from it. We've added structure to an otherwise structure-less
    blob of text.\n\n### Lexing & Parsing SQL\n\nSQLite does this same process when
    it reads in SQL queries. First, it groups characters together into tokens such
    as `SELECT` or `FROM`. Then the parser builds a structure to represent it.\n\nThe
    SQLite documentation provides helpful \"railroad diagrams\" to represent the paths
    the parser can take when consuming the stream of tokens. The [SELECT definition](https://www.sqlite.org/lang_select.html)
    shows how it can start with the `WITH` keyword (for [CTEs](https://www.sqlite.org/lang_with.html))
    and then move into the `SELECT`, `FROM`, and `WHERE` clauses.\n\n![](https://slabstatic.com/prod/uploads/p1b436gf/posts/images/t7Xvs_UNlDc_GnlwSjc6PuWe.png)\n\nWhen
    the parser is done, it outputs the aptly named `Select struct`. If you had a SQL
    query like this:\n\n```\nSELECT name, age FROM persons WHERE favorite_color =
    'lime green'\n```\n\nThen you'll end up with an AST that looks something like
    this:\n\n```\n{\n  \"ExprList\": [\"name\", \"age\"],\n  \"Src\": \"persons\",\n
    \ \"Where\": {\n    \"Left\": \"favorite_color\",\n    \"Right: \"lime_green\",\n
    \   \"Op\": \"eq\"\n  }\n}\n```\n\n## Determining the best course of action\n\nSo
    now that we have our sandwich order AST, we have a plan to make our sandwich,
    right?  Not quite.\n\nThe AST represents what you want—which is a couple of sandwiches.
    It doesn't tell us how to make the sandwiches. Before we get to the plan, though,
    we need to determine the optimal way to make the sandwiches.\n\nOur sandwich-making
    machine can assemble a plethora of different sandwiches, so we stock all kinds
    of ingredients. If we were making a monster sandwich loaded with most of our available
    toppings it might make sense for the machine to visit each ingredient’s location,
    using it, or not, according to the AST.\n\nBut for our BLT, we need only bacon,
    lettuce & tomato. It’ll be way faster if we can have the machine look up the locations
    of just these three toppings in an index and jump directly between them.\n\nSQLite
    has a similar decision to make when planning how to execute a query. For this,
    it uses statistics about its tables' contents.\n\n### Using statistics for faster
    queries\n\nWhen SQLite looks at an AST, there could be hundreds of ways to access
    the data to fulfill a query. The naive approach would be to simply read through
    the whole table and check every row to see if it matches. This what we in the
    biz call a _full table scan_ and it is painfully slow if you only need a few rows
    from a large table.\n\nAnother option would be to use an index to help you quickly
    jump to the rows you need.  An index is a list of row identifiers that are sorted
    by one or more columns, so if we have an index like this:\n\n```\nCREATE INDEX
    favorite_color_idx ON persons (favorite_color);\n```\n\nThen all the row identifiers
    for people who love \"mauve\" are all grouped together in our index. Using the
    index for our query means we have to first read from the index and then jump to
    a row in the table. This has a higher cost per row as it requires two lookups,
    however almost no one likes mauve so we don't have too many matching rows.\n\nBut
    what happens if you search for a popular color like \"blue\"? Searching the index
    first and then jumping to our table for so many rows would actually be slower
    than if we simply searched the entire table.\n\nSo SQLite does some statistical
    analysis on our data and uses this information to choose the (probably) optimal
    recipe for each query.\n\nSQLite's statistics are stored in several \"sqlite\\_stat\"
    tables. These tables have evolved over the years so there're 4 different versions
    of stats but only two are still in use with recent versions of SQLite: `sqlite_stat1`
    & `sqlite_stat4`.\n\nThe `sqlite_stat1` table has a simple format. It stores the
    approximate number of rows for each index and it stores the number of duplicate
    values for the columns of the index. These coarse-grained stats are the equivalent
    of tracking basic averages for a data set—they're not super accurate but they're
    quick to calculate and update.\n\nThe `sqlite_stat4` table is a bit more advanced.
    It will store a few dozen samples of  values that are spread across  an index.
    These finer-grained samples mean that SQLite can understand how unique different
    values are across the key space.\n\n## Executing on our plan\n\nOnce we have an
    optimized plan for building a sandwich, we should have our machine write it down.
    That way if we get the same order again in the future, we can simply reuse the
    plan rather than having to parse & optimize the order each time.\n\nSo what does
    this sandwich plan look like?\n\nThe plan will be recorded as a list of commands
    that the machine can execute to build the BLT again in the future. We don't want
    a command for each type of sandwich, as we may have a lot of different types.
    Better to have a set of common instructions that can be reused to compose any
    sandwich plan.\n\nFor example, we might have the following commands:\n\n- `FIND_INGREDIENT_BIN(ingredient_name)`
    - looks up the bin position of an ingredient.\n- `FETCH_INGREDIENT(bin)` - this
    grabs an ingredient with the machine's robot arm from the given ingredient bin
    number.\n- `APPLY_INGREDIENT` - this puts the ingredient from the robot arm onto
    the sandwich.\n- `GRILL` - this grills the current sandwich.\n\nWe also have one
    more requirement that's not immediately obvious. We only have so much space to
    hold our finished sandwiches so we need to make one sandwich at a time and have
    the customer take it before making the next sandwich. That way we can handle any
    number of sandwiches in an order.\n\nThis process of handing off is called _yielding_
    so we'll have a `YIELD` command when where we wait for the customer to take the
    sandwich.\n\nWe'll also need some control flow so we can make multiple of the
    same kind of sandwich so we'll add a `FOREACH` command.\n\nSo putting our commands
    together, our plan might look like:\n\n```\n// Make our BLT sandwiches\nFOREACH
    1...3\n  bin = FETCH_INGREDIENT_BIN(\"bacon\")\n  FETCH_INGREDIENT(bin)\n  APPLY_INGREDIENT\n\n
    \ bin = FETCH_INGREDIENT_BIN(\"lettuce\")\n  FETCH_INGREDIENT(bin)\n  APPLY_INGREDIENT\n\n
    \ bin = FETCH_INGREDIENT_BIN(\"tomato\")\n  FETCH_INGREDIENT(bin)\n  APPLY_INGREDIENT\n\n
    \ YIELD\nEND\n\n// Make our grilled cheese\nbin = FETCH_INGREDIENT_BIN(\"cheese\")\nFETCH_INGREDIENT(bin)\nAPPLY_INGREDIENT\nGRILL\nYIELD\n```\n\nThis
    set of domain-specific commands and the execution engine to run it is called a
    _virtual machine._ It gives us a level of abstraction that's appropriate for the
    task we're trying to complete (e.g. sandwich making) and it lets us reconfigure
    commands in different ways for different sandwiches.\n\n### Inspecting the SQLite
    Virtual Machine\n\nSQLite's virtual machine is structured similarly. It has a
    set of database-related commands that can execute the steps needed to fetch the
    results of a query.\n\nFor example, let's start with a table of people with a
    few rows added:\n\n```\nCREATE TABLE persons (\n  id INTEGER PRIMARY KEY AUTOINCREMENT,\n
    \ name TEXT,\n  favorite_color TEXT\n);\n\nINSERT INTO persons\n  (name, favorite_color)\nVALUES\n
    \ ('Vicki', 'black'),\n  ('Luther', 'mauve'),\n  ('Loren', 'blue');\n```\n\nWe
    can inspect this with two different SQLite commands. The first command is called
    `EXPLAIN QUERY PLAN` and it gives a very high level plan of the query. If we run
    it for a simple `SELECT` with a conditional then we'll see that it performs a
    table scan of the `persons` table:\n\n```\nsqlite> EXPLAIN QUERY PLAN SELECT *
    FROM persons WHERE favorite_color = 'blue';\n\nQUERY PLAN\n`--SCAN persons\n```\n\nThis
    command can give more information as you do more complex queries. Now let's look
    at the other command to further inspect the plan.\n\nConfusingly, it's called
    the `EXPLAIN` command. Simply drop the \"`QUERY PLAN`\" part of the first command
    and it will show a much more detailed plan:\n\n<div class=\"unwrap\">\n```\nsqlite>
    EXPLAIN SELECT * FROM persons WHERE favorite_color = 'blue';\n\naddr  opcode         p1
    \   p2    p3    p4             p5  comment      \n----  -------------  ----  ----
    \ ----  -------------  --  -------------\n0     Init           0     11    0                    0
    \  Start at 11\n1     OpenRead       0     2     0     3              0   root=2
    iDb=0; persons\n2     Rewind         0     10    0                    0   \n3
    \      Column       0     2     1                    0   r[1]=persons.favorite_color\n4
    \      Ne           2     9     1     BINARY-8       82  if r[1]!=r[2] goto 9\n5
    \      Rowid        0     3     0                    0   r[3]=rowid\n6       Column
    \      0     1     4                    0   r[4]=persons.name\n7       Column
    \      0     2     5                    0   r[5]=persons.favorite_color\n8       ResultRow
    \   3     3     0                    0   output=r[3..5]\n9     Next           0
    \    3     0                    1   \n10    Halt           0     0     0                    0
    \  \n11    Transaction    0     0     1     0              1   usesStmtJournal=0\n12
    \   String8        0     2     0     blue           0   r[2]='blue'\n13    Goto
    \          0     1     0                    0   \n```\n</div>\n\nThis is the \"plain
    English\" representation of the byte code that your query is compiled down to.
    This may look confusing but we can walk through it step-by-step to break it down.\n\n###
    The SQLite virtual machine instruction set\n\nJust like how a computer has low-level
    CPU operations such `MOV` and `JMP`, SQLite has a similar instruction set but
    it's just at a higher level. As of this writing, there are 186 commands, or _opcodes_,
    that the SQLite VM can understand. You can find the [full specification](https://sqlite.org/opcode.html#the_opcodes)
    on the SQLite web site but we'll walk through a couple of them here.\n\nThe first
    opcode is an [Init](https://sqlite.org/opcode.html#Init) which initializes our
    execution and then jumps to another instruction in our program. The parameters
    for the opcodes are listed as `p1` through `p5` and their definition is specific
    to each command. For the `Init` opcode, it jumps to the instruction listed in
    `p2` which is `11`.\n\nAt address `11` we arrive at the [Transaction](https://sqlite.org/opcode.html#Transaction)
    opcode which starts our transaction. For most opcodes, the VM will move to the
    next address after executing the instruction so we move to address `12`. This
    [String8](https://sqlite.org/opcode.html#String8) opcode stores string value `\"blue\"`
    into register `r[2]`. The registers act like a set of memory addresses and are
    used to store values during execution. We'll use this value later for our equality
    comparison.\n\nNext, we move to address `13` which is a [Goto](https://sqlite.org/opcode.html#Goto)
    instruction which has us jump to the instruction listed in its `p2` parameter,
    which is address `1`.\n\nNow we get into the row processing. The [OpenRead](https://sqlite.org/opcode.html#OpenRead)
    instruction opens a _cursor_ on the `persons` table. A cursor is an object for
    iterating over or moving around in a table. The next instruction, [Rewind](https://sqlite.org/opcode.html#Rewind),
    moves the cursor to the first entry of the database to begin our table scan.\n\nThe
    [Column](https://sqlite.org/opcode.html#Column) instruction reads the `favorite_color`
    column into register `r[1]` and the [Ne](https://sqlite.org/opcode.html#Ne) instruction
    compares it with the `\"blue\"` value in register `r[2]`. If the values don't
    match then we'll move to the [Next](https://sqlite.org/opcode.html#Next) instruction
    at address `9`. If they do match, we'll fill in registers `r[3]` , `r[4]`, & `r[5]`
    with the column `id`, `name`, & `favorite_color` for the row.\n\nFinally, we get
    to where we can yield the result back to the caller using the [ResultRow](https://sqlite.org/opcode.html#ResultRow)
    instruction. This will let the calling application copy out the values in registers
    `r[3…5]`. When the calling application calls `sqlite3_step()`, the program will
    resume from where it left off by calling [Next](https://sqlite.org/opcode.html#Next)
    and jumping back to re-execute the row processing at instruction `3`.\n\nWhen
    Next no longer produces any more rows, it'll jump to the [Halt](https://sqlite.org/opcode.html#Halt)
    instruction and the program is done.\n\n## Wrapping up our sandwich processing
    engine\n\nThe query execution side of SQLite follows this simple parse-optimize-execute
    plan on every query that comes into the database. We can use this knowledge to
    improve our application performance. By using bind parameters in SQL statements
    (aka those `?` placeholders), we can prepare a statement once and skip the parse
    & optimize phases every time we reuse it.\n\nSQLite uses a virtual machine approach
    to its query execution but that's not the only approach available. Postgres, for
    example, uses a node-based execution plan which is structured quite differently.\n\nNow
    that you understand the basics of how an execution plan works, try running `EXPLAIN`
    on one of your more complex queries and see if you can understand the step-by-step
    execution of how your query materializes into a result set for your application.\n"
- :id: phoenix-files-live-session
  :date: '2022-09-05'
  :category: phoenix-files
  :title: Live sessions in action
  :author:
  :thumbnail: live-session-thumbnail.jpg
  :alt:
  :link: phoenix-files/live-session
  :path: phoenix-files/2022-09-05
  :body: "\n<p class=\"lead\">This is a post about the benefits we can get from using
    `live_session`, and the super powers we get from combining it with hooks. If you
    want to deploy your Phoenix LiveView app right now, then check out how to [get
    started](/docs/elixir/). You could be up and running in minutes.</p>\n\nPhoenix
    LiveView often makes us feel like \"wow, that was really fast!\" and that is not
    a coincidence. Behind  [_LiveView's magic_](https://fly.io/blog/liveview-its-alive/),
    there are a bunch of design decisions, but also interesting features we can use.\n\nUnder
    the umbrella of LiveView navigation we have the [live_session/3](https://hexdocs.pm/phoenix_live_view/Phoenix.LiveView.Router.html)
    macro  to group live routes into live sessions. We can navigate between the routes
    in the same session over the existing websocket, without any additional HTTP  request,
    thus making navigation between live routes even faster!\n\nBut `live_session`
    also has some other interesting ramifications. Today we'll take a closer look
    at this feature and see that it has more going for it than is immediately obvious.\n\n##
    Live_session in action\n\nWhen we navigate between different pages within our
    application using LiveView, every time we want to mount a new root LiveView, an
    HTTP request is made. Then, a connection with the server is established and the
    LiveView to be rendered is mounted. Which means that a full page reload is being
    done.\n\nIf we look at our iex console, each of these steps is described in the
    logs:\n\n![](logs.jpg?card&center)\n\nWouldn't it be better if we could switch
    between LiveViews without making any HTTP requests (saving a couple ms in the
    process)? That's when `live_session` comes into play!\n\nLet's start learning
    how to group different live routes using `live_session`.\n\nIn our `router.ex`
    file, we define a `live_session` and give it an atom as a name:\n\n```elixir\nscope
    \"/\", MyAppWeb do\n  pipe_through :browser\n  \n  live_session :default do\n
    \   live \"/users/register\", UserRegistrationLive, :new\n    live \"/users/log_in\",
    UserLoginLive, :new\n    live \"/users/reset_password\", UserForgotPasswordLive,
    :new\n  end\nend \n```\n\nThis way, we can navigate between each route within
    our `:default` session without any additional HTTP requests, using [live_redirect](https://hexdocs.pm/phoenix_live_view/Phoenix.LiveView.Helpers.html#live_redirect/2)
    in our LiveView's templates:\n\n```elixir\n<%%= live_redirect \"Register\", to:
    \"/users/register\" %>\n```\n\nLet's see it in action:\n\n<%= video_tag \"navigating_with_live_redirect.mp4?card&center&1/2\",
    title: \"User is changing between pages and logs are printed in the iex console\"
    %>\n\n<aside class=\"right-sidenote\">\nHowever, keep in mind that live navigation
    between LiveViews of different sessions using `live_redirect` is not possible
    since it will be necessary to do a full page reload, going through the plug pipeline.\n</aside>\n\nEvery
    time we navigate through our authentication system a new LiveView is mounted,
    but this time, as we can see in the logs, there is no additional HTTP request!\n\n\nThis
    is nice and fast. But the separation between routes in different sessions is where
    we start to see more possibilities!\n\n## Attaching hooks to a group of routes\n\nWe
    can attach hooks in the mount lifecycle of each LiveView in the session just by
    combining the `live_session` macro with the `on_mount` callback.\n\nFirst, we
    define `on_mount` functions that will be invoked on our LiveView's mount. For
    example:\n\n```elixir\ndefmodule MyAppWeb.UserAuth do\n  \n  def on_mount(:default,
    _params, session, socket) do\n    {:cont, socket}\n  end\n  \n  def on_mount(:ensure_user_is_admin,
    _params, session, socket) do\n    if session[\"current_user_role\"] == \"admin\"
    do \n      {:cont, socket}\n    else\n      {:halt, socket}\n    end\n  end\nend\n```\n\nOur
    `:ensure_user_is_admin` hook stops the mounting process if the user is not an
    admin, and continues the process otherwise. These outcomes are accomplished by
    returning the tuples `{:halt, socket}` and `{:cont, socket}`, respectively.\n\nOnce
    our hook is defined, we can attach it to our session by using the `on_mount` option.
    We pass a tuple with our module's name and the name of the hook we defined above:\n\n\n```elixir\nlive_session
    :admin,\n  on_mount: {MyAppWeb.UserAuth, :ensure_user_is_admin} do\n  live \"/settings\",
    SettingsLive, :edit\n  ...\nend\n```\n\nWhat if we want to attach more than one
    hook per session? we can do it by defining a list of them:\n\n<aside class=\"right-sidenote\">\nYou
    can also attach a specific hook to a particular LiveView, by calling it at the
    beginning of the LiveView definition.\n</aside>\n\n```elixir\nlive_session :admin,
    \n  on_mount: [\n    MyAppWeb.UserAuth,\n    {MyAppWeb.UserAuth, :mount_current_user},
    \n    {MyAppWeb.UserAuth, :ensure_user_is_admin}\n  ] do\n  \n  live \"/settings\",
    SettingsLive, :index\n  ...\nend\n```\n\nDid you notice that the first element
    of the `on_mount` list is different? If you call a hook without specifying a name,
    LiveView will default to the `:default` hook.\n\nWe can use this to add custom
    behavior to our routes or to define different authorization strategies.\n\nFor
    example, we can protect routes from unauthenticated users; conversely we can redirect
    authenticated users from LiveViews that don't make sense for them (like a login
    page).\n\nLet's see how we can handle this example in a secure way.\n\n## Security
    considerations for authorization strategies\n\nWhen we have a regular web application
    and we want to perform authentication and authorization operations on each of
    the routes in a scope; we usually define plug functions with the necessary validations,
    then we group them into pipelines, and finally, we pipe each of the routes through
    those security pipelines.\n\nThinking in live routes, if we want to secure each
    of our live routes, is it enough to use the same security pipelines we define
    for regular routes? Let's think about it.\n\nWhen we first mount a LiveView within
    a session or redirect between different sessions, an HTTP request is made. Which
    means, all our routes will pipe through our security pipelines. That's good!\n\nHowever,
    what happens when we navigate between LiveViews within the same session? The same
    stateful connection is used to mount the new LiveView, and no HTTP requests are
    made. Which means: no security pipelines at all!\n\n![](initial_request.jpg?card&center)\n\n![](live_redirect.jpg?card&center)\n\nSo,
    if we want to secure each of the routes within a session, we must do it in another
    way. Fortunately, we've already learned how to do it: we just have to define all
    our authorization logic inside hooks!\n\nLet's do it:\n\n```elixir\nscope \"/\",
    MyAppWeb do\n  pipe_through [:browser, :redirect_if_user_is_authenticated]\n\n
    \ live_session :only_unauthenticated_users,\n    on_mount: [\n      {MyAppWeb.UserAuth,
    :redirect_if_user_is_authenticated}\n    ] do\n    live \"/users/register\", UserRegistrationLive,
    :new\n    live \"/users/log_in\", UserLoginLive, :new\n    live \"/users/reset_password\",
    UserForgotPasswordLive, :new\n  end\n\n  post \"/users/log_in\", UserSessionController,
    :create\nend\n\n```\n\nIn order to secure all the routes in the `/` scope, we
    applied both security mechanisms we mentioned earlier. Plugs/pipelines to secure
    web requests, and hooks to secure each of the routes in the session, on the LiveView's
    mount.\n\n## Using a different root layout for grouped like routes\n\nNavigating
    among live routes within a single live session lets you avoid the overhead of
    a full page reload. This prevents the root layout from changing, which you should
    keep in mind when grouping LiveViews into live sessions!\n\nConversely, putting
    routes into different live sessions forces a page reload through the plug pipeline
    on navigation between them. This is an opportunity that `live_session` doesn't
    waste: it takes a `:root_layout` option to let you specify the root layout for
    the member LiveViews all at once.\n\nLet's see how we can do it:\n\n```elixir\nlive_session
    :admin, root_layout: {MyAppWeb.LayoutView, \"root_admin.html\"} do\n  live \"/users/settings\",
    UserSettingsLive, :edit\nend\n```\n\nThis way we can customize what we show to
    our users. For example, we can show one navigation bar for admin users and a different
    one for regular users.\n\n<%= partial \"shared/posts/cta\", locals: {\n  title:
    \"Fly.io ❤️ Elixir\",\n  text: \"Fly.io is a great way to run your Phoenix LiveView
    app close to your users. It's really easy to get started. You can be running in
    minutes.\",\n  link_url: \"https://fly.io/docs/elixir/\",\n  link_text: \"Deploy
    a Phoenix app today!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>\"\n} %>\n\n##
    Bonus example: Set common assigns at the router level\n\nWe can use  `live_session`
    and `:on_mount` to set the common assigns for a group of live routes; all of this
    just in one place!\n\nSetting assigns at the router level is useful to avoid  setting
    assigns on every LiveView or forgetting to do it in some of them. For example,
    we can define different hooks to set our active menu item:\n\n```elixir\ndefmodule
    Web.MenuAssign do\n  @moduledoc \"\"\"\n  Ensures common `assigns` are applied
    to all LiveViews attaching this hook.\n  \"\"\"\n  import Phoenix.LiveView\n\n
    \ def on_mount(:settings, _params, _session, socket) do\n    {:cont, assign(socket,
    :active_item, :settings)}\n  end\n\n  def on_mount(:preferences, _params, _session,
    socket) do\n    {:cont, assign(socket, :active_item, :preferences)}\n  end\nend\n```\n\nThen
    in the router, we block off a whole section of routes and they will get the `:active_item`
    assign set automatically.\n\n```elixir\n  live_session :preferences,\n    on_mount:
    [\n      Web.InitAssigns,\n      {Web.InitAssigns, :require_current_user},\n      {Web.MenuAssign,
    :preferences}\n    ] do\n\n    scope \"/preferences\", as: :preferences do\n      live
    \"/avatar\", AvatarPreferenceLive.Index, :index\n      live \"/notifications\",
    NotificationsPreferenceLive.Index, :new\n    end\n  end\n```\n\nAll routes within
    the `live_session :preferences`  have set the assign `:active_item` with the value
    `:preferences`\n\n## Wrap up\n\n`live_session` by itself gives us faster navigation
    between live routes, but it gains super powers and becomes doubly useful in combination
    with the `on_mount` callback.\n\nWe can use these features to mark boundaries
    between routes that look or behave differently. In the first case, we can define
    a specific root layout for a group of LiveViews; in the second one, we can use
    hooks to modify the behavior of a session's routes.\n"
- :id: laravel-bytes-taking-laravel-global
  :date: '2022-08-30'
  :category: laravel-bytes
  :title: Taking Laravel Global
  :author: fideloper
  :thumbnail: global-laravel-thumbnail.png
  :alt:
  :link: laravel-bytes/taking-laravel-global
  :path: laravel-bytes/2022-08-30
  :body: "\n\n<p class=\"lead\">Fly.io empowers you to run apps globally, close to
    your users. If you want to ship a Laravel app, [try it out on Fly.io](/docs/laravel/).
    It takes just a couple of minutes.</p>\n\nFly.io puts your application close to
    your users by making global deployments easy. It's literally just [a few commands](/docs/reference/scaling/#regions-and-scaling).\n\nHere's
    an example of showing an app, currently in DFW (Dallas), scaled out to also include
    Frankfurt and Singapore:\n\n```bash\n# Add 2 additional regions\nfly regions add
    fra sin\n\n# Scale VM count to fill your regions\nfly scale count 3\n```\n\nTwo
    commands! User requests will be routed to the nearest application instance. \n\nThat's
    the easy part. The hard part is making sure your application behaves in it's new,
    globally available setup. Let's explore what you need to know.\n\n\n\n## Where's
    the data?\n\nI'm not burying the lede here. This TL;DR of this article is asking
    yourself \"Where's my data?\" and coming up with an answer.\n\nThis is similar
    to good-old-fashion load balancing, but with a twist. With a load balancer, you
    need to figure out how every application instance sees the same data. \nIn a global
    environment, we also need to figure out how to do that quickly.\n\nThese are solved
    problems, but the solutions involve trade-offs! Grab a cup of coffee while we
    explore the problems and solutions to global infrastructure.\n\n\n<%= partial
    \"shared/posts/cta\", locals: {\n  title: \"Deploy Globally on Fly\",\n  text:
    \"Fly.io makes running full-stack applications easier than ever before. Give it
    a whirl!\",\n  link_url: \"https://fly.io/docs/laravel/\",\n  link_text: \"Deploy
    your Laravel app!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>\"\n} %>\n\n\n##
    Problem: File Storage\n\nWe'll start with the lowly file. If your application
    accepts user-uploaded files, you have to decide where to put them.\n\nBy default,
    Laravel saves files to the local disk. If your application is served from multiple
    application instances, this is problematic.\n\nIf a user uploads a file into Dallas,
    that file file won't exist in Singapore.\n\nFly.io provides volumes for persistent
    storage, however volumes are pinned to a region. They don't replicate around the
    world. Maybe someday!\n\nThe answer here is to upload files to a central storage
    location - typically [S3 or some compatible storage](https://laravel.com/docs/9.x/filesystem#amazon-s3-compatible-filesystems)
    service.\n\nThe least-annoying way to accomplish this is using Laravel's [File
    Storage](https://laravel.com/docs/9.x/filesystem), which allows you to set a driver
    to some non-local place like S3, DO Spaces, CloudFlare R2, or what-have-you.\n\n##
    Problem: Session Storage\n\nSessions are usually stored as files on the local
    disk. Sounds familiar, right? We just talked about file storage!\n\nSimilar to
    storing files, we need to put our session data were every application instance
    can see it.\n\nRedis is a very popular place to store sessions.\n\nUnfortunately,
    replicating Redis across multiple, global regions is slow enough to be noticeable.
    Sessions are stored and retrieved on just about every web request, so replication
    lag could be killer.\n\n**The simplest solution here is to use Cookie-based session
    storage.** This is just a configuration change in Laravel - setting `SESSION_DRIVER=cookie`
    in your `.env` file.\n\nCookie-based sessions store data in a cookie. Cookies
    are stored in web browsers, instead of somewhere in your infrastructure. The user's
    browser will always send the cookie alongside web requests, and thus the session
    data is accessible by your Laravel application.\n\nIt's elegant in its simplicity
    - you pushed the storage problem off to your users, and they don't have to know
    nor care about that implementation detail. That's the best kind of solution!\n\nFor
    security, I recommend the following settings:\n\n```\n# Enable cookie-based sessions\nSESSION_DRIVER=cookie\n\n#
    Only let them work when https:// is used\nSESSION_SECURE_COOKIE=true\n```\n\nAdditionally,
    you may want to update `config/sessions.php` in order to set `encrypt => true`,
    ensuring the cookie session data is encrypted. There's a few more settings related
    to security available in `config/session.php`. Be sure to review them.\n\nHere's
    the main trade-off with cookie-based sessions. **Cookies can only store a limited
    amount of data.** That limit is roughly 4096 bytes (browser-dependent). If you
    use session storage to store a bunch of data - maybe don't! Or investigate other
    means of saving data between requests.\n\n<div class=\"callout\">Session storage
    in Laravel is typically used for things like flash messages, and validation error
    messages.</div>\n\n\n\n### Maybe Use File-Based Sessions Anyway?\n\nFly.io's global
    network uses [BGP](https://www.cloudflare.com/learning/security/glossary/what-is-bgp/)
    to send user requests to the nearest \"edge\" location. Fly.io knows what application
    instance is closest to that edge location, and routes requests to it.\n\nAssuming
    your users aren't zooming around the world, it's a safe bet that any given user
    will likely be sending requests to the same region every time.\nWith that assumption,
    we could actually use file-based sessions!\n\nThe trade-off is the odd case where
    a user get routed to another region for some reason. They'd no longer be logged
    in! This is possible if an application instance in a region becomes unhealthy
    or if there are some global BGP-related issues (it is DNS, after all).\n\n\n###
    Multiple App Instances in One Region\n\nIf you decide that users will always be
    served from one region anyway, you may still end up with a problem.\n\nWhat if
    you have multiple application instances in one region? Fly.io supports that use
    case - it will load-balance requests between all application instances in a region.\n\nThis
    means each application instance within a given region would need access to a central
    session store.\n\nIn that case, you might want to setup a single Redis instance
    per region to store sessions. You'd have to do a bit of extra work in your Laravel
    configuration to ensure each application instance connected to the correct Redis
    host\t.\n\n## Problem: Cache Storage\n\nCaching is an issue similar to session
    storage. You may first reach for file-based caching, but then realize that you
    end up with a cache per application instance. Each application instance's cache
    can get out of sync easily.\n\nLike with Sessions, you might get away with an
    application-specific cache.\n\nHowever you need to be careful about data \"correctness\".
    If your cache has different data in each region, is that an issue? I can't tell
    you - that depends on your application!\n\nOne thing I can say is that caching
    is one of those things where \"eventual consistency\" usually is okay. That means
    using a distributed Redis setup might work!\n\nIn fact, you may be able to have
    your cake and eat it too. We wrote up a Redis-backed caching strategy that could
    let you have [both global and regional cache storage using one Redis cluster](/blog/last-mile-redis/)!\n\nDon't
    sleep on that article.\n\n\n\n## Problem: Proxy-awareness\n\nApplications in Fly.io
    are served from behind a proxy. This proxy routes requests to the nearest application
    instance from your users.\n\nBecause a proxy is involved, your app needs to be
    aware of the `X-Forwarded-*` headers.\n\n<div class=\"callout\">If you use the
    `fly launch` command to initialize your Laravel application, Fly.io takes care
    of that.</div>\n\nWithin Laravel, the `X-Forwarded-*` headers effect your application's
    ability to:\n\n1. See correct client information (e.g. getting the correct IP
    address)\n1. Generate URL's correctly (e.g. ensuring `https://` is used)\n\nYour
    app needs to be told to trust Fly.io's proxy, which can be done in the `TrustProxies`
    middleware. You can update your `app/Http/Middleware/TrustProxies.php` file and
    set `protected $proxies = '*';`.\n\nPro-tip: You can see what headers the Fly-Proxy
    is sending your application using [https://debug.fly.dev/](https://debug.fly.dev/)\n\n##
    Problem: The Database\n\nThe phrase \"mission critical\" is often found just a
    few words away from \"database\". We need our data, and we like our [ACID](https://en.wikipedia.org/wiki/ACID)!\n\nFlying
    in the face of data consistency (and speedy queries) is running an application
    globally. Having the same data (with any consistency) in multiple regions around
    the world is a tough nut to crack.\n\nWe don't want an app server in Singapore
    having to talk to a database server in Dallas. The speed of light and the number
    of network hops required make that agonizingly slow for the typical full-stack
    application.\n\nSo, here's a solution that threads the needle between \"easy to
    implement\" and \"reasonably fast\":\n\n1. Have a primary database. It's the only
    place that accepts **write** queries\n1. Place **read-replicas** in multiple global
    regions - preferably each one is close to (if not within) each region hosting
    your application\n1. Setup [Laravel's built-in feature](https://laravel.com/docs/9.x/database#read-and-write-connections)
    to route write and read queries to the proper database instances\n1. Use  the
    [Fly-Replay header](/docs/reference/fly-replay/) to route traffic that result
    in write queries to the primary region\n\nHow are you expected to do all of that!?
    [Here's how to run MySQL globally on Fly.io with PlanetScale](/laravel-bytes/multi-region-laravel-with-planetscale/).\n\nThat
    article covers the complete setup, and goes into details about how to handle sending
    write queries across the globe in an efficient manner.\n\n## Oh, is that all?\n\nThat's
    a lot of stuff to care about! Full-stack frameworks need to store data for a bunch
    of use cases - your applications data, session storage, caching (if you use it),
    and more.\n\n\nThis all presents some new challenges in a global environment.
    We not only need every app instance to see the same data, but access it quickly
    as well. Fly.io makes this uniquely easy, but we still need to know how this affects
    our application configuration.\n"
- :id: blog-sqlite-internals-wal
  :date: '2022-08-24'
  :category: blog
  :title: How SQLite Scales Read Concurrency
  :author: ben
  :thumbnail: internals-thumbnail.jpg
  :alt: A photo album of all my beloved sandwiches.
  :link: blog/sqlite-internals-wal
  :path: blog/2022-08-24
  :body: |2


    <div class="lead">
      [Fly.io](http://fly.io/) runs apps close to users around the world, by taking containers and upgrading them to full-fledged virtual machines running on our own hardware around the world. Sometimes those containers run SQLite and we make that easy too. [Give us a whirl](https://fly.io/docs/speedrun/) and get up and running quickly.
    </div>

    If you scour Hacker News & Reddit for advice about databases, some common words of caution are that SQLite doesn't scale or that it is a single-user database and it's not appropriate for your web-scale application.

    Like any folklore, it has some historical truth. But it's also so wildly out-of-date.

    In our [last post](/blog/sqlite-internals-rollback-journal/), we talked about the _rollback journal._ That was SQLite's original transactional safety mechanism and it left much to be desired in terms of scaling.

    In 2010, SQLite introduced a second method called the _write-ahead log_, or as it's more commonly referred to: the WAL.

    ## Refresher on the rollback journal

    The rollback journal worked by copying the old version of changed pages to another file so that they can be copied back to the main database file if the transaction rolls back.

    The WAL does the opposite. It writes the new version of a page to another file and leaves the original page in-place in the main database file.

    So how does this simple change enable SQLite to scale? Let's revisit our sandwich shop example from the [last post](/blog/sqlite-internals-rollback-journal/) to see how the WAL would make things run more smoothly.

    ## Sandwiches at scale

    In our sandwich shop example, we had to choose between a single sandwich maker making sandwiches and one or more inventory specialists inventorying ingredients. We couldn't do both at the same time. This is how the rollback journal works; a writer can alter the database or readers can read from the database—but not both at the same time.

    This is a problem since making a complicated, time-consuming sandwich will prevent inventory from being taken. Also, a single slow inventory counter will prevent any sandwiches from being made.

    A simple solution would be to take a photo of every ingredient after a sandwich is made. That way inventory counters could look at the photos to take inventory. However, it would be slow and inefficient to take a photo of every ingredient after a sandwich since many ingredients wouldn't change. For example, if you're making a grilled cheese then you're not going to touch the pickles, right? Right!?

    A better solution would be to only take photos of the ingredients you took from after each sandwich. You can add these photos to a binder and now the inventory folks can see a point-in-time snapshot of the ingredients without interfering with the sandwich maker.

    This is how the WAL works, in concept.

    ## Enabling the sandwich log

    Let's create a `sandwiches.db` that will store our current count of ingredients:

    ```sql
    CREATE TABLE ingredients (
        id INTEGER PRIMARY KEY,
        name TEXT,
        count INTEGER
    );
    ```

    To enable our write-ahead log journaling mode, we just need to use the `journal_mode` PRAGMA:

    ```sql
    PRAGMA journal_mode = wal;
    ```

    Internally, this command performs a rollback journal transaction to update the database header so it's safe to use like any other transaction. The database header has a read & write version at bytes 18 & 19, respectively, that are used to determine the journal mode. They're set to `0x01` for rollback journal and `0x02` for write-ahead log. They're typically both set to the same value.

    ## Altering the ingredient count

    Now that we are using the WAL, we can add our initial inventory counts:

    ```sql
    INSERT INTO ingredients (name, count)
    VALUES ('onions', 10), ('tomatoes', 5), ('lettuce', 20);
    ```

    Instead of updating our `sandwiches.db` database file, our change is written to a `sandwiches.db-wal` file in the same directory. Let's fire up our `hexdump` tool and see what's going on.

    ### Starting with the WAL header

    The WAL file starts with a 32-byte header:

    ```
    377f0682 002de218 00001000 00000000 5a20ee38 f926b5d3 0dd5236d 9972220b
    ```

    Most SQLite files start with a [magic number](https://en.wikipedia.org/wiki/Magic_number_(programming)) and the WAL is no exception. Every WAL file starts with either `0x377f0682` or `0x377f0683` which indicate whether checksums in the file are in little-endian or big-endian format, respectively. Nearly all modern processors are little-endian so you'll almost always see `0x377f0682` as the first 4 bytes of a SQLite WAL file.

    Next, the `0x002de218` is the WAL format version. This is the big-endian integer representation of `3007000` which means it's the WAL version created in SQLite 3.7.0. There's currently only one version number for WAL files.

    The next four bytes, `0x00001000`, are the page size. We're using the default page size of 4,096 bytes. The next four after that are `0x00000000` which is the checkpoint sequence number. This is a number that gets incremented on each _checkpoint_. We'll discuss checkpointing later in this post.

    After that, we have an 8-byte "salt" value of `0x5a20ee38f926b5d3`. The term "salt" is typically used in cryptography (and sandwiches, actually) but in this case it's a little different. Sometimes the WAL needs to restart from the beginning but it doesn't always delete the existing WAL data. Instead it just overwrites the previous WAL data.

    In order for SQLite to know which WAL pages are new and which are old, it writes the salt to the WAL header and every subsequent WAL frame. If SQLite encounters a frame whose salt doesn't match the header's salt, then it knows that it's a frame from an old version of the WAL and it ignores it.

    Finally, we have an 8-byte checksum of `0x0dd5236d9972220b` which is meant to verify the integrity of the WAL header and prevent partial writes. If we accidentally overwrite half the header and then the computer shuts down, we can detect that by calculating and comparing the checksum.

    ### Adding WAL entries

    The WAL works by appending new data to the `-wal` file so we'll see an entry for a single page in our WAL file. It starts with a 24-byte header and then writes 4,096 bytes of page data.

    ```
    00000002 00000002 5a20ee38 f926b5d3 5333783e 42122b82 [page data...]
    ```

    The first 4 bytes, `0x00000002`, are the page number for the entry. This is saying that our page data following the header is meant to overwrite page 2.

    The next 4 bytes, also `0x00000002`, indicate the database size, in pages, after the transaction. This field actually performs double duty. For transactions that alter multiple pages, this field is only set on the last page in the transaction. Earlier pages set it to zero. This means we can delineate sections of the WAL by transaction. It also means that a transaction isn't considered "committed" until the last page is written to the WAL file.

    After that we see our salt value `0x5a20ee38f926b5d3` copied from the header. This lets us know that it is a contiguous block of WAL entries starting from the beginning of the WAL. Finally, we have an 8-byte checksum of `0x5333783e42122b82` which is computed based on the WAL header checksum plus the data in the WAL entry and page data.

    ### Overlaying our WAL pages

    Every transaction that occurs will simply write the new version of changed pages to the end of the WAL file. This append-only approach gives us an interesting property. The state of the database can be reconstructed at any point in time simply by using the latest version of each page seen in the WAL starting from a given transaction.

    In the diagram below, we have the logical view of a b-tree inside SQLite and an associated WAL file with 3 transactions. Before the WAL file exists, we have three pages in our database, represented in black.

    The first transaction (in green) updates pages 1 and 2. A snapshot of the database after this transaction can be constructed by using WAL entries for pages 1 and 2 and the original page from the database for page 3.

    Our second transaction (in red) only updates page 2. If we want a snapshot after this transaction, we'll use page 1 from the first transaction, page 2 from the second transaction, and page 3 from the database file.

    The last transaction (in orange) updates pages 2 & 3 so the entire b-tree is now read from the WAL.

    ![Several states of the b-tree as represented in transactions starting at different points of time.](btrees.png)

    The beauty of this approach is that we are no longer overwriting our pages so every transaction can reconstruct its original state from when it started. It also means that write transactions can occur without interfering with in-progress read transactions.

    ## Retiring our gastronomical photo album

    You may have noticed one problem with our append-only album of ingredient photos—it keeps getting bigger. Eventually it will become too large to handle. Also, we really don't care about the ingredient photos that we took 400 sandwiches ago. We only want to allow sandwich makers and inventory counters to do their work at the same time.

    In SQLite, we resolve this issue with the "checkpointing" procedure. Checkpointing is when SQLite copies the latest version of each page in the WAL back into the main database file. In our diagram below, page 1 is copied from the first transaction but pages 2 & 3 are copied from the third transaction. The prior versions of page 2 are ignored because they are not the latest.

    ![The latest version of each page is copied back to the main database in a process called checkpointing.](checkpointing.png)

    SQLite can perform this process incrementally when old transactions are not in use but eventually, it needs to wait until there are no active transactions if it wants to fully checkpoint and restart the WAL file.

    This naive approach can be problematic on databases that constantly have open transactions as SQLite will not force a checkpoint on its own and the WAL file can continue to grow. You can force SQLite to block new read & write transactions so it can restart the WAL by issuing a [wal_checkpoint](https://www.sqlite.org/pragma.html#pragma_wal_checkpoint) PRAGMA:

    ```sql
    -- restart but don't truncate WAL
    PRAGMA wal_checkpoint(RESTART);
    ```

    or

    ```sql
    -- restart and truncate WAL to zero bytes
    PRAGMA wal_checkpoint(TRUNCATE);
    ```

    ## Building a photo index

    As our photo album grows, it gets slower to find the latest version of each photo in order to reconstruct our sandwich shop state. You have to start from the beginning of the album and find the last version of a page every time you want to look up a photo.

    A better option would be to have an index in the photo album that lists all the locations of photos for each ingredient. Let's say you have 20 photos of banana peppers. You can look up "banana peppers" in the index and find the location of the latest one in the album.

    ### Writing our index

    SQLite builds a similar index and it's stored in the "shared memory" file, or SHM file, next to the database and WAL files.

    But SQLite's index is a bit funny looking at first glance: it's a list of page numbers and a hash map. The goal of the index is to tell the SQLite client the latest version of a page in the WAL up to a given position in the WAL. Since each transaction starts from a different position in the WAL, they can have different answers to exactly which version of a page they see.

    The SHM index is built out of 32KB blocks that each hold 4,096 page numbers and a hash map of 8,192 slots. When WAL entries are written, their page numbers are inserted into the SHM's page number list in WAL order. Then a hash map position is calculated based on the page number and the index of the page number is stored in the hash map.

    Clear as mud? Let's walk through an example.

    ![Diagram of the shared memory file which is constructed of a list of page number and a hash map to the indexes of the page number list.](shm.png)

    In the diagram above, our first transaction in the WAL (green) updates pages 1 & 2. They get written to the WAL, but the page numbers are also added to the page numbers list in the SHM file. SQLite calculates a hash map slot position for each page using the formula: `(pgno * 383)%8192`. In the hash map slot, we'll write the index of the page in our page number list. This is also `1` & `2`, respectively. Don't read too much into the exact hash map positions in the diagram. There's some loss of fidelity in simplifying 8,192 slots down to 14!

    This hash-based mapping will generally spread out our page numbers across our hash map and leave empty space between them. Also, we are guaranteed to have a lot of empty slots in the hash map since there are double the number of hash map slots as there are page number spots. This will be useful when looking up our pages later.

    Our next transaction (red) only updates page 2. We'll write that page to the third entry in our page number list. However, when we calculate our hash map position, we have a collision with the entry for page 2 in transaction 1. They both point at the same slot. So instead of writing to that slot, we'll write our page number list index, `3`, to the next empty slot.

    Finally, our third transaction updates pages 2 & 3. We'll write those to indexes `4` & `5` in our page number list and then write those indexes to our hash map slots. Again, our page 2 collides with updates in the first two transactions so we'll write the index to the first empty slot after the hash map position.

    ### Reading our index

    Now that we have our index built, we can quickly look up the latest version of any given page for a transaction. Let's say we've started a read transaction just after transaction #2 completed. We only want to consider versions of pages in the WAL up to entry #3 since WAL entries in index 4 & 5 occurred after our read transaction started.

    If we want to look up page 2, we'll first calculate the hash position and then read all the indexes until we reach an empty slot. For page 2, this is indexes 2, 3, & 4. Since our transaction started at entry #3, we can ignore entries after index 3 so we can discard index 4. Out of this set, index 3 is the latest version so our SQLite transaction will read page 2 from WAL entry 3.

    Astute readers may notice that we can have collisions across multiple pages. What happens if page 2 & page 543 in a database compute the same slot? SQLite will double check each entry in the page numbers list to make sure it's the actual page we're looking for and it will discard any others automatically.

    ## Choosing a journaling strategy

    While there are always trade-offs between design choices, the vast majority of applications will benefit from WAL mode. The SQLite web site [helpfully lists some edge cases](https://sqlite.org/wal.html) where the rollback journal would be a better choice such as when using multi-database transactions. However, those situations are rare for most applications.

    Now that you understand how data is stored and transactions are safely handled, we'll take a look at the query side of SQLite in our next post which will cover the SQLite Virtual Machine.
- :id: ruby-dispatch-making-sense-of-rails-assets
  :date: '2022-08-22'
  :category: ruby-dispatch
  :title: Making Sense of Rails Assets
  :author: brad
  :thumbnail: making-sense-of-rails-assets-thumbnail.jpg
  :alt:
  :link: ruby-dispatch/making-sense-of-rails-assets
  :path: ruby-dispatch/2022-08-22
  :body: |2


    **The Rails asset ecosystem is at peak complexity as it transitions from Sprockets to Importmaps and Propshaft, by way of Webpacker. How does it affect people who build Rails apps? How should Rails plugin developers navigate the transition?**

    Rails has always prided itself on convention over configuration and as a result, the community favors having one way to solve a problem vs. a bunch of different ways where a choice has to be made. That's why the state of the asset pipeline is so surprising—there's a lot of different ways to do it and a lot of different decisions that have to be made.

    ## Today in Rails 7

    Here's an over-simplified menu of your choices for an asset pipeline when creating a new Rails 7 application today:

    ### Basic Asset Pipeline

    Running `rails new` as of Rails 7 will default to bundling the `importmap-rails` gem for JavaScript and the `sprockets` gem for CSS and image fingerprinting. If you're building a Hotwire app or a simple Rails app that only requires a few JavaScript files, this approach is great since it doesn't require npm, yarn, or any other parts of a typical JavaScript toolchain.

    This approach does require HTTP/2 to be performant. Why? Because HTTP/2 allows websites to serve up separate files over a single connection.

    Compare that to HTTP/1, which requires several connections to serve up multiple files. Creating connections to servers that are located in galaxies far far away has overhead, which is why Sprockets concatenated all JavaScript and CSS files into one big file. It reduced the number of connections that needed to be made to those distant servers.

    There are some popular Rails web hosts out there that don't support HTTP/2, so be sure to check with your host for support. Fly supports HTTP/2 out of the box, so no additional configuration is required to use import maps.

    ### Complex Asset Pipeline

    If your project has a more complicated frontend that requires JavaScript or CSS compilation or manages dependencies with yarn or npm, you'll want to use the `jsbundling-rails` and `cssbundling-rails` gems. Each of those gems brings in more complexity, which is only worth it if you absolutely need it.

    ### But wait, there's more!

    If you're new to Rails, you should pick the Basic Asset Pipeline option and get back to shipping. Don't get caught up in analysis paralysis on the perfect asset pipeline because it doesn't exist.

    But just for fun, and to prove a point about how confusing this can be, know that there's a few other options for deploying Rails that work for more specialized needs:

    - [**Tailwind**](https://github.com/rails/tailwindcss-rails) **asset pipeline** - The `tailwindcss-rails` gem wraps the `tailwindcss` CLI, and is a great option if you plan to use only Tailwind and know you won't need to compile or manage other CSS dependencies.
    - [**Dart Sass**](https://github.com/rails/dartsass-rails) **asset pipeline** - Like the Tailwind asset pipeline above, the `dartsass-rails` gem wraps the `dart-sass` binary and is great if you're certain that's all your project needs.
    - **Sprockets-only asset pipeline** - If you're coming into Rails 7 with an application that's using Sprockets to manage JavaScript assets, nothing says you have to change that. Sprockets will keep on chugging.
    - **Webpacker asset pipeline** - Rails 5.2 shipped with Webpacker, a wrapper for webpack, that made it possible for Rails apps to ship complex JS front-ends, like React, out of the box. While this gem will work in Rails 7, its been retired and is recommended to migrate to [jsbundling-rails](https://github.com/rails/jsbundling-rails/).

    ## Pain points

    If you're coming into Rails 7 from an older Rails app, there are few pain points that you can expect to run into:

    ### Migrating pipelines

    The documentation and tools for migrating pipelines isn't fully baked yet, though efforts are being made to make this clearer, as evident in a [Rails forum post](https://discuss.rubyonrails.org/t/guide-to-rails-7-and-the-asset-pipeline/80851) and [Draft Github PR that updates the Asset Pipeline Guide](https://github.com/rails/rails/pull/45400). Improving documentation will help eliminate some of the confusion around the asset pipeline.

    Another thing to think about: if you create a new Rails 7 app with the basic pipeline, then find out in the future that you need to switch to `jsbundling-rails`, you'll have to manually move around your asset files and reconfigure a few things.

    ### Running the development server

    For a long time, the way to run a Rails development server was `bin/rails server` . Some asset pipeline configurations require running development servers in a separate process from the Rails development server. There is a slight learning curve for Rails developers who have to switch from running `bin/rails server` to `bin/dev` and learn about `Procfile`.

    ### Difficulty using assets from Rails plugins

    Where you're probably running into issues today is how to get assets from Rails plugins, like `local_time`, working in Rails. Why is that? Let's take a closer look starting with the Installation section of the project's README file:

    ```
    ## Installation

    1. Add `gem 'local_time'` to your Gemfile.
    2. Include `local-time.js` in your application's JavaScript bundle.

        Using the asset pipeline:
        ```js
        //= require local-time
        ```
        Using the [local-time npm package](https://www.npmjs.com/package/local-time):
        ```js
        import LocalTime from "local-time"
        LocalTime.start()
        ```
    ```

    The first direction for inclusion in the asset bundle is Sprockets syntax. Since Sprockets managed load paths, it knew that  `//= require local-time` should look in that project's `lib/assets/javascripts/src` path to find the asset.

    The second direction for inclusion makes life a little more difficult for Rails plugin developers. In addition to maintaining the Ruby gem, they also now have to maintain an npm package and version that separately from the gem.

    The third direction is missing, which would instruct Rails 7 application developers on how to pin the local-time npm package via `bin/importmap pin`. For the Rails plugin developer, there's not much direction on if they should distribute their JavaScript assets with the Rails gem or separately as an npm package.

    This confusion exists today because of the different asset management systems that Rails currently has in play: sprockets, webpacker, jsbundling-rails, and importmaps. There's no One Way™ for third party gem developers to integrate their assets with Rails.

    Fortunately work is being done by the Rails community to solve a few of these pain points.

    ## The Future

    In the future, [Propshaft](https://github.com/rails/propshaft) will serve as the interface between Rails applications and assets. It will replace all that Sprockets does and focus on these four things:

    1. **Load paths** - Propshaft will keep track of asset load paths from the app and Rails gem plugins. This should make installing Rails plugin gems much easier since they'll register with Propshaft, eliminating the guesswork needed to find assets.
    1. **Digest stamping -** Assets defined in the load paths will be digested and copied into the `./public` directory, along with a `manifest.json` file, so Rails can serve up assets with long cache expiries. This is a well known technique that makes web applications load faster.
    1. **Development server** - Propshaft [will run a development server inside of Rails](https://github.com/rails/propshaft/blob/main/lib/propshaft/railtie.rb#L36-L42), which means it will be possible to run some development environments via `bin/rails server` without a Procfile. This is still under active development, so expect changes in the final implementation.
    1. **Basic compilers**  - Propshaft will support basic compilation steps, like replacing `url(asset)` strings in CSS files with `url(digested-asset)`. Complex compilations will be delegated out to tools better suited for that purpose.

    Rails will continue recommending importmaps for JavaScript assets when running `rails new`, but instead of including Sprockets, it will include Propshaft. Sprockets will probably continue being supported for a while, but it will be recommended to switch to Propshaft as the way to manage Rails assets.

    ---

    ## The Past

    You might be thinking, &quot;how did the Rails asset pipeline get to be such a beast?&quot;. Like most questions that end with, &quot;get to be such a beast&quot;, it's a long story.

    Rails has been around for over a decade, a time long ago before HTTP/2 existed and the phrase &quot;Backend JavaScript developer&quot; was an oxymoron. Come on a journey with me and explore its past.

    ### The Time Before Asset Management

    When Rails first started, HTTP/2 didn't exist, JavaScript development as a full-time job was a foreign concept, and asset management was a matter of throwing [Scriptaculus](http://script.aculo.us),  [Prototype](http://prototypejs.org), and an `application.js` file into the `./public/assets` directory. No fingerprinting was necessary because there were so few moving parts.

    There were third-party gems, like [jammit](https://rubygems.org/gems/jammit), that were available for production websites that cared about asset efficiency, but for the most part it's something Rails devs didn't think about.

    Life was good, then JavaScript file sizes starting getting bigger and became a much bigger concern of web application development.

    ### Hello, my name is Sprockets

    A few years later, when we swore off Prototype because it extended core JavaScript prototypes and moved on to jQuery, Sprockets came onto the scene. Most of us remember Sprockets as a huge pain, but it was actually pretty good when it arrived into Rails because source maps, transpiling, and all the other fancy stuff that JS compilers do wasn't that pervasive.

    ### Move over Sprockets, Webpacker is in town

    At some point the JavaScript community flew past Sprockets in terms of pipeline sophistication. Rails developers envied source maps, tree shaking, and all sorts of other goodies that made for more efficient asset delivery. Sprockets couldn't keep up, so Webpacker was born.

    Remember, this was all before Turbo landed. The way to build a high budget compelling Rails app at the time was to deploy a front-end Framework like React, Vue, or Backbone.js.

    ### Hmm, Maybe Rails Apps Shouldn't Run so Much JavaScript and HTTP/2 Solves Some Asset Pipeline Problems

    At some point a few folks in the Rails community had the revelation that fast, responsive web applications that developers typically associate with JavaScript-rich applications could be built without all the complexity of a single-page JavaScript application that talks to its server via an API. Turbo was born!

    Turbo has nothing to do with the asset pipeline, so how is it relevant? Most Turbo apps require a few JavaScript files that don't need any fancy compiler technology, so it doesn't need a complex JavaScript compilation process. Turbo accomplishes this feat by keeping application state on the server, rendering HTML fragments, and delivering it over the wire to the browser where it swaps out parts of the DOM.

    At the same time it was realized that HTTP/2 could deliver a lot of small asset files without incurring the performance penalties imposed by HTTP/1, so importmaps was born.

    These two ideas came together in Rails 7 and gave us a much simpler way to build web applications without the complexity associated with front-end heavy JavaScript applications.
- :id: laravel-bytes-multi-region-laravel-with-planetscale
  :date: '2022-08-22'
  :category: laravel-bytes
  :title: Multi-Region Laravel with PlanetScale
  :author: fideloper
  :thumbnail: planetscale-mysql-with-fly-thumbnail.jpg
  :alt:
  :link: laravel-bytes/multi-region-laravel-with-planetscale
  :path: laravel-bytes/2022-08-22
  :body: "\n\n\n<p class=\"lead\">Fly.io empowers you to run apps globally, close
    to your users. If you want to ship a Laravel app, [try it out on Fly.io](/docs/laravel/).
    It takes just a couple of minutes.</p>\n\n\n\nWe're going to see how to serve
    Laravel globally.\n\nFly.io is great at serving your application from servers
    in multiple regions. PlanetScale is great at putting your data close to your application
    servers!\n\nWe're going to use PlanetScale Portals to replicate data from a main
    database across the world to read-replicas. Having the data close to the application
    servers speeds up our applications, as they don't need to reach for a database
    that might be on the other side of the world.\n\n## The Setup\n\nWe'll have 3
    application instances on Fly, each in a different region. Similarly, PlanetScale
    will have 3 database instances. One is the primary database, and 2 will be read-replicas.
    Each instance will be in a different region.\n\nPlanetScale has regions available
    close to Fly.io's. When our app serves a request, we want to connect to the nearest
    PlanetScale database.\n\nOur mapping of Fly.io region to PlanetScale region will
    look like this:\n\n1. **DFW** → us-east-1 (Virgina), the primary database\n1.
    **FRW** → eu-central (also Frankfurt), a read-replica\n1. **SIN** → ap-southeast
    (also Singapore), a read-replica\n\nFly.io has some regions closer to PlanetScale's
    Virginia region than Dallas, but that won't affect our results too much.\n\n##
    Scaling Laravel Globally\n\nFirst, let's see the easy part. What does it look
    like to scale a Laravel application out to multiple regions on Fly.io?\n\nLet's
    say you already have an application running in DFW (Dallas, Texas). You can add
    additional regions to your application like so:\n\n```bash\n# Head to your app,
    where fly.toml lives\ncd ~/Fly/my-app\n\n# Add additional regions Frankfurt and
    Singapore\nfly regions add fra sin\n```\n\nNow our application is allowed to run
    in 3 regions. However, it's still only running in DFW!\n\nTo get the app running
    in all 3 regions, we need to scale our application up. Fly will take care of splitting
    your app instances amongst the available regions:\n\n```bash\n# Head to your app,
    where fly.toml lives\ncd ~/Fly/my-app\n\nfly scale count 3\n```\n\n## Good news,
    everyone!\n\nYou're running your Laravel application in multiple regions! Customers
    are certain to flock to your site, likescribe to your content, and purchase your
    widgets.\n\n**But wait! It's slow!?**\n\nIt turns out that making your application
    code closer to your users may also require moving your data along with it. Shooting
    TCP packets around the globe is slow! **It would be much better if our database
    was just a short hop away from the application code querying it**.\n\nHow do we
    do this? The short answer is: Read Replicas.\n\n\n\n## Read Replicas\n\nYes, read
    replicas! An eventually-consistent copy of your data, propagated from a primary
    database instance to read-only replica instances spread around the world.\n\nYou
    can set this up yourself but…lets not. Save some of your life-hours and choose
    a quick swipe of the credit card. PlanetScale can do the work for you (without
    being super expensive).\n\nWithin PlanetScale, you just click some buttons. [Here
    are the buttons to click](/docs/app-guides/planetscale/). Just create a database
    and add some regions, we'll cover the rest here.\n\nFor the Laravel side of things,
    read on!\n\n\n\n## Laravel Configuration\n\nYou may recall that Laravel has configuration
    to help us [split read/write connections](https://laravel.com/docs/9.x/database#read-and-write-connections).
    Here's what it looks like:\n\n```php\n// Found in file config/database.php\n\n'mysql'
    => [\n    'read' => [\n        'host' => [\n            '192.168.1.1',\n            '196.168.1.2',\n
    \       ],\n    ],\n    'write' => [\n        'host' => [\n            '196.168.1.3',\n
    \       ],\n    ],\n    'sticky' => true,\n    // more stuff\n];\n```\n\nWith
    this, Laravel will send any \"write\" queries to the `write` connection, and any
    `read` queries to one of the hosts in the `read` query array.\n\n<div class=\"callout\">There's
    also the `sticky` feature, so that once a write query is made, all queries for
    the duration of that http request will use the `write` host. This makes sure data
    is immediately available, allowing us to ignore the unpleasant reality of replication
    lag.</div>\n\n## The Problem\n\nUnfortunately this isn't the whole story. This
    is a great feature, but has a few implementation details.\n\n1. Laravel is going
    to choose a read host [at random](https://github.com/laravel/framework/blob/9544b13b03871e560689aa920743426801aa57b0/src/Illuminate/Database/Connectors/ConnectionFactory.php#L140).
    However, we want a _specific_ database host to be chosen per Fly region.\n1. Write
    queries will always go to the database in DFW, even if the application serving
    a request is in Singapore. Database connections going across the globe are going
    to be 200-year-old-tortoise slow \U0001F422.\n\nWe know ahead of time what Fly.io
    regions and what PlanetScale regions we operate within (we chose them!). We'll
    use that knowledge to make Laravel smart about how it talks to the database.\n\n##
    Choosing the Database Connection\n\nTraffic will be routed to a specific region
    depending on where in the world you are. In our case, that's either DFW, FRA or
    SIN. We want to choose our database connection based on the region that is serving
    a request.\n\nLet's say we have some environment variables like this (in your
    `fly.toml`file):\n\n```\n[env]\nDB_HOST_dfw = \"db_closest_to_dallas.psdb.cloud\"\nDB_HOST_fra
    = \"db_closest_to_frankfurt.psdb.cloud\"\nDB_HOST_sin = \"db_closest_to_singapore.psdb.cloud\"\n```\n\n\n\nSomething
    like this allows us to hack some configuration on the `read` connection, like
    so:\n\n```php\n'read' => [\n    'host' => [\n        // Get hostname from env
    var: DB_HOST_xxx (e.v. DB_HOST_dfw)\n        // But then we couldn't use the config
    cache :/\n        env('DB_HOST_'.env('FLY_REGION', 'dfw'), 'default_host.psdb.cloud'),\n
    \   ],\n],\n```\n\n\n\nHowever, **this will break if we try to use Laravel's config
    cache** (which you want to do, as it speeds up Laravel a bunch).\n\nTo avoid that
    problem, we can turn to some code. Inside of your `AppServiceProvider` (or service
    provider of your choice), we can add some logic to override the `read` host set
    in the configuration.\n\n```php\npublic function boot()\n{\n    // \"match\" is
    fancy PHP 8+ stuff, we could use a switch{} instead\n    $dbHostname = match (env('FLY_REGION'))
    {\n        'mad' => env('DB_HOST_fra'), // Frankfurt\n        'sin' => env('DB_HOST_sin'),
    // Singapore\n        'dfw' => env('DB_HOST_dfw'), // Dallas\n        // Default
    to primary/write host:\n        default => config('database.connections.mysql.write.host'),\n
    \   };\n\n    // Only give Laravel one \"read\" host to choose from\n    // Based
    on the closest database server\n    config()->set('database.connections.mysql.read.host',
    [$dbHostname]);\n}\n```\n\nThis will set our `read` host to the appropriate hostname
    _on the fly_!\n\n### Wait, what's happening here?\n\nA few things!\n\nFirst, the
    `write` host is always going to be an array of length 1, containing our primary
    database hostname. This is where all write operations must go (the others are
    read-only replicas). We default to sending queries to this database if we don't
    match another region.\n\nThe `read` hosts can be set to an array of 1 or more
    hosts. However, Laravel will just choose one of those randomly. We need to make
    our application smarter than that. To make it smarter, we do the dumbest thing
    possible - only give it one option to choose from!\n\n### There's another issue!\n\n\n\nPlanetScale
    gives us a username, hostname, and password that's **unique** for each database
    instance.\n\nThis means our application needs to know the connection details for
    all 3 database instances.\n\nSo our application can use environment variables
    / secrets like this:\n\n```\n# DB DEFAULTS\nDB_DATABASE=laravel-multi-region\nMYSQL_ATTR_SSL_CA=/etc/ssl/certs/ca-certificates.crt\n\n#
    DFW\nDB_HOST_dfw=xxx.us-east-2.psdb.cloud\nDB_USERNAME_dfw=xxxyyyzzz\nDB_PASSWORD_dfw=pscale_pw_some_long_password1\n\n#
    FRA\nDB_HOST_fra=eu-central.connect.psdb.cloud\nDB_USERNAME_fra=aaabbbccc\nDB_PASSWORD_fra=pscale_pw_some_long_password2\n\n#
    SIN\nDB_HOST_sin=ap-southeast.connect.psdb.cloud\nDB_USERNAME_sin=dddeeefff\nDB_PASSWORD_sin=pscale_pw_some_long_password3\n```\n\nThe
    passwords should be set as [secrets](https://fly.io/docs/reference/secrets/) in
    your app.\n\n```bash\nfly secrets set DB_USERNAME_dfw=\"pscale_pw_some_long_password1\"
    \\\n    DB_USERNAME_fra=\"pscale_pw_some_long_password2\" \\\n    DB_USERNAME_sin=\"pscale_pw_some_long_password3\"\n```\n\nAssuming
    our env vars / secrets are set, our `config/database.php` file can be updated
    to this:\n\n```php\n# mysql connection info\n'mysql' => [\n    'read' => [\n        'host'
    => [\n            env('DB_HOST_dfw')\n        ],\n        'username' => env('DB_USERNAME_dfw'),\n
    \       'password' => env('DB_PASSWORD_dfw', ''),\n    ],\n    'write' => [\n
    \       'host' => [\n            env('DB_HOST_dfw')\n        ],\n        'username'
    => env('DB_USERNAME_dfw'),\n        'password' => env('DB_PASSWORD_dfw', ''),\n
    \   ],\n    'sticky' => true,\n    // other stuff\n],\n```\n\nLuckily, Laravel
    lets us set a unique username and password per `read` or `write` connection.\n\nTo
    ensure Laravel uses the correct connection information per region (and to connect
    to the correct read-replica when serving from Frankfurt or Singapore), we can
    update our `AppServiceProvider` with the following:\n\n```php\n  public function
    boot()\n  {\n      /**\n       * Get hostname, username, password for a given
    region\n       */\n\n      $dbHostname = match (env('FLY_REGION')) {\n          'fra'
    => env('DB_HOST_fra'), // Frankfurt\n          'sin' => env('DB_HOST_sin'), //
    Singapore\n          'dfw' => env('DB_HOST_dfw'), // Dallas\n          default
    => config('database.connections.mysql.write.host'),\n      };\n\n      $dbUsername
    = match (env('FLY_REGION')) {\n          'fra' => env('DB_USERNAME_fra'), // Frankfurt\n
    \         'sin' => env('DB_USERNAME_sin'), // Singapore\n          'dfw' => env('DB_USERNAME_dfw'),
    // Dallas\n          default => config('database.connections.mysql.write.username'),\n
    \     };\n\n      $dbPassword = match (env('FLY_REGION')) {\n          'fra' =>
    env('DB_PASSWORD_fra'), // Frankfurt\n          'sin' => env('DB_PASSWORD_sin'),
    // Singapore\n          'dfw' => env('DB_PASSWORD_dfw'), // Dallas\n          default
    => config('database.connections.mysql.write.password'),\n      };\n\n      config()->set('database.connections.mysql.read.host',
    [$dbHostname]);\n      config()->set('database.connections.mysql.read.username',
    $dbUsername);\n      config()->set('database.connections.mysql.read.password',
    $dbPassword);\n  }\n```\n\nNow our application will send read queries to a database
    pretty close to Fly's application servers!\n\n### Test it out\n\nTo test this
    out, I created an app with a route that measures times for one read query and
    and one write query.\n\nTo help us test timings, we can use the (semi-secret)
    header `fly-prefer-region`, which tells Fly's proxy which region to route a request
    to.\n\n```bash\n# The \"fly-prefer-region\" header tells FLy which region to route\n#
    an HTTP request through\n\n## Dallas\ncurl -i -H \"fly-prefer-region: dfw\" https://global-af.fly.dev/dump\n\nREGION:
    dfw\nCONNECTION: x0f5nogmbz0w.us-east-2.psdb.cloud via TCP/IP\nREAD QUERY TIME:
    77.87\nWRITE QUERY TIME: 445.74\n\n\n## Frankfurt\ncurl -i -H \"fly-prefer-region:
    fra\" https://global-af.fly.dev/dump\n\nREGION: fra\nCONNECTION: eu-central.connect.psdb.cloud
    via TCP/IP\nREAD QUERY TIME: 12.37ms \nWRITE QUERY TIME: 1068.77ms\n\n\n## Singapore\ncurl
    -i -H \"fly-prefer-region: sin\" https://global-af.fly.dev/dum\n\nREGION: sin\nCONNECTION:
    ap-southeast.connect.psdb.cloud via TCP/IP\nREAD QUERY TIME: 11.96ms\nWRITE QUERY
    TIME: 2892.84ms # nearly 3 seconds!\n```\n\nWe can see that our write query time
    gets higher the further away from DFW we are.\n\n## That's Not Ideal\n\nWe have
    Laravel splitting read/write connections. The `sticky` feature is pretty cool
    too. **But what's happening with write queries?**\n\nWell, a lot of our requests
    are going to make connections to a database that's far away. This is exasperated
    in an app that does a lot of database writes, and still further trouble with the
    `sticky` feature.\n\nWhile our HTTP requests are going to be routed to an app
    server pretty close to a user, the database connection may still be galavanting
    across the globe.\n\nSo that sucks.\n\nHere's a secret though - **It turns out
    it's usually much faster to route an _HTTP_ request to region closest to the primary
    database than it is to have a database connection from far away.**\n\nThis is
    because (among other reasons) the typical app has an asymmetry. A single HTTP
    request can easily result in a crap-ton (that's a technical term) of queries.\n\n![laravel
    post request resulting in many queries](http-queries.png)\n\n> These queries are
    perfect, don't @ me.\n\nIf we route this HTTP request to Singapore but then force
    our app to connect to a database in Dallas, we're gonna have a bad time.\n\nIt
    would be faster overall if we made the single, smaller HTTP request traverse the
    globe instead of the heavily-used database connection.\n\n\n\n## The Fly-Replay
    Header\n\nEnter, stage left, the [Fly Replay header](/docs/reference/fly-replay/).
    Any sufficiently advanced technology is indistinguishable from ~~magic~~ Rust,
    and we have some fairy Rust in the form of Fly's proxy layer.\n\n\U0001FA84 If
    your code returns an HTTP request with a `fly-replay` header, Fly's proxy will
    read the value of that header and **replay** the HTTP request to your app in _another_
    region of your choice (or even another app in your org)!!\n\nThis gives your application
    the opportunity to say \"this request probably is going to write to the database,
    how about you go to our primary region instead?\".\n\nMore often than not, this
    is orders of magnitude faster.\n\n\n\n### Fly-Replay in Laravel\n\nOne way we
    can make use of `fly-replay` is through an Http middleware. Let's create a new
    one:\n\n```bash\nphp artisan make:middleware ReplayWriteRequest\n```\n\nWe can
    edit our new middleware at `app/Http/Middleware/ReplayWriteRequest.php`:\n\n```php\npublic
    function handle(Request $request, Closure $next)\n{\n    // Perhaps make available
    as a config:\n    // config('fly.regions.primary');\n    $primaryRegion = 'dfw';\n\n
    \   // Any route that has this middleware applied\n    // will inform Fly to replay
    the request against dfw\n    // if the current region is not already dfw\n    if
    (env('FLY_REGION') && env('FLY_REGION') != $primaryRegion) {\n        return response('',
    200, [\n            'fly-replay' => $primaryRegion,\n        ]);\n    }\n\n    //
    Else, continue as normal\n    return $next($request);\n}\n```\n\nNote that we
    \ use `env('FLY_REGION')` and **not**  `$request→headers→get('fly-region')`. This
    got me at first. As per the [docs](https://fly.io/docs/reference/runtime-environment/#fly-region),
    the header is the edge location accepting your request, while env var is the region
    the application instance resides within.\n \n<div class=\"callout\">Pro tip: Use
    [debug.fly.dev](https://debug.fly.dev/) to see what headers and env vars your
    application will see.</div>\n\nThis middleware isn't meant to be global, but instead
    applied at your discretion.\n\nYou can register/assign this  middleware a name
    in `app/Http/Kernel.php`:\n\n```php\nprotected $routeMiddleware = [\n    // Other
    stock middleware omitted\n    'fly-replay' => \\App\\Http\\Middleware\\ReplayWriteRequest::class,\n];\n```\n\nThat's
    now named `fly-replay`. We can assign that to any route that we think will make
    write requests that should not traverse across the globe.\n\nThen we can apply
    this middleware to whatever route we want:\n\n```php\n// File routes/web.php\n\n//
    This route will tell Fly to replay the request against\n// the primary region
    (if it's not currently the primary region)\nRoute::delete('/user/{id}', \\App\\Http\\Controllers\\DeleteUserController::class)\n
    \   ->middleware(['auth', 'fly-replay']);\n```\n\nUsing this method evens out
    our write-query time, since they are all between Fly's DFW region and PlanetScale's
    Virginia region.\n\n```bash\ncurl -H \"fly-prefer-region: dfw\" https://global-af.fly.dev/insert\n\nREGION:
    dfw\nCONNECTION: x0f5nogmbz0w.us-east-2.psdb.cloud via TCP/IP\nREAD QUERY TIME:
    67.43ms\nWRITE QUERY TIME: 384.07ms\n\ncurl -H \"fly-prefer-region: fra\" https://global-af.fly.dev/insert\n\nREGION:
    dfw\nCONNECTION: x0f5nogmbz0w.us-east-2.psdb.cloud via TCP/IP\nREAD QUERY TIME:
    70.17ms\nWRITE QUERY TIME: 380.57ms\n\ncurl -H \"fly-prefer-region: sin\" https://global-af.fly.dev/insert\n\nREGION:
    dfw\nCONNECTION: x0f5nogmbz0w.us-east-2.psdb.cloud via TCP/IP\nREAD QUERY TIME:
    67.81ms\nWRITE QUERY TIME: 428.83ms\n```\n\nRequests being routed to Singapore,
    then being told to replay  the request in DFW will take a bit of extra time. However,
    it's still much faster (and consistent) than making long-distance database connections!\n\n\n\n###
    What if I miss a route?\n\nNote that I never said to undo the configuration where
    we split read/write connections!\n\nIf our middleware isn't applied to a route
    that writes to the databases, it should still work. Laravel will send the write
    queries to the `write` connection in the primary reason. It's just a lot slower.\n\n\n\n##
    What hath we wrought?\n\nRunning apps in multiple regions is traditionally really
    hard. The fact that I was able to set this all up in an afternoon is actually
    blowing my mind. Is it perfect? Not really! We can't escape the speed of light.
    But PlanetScale and Fly.io helps us solve the thorniest issue - keeping latency
    low when your database is far away from your users.\n"
- :id: ruby-dispatch-semi-static-websites
  :date: '2022-08-17'
  :category: ruby-dispatch
  :title: Semi-Static Websites
  :author: brad
  :thumbnail: semi-static-websites-thumbnail.jpg
  :alt:
  :link: ruby-dispatch/semi-static-websites
  :path: ruby-dispatch/2022-08-17
  :body: |2


    <div class="lead">
      Fly makes it easy to deploy server-side rendered (SSR) content sites to a global fleet of servers close to your readers _without_ a content distribution network (CDN). We'll look at how to accomplish that with [Sitepress](https://sitepress.cc/), a Ruby site generator that can ran as a stand-alone SSR, static website, or be embedded inside of a Rails app.
    </div>

    Static websites have exploded in popularity over the past few years. What is it that people like so much about static site generators?

    - **Low operational complexity** - Static websites can be deployed to a production environment without the need for a database, caching server, or other service dependencies. All that's needed is a pile of HTML, CSS, and JavaScript files and a fast server, like nginx.
    - **Things break _before_ they're deployed** - If there's an error, its usually caught during the compilation phase by the site compiler. The only exception to this are static sites that include tons of JavaScript.
    - **Pretty darn fast** - Static websites that keep an eye on asset sizes are generally pretty fast around the world when deployed behind a CDN, but only if those caches are warmed up. More on that later.
    - **Content is files** - Site generators typically manage content as a bunch of text, image, JavaScript, and CSS files in folders. This means they can be stored on a file system and tracked with amazing tools like git, which bills itself as `the stupid content tracker`. Dynamic systems, like Wordpress, use databases to store content, which requires versioning schemes that are implemented at the application level.

    Despite these dreamy characteristics of running static websites in production, it does come with a few trade-offs, especially if there's a few dynamic things that are needed, like publishing content with a future publish date.

    Let's look at a few approaches for how one might accomplish such a feat so we can better understand these trade-offs.

    ## Problem: How Do I Publish Future Content?

    A common use case when dealing with content, like a blog, is scheduling a post to be published in the future. It's a simple task for a dynamic content management system, but for static site generators, it requires some work. Here's a few different approaches to the problem.

    ### The Static Way: Use `cron` to Build and Publish The Site

    One way to publish future content with a static website is to schedule an hourly or daily task in `cron` that builds all the HTML pages in the website and uploads them to a production environment. This works fine for a small website that's not updated frequently, but it falls over for larger websites that need to publish content at more fine-grained intervals. Imagine wiring up a cronjob that runs every minute to compile and upload a website with a few thousand pages?

    ### The Database Way: Run a CMS like Refinery or Wordpress

    On the other end of the spectrum there's the Content Management System. The most popular CMS in the world is WordPress. For Rails, the most popular CMS is Refinery [according to the Ruby Toolbox](https://www.ruby-toolbox.com/categories/content_management_systems).

    This approach to managing content comes with a lot of complexity. For starters, these approaches require databases. Databases are hard to sync between production, staging, and development environments. Databases break. It's difficult to version and merge conflicting data in the database if people are editing the same content.  Usually some sort of caching layer needs to be built above the content generated by the database. It's a lot of extra complexity that might not be worth it.

    ### The Middle Road: Deploy a Semi-Static Website to Fly

    Semi-static websites, also known as server-side rendered (SSR) sites, run a small server in production that renders content per request. For a blog, that means the server could check a `publish_at` frontmatter key on a markdown file like this:

    ```md
    ---
    title: My First Blog Post
    publish_at: January 1, 2042
    ---

    Hello! I hope 2042 is a great year.
    ```

    And quickly figure out whether or not to display the post based on the server's current time.

    We're going to use a Ruby site generator called [Sitepress](https://sitepress.cc/) to deploy a semi-static website to Fly's global infrastructure. Why not Bridgetown, Jekyll, and Middleman? Those are all really great site generators, but they're focused on generating static HTML, CSS, and JavaScript assets, which won't work that great for server-side rendered sites. Outside of Ruby there's tons of site generators, but this is written for Ruby Dispatch where we talk about all things Ruby.

    ## How to build and deploy a semi-static website to Fly

    It's pretty quick! Here's how to do it:

    1. Clone the repo [https://github.com/sitepress/standalone-starter](https://github.com/sitepress/standalone-starter).

    ```cmd
    git clone git@github.com:sitepress/standalone-starter.git
    ```

    1. [Install the Fly CLI](https://fly.io/docs/getting-started/installing-flyctl/) and signup for an account.
    1. Now we're going to run a command that provisions the app and deploys it:

    ```cmd
    fly launch --copy-config --dockerfile Dockerfile --now
    ```

    1. Once that finishes, run `fly open` and you should see a website that looks like this:

    ![](https://slabstatic.com/prod/uploads/p1b436gf/posts/images/v5scGQhm4E_evdFPW480l0YC.png)

    Hooray! You've published your first semi-static website. It's running a webrick server that renders everything per-request.

    ### How fast is the website we just deployed?

    Now let's see what things look like for people on the other side of the world by running `fly curl`, a nifty little tool that loads your website from various Fly outposts.

    ```cmd
    fly curl https://$YOUR_SITE_NAME.fly.dev
    ```
    ```output
    REGION  STATUS  DNS   CONNECT TLS     TTFB    TOTAL
    ams     200     0.7ms 0.8ms   29ms    193.5ms 195.1ms
    cdg     200     0.7ms 0.9ms   29.1ms  179.5ms 181.1ms
    dfw     200     0.7ms 0.9ms   29.5ms  71.3ms  73.5ms
    ewr     200     0.5ms 0.7ms   25.1ms  90.6ms  91.8ms
    fra     200     0.6ms 0.8ms   30.5ms  204.7ms 205.9ms
    hkg     200     0.6ms 0.7ms   21.7ms  186.9ms 188.5ms
    lax     200     0.5ms 0.7ms   21.9ms  50.5ms  51.2ms
    lhr     200     0.8ms 1ms     29.2ms  174.3ms 175.9ms
    mia     200     0.8ms 1.1ms   33ms    116.4ms 160.6ms
    nrt     200     0.4ms 0.5ms   17.4ms  135.8ms 136.5ms
    ord     200     0.6ms 0.8ms   22.8ms  93.3ms  95.2ms
    scl     200     1.1ms 1.5ms   48.3ms  211.3ms 213.3ms
    sea     200     0.4ms 0.5ms   16.3ms  57.4ms  57.9ms
    sin     200     0.5ms 0.6ms   17.5ms  204.6ms 205.2ms
    sjc     200     3.3ms 3.4ms   34.9ms  46.1ms  46.4ms
    syd     200     0.7ms 0.9ms   24.9ms  192.9ms 193.6ms
    yyz     200     0.6ms 0.8ms   31.6ms  99ms    99.9ms
    ```

    The time-to-first-byte (TTFB) times on the other side of the world probably aren't all that great, which means people trying to read your website are sitting there waiting. TTFB is the amount of time people have to wait after typing `your website.com` into their browser and receiving the first bytes of HTML.

    ### Provision and deploy to servers around the world

    Let's fix that problem by deploying this website closer to them, without a CDN, by telling Fly we're cool running our site in these regions:

    ```cmd
    fly regions set sin fra ord
    ```
    ```output
    Region Pool:
    fra
    ord
    sin
    Backup Region:
    ```

    This command tells Fly the parts of the world you want to deploy the website, but the servers aren't running there yet. Let's spin them up.

    ```cmd
    fly scale count 3 --max-per-region=1
    ```

    Fly will scale up your semi-static site in each of these regions so they're ready to respond to requests as they come in.

    Let's see how those regions scaled up.

    ```cmd
    fly status
    ```
    ```output
    App
      Name     = $YOUR_SITE_NAME
      Owner    = personal
      Version  = 2
      Status   = running
      Hostname = $YOUR_SITE_NAME.fly.dev
      Platform = nomad

    Instances
    ID        PROCESS VERSION REGION  DESIRED STATUS  HEALTH CHECKS       RESTARTS  CREATED
    394677de  app     2       ord     run     running 1 total, 1 passing  0         1m6s ago
    4de70730  app     2       sin     run     running 1 total, 1 passing  0         1m6s ago
    8f6646aa  app     2       fra     run     running 1 total, 1 passing  0         2m21s ago
    ```

    ### Now how fast is our website?

    Let's run `fly curl` again and see what happened to the latency:

    ```cmd
    fly curl https://$YOUR_SITE_NAME.fly.dev
    ```
    ```output
    REGION  STATUS  DNS   CONNECT TLS     TTFB    TOTAL
    ams     200     0.7ms 1ms     29ms    37.7ms  39.4ms
    cdg     200     0.9ms 1.1ms   22.9ms  44.5ms  45.9ms
    dfw     200     0.8ms 1.2ms   25.9ms  68.1ms  69.3ms
    ewr     200     6.8ms 7.1ms   85.5ms  70.3ms  71.3ms
    fra     200     0.8ms 1.1ms   26.9ms  33ms    34.4ms
    hkg     200     0.9ms 1.1ms   22.7ms  70ms    71.7ms
    lax     200     0.5ms 0.7ms   21.7ms  86.4ms  87.1ms
    lhr     200     0.8ms 1.1ms   24.5ms  36.4ms  38ms
    mia     200     0.8ms 1.2ms   25.3ms  69ms    70.8ms
    nrt     200     0.4ms 0.6ms   18.1ms  94.1ms  94.6ms
    ord     200     0.4ms 0.6ms   21.8ms  51.9ms  53ms
    scl     200     1.1ms 1.4ms   39.8ms  168.1ms 170.3ms
    sea     200     0.4ms 0.5ms   16.1ms  83.2ms  91.6ms
    sin     200     0.4ms 0.5ms   17.4ms  26.3ms  26.9ms
    sjc     200     0.4ms 0.5ms   28.1ms  83.6ms  83.7ms
    syd     200     0.9ms 1.1ms   23.4ms  113.7ms 115.1ms
    yyz     200     1ms   1.5ms   41.8ms  60.8ms  62.3ms
    ```

    Much better! The TTFB time decreased, which means people who are reading the website see the content instantly (under 250ms) as far as they're concerned.

    ## Fly, Content Distribution Networks, Your Customers, and You

    With Fly, it is possible to cut out the middleman CDN and simply run content servers closer to your customers. CDNs are great, but they do add latency to your application for a few reasons:

    1. A CDN with a cold cache has to fetch the content from the origin. This adds latency to the initial request.
    1. When the cache from a CDN expires, it has to check the origin for a fresh resource which again, adds latency to the request. Some CDNs can be configured to serve up the stale content while requesting the new stuff from the origin, but nobody wants something that's stale. If people like stale stuff bakeries wouldn't mark down day-old-bread.

    For static websites, this isn't a huge deal, but for mixed websites where all application requests go through the CDN and the cache times are low like a blog or news website, its at least an extra hop that simply isn't necessary when running servers close to your users.

    ## There's lots of reasons to deploy semi-static websites

    Here's a few that you could try for your own projects:

    - **Always up-to-date project README files** - Lots of open source projects have a website that repeat content from the `README.md` file at the root of their project. With a semi-static site, its easy to fire off an HTTP request for an open-source projects latest `README.md` file and render it within the project website. The starter app [shows how a Github README could be rendered on a project page](https://github.com/sitepress/standalone-starter/blob/main/pages/projects/sitepress.html.md).
    - **Localization** - A semi-static site could localize content depending on the `FLY_REGION` or users IP address.
    - **Treat Your Content As Data** - Imagine a world where you could get a list of relevant help articles to your customers from within your application? It's possible with Sitepress via a line of code that looks like `HelpPages.tags(:login, :security, :account_management)`. Treating content as data opens up a lot of interesting use cases for tightly integrating content with your app, which can make for a great experience for your customers.
    - **Community Websites** - Git is the ultimate content management system for communities, especially when it's backed with workflows like &quot;Open a PR to edit content&quot;. It's how [Fly's docs are managed](https://github.com/superfly/docs). Be sure to include an &quot;Edit with Github&quot; link on the page that opens the content up in Github's edit view so people can make contributions for quick edits, like typos.
    - **Run it in your Rails Apps** - If you're a small team or solo developer, Sitepress can be [embedded in your Rails apps](https://sitepress.cc/getting-started/rails/) and integrate directly with your routes files. If the Rails application has a database, you'll want to read about [how to run ordinary rails apps globally](https://fly.io/blog/run-ordinary-rails-apps-globally/).
- :id: phoenix-files-phx-trigger-action
  :date: '2022-08-16'
  :category: phoenix-files
  :title: Triggering a Phoenix controller action from a form in a LiveView
  :author:
  :thumbnail: phx-trigger-action-thumbnail.jpg
  :alt:
  :link: phoenix-files/phx-trigger-action
  :path: phoenix-files/2022-08-16
  :body: "\n\n<p class=\"lead\">This is a post about getting a form in a LiveView
    to invoke a Phoenix controller action, on your terms. If you want to deploy a
    Phoenix LiveView app right now, then check out how to [get started](/docs/elixir/).
    You can be up and running in minutes.</p>\n\nHave you ever wanted to use LiveViews
    for a site's authentication? Among many other implementation details, you need
    to save some data to identify the logged-in user. This can be a token or some
    unique identifier, and it needs to persist even as the user navigates around your
    app and different LiveViews get created and destroyed.\n\nThe obvious solution
    is to store this token or unique identifier in the session. You can create a Phoenix
    controller with a `:create` action that generates a token, then saves it in the
    session using functions of the `Plug.Conn` module:\n\n```elixir\ndefmodule MyAppWeb.SessionController
    do\n  use MyAppWeb, :controller\n\n  def create(conn, %{\"user\" => user}) do\n
    \   token = Accounts.generate_user_session_token(user)\n    \n    conn\n    |>
    put_session(:user_token, token)\n    |> redirect(to: signed_in_path(conn))\n  end\nend\n```\n\nYou
    continue building your authentication system and decide that once a user signs
    up, using a form in a LiveView, they should be automatically logged in. This means
    saving the session data from within the LiveView&mdash;and only after the new
    user is finished signing up and you're happy for them to have access to the app.\n\n##
    Problem\n\nThe LiveView lifecycle starts as an HTTP request, but then a WebSocket
    connection is established with the server, and all communication between your
    LiveView and the server takes place over that connection.\n\nWhy is this important?
    Because session data is stored in cookies, and cookies are only exchanged during
    an HTTP request/response. So writing data in session can't be done directly from
    a LiveView.\n\nCan we call the controller's `:create` action from our LiveView
    form, and have it write the data for us? And can we make sure that happens only
    once the new user's registration process is complete: their data validated and
    saved?\n\n## Solution\n\nWe can make an HTTP route call to a controller when the
    form is submitted by adding the `:action` attribute to our forms, specifying the
    URL we want to use.\n\nAnd the [:phx-trigger-action](https://hexdocs.pm/phoenix_live_view/form-bindings.html#submitting-the-form-action-over-http)
    attribute allows us to make form submission conditional on some criteria.\n\nIn
    this case, we want to trigger the form submit, and log the new user in, after
    saving their registration data in the database without errors; if this doesn't
    happen, the action should not trigger, and instead we need to keep our LiveView
    connected and display any generated errors.\n\nLet's see how to do it.\n\nLet's
    start by defining, in our LiveView, the form that we'll use to fill out the user's
    data:\n\n```elixir\ndef render(assigns) do\n  ~H\"\"\"\n  <h1>Register</h1>\n\n
    \ <.form\n    id=\"registration_form\"\n    :let={f}\n    for={@changeset}\n    as={:user}\n
    \ >\n    <%%= label f, :email %>\n    <%%= email_input f, :email, required: true
    %>\n    <%%= error_tag f, :email %>\n\n    <%%= label f, :password %>\n    <%%=
    password_input f, :password, required: true %>\n    <%%= error_tag f, :password
    %>\n\n    <div>\n      <%%= submit \"Register\" %>\n    </div>\n  </.form>\n  \"\"\"\nend\n```\n\nThis
    form uses a changeset to build the necessary inputs. In this case, just a couple
    of inputs to save the user's email and password.\n\nWe define the changeset and
    add it to the LiveView assigns:\n\n```elixir\ndef mount(_params, _session, socket)
    do\n  changeset = Accounts.change_user_registration(%User{})\n  {:ok, assign(socket,
    changeset: changeset)}\nend\n```\n\nWe also add a couple of callbacks: `validate`
    to validate the data that the user enters into the form, and show us live errors
    if needed, and `save` to persist the user's information into the database.\n\n```elixir\ndef
    handle_event(\"validate\", %{\"user\" => user_params}, socket) do\n  changeset
    = Accounts.change_user_registration(%User{}, user_params)\n  {:noreply, assign(socket,
    changeset: changeset}\nend\n\ndef handle_event(\"save\", %{\"user\" => user_params},
    socket) do\n  case Accounts.register_user(user_params) do\n    {:ok, user} ->\n
    \     changeset = Accounts.change_user_registration(user)\n      {:noreply, assign(socket,
    changeset: changeset)}\n\n    {:error, %Ecto.Changeset{} = changeset} ->\n      {:noreply,
    assign(socket, :changeset, changeset)}\n  end\nend\n```\n\nWe add three more attributes
    to our form: `:phx-submit`, `:phx-change` and `:action`. The first two invoke
    the callbacks we defined above, and `:action` executes our controller's `:create`
    action using the URL `users/log_in/`.\n\n<aside class=\"right-sidenote\"> Spoiler
    alert! [Verified routes coming soon](https://twitter.com/chris_mccord/status/1554478915477028864)
    </aside>\n\n```elixir\n<.form\n  id=\"registration_form\"\n  :let={f}\n  for={@changeset}\n
    \ as={:user}\n  phx-submit=\"save\"\n  phx-change=\"validate\" \n  action={~p\"/users/log_in/\"}
    #{Routes.session_path(@socket, :create)}\n>\n```\nWith this, we get the `:create`
    action to run once the form is submitted; however, the action will run happily
    even if there was an error saving the user data. We don't want that!\n\nThis is
    where the `:phx-trigger-action` attribute comes into play. Let's use it to submit
    the form only if the user has been successfully saved to the database.\n\nFirst
    we add the `phx-trigger-action` attribute to the form:\n\n```elixir\n<.form\n
    \ id=\"registration_form\"\n  :let={f}\n  for={@changeset}\n  as={:user}\n  phx-submit=\"save\"\n
    \ phx-change=\"validate\"\n  action={~p\"/users/log_in/\"} #{Routes.session_path(@socket,
    :create)}\n  phx-trigger-action={@trigger_submit}\n>\n```\n\nYou can probably
    see where this is going: `phx-trigger-action` takes a boolean value, so when `@trigger_submit`
    is `true`, the form will get submitted and the action defined in our `action`
    attribute will be triggered. Let's add `trigger_submit` to the LiveView assigns:\n\n```elixir\ndef
    mount(_params, _session, socket) do\n  changeset = Accounts.change_user_registration(%User{})\n
    \ {:ok, assign(socket, changeset: changeset, trigger_submit: false)}\nend\n```\n\n\nWe
    change  `trigger_submit` to `true` only if the user has been saved correctly:\n\n```elixir\ndef
    handle_event(\"save\", %{\"user\" => user_params}, socket) do\n  case Accounts.register_user(user_params)
    do\n    {:ok, user} ->\n      changeset = Accounts.change_user_registration(user)\n
    \     socket = assign(socket, changeset: changeset, trigger_submit: true)\n      {:noreply,
    socket}\n\n    {:error, %Ecto.Changeset{} = changeset} ->\n      {:noreply, assign(socket,
    :changeset, changeset)}\n  end\nend\n```\n\nNow the `:create` action is only executed
    once the user is saved correctly. In case of error, the LiveView shows the registration
    errors to the user.\n\n<%= partial \"shared/posts/cta\", locals: {\n  title: \"Fly.io
    ❤️ Elixir\",\n  text: \"Fly.io is a great way to run your Phoenix LiveView app
    close to your users. It's really easy to get started. You can be running in minutes.\",\n
    \ link_url: \"https://fly.io/docs/elixir/\",\n  link_text: \"Deploy a Phoenix
    app today!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>\"\n} %>\n\n## Possible
    errors and how to fix them\n\nLet's prevent two common errors that can trip us
    up when using the `phx-trigger-action` option.\n\n### Form parameters are empty
    when phx-trigger-action is triggered\n\nThe first error is very specific to our
    use case and is related to our form fields: When the form is submitted and the
    form's parameters reach the controller, the parameter that stores the user's password
    is empty, even though we're sure we've entered a value.\n\nThis is related to
    the password type input and its [behavior](https://hexdocs.pm/phoenix_live_view/form-bindings.html#password-inputs).
    All we have to do is explicitly give the input a value by adding the `value` option
    like so:\n\n```elixir\n<%%= password_input f, :password, \n  required: true, \n
    \ value: input_value(f, :password) \n%>\n```\n\nWith this simple step, the value
    of our password input will be sent in the parameters of the form!\n\n### The controller
    route is not found, even though it is defined in the router\n\nThe second mystifying
    error is this: `phx-trigger-action` tries to execute the controller action we
    specified, but the route cannot be found on the router, even when it is the correct
    one.\n\n```elixir\n[debug] ** (Phoenix.Router.NoRouteError) \n  no route found
    for PUT /users/log_in (MyAppWeb.Router)\n```\n\nIn this case, it's related to
    how our changeset is interpreted when the form and its attributes are being built.\n\nIn
    our example, we insert the user into the database just before submitting the form,
    so our changeset contains the data of a record that already exists. Phoenix thinks
    that we're trying to modify that record; that's when the form is built using the
    `put` method instead of the `post` method.\n\nThe solution is simple; we just
    have to add the option `method=\"post\"` to our form's definition.\n\n```elixir\n<.form\n
    \ id=\"registration_form\"\n  :let={f}\n  for={@changeset}\n  as={:user}\n  phx-submit=\"save\"\n
    \ phx-change=\"validate\"\n  action={~p\"/users/log_in/\"} #{Routes.session_path(@socket,
    :create)}\n  phx-trigger-action={@trigger_submit}\n  method=\"post\"\n>\n```\n\n##
    Discussion\n\nThe `phx-trigger-action` option is ideal when you need to do final
    validations just before submitting form data via an HTTP request from a LiveView.\n\nIt's
    also so simple to use that you'd think nothing could go wrong. However, as we've
    seen, headaches can arise from the underlying form behavior, and they can be tricky
    to debug. We've highlighted two such problems to help you use the `phx-trigger-action`
    option painlessly.\n"
- :id: blog-remote-ide-machines
  :date: '2022-08-15'
  :category: blog
  :title: Building an In-Browser IDE the Hard Way
  :author:
  :thumbnail: ide-the-hard-way-thumbnail.png
  :alt: Long rubbery arms snaking their way into a phone screen and through the ether
    for the fingers to reach a laptop keyboard hovering in the void, surrounded by
    leaves and berries in a moody palette.
  :link: blog/remote-ide-machines
  :path: blog/2022-08-15
  :body: "\n\n<p class=\"lead\">Fly.io upgrades containers to full-fledged virtual
    machines running on our hardware around the world, connected with WireGuard to
    a global Anycast network. This post is about one fun thing you can run on a VM.
    [Check us out](https://fly.io/docs/speedrun/): your app can be running in minutes.</p>\n\n\"Remote
    development environment!\" \n\nWhether you reacted with a thrill of enthusiasm,
    a surge of derision or a waft of indifference, we're not really here to change
    your mind. That phrase means a lot of different things at this point in history.
    The meaning we pick today is \"nerd snipe.\"\n\nLet's set up a remote in-browser
    IDE, configured for Elixir / Phoenix development, the hard way&mdash;that is:
    using the command line and a Dockerfile.\n\n<aside class=\"right-sidenote\">Browser-based
    IDEs are a thing, and we happen to have a rather convenient infrastructure at
    our fingertips for deploying web apps (plug! CTA!). When we say \"the hard way\",
    we have to concede that there are harder ways! We're not writing an IDE; we're
    not forwarding X11; it's all relative.</aside>\n\n## What is this?\n\nWe've leaned
    away from blog posts with a lot of code blocks in them. Too many code blocks make
    our eyes glaze over. But we really wanted to show off a fun way to play with [Fly
    Machines](https://fly.io/blog/fly-machines/), which are VMs that you manage directly.
    And a personal remote development environment is just the ticket: for individual
    use, we don't need load-balancing, or lots of instances, or always-on&mdash;in
    fact, it's better if we can turn it on and off. \n\nSo here we are. Once we got
    those code blocks flowing, we didn't stop until we'd deployed, step-by-step, not
    one, but two separate apps on Fly.io.\n\n## What you're in for\n\nIf you perform
    the ritual to completion, you'll have deployed [an Elixir / Phoenix / SQLite hello-world
    app](https://github.com/fly-apps/hello_elixir_sqlite) to Fly.io, from your own
    personal Elixir / Phoenix development environment that you've configured and deployed
    on Fly.io, complete with [code-server](https://github.com/coder/code-server) IDE.
    You'll use the power of Machines to get the VM to go to sleep when you're not
    using it, making it cheaper to run.\n\nThe steps to get there look roughly like:\n\n1.
    Configure and deploy the development environment on a Fly.io Machine:\n   1. Clone
    a repo with the files needed; optionally, read these files and some explanation
    of what they do.\n   2. Create an app. Give it a storage volume and an IP address;
    set secrets.\n   3. Create and start the new VM, associated with the app. Pow!
    Remote development environment!\n2. Log into the remote dev environment. Mime
    developing, and actually deploy, a hello-world Elixir / Phoenix app:\n   1. Test
    the sample app on a dev server running on the remote VM; visit it in the browser.\n
    \  2. Create an app. Set a secret. Give it a storage volume. Deploy!\n\n<div class=\"callout\">\n\n###
    Costs\n\nBetween the remote development VM and the Phoenix app we'll deploy as
    part of the demo, we're planning to provision two VMs. The Phoenix app fits into
    the Fly.io [free allowance](https://fly.io/docs/about/pricing/) for compute. We
    recommend giving the remote development environment 1GB of RAM, which takes it
    past our free allowance: it would cost [a few dollars a month](https://fly.io/docs/about/pricing/#virtual-machines)
    to run it full-time. We'll provision two  volumes with a total of 3GB capacity,
    which exactly matches the free storage allowance on our [Hobby Plan](https://fly.io/docs/about/pricing/#hobby-plan).\n\nDestroying
    both new apps after finishing the tutorial (if you have no further use for them)
    will keep costs minimal.\n\n</div>\n\n## The thing we want to make\n\nAs we've
    mentioned, [code-server](https://github.com/coder/code-server), an in-browser
    VS Code, is our IDE of choice for today&#39;s exercise. It&#39;s far from the
    only possibility, looking at the range of VS Code-flavoured possibilities alone
    (starting with zero amount of VS Code, and stopping short of fully-managed services).
    Say we&#39;re connecting over SSH. A perfectly good remote dev environment, according
    to some people, would be tmux and Vim over SSH. [VS Code and an SSH tunnel](https://code.visualstudio.com/docs/remote/ssh)
    is a more comfortable option for most of us. If we can&#39;t, or don&#39;t want
    to, install VS Code on our local device, [code-server](https://github.com/coder/code-server)
    makes it into a web app. We can [get at code-server over SSH too](https://coder.com/docs/code-server/latest/guide#port-forwarding-via-ssh).\n\nWe
    can also get code-server in the browser over an HTTPS connection (with a Let&#39;s
    Encrypt certificate and a TLS-terminating proxy), and put a password in front
    of it. Since Fly.io will provide us the cert and proxy almost without us noticing,
    that's the instant-gratification route. We'll go that way today, and skip over
    questions of SSH and WireGuard. If you want to talk about SSH and Fly.io remote
    dev setup, [go see Amos](https://fasterthanli.me/articles/remote-development-with-rust-on-fly-io).\n\nWe'll
    build from a Dockerfile. Our Docker image will hold a clean-slate (but mostly-configured)
    development environment. If we ever get mired in dependency hell, or our SSD goes
    fizzle, we can deploy a fresh machine using that.\n\nWorking files will live on
    a persistent storage volume. Nothing stops us checking our work into a remote
    Git repository regularly too.\n\nWe want to shut it down when it's not in use
    (because we don't get charged for CPU or RAM while the Fly Machine is `stopped`).
    Machines can be started and stopped manually using their [REST API](https://fly.io/docs/reference/machines/)
    or [flyctl](https://fly.io/docs/flyctl/machine/), but here we'll also run a proxy
    called [Tired Proxy](https://github.com/superfly/tired-proxy) that shuts the server
    down if it doesn't get any HTTP requests for some time. As for waking it up: Fly's
    proxy itself tries to wake machines for HTTP requests (and TCP connections).\n\n##
    Begin construction \n\nLet's get our hands dirty, starting by walking through
    the three files we use to build our image. If you don't want to type them in,
    you can clone [the repo](https://github.com/fly-apps/code-server-dev-environment):
    `git clone https://github.com/fly-apps/code-server-dev-environment.git`\n\nIf
    you really don't want to bash out the commands, we do have a [Code Server Launcher](https://fly.io/launch/code-server).
    It will immediately deploy the same remote dev environment we're creating in this
    demo.\n\n### Dockerfile\n\n```docker\nFROM lubien/tired-proxy:2 as proxy\nFROM
    hexpm/elixir:1.12.3-erlang-24.1.4-debian-bullseye-20210902-slim\n\n# install build
    dependencies\nRUN apt-get update -y \\\n    && apt-get install -y build-essential
    git unzip curl \\\n    && apt-get clean && rm -f /var/lib/apt/lists/*_* \\\n    &&
    curl -fsSL https://code-server.dev/install.sh | sh\n\n# prepare build dir\nWORKDIR
    /app\n\n# Use bash shell\nENV SHELL=/bin/bash\n\nRUN curl -L https://fly.io/install.sh
    | sh \\\n    && echo 'export FLYCTL_INSTALL=\"/root/.fly\"' >> ~/.bashrc \\\n
    \   && echo 'export PATH=\"$FLYCTL_INSTALL/bin:$PATH\"' >> ~/.bashrc \\\n    &&
    code-server --install-extension elixir-lsp.elixir-ls\n\n# Apply VS Code settings\nCOPY
    settings.json /root/.local/share/code-server/User/settings.json\n\n# Use our custom
    entrypoint script first\nCOPY entrypoint.sh /entrypoint.sh\n\nCOPY --from=proxy
    /tired-proxy /tired-proxy\n\nENTRYPOINT [\"/entrypoint.sh\"]\n```\n\nTo summarize
    that: \n\nIt's a multi-stage build that gets the code for Tired Proxy from its
    public Docker image, and uses an official Elixir base image to save us the trouble
    of finding things like Elixir, Mix, etc., that every Elixir dev environment should
    have.\n\n<aside class=\"right-sidenote\">In case you're interested, here's the
    source for the Tired Proxy: https://github.com/superfly/tired-proxy. This is one
    of my first Go experiences, so try not to be too traumatized.</aside>\n\nOver
    that, it installs some other things we know we want, including code-server (our
    IDE), flyctl (so we can deploy apps from the code-server terminal), and the [Elixir
    Language Server](https://github.com/elixir-lsp/elixir-ls) extension for code-server
    (to make developing Elixir apps more comfy).\n\nIt copies in two other files from
    the local working directory: `settings.json` (just to get the dark theme in VS
    Code) and `entrypoint.sh` (a shell script which encapsulates all the things the
    VM should do every time it starts up). The Tired Proxy executable is copied from
    the first stage.\n\nFinally, it sets ENTRYPOINT to run `/entrypoint.sh`.\n\n###
    entrypoint.sh\n\n```\n#!/bin/bash\n\nTIME_TO_SHUTDOWN=3600\n\nmkdir -p /project\n\n#
    In case fly volumes put something there\nrm -rf '/project/lost+found'\n\nif [
    -z \"$(ls -A /project)\" ]; then\n    echo \"Preparing project\"\n    rm -rf /project\n
    \   git clone $GIT_REPO /project\n\n    cd /project\n\t\t\n    echo \"Setting
    up Elixir environment\"\n\n    mix local.hex --force\n    mix local.rebar --force\n
    \   mix deps.get\nfi\n\ncode-server --bind-addr 0.0.0.0:9090 /project &\n    /tired-proxy
    --port 8080 --host http://localhost:9090 --time $TIME_TO_SHUTDOWN\n```\n\nHere's
    what that does:\n\nFirst, it sets the TIME\\_TO\\_SHUTDOWN environment variable
    to 3600 seconds (1 hour). This is used in the `tired-proxy` command later on.\n\nIt
    creates a folder for the Elixir project to live in, if one doesn't already exist.
    The `-p` tag prevents errors in the case that `/project` already exists (as it
    should the second time you start the VM).\n\nIt initializes the environment, if
    that hasn't already happened, by cloning project files from the repo indicated
    in the GIT_REPO environment variable (which we'll set when we run the VM), installing
    Hex and Rebar locally (non-interactively, with the `--force` flag), and getting
    project dependencies.\n\nFinally, the trick we have up our sleeve: it spawns a
    code-server, with the `/project` folder open, listening on port 9090&mdash;but
    we don't expose this port directly. Tired Proxy maps port 8080 to 9090, and if
    there's no incoming HTTP connection for $TIME\\_TO\\_SHUTDOWN seconds, it exits.
    That's it. That's the whole trick.\n\n### settings.json\n\nWe provide a `settings.json`
    just to get the dark theme in our IDE.\n\nIf you're a VS Code user, you can provide
    your own preferences in this file.\n\n```\n{\n    \"workbench.colorTheme\": \"Default
    Dark+\",\n    \"git.postCommitCommand\": \"sync\",\n    \"git.enableSmartCommit\":
    true,\n    \"git.confirmSync\": false,\n    \"git.autofetch\": true\n  }\n```\n\n##
    Over to flyctl\n\nIf you're new to Fly.io, [install flyctl](https://fly.io/docs/getting-started/installing-flyctl/),
    the Fly.io CLI tool, and run [`fly auth signup`](https://fly.io/docs/hands-on/sign-up/).
    If you already have flyctl installed, it's worth making sure it's up to date with
    `fly version update`.\n\n<div class=\"callout\">\n### `flyctl` vs `fly`\n\nThis
    is more of a secret than it should be, but flyctl uses `fly` as an alias for `flyctl`.
    So if I type `fly secrets` and the docs are for `flyctl secrets`, that's all that's
    about.</div>\n\n## Prepare the Fly Machines App\n\n<div class=\"callout\">\n\n\"Fly
    Machines App?\" Let's back up just a bit. \n\n[Machines are basically just one
    level lower than apps.](https://fly.io/blog/fly-machines/) They're VMs you can
    create, destroy, start, and stop directly through a REST API or with flyctl. The
    Fly.io platform still needs to keep track of these VMs&mdash;who they belong to,
    where to route requests, all that. A machines app is where machine VMs store their
    important documents: their passports, Sears, Roebuck & Co. stock certificates,
    public IP addresses, etc. A machines app doesn't run your code unless it has at
    least one Machine to provide the code&mdash;not to mention the CPU and RAM.\n\nThis
    is not so different from the kind of app we're used to talking about: the kind
    that our orchestrator keeps running, to the best of its ability, until you scale
    to zero or destroy it. In both cases, you have VMs, and you have some centrally-stored
    information. Conceptually, it's probably not helpful to hold onto a distinction
    between \"machines apps\" and \"app apps\". But for the moment, there's a practical
    difference in the way these two cases are implemented, so when it's time to register
    a new app on Fly.io to shuffle paperwork for your code-server VM, you need to
    create a Machines App.\n</div>\n\n\n[Register](https://fly.io/docs/flyctl/apps-create/)
    a new machines app on Fly.io with a name and an organization. The remote IDE URL
    will be `<your-app-name>.fly.dev`, so choose well.\n\n```cmd\nfly apps create
    <your-app-name> --machines\n```\n\n<div class=\"callout\"> Machines, and flyctl,
    are evolving so fast that gingerly configuring everything step-by-step like this
    is going to look positively quaint any minute now. We're chasing a moving target
    here. </div>\n\n\n[Create a new volume](https://fly.io/docs/flyctl/volumes-create/)
    called `storage`, tied to this app, with size 2GB. You can choose to make it smaller.
    The VM will be tied to the hardware this volume is on.\n\n\n```cmd\nfly volumes
    -a <your-app-name> create storage --size 2 \n```\n```out\n? Select region:  [Use
    arrows to move, type to filter]\n  Amsterdam, Netherlands (ams)\n  Paris, France
    (cdg)\n  Dallas, Texas (US) (dfw)\n  Secaucus, NJ (US) (ewr)\n  Frankfurt, Germany
    (fra)\n> São Paulo (gru)\n  Hong Kong, Hong Kong (hkg)\n  Ashburn, Virginia (US)
    (iad)\n  Los Angeles, California (US) (lax)\n  London, United Kingdom (lhr)\n
    \ Chennai (Madras), India (maa)\n  Madrid, Spain (mad)\n  Miami, Florida (US)
    (mia)\n  Tokyo, Japan (nrt)\n  Chicago, Illinois (US) (ord)\n```\n\nBy default,
    Fly.io apps have private IPV6 addresses for use within their organization's WireGuard
    network. If you want to access this app without a WireGuard tunnel, it needs a
    public IP.\n\nSince you're not running `fly deploy` on this app, you need to [allocate
    this manually](https://fly.io/docs/flyctl/ips-allocate-v4/).\n\n```cmd\nfly ips
    allocate-v4 -a <your-app-name>\n```\n\nUse `fly secrets` to pass in secret environment
    variables. Note the `--stage` flag, which is needed (at this time) because setting
    secrets on a machines app triggers a deploy by default, and we don't want that
    in our case.\n\n```cmd\nfly secrets set -a <your-app-name> \\\nPASSWORD=<mypassword>
    \\\nFLY_API_TOKEN=$(fly auth token) \\\n--stage\n```\n```out\nSecrets are staged
    for the first deployment\n```\n\ncode-server asks for a password when you first
    open it. It will generate its own random password on installation if the PASSWORD
    environment variable isn't set. You can still ssh into the VM later and extract
    it from a config file, but it's easier to set it ahead.\n\nProviding your `fly
    auth token` allows you to deploy other apps to Fly.io from within this app. Trippy!\n\n##
    Run the machine for the first time\n\nHere's the invocation to bring the code-server
    VM into being! Your app will be discoverable on the Internet as soon as the VM
    is up and listening for requests.\n\n```cmd\nfly machine run . \\\n  -p 443:8080/tcp:tls
    \\\n  -p 4000:4000/tcp:tls \\\n  --memory 1024 \\\n  --region <your-region> \\\n
    \ --volume storage:/project \\\n  --env FLY_REMOTE_BUILDER_HOST_WG=1 \\\n  --env
    GIT_REPO=https://github.com/fly-apps/hello_elixir_sqlite.git \\\n  -a <your-app-name>\n```\n\n*
    `fly machine run .`: Run a new Fly Machine VM. The first argument is the image
    or the path to the Dockerfile. In this case it's the current folder (don't miss
    out the ` .`). \n* `-p 443:8080/tcp:tls`: Map port 8080 to the external HTTPS
    port (443) so we can access it via your-app-name.fly.dev. Remember this port belongs
    to the Tired Proxy so the server shuts down if no one uses it for a while.\n*
    `-p 4000:4000/tcp:tls` : Elixir Phoenix uses port 4000 for development. Open this
    port so you can visit the Phoenix app's dev server on `<your-app-name>.fly.dev:4000`.
    Note: This does mean the world can see it, too.\n* `--memory 1024`: For the best
    experience (i.e. to avoid OOM crashes), we'll go for the [recommended mimimum](https://coder.com/docs/code-server/latest/requirements)
    of 1GB. You can try a lower value, but keep this choice in mind if you have to
    debug a VM crash!\n* `--region <your-region>`: In order to mount the volume you
    provisioned earlier, the three-letter code &lt;your-region&gt; must match the
    volume's region.\n* `--volume storage:/project`: Mount the volume called `storage`
    to the path `/project`. That's where your project files will live.\n* `--env FLY_REMOTE_BUILDER_HOST_WG=1`:
    This machine is already on your organization's WireGuard private network, so it
    doesn't need to create a userland WireGuard tunnel to reach a remote builder.
    We use the FLY\\_REMOTE\\_BUILDER\\_HOST\\_WG environment variable to tell flyctl
    to use native WireGuard, which means it'll be faster to deploy apps from code-server.\n*
    `--env GIT_REPO=https://github.com/fly-apps/hello_elixir_sqlite.git`: Store the
    URL for the repo to be cloned to `/project`. We're using a prepared Phoenix /
    SQLite example for fun (and so we can focus on deployment here, and not databases).\n*
    `-a <your-app-name>`: Tell flyctl to deploy the VM under the dev server app.\n\nThe
    above command should result in output close to this:\n\n```\nSuccess! A machine
    has been successfully launched, waiting for it to be started\n Machine ID: d5683003add8e9\n
    Instance ID: 01G3XMBKS6F004P5W0Q51V466A\n State: starting\nWaiting on firecracker
    VM...\nWaiting on firecracker VM...\nWaiting on firecracker VM...\nMachine started,
    you can connect via the following private ip\n  fdaa:0:3335:a7b:1f61:d2d:3742:2\n```\n\n##
    Time to (pretend to) code!\n\nNow we can visit your freshly-minted remote development
    environment at `<your-app-name>.fly.dev`. It'll ask for the password you set earlier
    with `fly secrets set`. Fear not the light theme. Once `settings.json` is read
    in, it'll switch to dark.\n\nOnce you're in, you'll see the `/project` folder
    open, complete with the cloned `hello_elixir_sqlite` project files. README.md
    contains instructions for building, previewing, and deploying that app. In case
    you're just following along in your imagination, we'll repeat them here.\n\nOpen
    the integrated VS Code terminal with <code>^+`</code>.\n\n![A code-server workspace,
    looking a lot like normal VS Code, with the files and terminal panels open.](code-server.png?card&center)\n\nRun
    `mix phx.server` to compile the app and start up the dev server. When that finishes,
    you can check it out in the browser at `<your-app-name>.fly.dev:4000`. You'll
    recall you exposed port 4000 when you created the machine with `fly machine run`.
    This does mean everybody can see the dev server at that address, because this
    Phoenix app doesn't have any password protection.\n\nAfter dutifully clicking
    the button to run migrations, you should see something like this:\n\n![The \"Welcome
    to Phoenix\" hello-world Phoenix starter app landing page running.](sample-app.png?card&center)\n\n\n<%=
    partial \"shared/posts/cta\", locals: {\n  title: \"Reverse CTA\",\n  text: \"It
    was tempting to link back to this post, but you're already here. Read the Machines
    announcement to see more about why we're stoked.\",\n  link_url: \"https://fly.io/blog/fly-machines/\",\n
    \ link_text: \"Get stoked about Machines too&nbsp;&nbsp;<span class='opacity-50'>&rarr;</span>\"\n}
    %>\n\n\n## flyctl Round two: Deploy to Fly.io from Fly.io\n\nIn the VM's Dockerfile,
    you installed flyctl, so you can run any `fly` command from the integrated terminal.
    You're authed, because you set the FLY\\_API\\_TOKEN secret, which the CLI will
    read from the environment if it's available.\n\nIt's time for the second round
    of app-configuration, secret-setting, and volume-creation with flyctl, this time
    all for your Phoenix app.\n\nRegister the new app, but don't deploy it. New app,
    new name. Use the `fly.toml` provided by the project repo. Deploy it wherever
    you like&mdash;it's a whole independent app.\n\n```cmd\nfly launch --no-deploy\n```\n```out\nAn
    existing fly.toml file was found\n? Would you like to copy its configuration to
    the new app? Yes\nCreating app in /project\nScanning source code\nDetected a Dockerfile
    app\n? App Name (leave blank to use an auto-generated name): <your-phoenix-app-name>\n?
    Select organization: Lubien (personal)\n? Select region: gru (São Paulo)\nCreated
    app phoenix-from-fly-vscode in organization personal\nWrote config file fly.toml\nYour
    app is ready. Deploy with `flyctl deploy`\n```\n\n\nGenerate a new secret for
    the Phoenix app.\n\n```cmd\nmix phx.gen.secret\n```\n```out\nAWNbSPHIDuHpIXwLp0vaLJPRA8QSB0X363xiAEWPSS+7bI6n6rcqZltGKQBU1DoE\n```\n\nSet
    that as the secret SECRET\\_KEY\\_BASE that the app will have access to.\n\n```cmd\nfly
    secrets set SECRET_KEY_BASE=AWNbSPHIDuHpIXwLp0vaLJPRA8QSB0X363xiAEWPSS+7bI6n6rcqZltGKQBU1DoE\n```\n```out\nSecrets
    are staged for the first deployment\n```\n\nThis sample app is configured to use
    an SQLite database, so you need some storage.\n\nProvision a 1GB volume in the
    same region as the Phoenix app.\n\n```cmd\nfly volumes create database_data --size
    1 --region gru\n```\n```out\n        ID: vol_w1q85vgl7xprzdxe\n      Name: database_data\n
    \      App: <your-phoenix-app-name>\n    Region: gru\n      Zone: 2824\n   Size
    GB: 1\n Encrypted: true\nCreated at: 01 Jun 22 13:32 UTC\n```\n\nThat's it! Deploy
    the Phoenix app.\n\n```cmd\nfly deploy\n```\n\nIt'll go live at [https://your-phoenix-app-name.fly.dev](https://your-phoenix-app-name.fly.dev/).\n\nYou
    can `stop` the machine using [`fly machine stop`](https://fly.io/docs/flyctl/machine-stop/),
    and revive it just by visiting the app in your browser (or with [`fly machine
    start`](https://fly.io/docs/flyctl/machine-start/)). If you close the tab, it
    will just sleep after an hour without activity.\n\n[Delete](https://fly.io/docs/flyctl/destroy/)
    your Code Server app if you're done with it.  Also destroy the Phoenix app if
    you don't want that!\n\n## Whew\n\nThis project was built to demonstrate our new
    Fly Machines feature, and how simple it can be to launch an app like code-server
    with it. \n\nThe Dockerfile serves as an example of how you can customize your
    setup ready to do some work, and it does some heavy lifting: cloning the repo,
    installing flyctl, and installing an entire Elixir developer environment. It wouldn't
    be hard to swap out the Elixir bits for [Ruby](https://fly.io/docs/rails/) ones,
    if that's your bag!\n\nFly machines are very keen to start themselves if someone
    reaches them over HTTP (thanks, fly-proxy), but they won't stop by themselves
    with the code-server process running. You can reuse our Tired Proxy to send a
    VM to sleep, so you don't get billed for it 24/7. One caveat: with this simple
    setup, if a random bot hits your port 8080, fly-proxy will treat that the same
    as you opening up the app in a tab&mdash;and try to wake the machine. Oh. Two
    caveats: Our experiments indicate that leaving a code-server tab open with the
    terminal pane active may keep it alive too.\n\nYou can also start and stop and
    remove machines using [flyctl](https://fly.io/docs/flyctl/machine/) or the [REST
    API](https://fly.io/docs/reference/machines/); you can write a little app with
    \"start\" and \"stop\" buttons for your machines if you want to play. Obviously,
    the idea is that you could use the Machine API to write much bigger, more interesting
    apps than that. We leave that as an exercise for the reader!\n\n\n"
- :id: blog-sqlite-internals-rollback-journal
  :date: '2022-08-10'
  :category: blog
  :title: How SQLite helps you do ACID
  :author: ben
  :thumbnail: rollback-journal-thumbnail.png
  :alt: A pencil writing in a journal of sandwiches I've eaten.
  :link: blog/sqlite-internals-rollback-journal
  :path: blog/2022-08-10
  :body: |2


    <div class="lead">
      [Fly.io](http://fly.io/) runs apps close to users around the world, by taking containers and upgrading them to full-fledged virtual machines running on our own hardware around the world. Sometimes those containers run SQLite and we make that easy too. [Give us a whirl](https://fly.io/docs/speedrun/) and get up and running quickly.
    </div>

    When database vendors recite their long list of features, they never enumerate "doesn't lose your data" as one of those features. It's just assumed. That's what a database is supposed to do. However, in reality, the best database vendors [tell you exactly how their database will lose your data](https://www.sqlite.org/howtocorrupt.html).

    I've written before about [how SQLite stores your data](https://fly.io/blog/sqlite-internals-btree/). In order not to lose any of it when a transaction goes wrong, SQLite implements a journal. It has two different modes: the rollback journal & the write-ahead log. Today we're diving into the rollback journal: what it is, how it works, and when to use it.

    ## How to lose your data

    To understand why you need a database journal, let's look at what happens without one. [In the last post](/blog/sqlite-internals-btree/), we talked about how SQLite is split up into 4KB chunks called "pages". Any time you make a change—even a 1 byte change—SQLite will write a full 4KB page.

    If you tried to overwrite a page in your database file directly, it would work fine 99% of the time. However, that 1% of the time is catastrophic. If your server suddenly shut down halfway through a page write then you'll end up with a corrupted database.

    The database needs to ensure that all page writes for a transaction either get written or don't. No halfsies. This is called _atomicity_.

    But that's not all. If another process is querying the database, it'll have no consistent view of the data since you're overwriting pages willy-nilly. The database needs to ensure each transaction has a snapshot view of the database for its entire duration. This is called _isolation_.

    Finally, we need to make sure bytes actually get flushed to disk. This part is called _durability_.

    Those make up 3 of the 4 letters of the [ACID transactional guarantee](https://en.wikipedia.org/wiki/ACID) that every database blog post is required to mention. The "C" stands for _consistency_ but that doesn't involve the rollback journal so we'll skip that.

    ## All for one, or none at all

    Every textbook definition of transactions involves a bank transfer where someone withdraws money from one account and deposits in another. Both actions must happen or neither must happen.

    This example gets trotted out because atomicity is so unusual in the physical world that it's hard to find anything else that's as intuitive to understand.

    But it turns out that atomicity doesn't "just happen" in databases either. It's all smoke and mirrors. So let's use a better example that involves our favorite topic: sandwiches.

    ### Building a sandwich

    When you go to a sandwich shop, you walk up to the counter, announce your order, and you get a tasty sandwich in hand a short time after. To you, the consumer, this is atomic. If you order a ham-and-cheese sandwich, you won't receive just a slice of ham or two pieces of dry bread. You either get a sandwich or you don't.

    But behind the counter, there are multiple steps involved: grab the bread, add the ham, add the cheese, hand it to the customer. If the sandwich maker gets to the cheese step and realizes they're out of cheese, they can tell you they can't make the sandwich and then put the ham and bread back where they found it. The internal state of the sandwich shop is restored to how it was before the order started.

    The rollback journal is similar. It records the state of the database before any changes are made. If anything goes wrong before we get to the end, we can use the journal to put the database back in its previous state.

    ### Our first transaction

    Let's start our first transaction by creating a table in a `sandwiches.db` database:

    ```
    CREATE TABLE sandwiches (
        id INTEGER PRIMARY KEY,
        name TEXT
    );
    ```

    SQLite starts by creating a `sandwiches.db-journal` file next to our `sandwiches.db` database file and [writing a journal header](https://github.com/sqlite/sqlite/blob/3cf46ee508e97b46736a2607ded9c84c2c16229f/src/pager.c#L1425-L1439):

    ```
    00000000 00000000 00000000 f65ddb21 00000000 00000200 00001000
    ```

    The first 12 bytes are filled with zeros but they'll be overwritten at the end of our transaction so let's skip them for now.

    The value `0xf65ddb21` is called a nonce and it's a randomly generated number that we'll use to compute checksums for our entries in the journal. SQLite has [some journal modes](https://www.sqlite.org/pragma.html#pragma_journal_mode) where it'll overwrite the journal instead of delete it so the checksums help SQLite know when its working with contiguous set of entries and not reading old entries left behind from previous transactions.

    Next, we have `0x00000000` which is the size of the database before the transaction started. Since this is the first transaction, our database was empty before the transaction.

    Then we specify the sector size of `0x00000200` (or 512).  A disk sector is the smallest unit we typically work with for disk drives and SQLite keeps the journal header on its own sector. It does this because journal header is later rewritten and we don't want to accidentally corrupt one of our pages if a sector write fails.

    Finally, we have `0x00001000` (or 4,096) which is the page size for our database, in bytes.

    SQLite can now freely write changes to the database file while knowing that it has written down the state of the database from before the transaction started.

    When you go to commit the changes, SQLite will rewrite the first 12 bytes of the journal header with two new fields: [the magic number](https://en.wikipedia.org/wiki/Magic_number_(programming)) & the number of page entries in the journal.

    ```
    d9d505f9 20a163d7 00000000
    ```

    The "magic number" is a ridiculous name for a constant value that is written to the beginning of a file to indicate its file type. For journal files, this magic number is `d9d505f9 20a163d7`. We don't have any page entries since our database was empty so the page count stays as zeros.

    Next, we'll [sync the journal to disk](https://github.com/sqlite/sqlite/blob/3cf46ee508e97b46736a2607ded9c84c2c16229f/src/pager.c#L4184-L4218) to make sure we don't lose it.

    The final step that ends the commit is when SQLite deletes the file. If any of the previous steps fail then SQLite can use the rollback journal to revert the state of the database. Just like with your ham-and-cheese, the transaction doesn't happen until you have a sandwich in your hand.

    ![Diagram of the database with 2 pages and a journal file with only a header.](journal-header-only.png)



    ### Copying out to the journal

    Now let's see how the journal works with an existing database. Our first transaction left us with a 2-page database. The first page holds our database header and some metadata about our schema. The second page is an empty leaf page for our `sandwiches` table.

    We'll insert our sandwich into our table:

    ```
    BEGIN;
    INSERT INTO sandwiches (name) VALUES ('ham and cheese');
    ```

    This will create a new journal with the following header:

    ```
    00000000 00000000 00000000 0600399E 00000002 00000200 00001000
    ```

    It looks similar to before but we have a new randomly-generated nonce (`0x0600399e`) and our database size before the transaction is now `0x00000002` pages instead of zero.

    Since our transaction is updating the leaf page, SQLite needs to copy out the original version of the page to the journal as a page record. The journal page records are comprised of 3 fields.

    First, we have the page number to indicate that we're updating page 2:

    ```
    00000002
    ```

    Then it's followed by a copy of the 4,096 bytes that were in the page before the transaction started. Finally, it computes a 32-bit checksum on the data in the page:

    ```
    0600399e
    ```

    Interestingly, the checksum is only calculated on a very sparse number of bytes in the page and is primarily meant to guard against incomplete writes. Since SQLite 3.0.0 dates back to 2004 and it works on minimal hardware, reducing any overhead can be critical. You can see the evolution of computing power as the WAL mode, which was introduced in 2010, checksums the entire page.

    With our original copy of the page in the journal, we can update our copy in the main database file without having to re-copy the page. We can add a second sandwich to our transaction and SQLite will only update the main database file:

    ```
    INSERT INTO sandwiches (name) VALUES ('cheesesteak');
    ```

    The database and journal end up looking like this:

    ![Diagram of copying page 2 from the database to an entry in the journal file.](journal-with-entry.png)

    ## When sandwiches go bad

    Back to our sandwich shop example, let's say there is a catastrophic sandwich event that occurs in the middle of your order. Perhaps your sandwich artist couldn't stand to make one more ham-and-cheese sandwich and abruptly quit.

    So our shop owner subs out a new employee to replace the old one so the sandwich production can continue. But how do we deal with the in-process sandwich? The new employee could try to finish the sandwich but maybe the customer gave specific instructions to the old employee. When you're dealing with something as critical as lunch, it's best to start over and do it right.

    When SQLite encounters a failure scenario, such as an application dying or a server losing power, it needs to go through a process called "[recovery](https://github.com/sqlite/sqlite/blob/3cf46ee508e97b46736a2607ded9c84c2c16229f/src/pager.c#L2691-L2747)". For a rollback journal, this is simple. We can walk through our journal page records and copy each page back into the main database file. At the end, we truncate our main database file to the size specified in the journal header.

    ![Diagram of the journal entry being copied back to the database file during recovery.](recovery.png)

    Rollback journals are even resistant to failures during their own recovery. If your server crashes midway through a recovery process, SQLite will simply start the recovery process from the beginning of the journal file.

    The procedure is idempotent and is not considered complete until the pages copied back are synced to disk and the journal file is deleted. For example, let's say we'd only copied half of page 2 in our diagram from the journal back to the database file and then our server crashed. When we restart, we still have our journal file in place and we can simply try copying that page again.

    ## Keeping track of our ingredients

    Our sandwich shop owner begins to suspect that employees are skimming pickles off the line and decides to hire folks to inventory ingredients periodically. However, the owner quickly realizes that the inventory numbers are off because the inventory specialists are trying to count ingredients while sandwich makers are taking those same ingredients to put into sandwiches.

    To fix this, the owner decides that the store must be locked while a sandwich is being made. However, when a sandwich isn't being made, any number of inventory specialists can come in and count ingredients.

    This is how it works in SQLite when using the rollback journal. Any number of read-only transactions can occur at the same time. However, when we start a write transaction then we need to wait for the readers to finish and block all new readers until the write is done.

    This makes sense now that you know that we're changing the main database file during a write transaction. We'd have no way to give read transactions a snapshot view of the database if we're updating the same underlying data.

    ### Read/write locks on the file system

    Since SQLite allows multiple processes to access it, it needs to perform locking at the file system level. There are 3 lock bytes that are used to implement the read/write lock at the file system level:

    - `SHARED` - held by read transactions, prevents writers from starting
    - `RESERVED` - held by the write transaction
    - `PENDING` - held by the write transaction to prevent readers from starting

    When a read transaction starts, it checks the `PENDING` lock first to ensure a writer is not inside a write transaction or that a writer is not waiting to start a transaction. If the reader can obtain the `PENDING` lock then it obtains a shared lock on the `SHARED` lock byte and holds it until the end of the transaction.

    For write transactions, it first obtains an exclusive lock on the `PENDING` lock byte to prevent new read transactions from starting. It then tries to obtain an exclusive lock on the `SHARED` lock byte to wait for in-process read transactions to finish. Finally, it obtains an exclusive lock on the `RESERVED` lock byte to indicate that a write transaction is in-process.

    This series of steps ensure that only one write transaction is in effect at any time and that new readers won't block it.

    ![Diagram of the structure of the lock page.](lock-page.png)

    Locks are located on a page at the 1GB position in the database file and this page is unusable by SQLite as [some Windows versions use mandatory locks](https://github.com/sqlite/sqlite/blob/3cf46ee508e97b46736a2607ded9c84c2c16229f/src/os.h#L104-L158) instead of advisory locks. If a database is smaller than 1GB, this page is never allocated and only exists within the operating system's lock accounting system.

    Within the lock page, a byte is used for the `PENDING` lock and another byte for the `RESERVED` lock. After that, 510 bytes are used for the `SHARED` lock. A byte range is used here to accommodate older Windows versions with mandatory locks. In those cases, a randomly chosen byte is locked by a client within that range. On Unix, the entire range is locked using `fctnl()` and `F_RDLCK`.

    ## How to improve on journaling

    The rollback journal is a simple trick to simulate atomicity and isolation and to provide durability to a database. Simple tricks are the best kind of tricks when you write a database so it's a great place to start.

    But it certainly has its trade-offs. Kicking out all other transactions whenever you need to write something can become a bottleneck for many applications that have concurrent users. When people say that SQLite doesn't scale, it's typically because they used the rollback journal.

    However, SQLite continued to improve and eventually introduced the write-ahead log (WAL) journaling mode and even the `wal2` journaling mode. These provide significantly better support for concurrent readers.

    This means that our inventory specialists in our example could each have a point-in-time view of all the ingredients—even while the sandwich maker continues to make sandwiches! We'll get into how this works in our next post on WAL mode.
- :id: phoenix-files-making-tabs-mobile-friendly
  :date: '2022-08-08'
  :category: phoenix-files
  :title: Making Tabs Mobile Friendly
  :author: mark
  :thumbnail: tab-recipe-thumbnail.jpg
  :alt:
  :link: phoenix-files/making-tabs-mobile-friendly
  :path: phoenix-files/2022-08-08
  :body: "\n\n<p class=\"lead\">This is a post about making a Tailwind styled tabs
    component play nicely on mobile without using Alpine.js or other client-side frameworks.
    If you want to deploy your Phoenix LiveView app right now, then check out how
    to [get started](/docs/elixir/). You could be up and running in minutes.</p>\n\nThis
    recipe creates a [Tailwind UI styled tab component](https://tailwindui.com/components/application-ui/navigation/tabs#component-de43ff625fee032d234b14989e88422f)
    that gracefully switches to an HTML `select` input when viewed on smaller screens.
    It doesn't use [Alpine.js](https://alpinejs.dev/) or other client-side javascript
    frameworks for managing the UI. It is built as a client-side, reusable, LiveView
    component.\n\nThe finished component works and looks like this:\n\n![Finished
    working tab behavior animation](./finished-working-tab-behavior.gif?center&card&border)\n\n**Why
    is this tricky?**\n\nThe trick bit is that tab titles can be long. When displayed
    on a narrow mobile device, it can look ugly. [Tailwind UI](https://tailwindui.com/components/application-ui/navigation/tabs)
    styled tabs solve this by using browser media queries to dynamically switch to
    an HTML `select` input. The _extra_ tricky bit comes when rotating a device from
    wide to narrow and the currently selected tab needs to now show the appropriate
    active select option. So the HTML `select` and the tabs need to be kept in selection
    sync even when hidden.\n\nWell, we're going to figure how to build tabs that are
    mobile friendly in this way and work entirely client-side in the browser.\n\nThe
    template code to create this component looks like this.\n\n```html\n<.tab_list
    id=\"my-tabs\">\n  <:tab title=\"Tab 1\" current>\n    <p class=\"mt-2\">Lorem
    ipsum.</p>\n  </:tab>\n\n  <:tab title=\"Tab 2\">\n    <p class=\"mt-2\">Lorem
    2 ipsum.</p>\n  </:tab>\n\n  <:tab title=\"Tab3\">\n    <p class=\"mt-2\">Lorem
    3 ipsum.</p>\n  </:tab>\n</.tab_list>\n```\n\nIf this interests you, read on!\n\n##
    Backing up to answer the \"Why?\"\n\nWhy do it this way? Why do it without a client-side
    JS solution like Alpine.js? Let's talk through the thought process that brought
    us here.\n\n### `phx-click` Approach\n\nThe first pass solution at a tabs component
    used `phx-click` attributes to notify the server that a tab selection changed.
    This worked well and was really easy.\n\nProblems:\n- Didn't work well for the
    mobile-friendly `select` input. Keeping them in sync means going through the server.\n-
    Click events require talking to the server.\n\nThe real problem here is that the
    server doesn't _need_ to be involved with this UI-only change.\n\nCan we change
    tabs without involving the server?\n\n### [Alpine.js](https://alpinejs.dev/) Approach\n\nThere
    really was no reason to involve the server for changing active tabs. For this
    use case, it really should remain a client-side only feature.\n\nThe **PETAL**
    stack uses [Alpine.js](https://alpinejs.dev/) for client-side Javascript interactions.
    Here's a list of the different parts of PETAL:\n\n- [**P**hoenix](https://www.phoenixframework.org/)\n-
    [**E**lixir](https://elixir-lang.org/)\n- [**T**ailwind](https://tailwindcss.com/)\n-
    [**A**lpine.js](https://alpinejs.dev/)\n- [**L**iveView](https://github.com/phoenixframework/phoenix_live_view)\n\nThis
    means the default tool to reach for here is [Alpine.js](https://alpinejs.dev/).
    If you've ever inspected the Tailwind UI components in the browser, they are implemented
    using Alpine.js. That's another check in the Alpine column!\n\nBut I don't want
    to use Alpine.js. It's nothing against Alpine.js! It's a great framework. Using
    Alpine.js adds a JS dependency to any project wanting to use this tabs component.
    This means it's less portable between all my great unicorn app projects.\n\nCan
    we add the client-side tabs without using Alpine.js?\n\n### Hooks Approach\n\nPhoenix
    LiveView has a feature called [client hooks](https://hexdocs.pm/phoenix_live_view/js-interop.html#client-hooks-via-phx-hook).
    This is probably the way we _should_ do it. It offers the most power and versatility.\n\nWhy
    not do it this way?\n\nA major benefit to me of the **PETAL** stack is \"co-location\".\n\nWith
    a LiveView component, the HTML markup and rendering logic are all in one place.
    Additionally, the styles are in the markup (Tailwind CSS) and any custom JS can
    be included in the HTML using Alpine.js.\n\nWe get nifty benefits from \"co-location\".
    Everything is one place. The benefit is a mental one mostly. It's the benefit
    of not needing to jump between multiple CSS files, template markup, a separate
    code file and then separate JS file(s).\n\nCo-location brings a simplification
    and organizational benefit. It keeps my brain from melting down holding all that
    in _addition_ to the problem I'm trying to solve.\n\nThe reason not to use the
    hook approach here is because I want to see if I can keep the benefits of co-location
    without using Alpine.js for the JS part.\n\nCan we get the JS behavior we want
    without using hooks?\n\n### [JS Commands](https://hexdocs.pm/phoenix_live_view/Phoenix.LiveView.JS.html#content)
    Approach\n\nLiveView's JS commands are a recent addition (v0.17+). They provide
    commands for executing JavaScript utility operations on the client.\n\nThis is
    where Berenice's excellent post [Client-Side Tabs in LiveView With JS Commands](../tabs-with-js-commands/)
    came in. It was a great all-client-side solution using JS commands! Boom! Done!\n\nProblem:\n-
    It didn't include the `select` mobile behavior. That's a major goal for this particular
    component.\n\nThat shouldn't be hard to add, right?\n\n**Ahem.**\n\nThat's where
    this post comes in. With help, we got it solved. We'll cover what was learned
    along the way and suggest possible improvements.\n\n## Problem being solved\n\nLet's
    clearly define the problem so we know when it's solved.\n\nHow do I create a Tailwind
    UI styled tab component that switches to an HTML `select` input for mobile formats
    using only LiveView?\n\nConstraints:\n- Don't involve the server for something
    that is client-only UI.\n- Avoid adding a Javascript dependency like [Alpine.js](https://alpinejs.dev/).\n-
    Don't use [hooks](https://hexdocs.pm/phoenix_live_view/js-interop.html#client-hooks-via-phx-hook)
    in order to keep co-location benefits.\n- Do it using only LiveView provided features.\n\nSo
    how do we actually _do_ that?\n\n## Solution\n\nNOTE: This builds on Berenice's
    excellent post [Client-Side Tabs in LiveView With JS Commands](../tabs-with-js-commands/).\n\nThere
    are two parts to the solution. The LiveView component code and 4 lines of extra
    javascript (which comes in later).\n\nThe following Elixir code defines a stateless
    function component for our tabs. It supports a slot named `tab` for defining a
    tab's content. We'll talk a bit more about it after the code. You'll also notice
    the private function `show_tab` that builds the JS commands for activating a tab.\n\n```elixir\ndefmodule
    Web.Components do\n  import Phoenix.LiveView, only: [assign: 3, assign_new: 3],
    warn: false\n  import Phoenix.LiveView.Helpers\n  alias Phoenix.LiveView.JS\n\n
    \ def tab_list(assigns) do\n    assigns =\n      assigns\n      |> assign_new(:active_class,
    fn -> \"border-indigo-500 text-indigo-600\" end)\n      |> assign_new(:inactive_class,
    fn -> \"border-transparent text-gray-500 hover:text-gray-700 hover:border-gray-300\"
    end)\n      |> assign_new(:class, fn -> \"\" end)\n      |> assign_new(:tab, fn
    -> [] end)\n\n    ~H\"\"\"\n    <div id={@id} class={@class}>\n      <div class=\"sm:hidden\">\n
    \       <label for={\"#{@id}-mobile\"} class=\"sr-only\">Select a tab</label>\n
    \       <select\n          id={\"#{@id}-mobile\"}\n          name=\"tabs\"\n          phx-change={JS.dispatch(\"js:tab-selected\",
    detail: %{id: \"#{@id}-mobile\"})}\n          class=\"block w-full pl-3 pr-10
    py-2 text-base border-gray-300 focus:outline-none focus:ring-indigo-500 focus:border-indigo-500
    sm:text-sm rounded-md\"\n        >\n          <%%= for {tab, i} <- Enum.with_index(@tab)
    do %>\n            <option value={\"#{@id}-#{i}\"}><%%= tab.title %></option>\n
    \         <%% end %>\n        </select>\n      </div>\n      <div class=\"hidden
    sm:block\">\n        <div class=\"border-b border-gray-200\">\n          <nav
    class=\"-mb-px flex space-x-8\" aria-label=\"Tabs\">\n            <%%= for {tab,
    i} <- Enum.with_index(@tab), tab_id = \"#{@id}-#{i}\" do %>\n              <%%=
    if tab[:current] do %>\n                <.link id={tab_id} phx-click={show_tab(@id,
    i, @active_class, @inactive_class)} class={\"group inline-flex items-center py-4
    px-1 border-b-2 font-medium text-sm #{@active_class}\"} aria-current=\"page\">
    <%%= tab.title %> </.link>\n              <%% else %>\n                <.link
    id={tab_id} phx-click={show_tab(@id, i, @active_class, @inactive_class)} class={\"group
    inline-flex items-center py-4 px-1 border-b-2 font-medium text-sm #{@inactive_class}\"}>
    <%%= tab.title %> </.link>\n              <%% end %>\n            <%% end %>\n
    \         </nav>\n        </div>\n      </div>\n      <%%= for {tab, i} <- Enum.with_index(@tab)
    do %>\n        <div id={\"#{@id}-#{i}-content\"} class={if !tab[:current], do:
    \"hidden\"} data-tab-content><%%= render_slot(tab) %></div>\n      <%% end %>\n
    \   </div>\n    \"\"\"\n  end\n\n  defp show_tab(js \\\\ %JS{}, id, tab_index,
    active_class, inactive_class) do\n    tab_id = \"#{id}-#{tab_index}\"\n\n    js\n
    \   |> JS.add_class(\"hidden\", to: \"##{id} [data-tab-content]\")\n    |> JS.remove_class(\"hidden\",
    to: \"##{tab_id}-content\")\n    |> JS.remove_class(active_class, to: \"##{id}
    nav a\")\n    |> JS.add_class(inactive_class, to: \"##{id} nav a\")\n    |> JS.remove_class(inactive_class,
    to: \"##{tab_id}\")\n    |> JS.add_class(active_class, to: \"##{tab_id}\")\n    |>
    JS.remove_attribute(\"selected\", to: \"##{id}-mobile option\")\n    |> JS.set_attribute({\"selected\",
    \"\"}, to: \"##{id}-mobile option[value='#{tab_id}'\")\n  end\nend\n```\n\n###
    What is `show_tab` doing?\n\nThe `show_tab` function creates a series of JS commands
    that are linked to the `phx-click` events of the tab links.\n\nThe commands take
    the approach of applying a blanket change to a group of DOM elements before making
    a more specific change to display the active selection. It also updates the HTML
    `<select>` input even when it's not visible.\n\n### What's going on with that
    `for` comprehension?\n\nThis nifty bit of code in the template is worth closer
    examination. Here's a greatly simplified version of it:\n\n```elixir\nfor {tab,
    i} <- Enum.with_index(@tab), tab_id = \"#{@id}-#{i}\" do\n  <.link id={tab_id}
    <%%= tab.title %> </.link>\nend\n```\n\nFirst, the [`Enum.with_index/2`](https://hexdocs.pm/elixir/Enum.html#with_index/2)
    function is given a list of tabs. It returns a list of tuples shaped like `{tab,
    index}`. That's pretty cool on it's own. Within a [`for` comprehension](https://hexdocs.pm/elixir/Kernel.SpecialForms.html#for/1)
    it lets us iterate the list while also getting an incrementing index. It becomes
    similar to a regular `for` loop in non-functional languages.\n\nThe part that
    makes this look confusing is the extra comma and `tab_id = \"#{@id}-#{i}\"`. What's
    that all about?\n\nThis is where we are reminded that an Elixir [`for` comprehension](https://hexdocs.pm/elixir/Kernel.SpecialForms.html#for/1)
    is not a `for` loop. A `for` comprehension can include additional generators or
    filter functions. Our usage here is like a filter that always returns truthy (so
    nothing is filtered out), but we can use it to assign a variable with each iteration.\n\nHere's
    some IEx friendly code to try out and play with it yourself.\n\n```elixir\ntabs
    = [\"tab-1\", \"tab-2\", \"tab-3\"]\n\nfor {tab, i} <- Enum.with_index(tabs),
    tab_id = \"#{tab}-#{i}\" do\n  IO.inspect tab, label: \"TAB\"\n  IO.inspect i,
    label: \"INDEX\"\n  IO.inspect tab_id, label: \"TAB_ID\"\nend\n```\n\nWhen run
    in IEx, the results look like this:\n\n```\nTAB: \"tab-1\"\nINDEX: 0\nTAB_ID:
    \"tab-1-0\"\nTAB: \"tab-2\"\nINDEX: 1\nTAB_ID: \"tab-2-1\"\nTAB: \"tab-3\"\nINDEX:
    2\nTAB_ID: \"tab-3-2\"\n```\n\nThis trick is handy in templates because it's easy
    to define variables in our loop. The alternative you see often in other languages
    is frowned upon in LiveView because it's harder to detect when HEEx template chunks
    have changed.\n\n```erb\n<%%= for {tab, i} <- Enum.with_index(@tab) do %>\n  <%%
    tab_id = \"#{@id}-#{i}\" %>\n  Don't do it this way!\n<%% end %>\n```\n\nSo now
    you know better and won't do it that way. \U0001F642\n\n### Not quite there yet!\n\nJust
    using this code, it works great for non-mobile interfaces. The problem is when
    it switches to the `select` input. Changing the selection using the dropdown doesn't
    activate and show the selected tab contents!\n\nHere's how our solution looks
    to this point:\n\n![tab behavior selection not changing for select](./tab-behavior-selection-not-changing.gif?center&card&border)\n\nWhen
    we change the `<select>` dropdown, no matter what selection we make, it doesn't
    change the displayed tab contents.\n\nWe're soooo close!\n\nThis extra, 4 lines
    of Javascript bridges that chasm for us. This can live in our `app.js` file.\n\n```javascript\n//
    Tabs behavior\nwindow.addEventListener(\"js:tab-selected\", ({detail}) => {\n
    \ let select = document.getElementById(detail.id)\n  let link = document.getElementById(select.value)\n
    \ liveSocket.execJS(link, link.getAttribute(\"phx-click\"))\n})\n```\n\nWhat does
    this do? The select input has the following `phx-click` event.\n\n```\nphx-change={JS.dispatch(\"js:tab-selected\",
    detail: %{id: \"#{@id}-mobile\"})}\n```\n\nWhen the select changes, it dispatches
    a local (stays in the browser) event called `\"js:tab-selected\"`. It passes along
    the details which is the ID of the select input.\n\nThe Javascript adds an event
    listener that is called by the click event. If finds the select input and the
    tab link with the same value as our selected option. This links our selection
    from the list to our tab. It then executes the JS commands defined in the `phx-click`
    for the tab.\n\nAn alternative version of the Javascript triggers a \"click\"
    on the element rather than execute the JS commands. That looks like this:\n\n```javascript\n//
    Tabs behavior\nwindow.addEventListener(\"js:tab-selected\", ({ detail }) => {\n
    \ let select = document.getElementById(detail.id)\n  let link = document.getElementById(select.value)\n
    \ if (link) { link.click() }\n})\n```\n\nThe last line of code says, \"If we found
    the link, 'click' it.\" The `execJS` is a better general solution for running
    commands that might not be clickable.\n\nNice! Either approach bridges the selected
    option to our tab.\n\nThis little bit of extra JS isn't ideal because we were
    trying to co-locate everything. Still, with this little bit of Javascript, we're
    there! We have a Tailwind UI styled tab component that falls back to a `<select>`
    input on narrower mobile screens!\n\nBehold it in all its glory!\n\n![Finished
    working tab behavior animation](./finished-working-tab-behavior.gif?center&card&border)\n\n##
    What was learned?\n\nStretching to reach an ideal, even when we don't fully reach
    it, helps us better see both where we fall short but also to realize what we _are_
    able to do. With this exercise, we saw what we can do without Alpine.js. We also
    found ways that LiveView can improve.\n\nThere were some challenges to getting
    this working. In fact, I got some help from a colleague. Thanks Chris McCord!
    \U0001F642\n\n- A big challenged was solved with the 4 lines of JS that makes
    our select input execute the JS commands.\n- We learned that currently, we can't
    fully realize the dream of fully co-located JS logic without Alpine.js. Particularly
    when conditional JS logic is needed.\n- [Hooks](https://hexdocs.pm/phoenix_live_view/js-interop.html#client-hooks-via-phx-hook)
    may be the \"blessed\" way to do the Javascript portion. If the DOM were expressed
    in a hook, it might be more clear about what's happening. However, in this case,
    it wasn't too bad.\n- There were some more things learned along this journey that
    I'll save for a follow-up post.\n\nClosing this out, I am satisfied with how portable
    it ended up. As an experiment in creating a component with client-side behavior
    that keeps most of the benefits of co-location, I learned a lot. Hopefully you
    learned something helpful too!\n\n<%= partial \"shared/posts/cta\", locals: {\n
    \ title: \"Phoenix Apps Run Great on Fly\",\n  text: \"Fly is an awesome place
    to run your Elixir apps. Deploying, clustering, connecting Observer, and more
    are supported and even fun!\",\n  link_url: \"https://fly.io/docs/elixir/\",\n
    \ link_text: \"Deploy your Elixir app today!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>\"\n}
    %>\n\n### Credits\n\nSpecial thanks to:\n\n- Berenice Medel for her previous work
    on tabs that got me quite far.\n- Chris McCord for improving my component API
    and writing those critical 4 lines of JS.\n"
- :id: laravel-bytes-deploying-with-private-composer-packages
  :date: '2022-08-08'
  :category: laravel-bytes
  :title: Deploying with Private Composer Packages
  :author: fideloper
  :thumbnail: docker-secrets-thumbnail.jpg
  :alt:
  :link: laravel-bytes/deploying-with-private-composer-packages
  :path: laravel-bytes/2022-08-08
  :body: "\n\nIf you've read anything on Fly.io, you've probably noticed a snippet
    like this:\n\n> We transmogrify Docker containers into lightweight micro-VMs and
    run them on our own hardware in racks around the world, so your apps can run close
    to your users.\n\n\n\nFly deploys an application by building it into a Docker
    image, and then doing a whole lotta _stuff_ to run it as a \"for-real\" virtual
    machine.\n\nTo deploy an application, all we need to care about is [creating a
    working Docker image](https://fly.io/docs/getting-started/laravel/).\n\n\n\n##
    The Problem\n\nDeploying a Laravel application involves installing Composer dependencies
    \"at build time\" - when the Docker image is being built, just before releasing
    the deployment.\n\nHowever, installing a private package such as Laravel [Nova](https://nova.laravel.com/)
    or [Spark](https://spark.laravel.com/) will fail unless we provide authentication
    tokens!\n\nYou can set [secrets](https://fly.io/docs/reference/secrets/) for your
    application, but these are only available at _run-time_. They aren't available
    when building your Docker image. For that, we need [build-time secrets](/docs/reference/build-secrets/).\n\n\n\n##
    Authenticating Private Packages\n\nAuthenticating Composer packages comes in a
    few forms.\n\nYou can create an `auth.json` file in your `$COMPOSER_HOME` path,
    or perhaps populate a `$COMPOSER_AUTH` environment variable with some JSON (examples
    [here](https://chipperci.com/docs/builds/composer-auth/)).\n\nA short-cut to this
    is running the following command (or something like it, as it varies a bit per
    package):\n\n```bash\n# Authenticate Composer for Laravel Nova\ncomposer config
    http-basic.nova.laravel.com \\\n    \"username@example.org\" \"some-auth-token\"\n```\n\nLet's
    see how to use this command in our `Dockerfile` to pull in Laravel Nova.\n\n\n\n##
    Secrets in Docker Builds\n\nOur goal is to authenticate private packages within
    Docker builds, without leaking secrets.\n\nWe'll make use of [Docker secrets](https://docs.docker.com/develop/develop-images/build_enhancements/)
    to accomplish this.\n\nYou may have used the `fly launch` command to generate
    a `Dockerfile`, or perhaps you created your own. Either way, you should have a
    `Dockerfile` already.\n\nTo get some secrets into a build, edit the `Dockerfile`,
    find the `composer install` command (it may be `composer update`), and update
    it:\n\n```bash\n# Mount 2 secrets, configure composer, install dependencies\nRUN
    --mount=type=secret,id=nova_user \\\n    --mount=type=secret,id=nova_pass \\\n
    \   \\\n    composer config http-basic.nova.laravel.com \\\n        \"$(cat /run/secrets/nova_user)\"
    \"$(cat /run/secrets/nova_pass)\" \\\n    \\\n    && composer install --optimize-autoloader
    --no-dev \\\n    ...\n```\n\nWe defined 2 secrets. The secrets are named `nova_user`
    and `nova_pass`. Mounting secrets in Docker creates a file per secret in `/run/secrets`.
    These are available only during the building of the container image (e.g. when
    you run `fly deploy`).\n\nIn our case, we use the secrets to run `composer config...`
    and add the authentication details used to install Laravel Nova.\n\nThe `--mount`
    directive is not a shell command, so there's no need to add `&&` after it as you
    commonly see when chaining commands.\n\nWe can pass the values of those secrets
    when we run `fly deploy`:\n\n```\nfly deploy \\\n    --build-secret nova_user=fideloper@fly.io
    \\\n    --build-secret nova_pass=LARAVEL_NOVA_AUTH_TOKEN_HERE \n```\n\nYou should
    see that the `composer install` command works - it authenticates and pulls down
    Laravel Nova!\n"
- :id: blog-volumes-expand-restore
  :date: '2022-08-04'
  :category: blog
  :title: Volume Expansion and Snapshot Restores
  :author: chris-n
  :thumbnail: snapshot-restore-thumbnail.jpg
  :alt: A night landscape with tall, thin evergreen trees on the horizon and hot-air
    balloons floating among oversized stars, planets, and galaxies.
  :link: blog/volumes-expand-restore
  :path: blog/2022-08-04
  :body: "\n\n<div class=\"lead\">Fly.io operates hardware around the world to run
    your apps close to your users. This post is about our storage volumes, which are
    handy for running full stack apps. Deploy, say, [a Laravel app](/docs/getting-started/laravel/)
    now&mdash;it takes mere minutes.</div>\n\nFly Volumes are the persistent storage
    that makes it possible to run full stack apps entirely on the Fly.io platform,
    keeping your configuration, session or user data in place across deployments.
    Looking at them from another angle, volumes are space on NVMe drives on our servers
    that you can rent for your apps to use.\n\nWe've recently made two major improvements
    to volumes: extending volume size, and self-service snapshot restores.\n\n## Extending
    a volume\n\nUntil recently, if you needed your volume to be bigger, you'd have
    to provision a new empty one and copy your data to it. This is not ideal, to put
    it mildly.\n\nBut you can now extend a volume on the CLI with `fly volume extend
    <volume-id>`. \n\nThe app instance attached to the volume does have to restart
    to allow the file system to be resized. This will happen automatically for &quot;regular&quot;
    apps, but [Machines VMs](https://fly.io/docs/reference/machines/) will have to
    be restarted manually.\n\nMore details in [the announcement on the Fly.io community
    forum](https://community.fly.io/t/volume-expansion-is-now-available/5920) and
    in the [volumes docs](/docs/reference/volumes/#extending-volumes).\n\nIdeally,
    we'd be able to alert you when you hit a usage threshold on a storage volume,
    or even better, give you the option to increase your volume size automatically
    when you hit a threshold. We're not there yet! \n\n## Restoring data from a volume
    snapshot\n\nIt's been possible for some time to restore a Fly.io [HA Postgres](/docs/reference/postgres/)
    database from a snapshot, but if you were using another database, you had to ask
    us to dig up and restore a snapshot for you.\n\nBut now you can restore regular
    volumes: as of [flyctl v0.0.363](https://github.com/superfly/flyctl/releases/tag/v0.0.363),
    individual volume restores can now be performed by specifying `--snapshot-id`
    at creation time.\n\nWhich means you can get back `sandwiches.db` by yourself
    (is there something other than sandwiches that you might store in an app's database?),
    at your own convenience.\n\nVolume snapshots are taken daily (but not at the same
    time every day), and shunted off to object storage where they live for five days
    before they expire.\n\nIf you need to restore from a snapshot, you identify the
    volume you want, list its snapshots, get the ID of the one you want from the list,
    and create a new volume by pointing `fly volume create` at that ID. \n\nYou can
    restore to a bigger volume, if you like, but not a smaller one than the snapshot
    came from.\n\n[Forum announcement.](https://community.fly.io/t/volume-restores-are-now-available/6214)
    [Docs.](/docs/reference/volumes/#snapshots-and-restores)\n\n<%= partial \"shared/posts/cta\",
    locals: {\n  title: \"Start simple\",\n  text: \"It's just a couple commands to
    get an app deployed on Fly.io.\",\n  link_url: \"https://fly.io/docs/speedrun/\",\n
    \ link_text: \"Try Fly for free&nbsp;&nbsp;<span class='opacity-50'>&rarr;</span>\"\n}
    %>"
- :id: laravel-bytes-customize-a-laravel-select-component
  :date: '2022-08-01'
  :category: laravel-bytes
  :title: Customize a Laravel select component
  :author: aprod
  :thumbnail: laravel-livewire-custom-select-thumbnail.jpg
  :alt:
  :link: laravel-bytes/customize-a-laravel-select-component
  :path: laravel-bytes/2022-08-01
  :body: "\n\n<p class=\"lead\">This is a post about creating a custom select component
    for Laravel Livewire applications.  If you have a Laravel app you want deployed
    closer to your users then [get started now](/docs/getting-started/laravel/). You
    could be up and running in mere minutes.</p>\n\nWhen it comes to form elements,
    we might immediately reach for an open-source or paid library. Pre-built components
    speed up development and using well-tested, robust libraries take a lot of pressure
    off our shoulders.\n\nBut what about when we need something custom? Customizing
    third-party packages is often harder than making the component ourselves. Also,
    learning how to make a reusable component improves our general understanding of
    Livewire.\n\nToday, we will make a custom select component using [Livewire](https://laravel-livewire.com/)
    and [Tailwind](https://tailwindcss.com/). Then we will go further and consider
    ways of making it accessible using [Alpine.js](https://alpinejs.dev/). We will
    build it fully custom without using an HTML `<select>` tag, this gives us a lot
    of freedom in appearance and UX.\n\n## Let's roll\n\nTo keep it simple, we assume
    you already [created a new Laravel project](https://laravel.com/docs/9.x#your-first-laravel-project),
    [installed Livewire using composer](https://laravel-livewire.com/docs/2.x/quickstart#install-livewire),
    and [installed Tailwind using npm](https://tailwindcss.com/docs/guides/laravel).\n\nGenerate
    the Livewire component using `php artisan make:livewire select`.\n\nIt creates
    two files:\n\n- Component's class: `app/Http/Livewire/Select.php`\n- Component's
    view: `resources/views/livewire/select.blade.php`\n\nThen, edit our `welcome.blade.php`
    file and include the select component.\n\n```xml\n<html>\n  <head>\n    <meta charset=\"utf-8\">\n
    \   <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n\n
    \   <title>Livewire Select</title>\n\n    <link href=\"{{ mix('/css/app.css')
    }}\" rel=\"stylesheet\">\n\n    @livewireStyles\n  </head>\n  <body class=\"flex
    items-center justify-center min-h-screen\">\n\n    <div class=\"w-56\">\n      <livewire:select/>\n
    \   </div>\n\n    @livewireScripts\n  </body>\n</html>\n```\n\n## Making the layout\n\nHead
    to the `select.blade.php` file. The component has 3 parts, the label of the select,
    the selected item / placeholder and an absolute positioned list with all the selectable
    options.\n\n```html\n<div>\n  <label>\n  Label\n  </label>\n  <div class=\"relative\">\n
    \   <button>\n    Selected item\n    </button>\n    <ul class=\"absolute z-10\">\n
    \     <li>\n        Option 1\n      </li>\n      <li>\n        Option 2\n      </li>\n
    \   </ul>\n  </div>\n</div>\n```\n\nWe can make it pop with a few Tailwind classes.\n\n```html\n<div>\n
    \ <label class=\"text-gray-500\">\n  Label\n  </label>\n  <div class=\"relative\">\n
    \   <button class=\"w-full flex items-center h-12 bg-white border rounded-lg px-2\">\n
    \   Selected item\n    </button>\n    <ul class=\"bg-white absolute mt-1 z-10
    border rounded-lg w-full\">\n      <li class=\"px-3 py-2\">\n        Option 1\n
    \     </li>\n      <li class=\"px-3 py-2\">\n        Option 2\n      </li>\n    </ul>\n
    \ </div>\n</div>\n```\n\nAfter dressing it up, it looks like this:\n\n![](https://static.slab.com/prod/uploads/p1b436gf/posts/images/fZyVmst9pWPlAeu1kN0KRbkL.png?card&center&1/2)\n\n##
    Item rendering and toggle\n\nNow let's implement the rendering and the opening/closing
    of the options.\n\nCreate some property in the `Select.php` file and a `toggle()`
    function\n\n```php\nclass Select2 extends component\n{\n  public $items;\n\n  public
    $selected = null;\n\n  public $label;\n\n  public $open = false;\n\n  public function
    toggle()\n  {\n      $this->open = !$this->open;\n  }\n\n...\n\n}\n```\n\nTo make
    the component **reusable we pass these from the outside**, currently in the `welcome.blade.php`\n\n```\n<livewire:select\n
    \ :selected=\"1\"\n  :items=\"['Apple','Banana','Strawberry']\"\n  label=\"Favorite
    fruit\"\n />\n```\n\nReplace a few parts in the `select.blade.php` to render dynamically
    from the given props, and also add a click listener to the `<button>`  to  add
    the opening / closing functionality.\n\n```html\n<div>\n  <label class=\"text-gray-500\">\n
    \   {{ $label }}\n  </label>\n  <div class=\"relative\">\n    <button\n      wire:click=\"toggle\"\n
    \     class=\"w-full flex items-center h-12 bg-white border rounded-lg px-2\"\n
    \     >\n    @if ($selected !== null)\n      {{ $items[$selected] }}\n    @else\n
    \     Choose...\n    @endif\n    </button>\n    @if ($open)\n      <ul class=\"bg-white
    absolute mt-1 z-10 border rounded-lg w-full\">\n        @foreach($items as $item)\n
    \         <li class=\"px-3 py-2 cursor-pointer\">\n            {{ $item }}\n          </li>\n
    \       @endforeach\n      </ul>\n    @endif\n  </div>\n</div>\n```\n\nWith those
    few edits, our items are rendered and we can open and close the options list.\n\n![](https://static.slab.com/prod/uploads/p1b436gf/posts/images/a4gORa4auSI7zx9C4oiQHzvI.gif?card&center&1/2)\n\n##
    Making the selection\n\nLet's create a `select($index)` function in the `Select.php`
    class. This sets the selected item to the given index, and also handles deselection
    if the given index is the currently selected item.\n\n```php\npublic function
    select($index) {\n  $this->selected = $this->selected !== $index ? $index : null;\n
    \ $this->open = false;\n}\n```\n\nAdd the click event to the `<li>` and also a
    few conditional classes to highlight the selected option. We use the `@class`
    blade directive and the `$loop` variable, which is provided by the `@foreach`loop\n\n```html\n<li
    wire:click=\"select({{ $loop->index }})\"\n  @class([\n    'px-3 py-2 cursor-pointer',\n
    \   'bg-blue-500 text-white' => $selected === $loop->index,\n    'hover:bg-blue-400
    hover:text-white',\n  ])\n>\n  {{ $item }}\n</li>\n```\n\nNow we have a working
    select component! \U0001F680\n\n![](https://static.slab.com/prod/uploads/p1b436gf/posts/images/fAgGyP94dxWBF_Lo0ywURIKc.gif?card&center&1/2)\n\n##
    Making it prettier\n\nBefore we dive into the Alpine.js part, let's make some
    UI improvements. Since we are building this custom, we can make it display how
    we want for our application. We want an open/close indicator, and it would be
    cool to include a check icon on the currently selected item. For simplicity, we
    will use [Heroicons](https://heroicons.com/) and copy-paste SVG-s.\n\n![](https://static.slab.com/prod/uploads/p1b436gf/posts/images/a14T-nNuxlAkx0UpfL7B8r_r.gif?card&center&1/2)\n\nYou
    can find the full final markup of `select.blade.php` here:\n\n<style>\n.gist table.lines
    {\n font-size: 12px;\n}\n.smallscript .gist-data {\n max-height: 150px;\n}\n.gist-data
    {\n max-height: 350px;\n}\n.gist-meta {\n font-size: 80% !important;\n}\n</style>\n\n<div
    class=\"smallscript\">\n  <script src=\"https://gist.github.com/IllesAprod/4e0fe4137d9bc668cc366352420cc935.js?file=select.blade.php\"></script>\n</div>\n\n##
    Making it accessible using Alpine.js\n\nAccessibility is an important part of
    web development, not just when we think about people with disabilities but making
    things accessible gives a better UX for everyone.\n\nOur requirements:\n\n- Pressing
    `TAB`  we can focus on our component\n- Pressing `SPACE` opens / closes the component\n-
    Pressing `UP` and `DOWN`arrows navigate between the items\n- Pressing `ENTER`
    selects the currently highlighted element\n\nAll of the above _can_ be implemented
    by Livewire, but it would make a lot of requests to the backend. Since these things
    are UI state only, it makes sense to implement them in the browser using  [Alpine.js](https://alpinejs.dev/),
    a popular and lightweight Javascript framework.\n\nFirst, we need to include Alpine's
    JS in our `welcome.blade.php`\n\n```html\n<head>\n...\n  <script defer src=\"https://unpkg.com/alpinejs@3.x.x/dist/cdn.min.js\"></script>\n...\n</head>\n```\n\nThen,
    add an `x-data`attribute to our parent `div`\n\n```html\n<div x-data=\"{}\">\n...\n</div>\n```\n\nTo
    highlight an item even when the mouse is not hovering, we must track which element
    is currently highlighted. Also, we have to calculate the next and the previous
    item, and for that, we will need to pass the count of the items from PHP.\n\n```html\n<div
    x-data=\"{\n  highlighted: 0,\n  count: {{ count($items) }},\n}\">\n```\n\nWe
    also need three functions `next`, `previous` and `select`. We will use the magical
    modulo operator for the next/previous calculation, which returns the remainder
    after the division. E.g.: `3 % 2 = 1`, `8 % 3 = 2`\n\nTo select the currently
    highlighted item we will use `this.$wire` variable's `call()` method.\n\n```xml\n<div
    x-data=\"{\n  highlighted: 0,\n  count: {{ count($items) }},\n  next() {\n    this.highlighted
    = (this.highlighted + 1) % this.count;\n  },\n  previous() {\n    this.highlighted
    = (this.highlighted + this.count - 1) % this.count;\n  },\n  select() {\n    this.$wire.call('select',
    this.highlighted)\n  }\n}\">\n```\n\nNow that we have all the needed data and
    functions, let's hook these into our layout.\n\nAdd Alpine.js event-listeners
    to our `<button>`\n\n```html\n<button\n  wire:click=\"toggle\"\n  class=\"w-full
    flex items-center justify-between h-12 bg-white border rounded-lg px-2\"\n  @keydown.arrow-down=\"next()\"\n
    \ @keydown.arrow-up=\"previous()\"\n  @keydown.enter.prevent=\"select()\"\n>\n```\n\nNow,
    to highlight the proper item, we add an `x-data` to our `<li>` with the current
    `$index` and add some classes using Alpine if the `$index` matches the `highlighted`
    variable.\n\n```html\n<li wire:click=\"select({{ $loop->index }})\"\n  x-data=\"{
    index: {{ $loop->index }} }\"\n  class=\"px-3 py-2 cursor-pointer flex items-center
    justify-between\"\n  :class=\"{'bg-blue-400 text-white': index === highlighted}\"\n
    \ @mouseover=\"highlighted = index\"\n>\n  ...\n</li>\n```\n\n## Final touches\n\nWe
    are almost done here, just a few hiccups left.\n\nWhen we open the select for
    the first time, it should highlight the currently selected item. Let's give this
    information to Alpine using the `x-init` attribute.\n\n```html\n<div x-data=\"{...}\"\n
    \ x-init=\"highlighted =  {{ $selected ?: 0 }}\"\n>\n```\n\nWe can handle &quot;click
    outside&quot; by adding a `close()` function to Alpine, and `@click.outside` listener
    to the parent `div` to close the popup when the user clicks somewhere else.\n\n```html\n<div
    x-data=\"{\n  ...\n  close() {\n    if (this.$wire.open) {\n     this.$wire.open
    = false;\n    }\n  }\n}\"\n...\n@click.outside=\"close()\"\n>\n```\n\nWhen the
    select isn't open and we hit `ENTER` it deselects the current item, let's fix
    this in `Select.php`\n\n```php\npublic function select($index) {\n  if (!$this->open)
    {\n    return;\n  }\n\n  $this->selected = $this->selected !== $index ? $index
    : null;\n  $this->open = false;\n}\n```\n\nWhen the highlighted and selected item
    isn't the same, we need to change the color of the check icon to blue; otherwise
    it can't be seen on the white background. Let's fix this with some dynamic classes.\n\n```php\n@if
    ($selected === $loop->index)\n  <div :class=\"index === highlighted ? 'text-white'
    : 'text-blue-500'\">\n    <svg xmlns=\"http://www.w3.org/2000/svg\" class=\"h-5
    w-5\" viewBox=\"0 0 20 20\" fill=\"currentColor\">\n      <path fill-rule=\"evenodd\"
    d=\"M10 18a8 8 0 100-16 8 8 0 000 16zm3.707-9.293a1 1 0 00-1.414-1.414L9 10.586
    7.707 9.293a1 1 0 00-1.414 1.414l2 2a1 1 0 001.414 0l4-4z\" clip-rule=\"evenodd\"
    />\n    </svg>\n  </div>\n@endif\n```\n\nThat's it, we are done! \U0001F680 \U0001F44F\n\nAs
    you can see, by dropping multiple `<livewire:select />` components into our view
    each piece does its job.\n\n![](https://static.slab.com/prod/uploads/p1b436gf/posts/images/eyAzTvRhbrxeIrT_he8dWwpw.gif?card&center&1/2)\n\n##
    Closing Thoughts\n\nSometimes, we don't appreciate how many things are going on
    under the hood when using third-party component packages. When we do it ourselves,
    in addition to learning what's involved, we also realize it's something we _can_
    do ourselves. So when we _need_ something custom, we won't be afraid of it.\n\nAnother
    take-away is: If you want Livewire components to be reusable, you should pass
    every dependency from the outside into them and not use global events and listeners.\n\nWe
    can use Livewire for lots of things, but we should  be on the lookout for  situations
    where using Javascript makes more sense. Alpine.js and Livewire work great together,
    and when we combine them, we can solve lots of problems for our customers and
    users.\n\n<%= partial \"shared/posts/cta\", locals: {\n  title: \"Launching Laravel\",\n
    \ text: \"Fly.io is a great place to deploy your Laravel app. It's really easy
    to get started. You can be running in minutes.\",\n  link_url: \"https://fly.io/docs/getting-started/laravel/\",\n
    \ link_text: \"Launch your Laravel app today!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>\"\n}
    %>\n\n## Final Component\n\nHere's the **final** version of our component with
    the Alpine.js improvements included.\n\n```html\n<div x-data=\"{\n  highlighted:
    0,\n  count: {{ count($items) }},\n  next() {\n  console.log(this.highlighted);\n
    \   this.highlighted = (this.highlighted + 1) % this.count;\n  },\n  previous()
    {\n    this.highlighted = (this.highlighted + this.count - 1) % this.count;\n
    \ },\n  select() {\n    this.$wire.call('select', this.highlighted)\n  },\n  close()
    {\n    if (this.$wire.open) {\n      this.$wire.open = false;\n    }\n  }\n }\"\n
    \  x-init=\"highlighted =  {{ $selected ?: 0 }}\"\n   @click.outside=\"close()\"\n>\n
    \ <label class=\"text-gray-500\">\n    {{ $label }}\n  </label>\n  <div class=\"relative\">\n
    \   <button\n      wire:click=\"toggle\"\n      class=\"w-full flex items-center
    justify-between h-12 bg-white border rounded-lg px-2\"\n      @keydown.arrow-down=\"next()\"\n
    \     @keydown.arrow-up=\"previous()\"\n      @keydown.enter.prevent=\"select()\"\n
    \   >\n      @if ($selected !== null)\n        {{ $items[$selected] }}\n      @else\n
    \       Choose...\n      @endif\n\n      <div class=\"text-gray-400\">\n        @if
    ($open)\n          <svg xmlns=\"http://www.w3.org/2000/svg\" class=\"h-5 w-5\"
    viewBox=\"0 0 20 20\" fill=\"currentColor\">\n            <path fill-rule=\"evenodd\"\n
    \             d=\"M14.707 12.707a1 1 0 01-1.414 0L10 9.414l-3.293 3.293a1 1 0
    01-1.414-1.414l4-4a1 1 0 011.414 0l4 4a1 1 0 010 1.414z\"\n              clip-rule=\"evenodd\"/>\n
    \         </svg>\n        @else\n          <svg xmlns=\"http://www.w3.org/2000/svg\"
    class=\"h-5 w-5\" viewBox=\"0 0 20 20\" fill=\"currentColor\">\n            <path
    fill-rule=\"evenodd\"\n              d=\"M5.293 7.293a1 1 0 011.414 0L10 10.586l3.293-3.293a1
    1 0 111.414 1.414l-4 4a1 1 0 01-1.414 0l-4-4a1 1 0 010-1.414z\"\n              clip-rule=\"evenodd\"/>\n
    \         </svg>\n        @endif\n      </div>\n    </button>\n    @if ($open)\n
    \     <ul class=\"bg-white absolute mt-1 z-10 border rounded-lg w-full\">\n        @foreach($items
    as $item)\n          <li wire:click=\"select({{ $loop->index }})\"\n            x-data=\"{
    index: {{ $loop->index }} }\"\n            class=\"px-3 py-2 cursor-pointer flex
    items-center justify-between\"\n            :class=\"{'bg-blue-400 text-white':
    index === highlighted}\"\n            @mouseover=\"highlighted = index\"\n          >\n
    \           {{ $item }}\n\n            @if ($selected === $loop->index)\n              <div
    :class=\"true ? 'text-white' : 'text-blue-500'\">\n                <svg xmlns=\"http://www.w3.org/2000/svg\"
    class=\"h-5 w-5\" viewBox=\"0 0 20 20\"\n                   fill=\"currentColor\">\n
    \                 <path fill-rule=\"evenodd\"\n                    d=\"M10 18a8
    8 0 100-16 8 8 0 000 16zm3.707-9.293a1 1 0 00-1.414-1.414L9 10.586 7.707 9.293a1
    1 0 00-1.414 1.414l2 2a1 1 0 001.414 0l4-4z\"\n                    clip-rule=\"evenodd\"/>\n
    \               </svg>\n              </div>\n            @endif\n          </li>\n
    \       @endforeach\n      </ul>\n    @endif\n  </div>\n</div>\n```\n\nHappy hacking!\n"
- :id: blog-sqlite-internals-btree
  :date: '2022-07-27'
  :category: blog
  :title: 'SQLite Internals: Pages & B-trees'
  :author: ben
  :thumbnail: sqlite-thumbnail.jpg
  :alt: A parliament of owls meticulously inspecting various pages while in a tree.
    There's also two feet.
  :link: blog/sqlite-internals-btree
  :path: blog/2022-07-27
  :body: |2


    <div class="lead">
      [Fly.io](http://fly.io/) runs apps close to users around the world, by taking containers and upgrading them to full-fledged virtual machines running on our own hardware around the world. Sometimes those containers run SQLite and we make that easy too. [Give us a whirl](https://fly.io/docs/speedrun/) and get up and running quickly.
    </div>

    Ok, I'll admit it—I'm a SQLite shill. There are few holes that I don't try to put a SQLite-shaped peg into. It's not that I dislike other databases, they're great. But SQLite is so easy to use and, more importantly, it's simple. Simplicity leads to reliability and I don't know of a more reliable database than SQLite.

    There's a comfort with being able to read through a spec or a code repository and know that you've covered the full breadth of a tool. That's why I love [Go's simple 100-page language specification](https://go.dev/ref/spec) and that's why I love SQLite's 130KLOC core code base. It's something that you can read through in a long weekend if, ya know, that's what you do on weekends.

    This constrained size means that SQLite doesn't include every bell and whistle. It's careful to include the 95% of what you need in a database—strong SQL support, transactions, windowing functions, CTEs, etc—without cluttering the source with more esoteric features. This limited feature set also means the structure of the database can stay simple and makes it easy for anyone to understand.

    We here at Fly.io have an unreasonable affinity for explanations involving sandwiches and this post will continue in that sacred tradition.

    ## I have binders full of sandwiches

    Recently, I tried to remember a sandwich I ate last week but to no avail. Like any 10x engineer, I quickly over-engineered a solution by making a SQLite database to track every sandwich consumed.

    We'll start off with our table definition:

    ```
    CREATE TABLE sandwiches (
        id INTEGER PRIMARY KEY,
        name TEXT,
        length REAL,
        count INTEGER
    )
    ```

    Now we'll have a record of every sandwich, its size in inches, and the number eaten.

    I live in Denver where we were known in the early 2000s for [toasted sandwiches](https://www.quiznos.com/), and then in the 2010s for [bad toasted sandwiches](https://www.quiznos.com/) so we'll kick off with a toasted Italian sub.

    ```
    INSERT INTO sandwiches (name, length, count) VALUES ('Italian', 7.5, 2)
    ```

    Voila! Our data is safe on disk. It's easy to gloss over all the steps it takes to get from hitting the "enter" key to bytes getting saved to disk. SQLite virtually guarantees that your database will never be corrupted or that your transaction will be half-written. But instead of glossing over, let's dive in deep and see how our Italian sub looks on disk.

    ### Efficient sandwich encoding

    Our row of sandwich data exists as an array of bytes inside SQLite that's encoded using its [_record format_](https://www.sqlite.org/fileformat.html#record_format). For our inserted row, we see the following bytes on disk:

    ```
    15 01 05 00 1b 07 01 49 74 61 6c 69 61 6e 40 1e 00 00 00 00 00 00 02
    ```

    Let's break these bytes down to see what's going on.

    ### The header & variable-length integers

    The first byte of `0x15` is the size of our row's payload, in bytes. After this is our rowid which is used as our `PRIMARY KEY`. Since this is the first row, its `id` has a value of `0x01`.

    These first two fields use what's called a variable-length integer ("varint") encoding. This encoding is used so that we don't use a huge 8-byte field for every integer and waste a bunch of space. It'd be like if a sandwich shop packaged every sandwich in enormous 6-foot party sub containers because they only could use one size of container. That'd make no sense! Instead, each size of sandwich gets its own container size.

    Varints use a simple trick. The high bit is used as a flag to indicate if there are more bytes to be read and the other 7 bits are our actual data. So if we wanted to represent 1,000, we start with its binary representation split into 7 bit chunks:

    ```
    0000111 1101000
    ```

    Then we add a "1" flag bit to the beginning of the first chunk to indicate we have more chunks, and a "0" flag bit to the beginning of the second chunk to indicate we don't have any  more chunks:

    ```
    10000111 01101000
    ```

    With varints, we can now store our integer in 2 bytes instead of 8. This may seem small but many SQLite databases have a lot of integers so it's a huge win!

    The next two bytes after the rowid specify the data that is not spilled to overflow pages but [the explanation is lengthy](https://www.sqlite.org/fileformat2.html#b_tree_pages) so we're gonna wave our hands over that part.

    ### Type encoding

    Next, we have a list of column types for our `name`, `size`, and `count` fields. Each data type has a different encoding that's specified as a varint.

    For our name column, the `0x1b` value specifies that it is a `TEXT` type and has a length of 7 bytes. Type values that are odd and are greater or equal to 13 are `TEXT` fields and can be calculated with the formula `(n*2) + 13`. So our 7-byte string is `(7*2) + 13` which is 27, or `0x1b` in hex.

    `BLOB` fields are similar except they're even numbers calculated as `(n*2) + 12`. SQLite alternates these `TEXT` and `BLOB` type values so small lengths of both types can be encoded efficiently as varints.

    Next, we have our "length" field which is a floating-point number. These are always encoded as a `0x07`.

    After that, we have our "count" field which is an integer. These get packed down similar to [varints](https://flyio.slab.com/posts/blog-sq-lite-internals-pages-b-trees-55cw0y3o#hwetq-the-header-variable-length-integers) but in a slightly different format. Integers that can fit in an 8-bit integer are represented with a type value of `0x01`. 16-bit integers are `0x02`, 24-bit integers are `0x03` and so on.

    ### Value encoding

    Once our types are all encoded, we just need to pack our data in. The text value of _"Italian"_ is represented as UTF-8:

    ```
    49 74 61 6c 69 61 6e
    ```

    Then our length of _7.5_ is represented as an [IEEE-754-2008 floating-point number](https://en.wikipedia.org/wiki/IEEE_754-2008_revision). SQLite can optimize integer floating-point values by storing them as pure integer fields but since we have a decimal place it is stored with 8-bytes:

    ```
    40 1e 00 00 00 00 00 00
    ```

    And finally we use a single byte to hold our count of 2:

    ```
    02
    ```

    Congrats! You're now an expert on SQLite record formatting.

    ## E_TOOMANYSANDWICHES

    As my sandwich addiction continues unabated, I fill my SQLite database with more and more rows. I even make friends on the [/r/eatsandwiches](https://www.reddit.com/r/eatsandwiches/) subreddit and start collecting their sandwiches. My sandwich database seems to grow without bound.

    Surprisingly though, adding or updating rows is still nearly as instantaneous as when I had a single row. So how does SQLite update a multi-gigabyte sandwich database in a fraction of a second? The answer is pages & b-trees.

    A naive approach to a database would just be to pack records in sequentially in a file. However, there's no way to insert or update rows in the middle of the file without shifting and rewriting all the bytes after the new row.

    Instead, SQLite groups rows together into 4KB chunks called "pages". Why 4KB? That's what file systems typically use as their page size so keeping everything aligned reduces page fetches from disk. Disks are usually the slowest part of a database so limiting page fetches can have a huge performance win.

    ### Inspecting the page format

    If we inspect our page, we can see its header:

    ```
    0d xx xx 00 03 xx xx xx
    ```

    There are several parts of this header but I masked out several bytes so we can focus on two particularly important fields. The first byte, `0x0d`, indicates the _page type_. This page type is a _table leaf_. We'll talk about those more with b-trees.

    The second important field is the _cell count_, which is `0x0003`. This tells us that 3 records exist in this page.

    After the header, we have the _cell pointer index_:

    ```
    0f e9 0f d2 0f bb
    ```

    This is a list of 2-byte values that represent offsets in the page for each record. The first record exists at offset 4,073 (`0x0fe9`), the second record exists at offset 4,050 (`0x0fd2`), etc. SQLite packs rows at the end of the page and then works backwards.

    After the index, we have a bunch of zeros until we reach the content area of the page which holds our row data in record format.

    ### Structuring pages into trees

    Now that we've chunked our data into pages, we can update a subset of our data without having to rewrite the whole file. That's great for writing but now we have a list of pages to search through if we want to query our data and that won't scale as-is.

    The simplest approach would be to start at the first page and then search every page for a given row. This is called a "table scan" and it can be really slow—especially if your data is at the end of your table. If you're into ["big-O" notation](https://en.wikipedia.org/wiki/Big_O_notation), it's also referred to as "linear time complexity", or `O(n)`. That means the amount of time it takes to search for a record has a linear relationship to the number of records you have, which is referred to as "`n`".

    ### Searching faster with binary search

    If you are searching by your primary key, you could perform a binary search since the table is sorted by primary key. For a binary search, you search the page right in the middle of your table for your sandwich record. If the record exists, great! You're done.

    If the sandwich you're searching for is before that page, then you find a new "middle" page in the first half of your table. If it's after the page, then you find a new middle page in the second half of your table. Keep slicing the search space in half and searching until you find your sandwich. If you squint a bit, this slicing and subdividing has the feel of a tree-like structure.

    A binary search has a logarithmic time complexity, or `O(log n)`. That's considered pretty good for data structures since it means you can scale up to a large number of records, `n`, while the cost grows at a much slower rate.

    ### Improving binary search by persisting the tree

    While logarithmic time complexity is great, we still have a problem. Let's run some numbers.

    If we have a small 2MB database with 4KB pages then that means we have 512 pages. A binary search of that many pages would have a worst-case scenario of `log₂(512)`, or 9 pages. That means we might have to read nine pages in our tiny database just to find one record! Page fetches are painfully slow in databases so we want to reduce that as much as possible.

    SQLite is structured as a b-tree, which is a data structure where each node can point to two or more child nodes and the values in these child nodes are all sorted. There are a ton of different variants of b-trees but the one that SQLite uses is called a b+tree. This type of tree stores all data in the leaf nodes, which is what our sorted list of pages represents, but also have an index of key ranges in the branch pages. SQLite refers to these branch pages as "interior pages".

    To illustrate this, let's say our leaf pages hold sandwich records that are each 40 bytes. The record also contains an integer primary key that is 4 bytes on average. That means we can fit about 100 records into one 4KB page. If we have less than 100 records, we only need one page. Easy peasy.

    But what happens when we add a 101st sandwich and it doesn't fit anymore? SQLite will split the page into two leaf pages and add an interior page as the root of our b+tree that points to our child leaf pages. This interior page stores the key ranges for the leaf pages so that when you search, you can see what ranges each child page holds without having to actually read that child page.

    This doesn't seem like a big improvement over our binary search until we start adding more data. Interior pages are also 4KB and they store pairs of child primary keys and their page numbers so each entry in our interior page takes about 8 bytes. That means we can store about 500 entries in an interior page. For our 2MB database, that means we can hold the key range for all of our leaf pages in a single interior page. To find a given record, we only need to search the root interior page to find the correct leaf page. That means our worst case search is only 2 pages.

    ### Growing a tree

    What happens when our root interior page fills up and we need a database bigger than 2MB? Similar to leaf pages, we split the interior page in two and add a new root parent interior page that points to the split interior pages. This means that we need to search the new root interior page to find the correct second-level interior page and then we search page that to find our leaf page. We now have a tree depth of 3.

    Since our new root can hold about 500 references to second-level interior pages and those second-level pages hold about 500 references to leaf pages, we can store about 250,000 pages, or about 1GB of data. If we continue adding rows, we'll need to split the root page again and increase our tree depth to 4.

    At a tree depth of 4, we can hold about 500³ leaf pages, or about a 476GB database. That means we only need to read 4 pages to find a given record—even in a huge database!

    ## OK, But Why?

    While it's interesting to noodle around with internals, how does this actually apply to real-world scenarios?

    Well, knowing about record formatting tells us that storing integers instead of floating-point numbers is wildly more efficient as SQLite doesn't compress floats.

    Or perhaps knowing that b-tree time cost grows logarithmically will let you feel more comfortable designing an application with a multi-gigabyte table.

    Or maybe, just maybe, discovering the [/r/eatsandwiches](https://www.reddit.com/r/eatsandwiches/) subreddit will inspire your dinner tonight.

    Learning about the internals of our tools lets us feel comfortable with them and use them confidently. Hopefully low-level features like SQLite's [PRAGMAs](https://www.sqlite.org/pragma.html) seem a little less opaque now.

    I'll be writing more on SQLite internals in future posts—from rollback journals to write-ahead logs to the SQLite virtual machine. Want to know more about a specific topic? Hit me up on the [Fly Community forum](https://community.fly.io/) or [ping us on Twitter](https://twitter.com/flydotio).
- :id: laravel-bytes-full-stack-laravel
  :date: '2022-07-26'
  :category: laravel-bytes
  :title: Full Stack Laravel
  :author: fideloper
  :thumbnail: full-stack-laravel-thumbnail.jpg
  :alt:
  :link: laravel-bytes/full-stack-laravel
  :path: laravel-bytes/2022-07-26
  :body: |2


    <div class="lead">Fly runs apps close to users by taking Docker images and transmogrifying them into Firecracker micro-vms, running on our hardware around the world. If you want to ship a Laravel app, [try it out on Fly.io](https://fly.io/docs/getting-started/laravel/). It takes just a couple of minutes.</div>

    We have [docs on using `fly launch`](https://fly.io/docs/getting-started/laravel/) to get you up and running quickly with Laravel, but let's talk about all the other *stuff* you probably want - Redis, MySQL, cron, and queues!

    We're gonna spin up a new Laravel installation, add some user authentication (backed by MySQL), setup Laravel's scheduler, and then see how to use Redis for queues, session storage, and caching.

    ## The Laravel Application

    The first thing we need is a Laravel application. We can use `fly launch` to get one running really quickly.

    ```bash
    cd ~/Fly

    # 1. Create a shiny, new Laravel app
    composer create-project laravel/laravel laravel-fly
    cd laravel-fly


    # 2. Launch on Fly!
    fly launch
    # Set app name, select region, deploy now


    # 3. View the site in your browser:
    fly open
    ```

    ## Redis

    Rather than trying to run everything in one [VM](https://fly.io/docs/reference/architecture/) on Fly, extra &quot;services&quot; are best run as separate VMs.

    Following along in the [Fly Redis docs](https://fly.io/docs/reference/redis/), we can create a small Redis service for ourselves.

    ```bash
    # A new, empty directory
    mkdir ~/Fly/redis \
        && cd ~/Fly/redis


    # Create a new redis app (don't deploy it yet!)
    fly launch --image flyio/redis:6.2.6 --no-deploy --name lara-redis


    # Add a persistent volume so Redis data sticks around
    # Note: Volumes only live within the region they are created
    fly volumes create redis_server --size 1


    # Update fly.toml config to use the volume
    cat >> fly.toml <<TOML
      [[mounts]]
        destination = "/data"
        source = "redis_server"
    TOML


    # Add a password (before launching!)
    fly secrets set REDIS_PASSWORD=somepassword
    ```

    A few things to note!

    1. We created a [Volume](https://fly.io/docs/reference/volumes/). In conjunction with a tweak to `fly.toml`, using persistent storage makes sure data isn't lost when the app is re-deployed (however, volumes are specific to a single region).
    1. We gave Redis a password (by setting a secret), we didn't leave it open for just any old connection. *This is required.*
    1. [As documented](https://fly.io/docs/reference/redis/), don't forget to edit the generated `fly.toml` file to delete everything under `[[services]]` (but keep the `[[services]]` heading).

    After that, we're ready to launch it.

    ```bash
    fly launch
    ```

    ### Private Networking

    We'll want our application to talk to Redis. Luckily, [private networks](https://fly.io/docs/reference/private-networking/) are a thing in Fly.

    We named our Redis service `lara-redis`, which means other services (within our Fly organization) can reach the Redis service using hostname `lara-redis.internal`.

    ### Laravel Config

    We need to update the Laravel configuration to use Redis. Laravel needs to know the hostname and password of our Redis service.

    Passwords are secrets, so we'll set it as such in our app service.

    ```bash
    cd ~/Fly/laravel-fly

    # Add in our redis password secret to this app as well
    fly secrets set REDIS_PASSWORD=somepassword
    ```

    Adding a secret triggers a quick re-deploy. Neat!

    The secret is set, but we also need additional (not-so-secret) configuration to tell Laravel where and how use Redis. Update the `fly.toml` file:

    ```
    [env]
      APP_ENV = "production"
      CACHE_DRIVER = "redis"
      SESSION_DRIVER = "redis"
      REDIS_HOST = "lara-redis.internal"
    ```

    Let's test that this works! If we edit file `routes/web.php`, we can add a route that uses Redis:

    ```php
    Route::get('/test', function() {
        Cache::put('new-key', Str::random(16));

        return "The new cached string is " . Cache::get('new-key');
    });
    ```

    We updated some files, we need to deploy again to get that change in place:

    ```bash
    fly deploy
    ```

    You should be able to head to `your-app.fly.dev/test` to see the result. If you don't get any errors, then everything is working!

    ![](01-redis-result.png?center)

    ## MySQL

    Fly offers to set up an [HA cluster of PostgreSQL for you](https://fly.io/docs/getting-started/multi-region-databases/).

    There's no need to brag about it, but we know that MySQL is pretty great. For that, we can [use PlanetScale in our Fly apps](https://fly.io/docs/app-guides/planetscale/).

    That link shows how to sign up and create a MySQL database on PlanetScale. Once created, we just need to add the proper configuration into our application.

    You can also <a href="/docs/app-guides/mysql-on-fly/">run MySQL as "just another Fly.io app"</a> as well.

    ![](02-planetscale.png?center)

    Edit `fly.toml` and update the environment variables:

    ```
    [env]
      APP_ENV = "production"
      CACHE_DRIVER = "redis"
      SESSION_DRIVER = "redis"
      REDIS_HOST = "lara-redis.internal"

      # New stuff here
      DB_CONNECTION = "mysql"
      DB_HOST = "some_hostname.us-east-2.psdb.cloud"
      DB_DATABASE= "laravel-fly"
      MYSQL_ATTR_SSL_CA = "/etc/ssl/certs/ca-certificates.crt"
    ```

    Note the addition of `MYSQL_ATTR_SSL_CA=/etc/ssl/certs/ca-certificates.crt`, which is required for connecting to databases on PlanetScale (Using TLS for remote database connections is **extremely important**)!

    <div class="callout">The value `/etc/ssl/certs/ca-certificates.crt` is correct for the container Fly setup for you via `fly launch`. See [here](https://docs.planetscale.com/concepts/secure-connections#ca-root-configuration) for other possible values.</div>

    Laravel needs to know the username/password for the database as well. Those seem pretty secret, so let's use secrets!

    I opted to keep the hostname an environment variable, but you might want to set it as a secret.

    ```bash
    fly secrets set DB_USERNAME=your-username DB_PASSWORD=your-password
    ```

    ### Authentication with Laravel Breeze

    Many Laravel apps will have user authentication. Let's use Laravel Breeze to quickly scaffold auth into our app.

    ```bash
    cd ~/Fly/laravel-fly

    composer require laravel/breeze --dev

    php artisan breeze:install

    # If you're running your app locally
    # you'll want to get static assets/run migrations
    npm install
    npm run dev

    php artisan migrate
    ```

    Check that it works locally if you'd like, then deploy it!

    ```bash
    fly deploy
    ```

    Did you know you can ssh into your apps? We'll use that to run our migrations:

    ```
    fly ssh console -C "php /var/www/html/artisan migrate --force"
    Connecting to top1.nearest.of.black-rain-2336.internal... complete
    Migration table created successfully.
    Migrating: 2014_10_12_000000_create_users_table
    Migrated:  2014_10_12_000000_create_users_table (216.52ms)
    Migrating: 2014_10_12_100000_create_password_resets_table
    Migrated:  2014_10_12_100000_create_password_resets_table (199.27ms)
    Migrating: 2019_08_19_000000_create_failed_jobs_table
    Migrated:  2019_08_19_000000_create_failed_jobs_table (197.86ms)
    Migrating: 2019_12_14_000001_create_personal_access_tokens_table
    Migrated:  2019_12_14_000001_create_personal_access_tokens_table (310.91ms)
    ```

    ## Migrate on Deploy

    If you're wondering how to automate the running of your migrations, you're in luck! That's configurable in the [`deploy` section of the fly.toml file](https://fly.io/docs/reference/configuration/#the-deploy-section).

    The following will run your migrations just before making the latest version of the application available.

    ```toml
    # File: fly.toml
    [deploy]
      release_command = "php /var/www/html/artisan migrate --force"
    ```

    You should now be able to register and log in.

    ![](03-fly-app-register.png?center)


    It works, we've got a full(ish) Laravel app running!

    ## Cron and Queues

    The `fly launch` command gave us some boilerplate to easily use Laravel's scheduler (cron) and queue workers.

    To enable those, we can configure some processes in our `fly.toml` file. Add the following:

    ```toml
    [processes]
      # Yes, this should be an empty string
      app = ""
      cron = "cron -f"
      worker = "php artisan queue:work"
    ```

    You may need to do further configuration to your queue worker (setting the driver).

    Don't forget to run `fly deploy` after making changes.

    Be sure to check the docs on [Laravel's cron and queues](/docs/laravel/the-basics/cron-and-queues/) to see more details, like how to scale them out separately.

    ## Poke the Bear

    Let's see what's going on in our VM. We can use the `logs` command to see what's going on behind the scenes, or the `ssh` command to poke around!

    ```bash
    cd ~/Fly/laravel-fly

    # See some logs!
    fly logs

    # SSH and interact with the VM!
    fly ssh console

    # We can see that our php-fpm, queue, and cron daemons are running:
    > ps aux | grep [f]pm
    > ps aux | grep [q]ueue
    > ps aux | grep [c]ron
    ```

    ## What We Did

    We have a Laravel application running with all the usual things:

    1. MySQL
    1. Redis
    1. cron (for the scheduler)
    1. Queues

    That covers the most popular use cases for Laravel, but there's lots more you can do as well! My favorite part of this is how easy it is to setup extra services in Fly (Redis in our case). The private networking makes it super easy.

    Fly is working on making database integrations easier as well. PlanetScale is super cool, and even supports multi-region read replicas (which works well with [scaling Fly apps](https://fly.io/docs/reference/scaling/))!
- :id: blog-launching-laravel-bytes
  :date: '2022-07-26'
  :category: blog
  :title: Launching Laravel Bytes
  :author: fideloper
  :thumbnail: laravel-announcement-thumbnail.png
  :alt:
  :link: blog/launching-laravel-bytes
  :path: blog/2022-07-26
  :body: |2


    <div class="lead">Fly runs apps close to users by taking Docker images and transmogrifying them into Firecracker micro-vms, running on our hardware around the world. If you want to ship a Laravel app, [try it out on Fly.io](https://fly.io/docs/getting-started/laravel/). It takes just a couple of minutes.</div>

    Today we're launching [Laravel Bytes](/laravel-bytes/) - our new home for anything Laravel.



    We're excited to support deploying Laravel across the globe - and we have lots to talk about!



    We've already made it [easy to run Laravel](/docs/laravel/) on Fly, but with the possibilities unlocked by global deployment, there's still plenty more to consider. In future posts, we'll talk about the considerations that come with a global user base.

    Finally, we'll have lots of tips, tricks, strategies, how-to's, and more (especially about [Livewire](https://laravel-livewire.com/) - we love real-time apps)!

    Keep a lookout for new stuff in Laravel Bytes!

    ## Getting Started with Laravel

    Laravel is supported in the `fly launch` command, so the easiest way to get a feel for running Laravel is in our docs for [Deploying a Laravel Application](/docs/laravel/).

    The quickest version of that boils down to this (assuming the [fly command is installed](/docs/getting-started/installing-flyctl/)):

    ```bash
    # Head into a Laravel app
    cd ~/some-laravel-app

    # Create and launch a Laravel app on Fly
    fly launch
    ```

    For something a bit more in depth, we've also written up a more complete [guide on running a Laravel apps on Fly](/laravel-bytes/full-stack-laravel/) with Redis, MySQL, queues workers, and cron tasks.

    <%= partial "shared/posts/cta", locals: {
      title: "Launch a Laravel app, super quick",
      text: "It'll take just a few minutes to get Laravel running on Fly!",
      link_url: "/docs/getting-started/laravel/",
      link_text: "Try Fly for free&nbsp;&nbsp;<span class='opacity-50'>&rarr;</span>"
    } %>
- :id: phoenix-files-phoenix-liveview-tailwind-variants
  :date: '2022-07-22'
  :category: phoenix-files
  :title: Phoenix LiveView Tailwind Variants
  :author: chris
  :thumbnail: tailwind-variants-thumbnail.jpg
  :alt: A person clutching an open laptop to their chest, face-out so we can read
    the screen, which just says 'HTML'. Little love-hearts emanate from the screen.
    There's a ribbon spiralled loosely around person and computer, with 'CSS' printed
    repeatedly along its length.
  :link: phoenix-files/phoenix-liveview-tailwind-variants
  :path: phoenix-files/2022-07-22
  :body: |2-


    ## Problem

    Users of [Tailwind CSS](https://tailwindcss.com) know the productivity gains the utility-first CSS framework provides. One of Tailwind's biggest advantages is that you can rapidly build applications without ever leaving your HTML. There's no context-switching between markup and CSS files, or searching for where classes are or aren't defined.

    You almost never have to touch individual CSS files – except when you want to customize the styling of the classes that Phoenix LiveView uses for loading indicators and form feedback. Yuck.

    Worse than leaving your markup to trudge through CSS files, is the strict nature of the Phoenix LiveView classes that make them hard to customize without many one-off CSS rules.

    Fortunately, Tailwind has a plugin feature that solves this beautifully.

    ## Solution

    Phoenix LiveView uses the following CSS classes to provide user feedback:

    - `phx-no-feedback` - applied when feedback should be hidden from the user
    - `phx-click-loading` - applied when an event is sent to the server on click while the client awaits the server response
    - `phx-submit-loading` - applied when a form is submitted while the client awaits the server response
    - `phx-change-loading` - applied when a form input is changed while the client awaits the server response

    Customizing each of these for each scenario, breakpoint, desktop and mobile size, etc, gets cumbersome and error-prone. We can sidestep these issues by defining a [Tailwind plugin](https://tailwindcss.com/docs/plugins) which provides variants for each of these Phoenix LiveView specific classes inside your `assets/tailwind.config.js` file:

    ```javascript
    const plugin = require('tailwindcss/plugin')

    module.exports = {
      content: [
        "./js/**/*.js",
        "../lib/*_web.ex",
        "../lib/*_web/**/*.*ex"
      ],
      theme: {
        extend: {},
      },
      plugins: [
        require("@tailwindcss/forms"),
        plugin(({addVariant}) => addVariant('phx-no-feedback', ['&.phx-no-feedback', '.phx-no-feedback &'])),
        plugin(({addVariant}) => addVariant('phx-click-loading', ['&.phx-click-loading', '.phx-click-loading &'])),
        plugin(({addVariant}) => addVariant('phx-submit-loading', ['&.phx-submit-loading', '.phx-submit-loading &'])),
        plugin(({addVariant}) => addVariant('phx-change-loading', ['&.phx-change-loading', '.phx-change-loading &']))
      ]
    }
    ```

    We added four `plugin` definitions which add variants for each Phoenix LiveView class. The `&.phx-click-loading` notation specifies that the variant is applied when an element directly has the `phx-click-loading` class applied. Additionally, the `.phx-click-loading &` notation tells Tailwind to also apply the variant to children of a container with the `phx-click-loading` class.

    This allows us to customize our markup like so:

    ```html
    <button
      phx-click="send"
      phx-disable-with="Sending..."
      class="p-4 rounded-lg bg-indigo-600 phx-click-loading:animate-pulse"
    >
      Send!
    </button>
    ```

    We style our button with the regular Tailwind classes, then we customize which Tailwind classes are applied on `phx-click`. We do this by simply prefixing the Tailwind classes by `phx-click-loading`, such as `phx-click-loading:animate-pulse` to show animation feedback while the server processes our event. Here's what it looks like in action:


    <%= video_tag "tailwind_variant_button.mp4?card&center?1/3", title: "A 'send' button with a visual pulsing effect while it waits for the event to be processed." %>

    You can expand this to apply red borders to form inputs when the inputs are invalid, or hide and show input errors – all without leaving your markup. Since elements are all stylized inline, you can also customize your Phoenix LiveView feedback on a case-by-case basis. Happy hacking!
- :id: phoenix-files-setup-vscode-for-elixir-development
  :date: '2022-07-21'
  :category: phoenix-files
  :title: Set Up VSCode for Elixir Dev
  :author: mark
  :thumbnail: desktop-thumbnail.jpg
  :alt:
  :link: phoenix-files/setup-vscode-for-elixir-development
  :path: phoenix-files/2022-07-21
  :body: |2


    Since [Elixir](https://elixir-lang.org/) and [Phoenix](https://www.phoenixframework.org/) appeared prominently in the [Stack Overflow Survey Results for 2022](https://survey.stackoverflow.co/2022/#technology-most-loved-dreaded-and-wanted), more people have been discovering the joy of Elixir and the power of LiveView. This article introduces developers to getting the popular VS Code editor up and running for productive Elixir development.

    ## Problem

    You want to use [Microsoft's VS Code](https://code.visualstudio.com/) for [Elixir](https://elixir-lang.org/) development but there are [many extension options](https://marketplace.visualstudio.com/search?term=elixir&target=VSCode&category=All%20categories&sortBy=Relevance)! Some extensions conflict, some are old, others new, and some require extra config.

    How do we setup VS Code for productive Elixir and Phoenix development?

    ## Solution

    Let's assume you already have [Elixir](https://elixir-lang.org/install.html) and [Phoenix](https://hexdocs.pm/phoenix/installation.html) installed. We are focused on setting up your development environment using VS Code.

    There are **many** extension options and some have been replaced by newer, official extensions. We'll start with the "must have" and then cover some great optional ones.

    ### Must have extensions

    Let's start with the 2 absolute must-have extensions!

    **[ElixirLS: Elixir support and debugger](https://marketplace.visualstudio.com/items?itemName=JakeBecker.elixir-ls)** - Elixir support with debugger, autocomplete, and more. Powered by ElixirLS.

    Refer to the extension information for learning what it can do and troubleshooting steps as well. Just note that it runs a project analysis on first run which can take some time. You may hear laptop fans kick up while that's going. Wait for that to finish before expecting code completion and formatting to work.

    <aside class="callout">
    #### Code management tip

    ElixirLS creates an `.elixir_ls` directory in your project root. You don't want that checked in with your code so adding the directory to your `.gitignore` file is recommended.
    </aside>

    **[Phoenix Framework](https://marketplace.visualstudio.com/items?itemName=phoenixframework.phoenix)** - Syntax highlighting support for Phoenix templates. Supports `.heex` and `~H` embedded templates as well.

    <aside class="right-sidenote">
    View > Command Palette... > "Preferences: Open Settings (JSON)"
    </aside>

    The [extension Github project](https://github.com/phoenixframework/vscode-phoenix) recommends the following config change in your `settings.json` file:

    ```json
    "emmet.includeLanguages": {
      "phoenix-heex": "html"
    },
    ```

    Note: If you don't see the improvements after making the changes, try reloading the window (Command Palette > Developer: Reload Window) or restart VS Code.

    ### Nice to have extensions

    With the critical pieces in place, let's add some frosting!

    **[Elixir Test](https://marketplace.visualstudio.com/items?itemName=samuel-pordeus.elixir-test)** - An extension with a few commands that helps you with your Elixir tests

    Make sure to check out the extension instructions page to see what it can do and especially the keyboard shortcuts for your platform.

    ### Optional extensions

    If you use [Tailwind CSS](https://tailwindcss.com/) for styling your web application, then you'll want to install the **[Tailwind CSS IntelliSense](https://marketplace.visualstudio.com/items?itemName=bradlc.vscode-tailwindcss)** extension that gives you helpful code completion and documentation lookups.

    This config change lets it work well with Phoenix templates and even inside your embedded `~H` function components!

    <aside class="right-sidenote">
    View > Command Palette... > "Preferences: Open Settings (JSON)"
    </aside>

    Add the following config to your `settings.json` file:

    ```json
    "tailwindCSS.includeLanguages": {
        "elixir": "html",
        "phoenix-heex": "html"
    },
    ```

    ## Discussion

    With your development environment setup, you are ready to productively hack on some Elixir code!

    If you are new to VS Code, then it may make sense to check out the official [learning resources](https://code.visualstudio.com/learn) for how to be productive with the tool.

    <%= partial "shared/posts/cta", locals: {
      title: "Fly.io ❤️ Elixir",
      text: "Need a place to deploy that shiny new Phoenix app? Fly.io is a great place to deploy LiveView applications. You can be running in minutes.",
      link_url: "https://fly.io/docs/elixir/",
      link_text: "Deploy your Phoenix app today!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>"
    } %>
- :id: blog-liveview-its-alive
  :date: '2022-07-19'
  :category: blog
  :title: How LiveView Took on a Life of Its Own
  :author: chris
  :thumbnail: phoenix-birth-thumbnail.png
  :alt: A baffled phoenix looks down at a serene-looking chick emerging from an egg.
  :link: blog/liveview-its-alive
  :path: blog/2022-07-19
  :body: "\n\n<div class=\"lead\">Phoenix LiveView lets you build interactive, real-time
    applications without dealing with client-side complexity. This is a post about
    the guts of LiveView. If you just want to ship a Phoenix app, the easiest way
    to learn more is to [try it out on Fly.io](https://fly.io/docs/elixir/); you can
    be up and running in just a couple minutes.\n</div>\n\nLiveView started with a
    simple itch. I wanted to write dynamic server-rendered applications without writing
    JavaScript. Think realtime validations on forms, or updating the quantity in a
    shopping cart. The server would do the work, with the client relegated to a dumb
    terminal. It did feel like a heavy approach, in terms of resources and tradeoffs.
    Existing tools couldn't do it&mdash;but I was sure that this kind of application
    was at least possible. So I plowed ahead.\n\nThree years later, what's fallen
    out of following this programming model often feels like cheating. \"This can't
    possibly be working!\". But it does. Everything is super fast. Payloads are tiny.
    Latency is best-in-class. You write less code. More than that: there's simply
    less to think about when writing features. \n\nThis blows my mind! It would be
    fun to say I'd envisioned ahead of time that it would work out like this. Or maybe
    it would be boring and pompous. Anyway, that's not how it happened.\n\nTo understand
    how we arrived where we are, we'll break down the ideas in the same way we evolved
    LiveView to what it is today: we'll start with a humble example, then we'll establish
    the primitives LiveView needed to solve it. Then we'll see how, almost by accident,
    we unlocked a particularly powerful paradigm for building dynamic applications.
    Let's go.\n\n## A Humble Start\n\nWe&#39;ll start small. Say you want to build
    an e-commerce app where you can update the quantity of an item like a ticket reservation.
    Ignoring business logic, the actual mechanics of the problem involve updating
    the value on a web page when a user clicks a thing. A counter.\n\nThis shouldn&#39;t
    be difficult. But we&#39;re used to navigating labyrinths of decisions, configuring
    build tools, and assembling nuts and bolts, to introduce even the simplest business
    logic. We go to build to a counter, and first we must invent the universe. Does
    it have to be this way?\n\nConceptually, what we really want is something like
    what we do in client-side libraries like React: render some dynamic values within
    a template string, and the UI updates with those changed values. But instead of
    a bit of UI running on the client, what if we ran it on the server? The live view
    might look like this:\n\n```elixir\ndef render(assigns) do\n  ~H\"\"\"\n  <span><%%=
    ngettext(\"One item left\", \"%{count} items left\", @remaining) %></span>\n  <%%=
    if @remaining > 0 do %>\n    <button phx-click=\"reserve\"><%%= gettext(\"reserve\")
    %></button>\n  <%% else %>\n    <%%= gettext(\"Sorry, sold out!\") %>\n  <%% end
    %>\n  \"\"\"\nend\n\ndef mount(%{\"id\" => id}, _session, socket) do\n  {:ok,
    assign(socket, remaining: 10, id: id)}\nend\n```\n\nWe can render a template and
    assign some initial state when the live view mounts.\n\nNext, our interface to
    handle UI interactions could look something like traditional reactive client frameworks.
    Your code has some state, a handler changes some state, and the template gets
    re-rendered. It might look like this:\n\n```elixir\n  def handle_event(\"reserve\",
    _, socket) do\n    %{current_user: current_user, id: id} = socket.assigns\n    case
    Reservations.reserve_ticket(current_user, id) do\n      {:ok, remaining} ->\n
    \       {:noreply,\n         socket\n         |> put_flash(:info, gettext(\"1
    ticket reserved!\"))\n         |> assign(:remaining, remaining)}\n\n      {:error,
    :nostock} ->\n        {:noreply, assign(socket, :remaining, 0})\n    end\n  end\n```\n\nUI
    components are necessarily stateful creatures, so we know we&#39;ll need a stateful
    WebSocket connection with [Phoenix Channels](https://hexdocs.pm/phoenix/channels.html)
    which can delegate to our `mount` and `handle_event` callbacks on the server.\n\nWe
    want to be able to update our UI when something is clicked, so we wire up a click
    listener on `phx-click` attributes. The `phx-click` binding will act like an RPC
    from client to server. On click we can send a WebSocket message to the server
    and update the UI with an `el.innerHTML = newHTML` that we get as a response.
    Likewise, if the server wants to send an update to us, we can simply listen for
    it and replace the inner HTML in the same fashion. The first pass would look like
    this on the client:\n\n```javascript\nlet main = document.querySelector(\"[phx-main]\")\nlet
    channel = new socket.channel(\"lv\")\nchannel.join().receive(\"ok\", ({html})
    => main.innerHTML = html)\nchannel.on(\"update\", ({html}) => main.innerHTML =
    html)\n\nwindow.addEventListener(\"click\", e => {\n  let event = e.getAttribute(\"phx-click\")\n
    \ if(!event){ return }\n  channel.push(\"event\", {event}).receive(\"ok\", ({html})
    => main.innerHTML = html)\n})\n```\n\nThis is where LiveView started&mdash;go
    to the server for interactions, re-render the entire template on state change,
    send the entire thing down to the client, and replace the UI.\n\nIt works, but
    it&#39;s not great.\n\nThere&#39;s a lot of redundant work done on the server
    for partial state changes, and the network is saturated with large payloads to
    only update a number somewhere on a page.\n\nStill, we have our basic programming
    model in place. It only takes annotating the DOM with `phx-click` and a few lines
    of server code to dynamically update the page. Excited, we ignore the shortcomings
    of our plumbing and press on to try out what we&#39;re really excited about – realtime
    applications.\n\n## Things get realtime\n\nPhoenix PubSub is distributed out of
    the box.  We can have our LiveView processes subscribe to events and react to
    platform changes regardless of what server broadcasted the event. In our case,
    we want to level up our humble reservation counter by having the count update
    as other users claim tickets.\n\nWe get to work and crack open our `Reservations`
    module:\n\n```elixir\ndefmodule App.Reservations do\n  ...\n  def subscribe(event_id),
    do: Phoenix.PubSub.subscribe(\"events:#{event_id}\")\n\n  def reserve_ticket(%User{}
    = user, %Event{id: id}) do\n    ...\n    broadcast(id, :reserved, %{event_id:
    id, remaining: remaining}})\n  end\n\n  defp broadcast(id, event, msg) do\n    Phoenix.PubSub.broadcast(App.PubSub,
    \"events:#{id}\", {__MODULE__, event, msg})\n  end\nend\n```\n\nWe add a subscription
    interface to our reservation system, then we modify our business logic to broadcast
    a message when reservations occur. Next, we can listen for events in our LiveView:\n\n```elixir\n
    \ def mount(%{\"id\" => id}, _session, socket) do\n    if connected?(socket),
    do: Reservations.subscribe(id)\n    {:ok, assign(socket, remaining: 2, id: id)}\n
    \ end\n\n  def handle_info({Reservations, :reserved, %{remaining: remaining}},
    socket) do\n    {:noreply, assign(socket, remaining: remaining)}\n  end\n```\n\nFirst,
    we subscribe to the reservation system when our LiveView mounts, then we receive
    the event in a regular Elixir `handle_info` callback. To update the UI, we simply
    update our state as usual.\n\nHere&#39;s what&#39;s neat – now whenever someone
    \ clicks reserve, _all users_ have their LiveView re-render and send the update
    down the wire. It cost us 10 lines of code.\n\nWe test it out side-by-side in
    two browser tabs. It works! We start doubting the scalability of our naive approach,
    but we marvel at what we _didn&#39;t write_.\n\n\n<%= partial \"shared/posts/cta\",
    locals: {\n  title: \"Phoenix screams on Fly.io.\",\n  text: \"Phoenix is a win
    anywhere. But Fly.io was practically born to run it. With super-clean built-in
    private networking for clustering and global edge deployment, LiveView apps feel
    like native apps anywhere in the world.\",\n  link_url: \"https://fly.io/docs/elixir/\",\n
    \ link_text: \"Deploy your Phoenix app in minutes.&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>\"\n}
    %>\n\n## Happy accident #1: HTTP disappears \n\nTo make the reservation count
    update on all browsers, we only wrote a handful of lines of server code. We didn&#39;t
    even touch the client. The existing LiveView primitives of a dumb bidirectional
    pipe pushing RPC and HTML were all we needed to support cluster-wide UI updates.\n\nThink
    about that.\n\nThere was no library or user-land JavaScript to add. Our reservation
    LiveView didn&#39;t even consider how the data makes it to the client. Our client
    code didn&#39;t have to become aware of out-of-band server updates because we
    already send everything over WebSockets.\n\nAnd a revelation hits us. HTTP completely
    fell away from our thoughts while we implemented our reservation system. There
    were no routes to define for updates, or controllers to create. No serializers
    to define for payload contracts. New features are now keystrokes away from realization.\n\nUnfortunately,
    this comes at the cost of server resources, network bandwidth, and latency. Broadcasting
    updates means an arbitrary number of processes are going to recompute an entire
    template to effectively push an integer value change to the client. Likewise,
    the entire templates string goes down the pipe to change a tiny part of the page.\n\nWe
    know there&#39;s something special here, but we need to optimize the server and
    network.\n\n## Making it fast\n\nOptimizing the server to compute minimal diffs
    is some of the most complex bits of the LiveView codebase, but conceptually it&#39;s
    quite simple. Our optimizations have two goals. First, we only want to execute
    those dynamic parts of a template that actually changed from the previous render.
    Second, we only want to send the minimal data necessary to update the client.\n\nWe
    can achieve this in a remarkably simple way. Rather than doing some advanced virtual
    DOM on the server, we simplify the problem. An Elixir HTML template is nothing
    more than a bunch of HTML tags and attributes, with Elixir expressions mixed in
    the places we want dynamic data.\n\nLooking at it from that direction, we can
    optimize simply by splitting the template into static and dynamic parts. Considering
    the following LiveView template:\n\n```\n<span class={@class}>Created: <%%= format_time(@created_at)
    %></span>\n```\n\nAt compile time, we can compile the template into a datastructure
    like this:\n\n```elixir\n%Phoenix.LiveView.Rendered{\n  static: [\"<span class=\\\"\",
    \\\">Created :\", \"</span>\"]\n  dynamic: fn assigns -> \n    [\n      if changed?(assigns,
    :class), do: assigns.class,\n      if changed?(assigns, :created_at), do: format_time(assigns.created_at)\n
    \   ]\n  end\n}\n```\n\nWe split the template into static and dynamic parts. We
    know the static parts never change, so they are split between the dynamic elixir
    expressions. For each expression, we compile the variable access and execution
    with change tracking. Since we have a stateful system, we can check the previous
    template values with the new, and only execute the template expression if the
    value has changed.\n\nInstead of sending the entire thing down on every change,
    we can send the client all the static and dynamic parts on `mount`, then only
    send the partial diff of dynamic segments for each update.\n\nWe can do this by
    sending the following payload to the client on mount:\n\n```javascript\n{\n s:
    [\"<span class=\\\"\", \\\">Created :\", \"</span>\"],\n 0: \"active\",\n 1: \"2022-04-27\"\n}\n```\n\nThe
    client receives a simple map of static values in the `s` key, and dynamic values
    keyed by the index of their location in the template. For the client to produce
    a full template string, it simply zips the static list with the dynamic segments,
    for example:\n\n```javascript\n[\"<span class=\\\"\", \"active\", \"\\\">Created
    :\", \"2022-04-27\", \"</span>\"].join(\"\")\n\"<span class=\"active\">Created:
    2022-04-27</span>\n```\n\nWith the client holding the initial payload of static
    and dynamic values, optimizing the network on update becomes easy. The server
    now knows which dynamic keys have changed, so when a state change occurs, it renders
    the template, which lazily executes only the changed dynamic segments. On return,
    we receive a map of new dynamic values keyed by their position in the template.
    We then pass this payload to the client.\n\nFor example, if the LiveView performed
    `assign(socket, :class, \"inactive\")`, the following diff would fall out of the
    `render/1` call and be sent down the wire:\n\n```javascript\n{0: \"inactive\"}\n```\n\nThats
    it! And to turn this little payload back into an updated UI for the client, we
    only need to merge this object with our static dynamic cache:\n\n```javascript\n{
    \                    {\n                        s: [\"<span class=\\\"\", \\\">Created
    :\", \"</span>\"],\n  0: \"inactive\"   =>    0: \"active\",     \n                        1:
    \"2022-04-27\"\n}                     }\n```\n\nThen we zip the merged data together
    and now our new HTML can be applied like before via an `innerHTML` update.\n\nReplacing
    the DOM container&#39;s innerHTML works, but wiping out the entire UI on every
    little change is slow and problematic. To optimize the client, we can pull in
    [morphdom](https://github.com/patrick-steele-idem/morphdom), a DOM diffing library
    that can take two DOM trees, and produce the minimal amount of operations to make
    the source tree look like the target tree. In our case, this is all we need to
    close the client/server optimization loop.\n\nWe build a few prototypes and realize
    we&#39;ve created something really special. Somehow our naive heavy templates
    are more nimble than those React and GraphQL apps we used to sling. But How?\n\n##
    Happy accident #2: best in class payloads, free of charge\n\nOne of the most mind
    blowing moments that fell out of the optimizations was seeing how naive template
    code somehow produced payloads smaller than the best hand rolled JSON apis and
    even sent less data than our old GraphQL apps.\n\nOne of the neat things about
    GraphQL is the client can ask the server for only the data it wants. Put simply,
    the client can ask for a user&#39;s username and birthday, and it won&#39;t be
    sent any other keys of the canonical user model. This is super powerful, but it
    must be specified on the server via typed schemas to work.\n\nHow then, does our
    LiveView produce nearly keyless payloads with no real typed information?\n\nThe
    answer is it was mostly by accident. Going back to our static/dynamic representation
    in the `Phoenix.LiveView.Rendered` struct, our only goal initially was thinking
    about how to represent a template in a way that allowed us to avoid sending all
    the markup and HTML bits that don&#39;t change. We weren&#39;t thinking about
    solving the problem of client/server payload contracts at all.\n\nThere&#39;s
    a lesson here that I still haven&#39;t fully unpacked. In the same way as a user
    of LiveView I stopped concerning myself with HTTP client/server contracts, as
    an _implementer_ of LiveView, I also had moved on from thinking about payload
    contracts. Yet somehow this dumb bidirectional pipe that sends RPC&#39;s and diffs
    of HTML now allows users to achieve best in class payloads without actually spec&#39;ing
    those payloads. This still tickles my mind.\n\nOne of the other really interesting
    parts of the LiveView journey is how the programming model never changed beyond
    our initial naive first-pass. The code we actually wanted to write in the beginning
    never needed to change to enable all these powerful optimizations and code savings.
    We simply took that naive first-pass and kept chipping away to make it better.
    This lead to other happy accidents.\n\n## Happy accident #3: lazy loading and
    bundle-splitting without the bundles\n\nAs we built apps, we&#39;d examine payloads
    and find areas where more data would be sent than we&#39;d like. We&#39;d optimize
    that. Rinse and repeat. For example, we realized the client often receives the
    same shared fragments of static data for different parts of the page. We optimized
    by sending static shared fragments and components only a single time.\n\nImagine
    our surprise on the other side of finishing these optimizations when we realized
    we solved a few problems that all client-side frameworks must face, without the
    intention of doing so.\n\nBuild tools and client side frameworks have [code splitting](https://reactjs.org/docs/code-splitting.html)
    features where developers can manage how their various JavaScript payloads are
    loaded by the client. This is useful because bundle size is ever increasing as
    more templates and features are added. For example, if you have templates and
    associated logic for a subset of users, your bundle will include all supporting
    code even for users who never need it. Code splitting is a solution for this,
    but it comes at the cost of complexity:\n\n- Developers now have to consider where
    to split their bundles\n- The build tools must be configured to perform code splitting\n-
    Developers have to refactor their code to perform lazy loading\n\nWith our optimizations,
    lazy-loading of templates comes free for free, and the client never gets a live
    component template it already has seen from the server. No bundling required,
    or build tools to configure.\n\nHere&#39;s how it works. Imagine we want to update
    our reservation system to render a list of available events that can be reserved.
    It might look something like this:\n\n\n\n![Two entries, called \"Lunch with the
    founder\" and \"Keynote\", each with a number of tickets remaining and a \"Reserve\"
    button.](lunch.png)\n\nWhich we could render with a reusable `reserve_item` component:\n\n```elixir\ndef
    render(assigns) do\n  ~H\"\"\"\n  <%%= for event <- @events do %>\n    <.reserve_item
    event={event}/>\n  <%% end %>\n  \"\"\"\nend\n\ndefp reserve_item(assigns) do\n
    \ ~H\"\"\"\n  <div class=\"m-2 p-2 border-y-2 border-dotted\">\n    <%%= ngettext(\"One
    ticket left\", \"%{count} tickets left\", @event.remaining) %>\n    <%%= if @event.remaining
    > 0 do %>\n      <.button phx-click={JS.push(\"reserve\", value: %{id: @event.id})}>\n
    \       <%%= gettext(\"reserve\") %>\n      </.button>\n    <%% else %>\n      <%%=
    gettext(\"Sorry, sold out!\") %>\n    <%% end %>\n  </div>\n  \"\"\"\nend\n```\n\nWe
    modified our initial reserve template to render a reservation button from a list
    of events. When LiveView performs its diffing, it recognizes the use of the shared
    statics and the following diff is sent down wire:\n\n![A LiveView diff, highlighting
    how instances of `\"s\":2` refer back the key `\"2\"` in the map of shared static
    template values.](diff.png)\n\nNote the `\"p\"` key inside the diff. It contains
    a map of shared static template values that are referenced later for each dynamic
    item. When LiveView sees a static (`\"s\"`) reference an integer, it knows that
    it points to a shared template value. LiveView also expands on this concept when
    using a live component, where static template values are cached on the client,
    and the template is never resent because the server knows which templates the
    client has or hasn&#39;t seen.\n\nEven with our humble reservation counter, there
    are other bundling concerns we skipped without noticing. Our template used localization
    function calls, such as `<%%= gettext(\"Sold out!\") %>`. We localized our app
    _without even thinking about the details._ For a dynamic app, you&#39;d usually
    have to serialize  a bag of localization data, provide some way to fetch it, and
    code split languages into different bundles.\n\nAs your LiveView application grows,
    you don&#39;t concern yourself with bundle size because there is no growing bundle
    size. The client gets only the data it needs, when it needs it. We arrived here
    without every thinking about the problem of client bundles or carefully splitting
    assets because LiveView applications hardly have client assets.\n\n## Less code,
    fewer moving parts\n\nThe best code is the code you don&#39;t have to write. In
    an effort to make a counter on a webpage driven by the server, we accidentally
    arrived at a programming model that sidesteps HTTP. We now have friction-free
    dynamic feature development with push updates from the server. Our apps have lower
    latency than those client apps we used to write. Our payloads are more compact
    than those GraphQL schemas we carefully constructed.\n\nLiveView is, at its root,
    just a way to generate all the HTML you need on the server, so you can write apps
    without touching JS. More dynamic apps than you might expect, thanks to Elixir’s
    concurrency model. But even though I wrote the internals of it, I'm still constantly
    blown away by how well things came together, and finding surprising new ways to
    apply it to application problems.\n\nAll this from a hack that started with 10
    lines of JavaScript pushing HTML chunks down the wire."
- :id: blog-logbook-2022-07-18
  :date: '2022-07-18'
  :category: blog
  :title: Logbook - 2022-07-18
  :author: fly
  :thumbnail: logbook-default2.jpg
  :alt: A 1950s style cartoon lady pointing at a vintage flight logbook
  :link: blog/logbook-2022-07-18
  :path: blog/2022-07-18
  :body: "\n\n<div class=\"lead\">\nThis is a Logbook post. It tells you what we’ve
    been up to here at Fly.io, to make running your code close to users better in
    all the ways. The proof of the pudding is in the eating, though, so [you should
    dig in now](https://fly.io/docs/speedrun/); you can have an app up and running
    in mere minutes.\n</div>\n\nWe have some real gems in this edition. Have you ever
    wished you could grow the storage volume on a Fly.io app? Now you can!! \n\nWhat
    about this one: Ever wished that the $99 [Pro Plan](https://fly.io/plans) would
    include $99 of usage credits? OK, that one may have been a little bit specific.
    Conversely, have you ever wished that $99+ of monthly resource usage would come
    with support by email? Now it can!\n\nI don't want to write spoilers for everything,
    so read on.\n\nThe changelog's broken up into sections this time around, largely
    to corral the ongoing torrent of [flyctl](https://fly.io/docs/flyctl/) improvements.
    Keep your flyctl up to date to take advantage. If you're playing (or working)
    with our new fast-booting [machines VMs](https://fly.io/blog/fly-machines/) and
    flyctl, be sure to scan these changes! As always, if you're interested in digging
    deeper into flyctl changes, dive into the [releases page on GitHub](https://github.com/superfly/flyctl/releases).\n\n##
    Grab bag! \n\n- **[Feature]** changelog The [Pro Plan](https://fly.io/plans) now
    includes usage credits equivalent to its price: $99. [Read more](https://community.fly.io/t/coming-soon-paid-plans-and-support-oh-my/3221/37).\n-
    **[Feature]** We are now SOC2-compliant. If you know, you know.  But you should
    read the [blog post](https://fly.io/blog/soc2-the-screenshots-will-continue-until-security-improves/)
    regardless. It&#39;s fun. [SOC2: The Screenshots Will Continue Until Security
    Improves](https://fly.io/blog/soc2-the-screenshots-will-continue-until-security-improves/)\n-
    **[Feature]** Added support for Volume expansion via `fly volumes extend <id>`.
    This feature is now available with flyctl version `v0.0.350`.\n- **[Feature]**
    Added SFTP support to [Hallpass](https://fly.io/blog/ssh-and-user-mode-ip-wireguard/).
    It&#39;s a pain to move files to and from a Fly app. Now you can `scp` or `sftp`
    files by connecting to [6PN](https://fly.io/docs/reference/private-networking/)
    over [WireGuard](https://fly.io/docs/reference/private-networking/#private-network-vpn).
    We haven&#39;t yet integrated this into flyctl.\n- **[Feature]** Personal Organizations
    are no longer special  \U0001F389 meaning the default organization we expect every
    user to have is less restricted. You can now add/remove members and upgrade your
    plan.\n- **[Feature]** Added a new transit provider and some BGP routing announcement
    changes with Netactuate. This should improve routing, which should improve latency
    for customers.\n- **[Feature]** Updated our Heroku webhook handling to email users
    when we’ve been unable to refresh their tokens on our end, and thus unable to
    auto-deploy their Heroku app releases to Fly.io.\n- **[Feature]** Wrote an internal
    walkthrough on updating customer account information in Stripe, to make it easier
    for team members to help a customer update their info—which means it&#39;s done
    faster for the customer!\n- **[Fix]** Fixed a bug that may have been preventing
    autoscaling from working properly  for ~75% of all apps using the feature.\n-
    **[Fix]** Fixed the Fly Redis image to start correctly, error if something goes
    wrong at startup, and only require REDIS_PASSWORD to boot.\n- **[Fix]** Creating
    an app via the GraphQL API with a `null`  `runtime` used to result in a cryptic
    error for people using the API directly. The API now properly enforces our default
    “Firecracker” runtime on app create, so this shouldn&#39;t happen anymore.\n-
    **[Fix]** Resolved some more issues folks have found by using the API directly.
    Trying to execute a template deployment via the API without defining apps in the
    template input would respond with a generic server error; it should now respond
    with a validation message. Also, fetching the flyctl version via GraphQL was causing
    generic errors - this is no longer the case.\n- **[Docs]** Added docs for [mounting
    persistent volumes via the machines API](https://fly.io/docs/reference/machines/#create-and-start-a-machine).\n-
    **[Docs]** [Updated](https://github.com/superfly/docs/issues/190) the Laravel
    docs with details on changing the PHP version, and getting full stack traces.\n-
    **[Blog]** Published [The Serverless Server](https://fly.io/blog/the-serverless-server/),
    a delightful romp through the building of a fictitious, yet somehow very familiar,
    Functions-as-a-Service service, from the metal up. \n\n## flyctl\n\n- **[Feature]**
    \ `flyctl` (=`fly`) with `LOG_LEVEL=debug` now formats and colorizes JSON requests
    and responses for readability.\n- **[Feature]** Deprecated `fly create` in favor
    of `fly apps create` so as not to have to keep arguments/flags in sync.\n- **[Feature/Fix]**
    We fixed a regression in `fly launch` for Phoenix apps causing the config file
    to lose the `PORT` environment variable. We also added full-stack tests that detect
    `fly launch` regressions for future Phoenix releases and `flyctl` versions.\n-
    **[Feature]** Improved the Laravel launcher for the `fly launch` command via [Laravel
    Launcher improvements](https://github.com/superfly/flyctl/issues/1034).\n- **[Feature]**
    [Split up flyctl launch scanners](https://github.com/superfly/flyctl/pull/1067)
    to make them easier to work on, and to more clearly indicate which frameworks
    we support and where to contribute.\n- **[Fix]** Fixed a [flyctl regression](https://github.com/superfly/flyctl/issues/1069)
    preventing the `api` module from being imported externally.\n\n### flyctl &amp;
    machines\n\n- **[Feature]**  `fly deploy` now supports machine-based apps by adding
    `version = 2` to `fly.toml`. `fly status` also works for them, and new machine
    apps may be launched with `fly machine launch`. More details about this coming
    soon, along with docs on the new config format.\n- **[Feature]**  `flyctl machine`
    (=`fly machine`) commands now will work without having to specify an app name
    in `fly.toml` or on the command line. This is useful when working with individual
    machines across multiple apps.\n- **[Feature]** Mounted volumes are now preserved
    correctly across machine app deployments handled by `fly deploy`.\n- **[Feature]**
    \ `fly machine status` now displays attached volumes.\n- **[Feature]** Machines
    cloned with `fly machine clone` will no longer pass volume config to the clone.\n-
    **[Feature]** `fly apps create --machines` will create an app for the new machines
    platform up-front.\n- **[Feature]** For stopped/exited machines, `fly machine
    status` now shows the exit code, whether the machine was killed by OOM, and whether
    it was requested to be stopped.\n- **[Feature]**  [Added](https://github.com/superfly/flyctl/pull/1018)
    \ `fly machine status --display-config` to display the full syntax-highlighted
    machine config. Useful for debugging.\n- **[Feature]** Updated `fly machine clone`
    for the new machines API, allowing machines users to quickly scale an app up,
    or out to other regions.\n- **[Feature]**  `fly deploy` for machines apps will
    now run rolling deploys, waiting for each VM to start before moving on.\n\n##
    Dashboard UI\n\n- **[Feature]** Added a volume size column to the volumes list
    on the dashboard app page (and formatted the date properly).\n- **[Feature]**
    Applied the cool new layout to our most-frequently-sent emails.\n\n\n![Screenshot
    of a password-reset email. It says \"click this link to change your password,\"
    then there's a \"Reset Password\" button, and then \"Be fast, this expires in
    a few minutes!\"](password_reset.png?3/4&center)\n\n\n- **[Feature]** Changed
    the font in the image details section (in the [dashboard](https://fly.io/dashboard)
    app overview tab) to monospace to make it nicer to read.\n- **[Feature]** Changed
    date formatting for app secrets tab to make it easier to read when an environment
    variable was created.\n- **[Feature]** Simplified the flow on [fly.io/documents](http://fly.io/documents).
    Clicking on an action button will just send you to the documents page for your
    personal org.\n- **[Feature/Fix]** Styling improvements for the app metrics page,
    for readability. Fixed some weirdness due to conflicts with leftover styling from
    the old Rails UI.\n\n\n\n\n"
- :id: phoenix-files-server-triggered-js
  :date: '2022-07-11'
  :category: phoenix-files
  :title: 'Triggering JS from the server in LiveView: showing a spinner'
  :author:
  :thumbnail: js-loader-thumbnail.jpg
  :alt:
  :link: phoenix-files/server-triggered-js
  :path: phoenix-files/2022-07-11
  :body: "\n\nIt's always frustrating when we click something—and it looks like nothing's
    happening. The default solution to this is to throw up a spinner while the server
    is chewing on a request, to help users resist the temptation to keep clicking.
    UX win!\n\nWe can do this on the client, and poll the server repeatedly, asking
    “Are you done yet?” But polling adds waiting time as well as traffic and load
    on the server. The server knows what it’s doing, and it knows when it’s done.
    We should get the server to show and hide the \"Please wait while I get that for
    you…” spinner.\n\nWith LiveView, we have the tools!\n\n## Problem\n\nHow can we
    create a loader that the **server** makes appear and disappear? And can we make
    that into a reusable component?\n\n## Solution\n\nToday we'll create a loader
    component that appears asynchronously when we make a request to an external API
    that may take time to respond. For that we'll apply an interesting trick; we'll
    trigger JS commands from the server side!\n\n### Defining a loader component\n\nBefore
    we start, I want to mention that my abilities developing CSS and HTML are not
    the best, so I used the amazing spinner designed and developed by Vasili Savitski
    and [Epicmax](http://epicmax.co/); you can find many others [here](https://epic-spinners.epicmax.co/).\n\nWe
    package up the spinner into a `loader` function component that we can reuse:\n\n```elixir\ndef
    loader(assigns) do\n  ~H\"\"\"\n  <div class=\"hidden h-full bg-slate-100\" id={@id}>\n
    \   <div class=\"flex justify-center items-center h-full\">\n      <div class=\"flower-spinner\">\n
    \       <div class=\"dots-container\">\n          <div class=\"bigger-dot\">\n
    \           <div class=\"smaller-dot\"></div>\n          </div>\n        </div>\n
    \     </div>\n    </div>\n  </div>\n  \"\"\"\nend\n```\n\nThe spinner itself is
    just a collection of HTML elements within our component container, and its own
    CSS takes care of the fancy positioning and animation of its nested elements to
    create the pretty spinny pattern.\n\nWe use Tailwind classes to match the loader
    container size to its parent HTML element, and to center the spinner within its
    parent. The `hidden` class makes the spinner invisible by default.\n\nLet's render
    our spinner (without the `hidden` class) to see how it looks:\n\n```elixir\ndef
    render(assigns) do\n  ~H\"\"\"\n    <.loader id=\"my_spinner\"/>\n  \"\"\"\nend\n```\n\n<%=
    video_tag \"js-loader-01.mp4?card?center?1/3\" %>\n\n### Showing and hiding the
    loader\n\nNow, how do we make our loader appear and disappear? [JS commands](https://hexdocs.pm/phoenix_live_view/Phoenix.LiveView.JS.html)!\n\n```elixir\ndefp
    show_loader(js \\\\ %JS{}, id) do\n  JS.show(js, \n    to: \"##{id}\", \n    transition:
    {\"ease-out duration-300\", \"opacity-0\", \"opacity-100\"}\n  )\nend\n\ndefp
    hide_loader(js \\\\ %JS{}, id) do\n  JS.hide(js, \n    to: \"##{id}\", \n    transition:
    {\"ease-in duration-300\", \"opacity-100\", \"opacity-0\"}\n  )\nend\n```\n\nWe
    use the `JS.show` and `JS.hide` commands, each one with a simple transition that
    changes the opacity of the loader's container.\n\nThe commands are encapsulated
    inside the `show_loader/2` and `hide_loader/2` functions just for simplicity,
    as we'll use them later.\n\nUsing the commands we defined above, we can show and
    hide our loader on the client side just by using [phx-bindings](https://hexdocs.pm/phoenix_live_view/bindings.html)
    like `phx_click`:\n\n```elixir\n<button phx-click={show_loader(\"my_spinner\")}>\n
    \ Without the server!\n</button>\n```\n\nHowever, sometimes only the server (and
    the logic we define) knows when the processing has finished and the loader can
    be hidden again. This is where we apply the most important trick of this recipe!
    \n\n### Triggering a JS command from the server side\n\nThe idea here is to push
    an event to the client from the server side each time we want to show or hide
    our loader, and have the event's JS listener trigger the JS command we want. Neither
    the server nor the listener really needs to know exactly what should happen on
    the client when this event arrives! So here's how we break it up:\n\n* The loader's
    outer HTML element has `data-*` attributes that store the JS Commands or functions
    that should be invoked when we start waiting, and when we’re done waiting.\n*
    The server pushes an event whose payload consists of 1) an element id to target
    and 2) the name of a data attribute, like `data-ok-done`.\n* The listener executes
    the JS indicated by the specified `data-*` attribute on the target element.\n\nThis
    is a highly-reusable pattern! Let's use it:\n\nFirst we specify the JS commands
    we want to trigger by adding them as attributes of our loader main container:\n\n```elixir\ndef
    loader(assigns) do\n  ~H\"\"\"\n  <div \n    class=\"hidden h-full bg-slate-100\"
    id={@id}\n    data-plz-wait={show_loader(@id)} \n    data-ok-done={hide_loader(@id)}\n
    \ >\n    .\n    .\n    .\n  </div>\n  \"\"\"\nend\n\n```\n\nThe `JS.show` command
    we defined in the `show_loader/2` function is embedded inside the `data-plz-wait`
    attribute (the same happens with the `data-ok-done` attribute). In both cases,
    we pass the identifier of our loader as a parameter.\n\nIn our `app.js` file we
    add an event listener for the server-pushed event `js-exec`:\n\n<aside class=\"right-sidenote\">Note
    that when we push an event from the server using `Phoenix.LiveView.push_event/3`
    (as we'll do later), the event name is dispatched in the browser with the `phx:`
    prefix.</aside>\n\n```elixir\nwindow.addEventListener(\"phx:js-exec\", ({detail})
    => {\n    document.querySelectorAll(detail.to).forEach(el => {\n        liveSocket.execJS(el,
    el.getAttribute(detail.attr))\n    })\n  })\n```\n\n\nThis listener is a generic
    solution: we can trigger any JS command (we can even execute a set of JS commands!)
    just by embedding them inside an HTML attribute.\n\nLet's see how it works:\n\nThe
    listener function receives a `detail` object, which has two attributes:  `to`
    and `attr`.  `to` contains the identifier of one or more HTML elements, and `attr`
    is the name of the HTML attribute that embeds the JS command we want to trigger.\n\nFor
    each element matching the `to` identifier, we trigger the JS command contained
    in the element's HTML attribute `attr`.\n\nFinally, we can trigger our `js-exec`
    event by adding it to the socket and pushing it to the client by using the [push_event/3](https://hexdocs.pm/phoenix_live_view/Phoenix.LiveView.html#push_event/3)
    function:\n\n```elixir\npush_event(socket, \"js-exec\", %{\n  to: \"#my_spinner\",
    \n  attr: \"data-ok-done\"\n})\n```\n\nWe send a map with the details that the
    listener is waiting for: the identifier of our spinner, and the name of the attribute
    that embeds the JS command we want to trigger.\n\nThis way we push the `js-exec`
    event to the client, and the listener receives the event and triggers the command
    embedded in the `data-ok-done` attribute.\n\n### Loader in action\n\nSpeaking
    of limited skills, it's difficult for me to choose colors that look good together,
    so an API like [this one](https://palett.es/api ) that generates a random color
    palette is very useful.\n\nFor our example, when we click a button, we send a
    request to the [Palett API](https://palett.es/api) and display the generated colors
    in an interesting way:\n\n\n<%= video_tag \"js-loader-02.mp4?card?center?1/3\"
    %>\n\n\nWe'll define a new LiveView for our example. We won't include _all_ the
    content here. You can check out the full [repo](https://github.com/bemesa21/phoenix_files/pull/1)
    \ to see everything. In this recipe we'll go over the most important details.\n\nThe
    trick to using our loader lies in just a couple of functions:\n\n```elixir\ndef
    handle_event(\"create_palette\", _value, socket) do\n  send(self(), :run_request)\n
    \ \n  socket = push_event(socket, \"js-exec\", %{\n    to: \"#my_spinner\", \n
    \   attr: \"data-plz-wait\"\n  })\n\n  {:noreply, socket}\nend\n\ndef handle_info(:run_request,
    socket) do\n  socket =\n    socket\n    |> assign(:colors, get_colors())\n    |>
    push_event(\"js-exec\", %{to: \"#my_spinner\", attr: \"data-ok-done\"})\n\n  {:noreply,
    socket}\nend\n```\n\nWhen the `create_palette` event is received from the button,
    we send the `:run_request` message to our own component and add the `js-exec`
    event to the socket just before returning the `{:noreply, socket}` tuple. This
    way, we process the `:run_request` message while the spinner is displayed and
    it stays there.\n\nOn the other hand, the `handle_info` callback is in charge
    of asynchronously calling the API and adding the event to the socket to hide the
    spinner. Once the colors are fetched from the API, no matter how long it took
    to respond, the loader is hidden.\n\n**Discussion**\n\nWe created a reusable loader,
    and with only 4 lines of JavaScript, got the server to display it using JS commands!
    I can say that it took me more effort to think and come up with a fun example
    than to create the solution.\n\nYou can build on this. Maybe you have a background
    job that builds an invoice on demand. If you use Phoenix PubSub, then your far-away
    server can notify the waiting LiveView when the job is done, and your spinner
    can vanish as the invoice or link appears. The possibilities are myriad. I'll
    be keeping an eye out for other solutions that involve triggering JS commands
    with server events!\n"
- :id: blog-soc2-the-screenshots-will-continue-until-security-improves
  :date: '2022-07-07'
  :category: blog
  :title: 'SOC2: The Screenshots Will Continue Until Security Improves'
  :author: thomas
  :thumbnail: soc2-thumbnail.png
  :alt: Fly.io-themed demonology-style sigils
  :link: blog/soc2-the-screenshots-will-continue-until-security-improves
  :path: blog/2022-07-07
  :body: "\n\n<p class=\"lead\">[Fly.io](http://Fly.io) runs apps close to users by
    taking containers and upgrading them to full-fledged virtual machines running
    on our own hardware around the world. We’ll come right to the point: if you were
    waiting for us to be SOC2-compliant before giving us all your money, well, we’re
    SOC2 now, so [take us for a spin](https://fly.io/docs/speedrun/) and make your
    checks payable to [Kurt](https://twitter.com/mrkurt).\n\nIf you’re off getting
    your app up and running on [Fly.io](http://Fly.io) and finding your checkbook,
    great! I won’t get in your way. The rest of you, though, I want to talk to you
    about what SOC2 is and how it works.\n\n<aside class=\"right-sidenote\">Spoiler:
    the SOC2 Starting Seven post held up pretty well.</aside>\nSOC2 is the best-known
    infosec certification, and the only one routinely demanded by customers. I have
    [complicated feelings about SOC2](https://latacora.micro.blog/2020/03/12/the-soc-starting.html),
    which you will soon share. But also, a few years ago, I wrote a [blog post](https://latacora.micro.blog/2020/03/12/the-soc-starting.html)
    about what startups need to do to gear up for SOC2. Having now project-managed
    [Fly.io](http://Fly.io)’s SOC2, I’d like to true that post up, since I’m officially
    a leading authority on the process.\n\nSOC2 is worth talking about. It’s arcane
    in its particulars. Startups that would benefit from SOC2 are held back by the
    belief that it’s difficult and expensive to obtain. Consumers, meanwhile, split
    down the middle between cynics who’re certain it’s worthless and true-believers
    who think it sets the standard for how security should work.\n\nEverybody would
    be better off if they stopped believing what they believe about SOC2, and started
    believing what I believe about SOC2.\n\n## I’m a Customer. What Should I Know?\n\nBottom-line:
    SOC2 is a weak positive indicator of security maturity, in the same ballpark of
    significance as a penetration test report (but less significant than multiple
    pentest reports).\n\n- SOC2 is an accounting-style, or \"real\", audit. That means
    it confirms on-paper claims companies make about their security processes. They’re
    nothing at all like “security audits” or “penetration tests”, which are heavily
    adversarial, engineering-driven, and involve giving third parties free rein to
    find interesting problems. \n- SOC2 is about the security of the company, not
    the company’s products. A SOC2 audit would tell you something about whether the
    customer support team could pop a shell on production machines; it wouldn’t tell
    you anything about whether an attacker could pop a shell with a SQL Injection
    vulnerability.\n- The guts of a SOC2 audit are a giant spreadsheet questionnaire
    (the “DRL”) and a battery of screenshots serving as evidence for the answers in
    the questionnaire.\n- The SOC2 DRL is high-level, abstract, and the product of
    accounting industry standards like the [COSO framework](https://pathlock.com/learn/internal-control-framework-a-practical-guide-to-the-coso-framework/);
    it understands things like “git” and “multi-factor authentication”, but nothing
    lower-level than that.\n- There are two kinds of SOC2 reports, done in sequence:
    the “Type I”, which takes a point-in-time snapshot of a company’s processes, and
    the “Type II”, which confirms over several months that the company consistently
    adhered to those processes. You can't “flunk” a Type I audit. But it’s more annoying
    to pass a Type II, because you can’t travel back in time to close a gap you had
    last year. \n\nI’m underselling SOC2. It assures some things pentests don't:\n\n-
    consistent policies for who in the company gets access to what\n- that everyone
    in the company has to log in with 2FA\n- the capability, at least, to fire alerts
    when weird things show up in logs\n- severed employees reliably have their access
    terminated\n- that the company is not actually run by 4 raccoons in a trenchcoat
    (or, if it is, that suitable policy has been written documenting and accepting
    the risk)\n\nDepending on the kind of company you’re looking at, a SOC2 certification
    might be more or less meaningful. Intensely technical company? High-risk engineering?
    Look for the pentest. Huge number of employees? Get the SOC2 report.\n\nSo: if
    you’re clicking on SOC2 blog posts because you’re wondering how seriously you
    should take SOC2, there's your answer. Go in peace.\n\nThe rest of you, buckle
    up.\n\n## What’s SOC2?\n\n<aside class=\"right-sidenote\">You might care about
    transaction integrity if you’re a Stripe, or confidentiality if you do e-Discovery
    for lawsuits, or privacy if you're Equifax.</aside>\n\nThere’s a structure to
    the things you claim in SOC2, the AICPA’s “[Trust Services Criteria](https://us.aicpa.org/content/dam/aicpa/interestareas/frc/assuranceadvisoryservices/downloadabledocuments/trust-services-criteria.pdf)”
    and something called “[the COSO framework](https://pathlock.com/learn/internal-control-framework-a-practical-guide-to-the-coso-framework/)”.
    These are broken down into categories: security, availability, transaction integrity,
    confidentiality, and privacy. “Security” is mandatory, and is the only one that
    matters for most companies.\n\n<aside class=\"right-sidenote\">”DRL” is “Document
    Request List”</aside>\n\nThe ground truth of SOC2 is something called the DRL,
    which is a giant spreadsheet  that your auditor customizes to your company after
    a scoping call. You can try to reason back every line item on the DRL to first
    accounting principles, but that’d be approximately as productive as trying to
    reason about contract law from first principles after paying a lawyer to review
    an agreement. Just take their word for it.\n\nWith me so far? SOC2. It’s a big
    spreadsheet an accounting firm gives you to fill out.\n\n## When Should You SOC2?\n\nWe
    waited as long as we felt we could.\n\nCareful, now. “Getting SOC2-certified”
    isn't the same as “doing the engineering work to get SOC2-certified”. Do the engineering
    now. As early as you can. The work, and particularly its up-front costs, scale
    with the size of your team.\n\nThe audit itself though doesn't matter, except
    to answer the customer request “can I have your SOC2 report?”\n\nSo, “when should
    I SOC2?”  is easy to answer.  Do it when it’s more economical to suck it up and
    get the certification than it is to individually hand-hold customer prospects
    through your security process.\n\nThere's a reason customers ask for SOC2 reports:
    it's viral, like the GPL. The DRL strongly suggests you have a policy to “collect
    SOC2 reports from all your vendors”. Some SOC2 product vendors offer automated
    [security questionnaires](https://medium.com/starting-up-security/understanding-the-security-questionnaire-cbf02b4715)
    for companies to fill out, and they ask for SOC2 reports as well. Your customers
    ask because they themselves are SOC2, and the AICPA wants them to force you to
    join the coven.\n\nThat doesn’t mean you have to actually do it. If you can speak
    confidently about your security practice, you can probably get through anybody’s
    VendorSec process without a SOC2 report. Or you can pay an audit firm to make
    that problem go away.\n\nIt makes very little sense to get SOC2-certified before
    your customers demand it. You can get a long way without certification. If it
    helps, remember that you can probably make a big purchase order from that Fortune
    500 customer contingent on getting your Type I in the next year.\n\n## What SOC2
    Made Us Do\n\nWe started preparing for SOC2 more than a year before engaging an
    auditor, following the playbook from [the “Starting 7” blog post](https://latacora.micro.blog/2020/03/12/the-soc-starting.html).
    That worked, and I’m glad we did it that way. But to keep this simpler to read,
    I’m just going to write out all the steps we took as if they happened all at once.\n\n**Single
    Sign-On**: Every newly-minted CSO I’ve ever asked has told me that SSO was one
    of the first 3 things they got worked out when they took the position. Put compliance
    aside, and it’s just obvious why you want a single place to control credentials
    (forcing phishing-proof MFA, for instance), access to applications, and offboarding.
    We moved everything we could to our Google SSO.\n\nInside our network, we also
    use a certificate-based SSH access control system (I’ll stop being coy, [we use
    Teleport](https://goteleport.com/)). To reach Teleport, you need to be [on our
    VPN](https://tailscale.com/); to get to [our VPN](https://tailscale.com/), you
    need Google SSO. Teleport, however, is authenticated separately, via Github’s
    SSO. So, for SSH, we have two authentication sources of truth, both of which need
    to work to grant access.\n\n<div class=\"callout\">\nIn addition to SSO, Teleport
    has the absolutely killer feature of transcript-level audit logs for every SSH
    session; with the right privilege level, you can watch other team members sessions,
    and you can go back in time and see what everyone did. This has the knock-on benefit
    of providing a transcript-level log of any REPL anyone has to drop into in prod.\n</div>\n\nThere
    is, infamously, an “[SSO tax](https://sso.tax/)” that companies pay to get access
    to the kinds of SAAS accounts that support SAML or OIDC. I have [opinions about
    the SSO tax](https://news.ycombinator.com/item?id=29892664). It's definitely a
    pain in our asses. If it’s early days for your company, for SAAS vendors that
    don’t deal in sensitive information, you can skip the SSO and just restrict who
    you give access to for the app. But mostly, you should just suck it up and pay
    for the account that does SSO.\n\n**Protected Branches**: I was surprised by how
    important this was to our auditors. If they had one clearly articulable concern
    about what might go wrong with our dev process, it was that some developer on
    our team might “go rogue” and install malware in our hypervisor build.\n\nIt’s
    easy to enable protected branches in Github. But all that does is make it hard
    for people to push commits directly to `main`, which people shouldn’t be doing
    anyways. To get the merit badge, we also had to document an approval process that
    ensured changes that hit production were reviewed by another developer.\n\nThis
    isn’t something we were doing prior to SOC2. We have components that are effectively
    teams-of-one; getting reviews prior to merging changes for those components would
    be a drag. But our auditors cared a lot about unsupervised PRs hitting production.\n\nWe
    asked peers who had done their own SOC2 and stole their answer: post-facto reviews.
    We do regular reviews on large components, like the [Rust fly-proxy](https://fly.io/blog/the-tokio-1-x-upgrade/)
    that powers our [Anycast network](https://fly.io/blog/32-bit-real-estate/) and
    the Go flyd that [drives Fly machines](https://fly.io/blog/fly-machines/). But
    smaller projects like our private DNS server, and out-of-process changes like
    urgent bug fixes, can get merged unreviewed (by a subset of authorized developers).
    We run a Github bot that flags these PRs automatically and hold a weekly meeting
    where we review the PRs.\n\n**Centralized Logging**: A big chunk of the SOC2 DRL
    is about monitoring systems for problems. Your auditors will gently nudge you
    towards centralizing this monitoring, but you’ll want to do that anyways, because
    every logging system you have is one you’ll have to document, screenshot, and
    write policy about.\n\nWe got off easy here, because logging is a feature of our
    platform; we run [very large ElasticSearch](https://fly.io/docs/flyctl/logs/)
    and [VictoriaMetrics clusters](https://fly.io/blog/measuring-fly/), fed from [Vector](https://vector.dev/)
    and [Telegraf](https://github.com/influxdata/telegraf/), and we’re generally a
    single ElasticSearch query away from any event happening anywhere in our infrastructure.\n\nOne
    thing SOC2 did force us to do was pick a ticketing system, which is something
    we’d done our best to avoid for several years. We send alerts to Slack channels
    and PagerDuty, and then have documented processes for ticketing them.\n\nAnother
    thing that surprised me was how much SOC2 mileage we got out of HelpScout. HelpScout
    is where our support mails (and `security@` mails) go to, and while I’m not a
    HelpScout superfan, it is a perfectly cromulent system of record for a bunch of
    different kinds of events SOC2 cares about, like internal and external reports
    of security concerns.\n\n**CloudTrail**: We barely use anything in AWS other than
    storage. We compete with AWS! We run our own hardware! But SOC2 audit firms have
    spent the last 10 years certifying  AWS-backed SAAS companies, and have added
    a whole bunch of AWS-specific line-items to their DRLs. We're now much better
    at indexing and alerting on CloudTrail than we were before we did SOC2. It’s too
    bad that’s not more useful to our security practice.\n\n**Infrastructure-as-Code**:
    Your auditor will probably know what Terraform and CloudFormation are, and they
    will want you to be using it. Your job will be documenting how your own deployment
    system (the bring-up for new machines in your environment) is similar to Terraform.
    Sure, whatever.\n\nAn annoyance I did not see coming from previous experience
    was host inventory. Inventory is trivial if you’re an AWS company, because AWS
    gives you an API and a CLI tool to dump it. We run our own hardware, and while
    we have several different systems of record for our fleet of machines, they’re
    idiosyncratic and don’t document well; we ended up taking screenshots of SQL queries,
    which wasn’t as smooth as just taking a screenshot of our [Tailscale ACLs](https://tailscale.com/kb/1018/acls/)
    or Google SSO settings.\n\n<a name=\"endpoint\"></a>\n**MDM and Endpoint Management**:
    Here’s a surprise: in the year of our lord 2022, doing endpoint security in your
    SOC2 has fallen out of fashion. We were all geared up to document our endpoint
    strategy, but it turned out we didn’t have to.\n\nI should have some snarky bit
    of “insight” to share about this, but I don’t, and mostly all I can tell you is
    that you can probably cross this off your checklist of big projects you’ll need
    to get done simply to get a SOC2 certification. You should do the work anyways,
    on its own merits.\n\n<aside class=\"right-sidenote\">SOC2’s company control standards
    are firmly rooted in the accounting scandals of 2001, and it shows.</aside>\n\n**Boring
    Company Stuff**: I’d been mercifully insulated from this aspect of SOC2 in my
    former role as engineering support for SOC2’s, but had no such luck this time.
    I knew there was a lot of annoying company documentation involved in doing SOC2.
    I won’t put you to sleep with many of the details; if you’re the kind of company
    that should get a SOC2, the company and management stuff isn’t going to be an
    obstacle.\n\nWe had three “boring company stuff” surprises that stick out:\n\nFirst,
    We needed a formal org chart posted where employees could find it. We’re not a
    “titles and management” kind of company (we’re tiny), so this was a bother. But,
    whatever, now we have an org chart. Exciting!\n\nNext, our auditors wanted to
    see evidence of annual performance reviews. We don’t do annual performance reviews
    (we’re a continuous feedback, routine scheduled 1-on-1 culture). But if you’re
    not doing annual performance reviews, the AICPA can’t give assurances that an
    employee who exfiltrated our production database to Pastebin would be terminated.
    So now we have pro-forma annual reviews.\n\nThis kind of SOC2 thing falls under
    the category of “[things you need to carefully explain to your team](SOC2-MESSAGE.png)
    so they don’t think you’ve suddenly decided to start stack ranking everyone”.
     \n\nFinally, background checks.\n\nBackground checks are performative and intrusive.
    Ask around for horror stories about how they flag candidates for not being able
    to source the right high school transcripts. Also, for us, they’re occasionally
    illegal: we have employees around the world, including several in European jurisdictions
    that won’t allow us to background check.\n\nThis is the only issue we ended up
    having to seriously back-and-forth with our auditors about. We held the line on
    refusing to do background checks, and ultimately got out of it after tracking
    down another company our auditors had worked with, finding out what they did instead
    of background checks, stealing their process, and telling our auditors “do what
    you did for them”. This worked fine.\n\n**Policies**: You’re going to write a
    bunch of policies. It’s up to you how seriously you’re going to take this. I can
    tell you from firsthand experience that most companies phone this in and just
    Google `[${POLICY} template]`, and adopt something from the first search engine
    result page.\n\n2019 Thomas would have done the same thing. But actually SOC2'ing
    a company I have a stake put me in a sort of CorpSec state of mind. You read a
    template incident response or data classification policy. You start thinking about
    why those policies exist. Then you think about would could go wrong if there was
    a major misunderstanding about those policies. Long story short, we wrote our
    own.\n\nThis part of the process was drastically simplified by the work [Ryan
    McGeehan](https://twitter.com/magoo) has published, for free, on his “[Starting
    Up Security](https://scrty.io/)” site. We have, for instance, an Information Security
    Policy. It’s all in our own words (and ours quotes [grugq](https://twitter.com/thegrugq)).
    But it’s based almost heading-for-heading on [Ryan’s startup policy](https://medium.com/starting-up-security/starting-up-security-policy-104261d5438a),
    which is brilliant.\n\nOne thing about writing policies inspired by Ryan’s examples
    is that it liberates you to write things that make sense and read clearly and
    don’t contain the words “whereas” or “designee”. Ryan hasn’t published a Data
    Classification policy. But our Data Classification policy was easy to write, in
    a single draft, just by using Ryan’s voice.\n\n<div class=\"callout\">\nIf you’re
    concerned about what you’re up against here, we ended up writing: (1) an Information
    Security Policy, which everyone on our team has to sign, (2) a Data Classification
    Policy that spells out how to decide which things can go in Slack or Email and
    which things you have to talk to management before transmitting at all, (3) a
    Document Retention policy (OK, this one I just sourced directly from our lawyers),
    (4) a Change Management policy, (5) a Risk Assessment policy, which says that
    sometime this year we will build a Risk Assessment spreadsheet explaining how
    we’ll handle a zombie apocalypse (you think I'm joking), (6) a Vulnerability Management
    policy that roughly explains how to run nmap, (7) an Access Request policy that
    tells people which Slack channel to join to ask for access to stuff, (8) a Vendor
    Management policy that propagates the SOC2 virus to all our vendors, (9) an Incident
    Response policy, which [we cribbed from Ryan](https://medium.com/starting-up-security/an-incident-response-plan-for-startups-26549596b914),
    (10) a Business Continuity plan that says that Jerome is in charge if Kurt is
    ever arrested for robbing a bank, and (11) an employee handbook.\n\nI want to
    say more about our Access Management policy, which I shoplifted, at least in spirit,
    from a former client of mine that now works with us at [Fly.io](http://Fly.io),
    but this is getting long, so you should just bug me about it online.\n</div>\n\nWe
    use [Slab](https://slab.com/) as our company wiki / intranet. It’s great. Slab
    surprised me midway through the audit with a feature for “verifying” pages, which
    might be the highest ROI feature/effort ratio I’ve come across: I click a button
    and Slab adds a green banner to a page saying that it’s “verified” for the next
    several months. That’s it, that’s the feature. Several DRL line items are about
    “recertifying” policies annually, and this gave us the screenshots we needed for
    that. Well done, Slab! Betcha didn't even realize you implemented policy recertification.\n\n**What
    We Didn’t Let SOC2 Make Us Do**\n\nOrdered from most to least surprising (that
    is, there was no way we were going to do the stuff at the bottom of the list).\n\n1.
    Install endpoint software. [See above](#endpoint).\n1. Run a vulnerability scanner.
    Our attack surface is overwhelmingly comprised of software we wrote ourselves,
    and we’re not cool enough yet to have Nessus checks for our code. We [took a note
    from CloudFlare](https://blog.cloudflare.com/introducing-flan-scan/) and, for
    compliance-driven scanning, just automated nmap. \n1. Actually collect SOC2 reports
    from all of our vendors, or document why we didn’t. We’ll have to get that done
    soon-ish, but it wasn’t a blocker for the certification. \n1. Clear any of the
    random Prototype Injection vulnerability alerts Dependabot generates for any project
    we have that uses Javascript.\n1. Install any agent software on any server anywhere.
    [I’m terrified of agents](https://taosecurity.blogspot.com/2006/12/matasano-is-right-about-agents.html).
    Also: of  work. The work we allowed ourselves to do for SOC2 was, uh, carefully
    curated. Software to generate extra line items for us to remediate? Not helpful
    for our process.\n1. Run antivirus on our servers. We did have to explain why
    that didn’t make any sense, but it boiled down to documenting our deployment and
    CI/CD systems.\n1. Run any other kind of security product, be it [IDS](https://lmgtfy.app/?q=gartner+security+quadrant+1997)
    or [WAF](https://lmgtfy.app/?q=gartner+security+quadrant+2005) or [SEM](https://lmgtfy.app/?q=gartner+security+quadrant+2003)
    or [DAST](https://lmgtfy.app/?q=gartner+security+quadrant+2008) or [ASM](https://lmgtfy.app/?q=gartner+security+quadrant+2021)
    or [XDR](https://lmgtfy.app/?q=gartner+security+quadrant+2012) or [CASB](https://lmgtfy.app/?q=gartner+security+quadrant+2019)
    or anything with “[mesh](https://www.youtube.com/watch?v=6lWgXDOAJ5s)” in the
    name.\n\n**The Audit Itself**\n\nIf you talk to people who’ve done SOC2 before,
    you’ll hear a lot of joking about screenshots. They're not joking.\n\nThe whole
    SOC2 audit is essentially a series of screenshots. This is jarring to people who
    have had “security audits” done by consulting firms, in which teams of technical
    experts jump into your codebase and try to outguess your developers and uncover
    flaws in their reasoning. Nothing like that happens in a SOC2 audit, because a
    SOC2 audit is an actual audit.\n\nInstead, DRL line items are going to ask for
    evidence supporting claims you make, like “our production repositories on Github
    have protected branches enabled so that random people can’t commit directly to
    `main`” . That evidence will almost always take the form of one or more screenshots
    of some management interface with some feature enabled.\n\nAnd that’s basically
    it? We divided the evidence collection stage of the audit up into a series of
    calls over the course of a week, each of which ate maybe twenty minutes of our
    time, most of which was us sharing a screen and saying “that checkbox over there,
    you should screenshot that”. I was keyed up for this before the calls started,
    prepared to be on my A-game for navigating tricky calls, and, nope, it was about
    as chill a process as you could imagine.\n\n**So, We’re Secure Now, And You Could
    Be Too**\n\nThis was a lot of words, but SOC2 gives a lot of people a lot of feels,
    and I’d wished someone had written something like this down before I started doing
    SOC2 stuff.\n\nThe most important thing I can say about actually getting certified
    is to keep your goals modest. I’ve confirmed this over and over again with friends
    at other companies: the claims you make in your Type I will bind on your Type
    II; claims you don’t make in your Type I won’t. It stands to reason then that
    one of your Type I goals should be helping your future self clear a Type II.\n\nI
    was a little concerned going into this that the quality of our SOC2 report (and
    our claims) would be an important factor for customers. And, maybe it will be.
    We got good auditors! I like them a lot! It wasn’t a paint-by-numbers exercise!
    But in talking to a couple dozen security people at other companies, my takeaway
    is that for the most part, having the SOC2 report is what matters, not so much
    what the SOC2 report says.\n\n<div class=\"callout\">\nI can’t not mention this,
    even though our auditors might see it and preemptively refuse to do this for us.\n\nAt
    least one peer, at a highly clueful, highly security-sensitive firm, described
    to us a vendor that had given them not one, not two, but five consecutive Type
    I reports. It is possible to synthesize excited bromide in an argon matrix! You
    can skip all the real work in SOC2!\n\nI’ve spent the last several weeks trying
    to convince Kurt that we’re not going to do this. You’ll know how I fared sometime
    next year.\n</div>\n\n<aside class=\"right-sidenote\">We’ll have more to say about
    pentesting soon.</aside>\nWe do a lot of security work that SOC2 doesn’t care
    about; in fact, SOC2 misses most of our security model. We build software in memory-safe
    languages, work to minimize trust between components, try to find simple access
    control models that are easy to reason about, and then get our code [pentested
    by professionals](https://doyensec.com/).\n\nSOC2 doesn’t do a good job of communicating
    any of that stuff. And that’s fine; it doesn’t have to. We can write our own security
    report to explain what we do and how our security practice is structured, which
    is something I think more companies should do; I’d rather read one of those than
    a SOC2 report.\n\nAnd for all my cynicism, SOC2 dragged us into some process improvements
    that I’m glad we’ve got nailed down. It helped to have clueful auditors, and a
    clear eye about what we were trying to accomplish with the process, if you get
    my drift.\n\nI expected “entire new cloud provider” to be a complicated case for
    SOC2 audits. But the whole thing went pretty smoothly (and the un-smooth parts
    would have hit us no matter what we were building). If it was easy for us, it’ll
    probably be easier for you. Just don’t do it until you have to.\n"
- :id: blog-the-serverless-server
  :date: '2022-06-30'
  :category: blog
  :title: The Serverless Server
  :author: will
  :thumbnail: not-a-server-thumbnail.jpg
  :alt: An anthropomorphic bird wearing a vest, shocked to be revealed, standing alongside
    a large computer-like console, by the dramatic opening of emerald-green curtains.
    It's reminiscent of The Wizard of Oz but with slightly less-steampunk machinery.
    And, you know, a bird.
  :link: blog/the-serverless-server
  :path: blog/2022-06-30
  :body: "\n\n<p class=\"lead\">I'm Will Jordan, and I work on SRE at Fly.io. We transmogrify
    Docker containers into lightweight micro-VMs and run them on our own hardware
    in racks around the world, so your apps can run close to your users. [**Check
    it out**](https://fly.io/docs/speedrun/)&mdash;your app can be up and running
    in minutes. This is a post about how services like ours are structured, and, in
    particular, what the term \"serverless\" has come to mean to me.</p>\n\nFly.io
    isn't a \"Gartner Magic Quadrant\" kind of company. We use terms like \"FaaS\"
    and \"PaaS\" and \"serverless\", but mostly to dunk on them. It's just not how
    we think about things. But the rest of the world absolutely does think this way,
    and I want to feel at home in that world.\n\nI think I understand what \"serverless\"
    means, so much so that I'm going to stop putting quotes around the term. Serverless
    is a magic spell. Set a cauldron to boil. Throw in some servers, a bit of code,
    some [eye of newt](https://fly.io/blog/a-foolish-consistency/), and a credit card.
    Now behold, a bright line appears through your application, dividing servers from
    services&hellip;and, look again, now the servers have disappeared. Wonderful!
    Servers are annoying, and services are just code, the same kind of code that runs
    when we push a button in VS Code. Who can deny it: \"[No server is easier to manage
    than no server.](https://twitter.com/awsreinvent/status/652159288949866496)\"\n\nBut,
    see, I work on servers. I'm a fan of magic, but I always have a guess at what's
    going on behind the curtain. Skulking beneath our serverless services are servers.
    The work they're doing isn't smoke and mirrors.\n\nLet's peek behind the curtain.
    I'd like to perform the exercise of designing the most popular serverless platform
    on the Internet. We'll see how close I can get. Then I want to talk about what
    the implications of that design are.\n\nClose your eyes, tap your keyboard three
    times and think to yourself, \"There's no place like `us-east-1`\".\n\n## Let's
    Start Building\n\nThe year is 2014, and the buzzword \"elastic\" is still on-trend.
    Our goal: liberate innocent app developers from the horrors of server management,
    abstracting services written in Python or Javascript from the unruly runtimes
    they depend on. You'll give us a function, and we'll run it.\n\nOnce this is invented,
    you'll probably want to use it to optimize sandwich photos uploaded by users of
    your social sandwich side project.\n\n<aside class=\"right-sidenote\">ElasticSearch
    would [shorten its name to Elastic](https://www.elastic.co/about/press/elasticsearch-changes-name-to-elastic-to-reflect-wide-adoption-beyond-search)
    in 2015, marking the peak of this fad.</aside>\n\nThe first tool in our toolbox
    is the virtual machine. VMs were arguably \"serverless\" _avant la lettre_, and
    Lambda itself literally stood on the shoulders of  EC2, so that's where we'll
    begin.\n\nTake a big, bare-metal x86 server sitting in a datacenter with all the
    standard hookups. Like every server, it has an OS. But instead of running apps
    on that OS, install a Type 1 (bare-metal) hypervisor_,_ like the [open-source
    Xen](https://xenproject.org/developers/teams/xen-hypervisor/).\n\nThe hypervisor
    is itself like a tiny operating system, but it runs guest OSs the way a normal
    OS would run apps. Each guest runs in a facsimile of a real machine; when the
    guest OS tries to interact with hardware, the hypervisor traps the execution and
    does a close-up magic trick to maintain the illusion. It seems complicated, but
    in fact the hypervisor code can be made a good deal simpler than the OSs it runs.\n\nNow
    give that hypervisor an HTTP API. Let it start and stop guests, leasing out small
    slices of the bare metal to different customers. To the untrained eye, it looks
    a lot like EC2.\n\nEven back in 2014, EC2 was boring. What we want is Lambda:
    we want to run functions, not a guest OS. We need a few more components. Let's
    introduce some additional characters:\n\n- The `Placement` service, with an API,
    that can start and stop VMs across a pool of `Workers`. \n- The `Manager` is a
    service with an API that tracks the VMs — we'll start calling them `Workers` —
    running across that pool, and can tell us how to reach them.\n- The `Frontend`
    handles requests for things our `Workers` will actually do.\n- The `function`
    is the code the customer wants us to run. For your sandwich app, the `function`
    resizes and optimizes an image, and sends it to an S3 bucket.  \n\nThe `Frontend`
    reads an `Invoke` request for a `function` we want to run. (Someone's just uploaded
    an image to your S3 sandwich bucket through your app.)  `Frontend` asks a `Manager`
    to provide the network address of a Worker VM containing an instance of your `function`,
    where it can forward the request. The `Manager` either quickly returns an existing
    idle `function instance`, or if none are currently available, asks a `Placement`
    service to create a new one.\n\nThis is all easier said than done. For instance,
    we don't want to send multiple requests racing toward a single idle instance,
    and so we need to know when it's safe to forward the next request. At the same
    time, we need `Manager` to be highly available; our `Manager` can't just be a
    Postgres instance. Maybe we'll use [Paxos or Raft](https://fly.io/blog/a-foolish-consistency/)
    for strongly-consistent distributed consensus, or maybe [gossiping load and health
    hints](https://fly.io/blog/a-foolish-consistency/) will be more resilient.\n\nWe
    can straightforwardly run a `function instance` on a `Worker` VM. But we can't
    just use any old VM; we can't trust a shared kernel with multitenant workloads.
    So: give each customer its own collection of dedicated EC2-instance `Workers`.
    Have `Placement` bin-pack `function instances` onto them. Boot up new `Workers`
    as needed.\n\nAnother catch: it takes seconds or even minutes to boot a new `Worker`.
    This means some of our requested functions have unacceptably (and unpredictably)
    high \"cold start\" time. (Imagine, in 2022, holding on to your excitement for
    over a minute waiting for your image of the local sandwicherie's scorpion-pepper
    grilled cheese to insert itself into your chat.) Have `Placement` manage a \"warm
    pool\" of running VMs, shared across all customers. Now functions can scale up
    quickly. To scale down, `Manager`  periodically vacuums idle VMs, returning them
    to the warm pool for reuse.\n\nScale is our friend. We have lots of customers,
    so the warm pool  smooths out unpredictable workloads, reducing the total number
    of EC2 instances we need. But we're not out of the woods yet. We can get huge
    spikes of consumption: say, an accidentally-recursive function. One broken customer
    brings everyone else back to cold-start latency.  The easiest fix: soft limits
    (\"contact us if you need more than 100 concurrent executions\"). Beyond that,
    the service could adopt a token bucket rate-limiting mechanism to allow a controlled
    amount of [sustained/burst scaling](https://docs.aws.amazon.com/lambda/latest/dg/invocation-scaling.html)
    per customer or function.\n\nWe've sketched most of orchestration, but hand-waved
    the actual function invocation. It's not all that complicated, though.\n\nOnce
    `Placement` allocates enough resources on a `Worker`, it can load up the `function
    instance` there. Remember, it's still 2014, and [Docker only _just_ became production-ready](https://www.docker.com/blog/its-here-docker-1-0/),
    so we'll roll our own container the old-fashioned way.  A daemon on the `Worker`
    VM:\n\n- handles the function initialization request, \n- fetches the [application
    code .zip file](https://docs.aws.amazon.com/lambda/latest/dg/gettingstarted-package.html#gettingstarted-package-zip)
    from object storage (S3), \n- unpacks it on top of a [ready-made runtime environment](https://docs.aws.amazon.com/lambda/latest/dg/lambda-runtimes.html),\n-
    launches the function-handler process in a chroot, \n- drops privileges,\n- uses
    namespaces and [seccomp profiles](https://www.kernel.org/doc/html/v4.19/userspace-api/seccomp_filter.html)
    to run in Docker-like [incarceration](https://fly.io/blog/sandboxing-and-workload-isolation/#incarceration),\n-
    enforces configured CPU and memory resource limits with cgroups,\n- uses the [cgroup
    freezer](https://www.kernel.org/doc/Documentation/cgroup-v1/freezer-subsystem.txt)
    to ensure that idle functions consume no resources outside of active requests
    proxied to the function instance. \n\n<aside class=\"right-sidenote\">Google Cloud
    Functions [originally didn't freeze its function instances](https://www.usenix.org/system/files/conference/atc18/atc18-wang-liang.pdf#page=12)
    and only billed the function execution- so you could run a Bitcoin miner in a
    background process on an idle function without paying a dime.</aside>\n\n## Iterating
    On The Design\n\nWe've come up with a relatively naive design for Lambda. That's
    OK! We're Amazon and we can paper over the gaps with money and still have enough
    left over to make hats. More importantly, we're out in front of customers, and
    we can start learning.\n\nFast forward to 2018. We made it. \"Serverless\" is
    the new \"elastic\" and it's all the rage. Now let's make it fast.\n\nWhat's killing
    us in our naive design is Xen. Xen is a bare-metal hypervisor designed to run
    arbitrary operating systems in arbitrary hardware configurations. But our customers
    don't want that. They're perfectly happy running arbitrary Linux applications
    on a specific, simplified Linux configuration.\n\n[Enter Firecracker](https://www.usenix.org/conference/nsdi20/presentation/agache).\n\nFirecracker
    is modern hypervisor built on KVM and exploits paravirtualization: the guest and
    the hypervisor are aware of each other, and cooperate. Unlike Xen, we don't emulate
    arbitrary devices, but rather [virtio devices](https://ozlabs.org/~rusty/virtio-spec/virtio-paper.pdf)
    designed to be efficient to implement in software. With no wacky device support,
    we lose hundreds of milliseconds of boot-time probing. We can be up and running
    in under 125ms.\n\nFirecracker can fit thousands of micro-VMs on a single server,
    paying less than 5MB per instance in memory.\n\nThis has profound implications.
    Before, we were carefully stage-managing how `function instances` made their way
    onto EC2 VMs, and the lifecycle of those EC2 VMs. But now, `function instances`
    can potentially just be VMs. It's safe for us to mix up tenants on the same hardware.\n\nWe
    can oversubscribe.\n\nOversubscription is a way of selling the same hardware to
    many people at once. We just bet they won't all actually ask to use it at the
    same time. And, at scale, this works surprisingly well. The trick: get really
    good at spreading around the load across machines to keep resource usage as uncorrelated
    as possible. We want to maximize server usage, but minimize contention.\n\nFirecracker
    lets us spread load more evenly, because we can run thousands of different customers
    on the same server.\n\nOur `Workers` are now bare-metal servers, not EC2 VMs.
    We need a warm pool of them, too. It's a lot of extra micro-management. And it's
    worth it. The resource-sharing shell game is way more profitable.  Reportedly,
    Lambda runs in production with CPU and memory oversubscription ratios as high
    as 10x. Not too shabby!\n\nThere's a tradeoff to this. We've aggressively decorrelated
    our server workloads, shuffling customers onto machines like suits in a deck of
    cards. But now we can't share memory across functions, like the classic [pre-forking
    web server](https://httpd.apache.org/docs/2.4/mod/prefork.html) model.\n\nOn a
    single server, a function with `n` concurrent executions might consume only slightly
    more memory than a single function. Shuffled onto `n` machines, those executions
    cost `n` times more. Plus, on the single server, instances can fork instantly
    from a parent, effectively eliminating cold-start latency.\n\nAnd now we have
    a [network-sized hole](https://rule11.tech/the-network-sized-holes-in-serverless/)
    in performance. Functions are related; they're intrinsically correlated. Think
    about serverless databases, or map-reduce functions, or long chains of functions
    in a microservice ensemble. What we want is network locality, but we also want
    related loads spread across different hardware to minimize contention. Our goals
    are in tension.\n\nSo some functions might perform best packed tightly to optimize
    performance, while others are best spread thin for more distributed resource usage
    across servers. Some kind of hinting along the lines of [EC2 placement groups](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html)
    could help thread the needle, but it's still a hard problem.\n\nAt any rate, we
    have a design, and it works. Now let's start thinking about the ramifications
    of the decisions we've made so far, and the decisions that we have yet to make.\n\n<%=
    partial \"shared/posts/cta\", locals: {\n  title: \"Roll your own FaaS\",\n  text:
    \"Fly Machines are Firecracker VMs with a fast REST API that can boot instances
    in about 300ms, in any region supported by Fly.io. Care to craft your own twist
    on serverless?\",\n  link_url: \"https://fly.io/docs/app-guides/functions-with-machines/\",\n
    \ link_text: \"Learn more&nbsp;&nbsp;<span class='opacity-50'>&rarr;</span>\"\n}
    %>\n\n## Ramifications for Concurrency\n\nLambda's one-request-per-instance concurrency
    model is simple and flexible: each function instance can handle one single request
    at a time. More load, more instances.\n\nThis works like [Common Gateway Interface](https://datatracker.ietf.org/doc/html/rfc3875)
    (CGI) of yore, or more precisely, like implementations of its successor [FastCGI](http://www.mit.edu/~yandros/doc/specs/fcgi-spec.html)
    which reuse instances across requests.\n\nScaling is simple and straightforward.
    Each request is handled in its own instance, separate from all other concurrent
    requests. No locks, thread-safety or any other parallel programming concepts.\n\nBut
    handling concurrent requests in a single instance can be more efficient, especially
    for high-performance web application servers that can leverage asynchronous I/O
    event loops and user-space threads to minimize context-switching overheads. Google's
    Cloud Run product supports [configurable per-instance concurrency](https://cloud.google.com/run/docs/about-concurrency).
    Lambda's design makes it harder for us to pull off tricks like that.\n\n## Ramifications
    for Pricing\n\nIf we're Lambda, we bill per-second duration based on memory use,
    with a per-request surcharge; like a taxi meter, we have a base fee, and then
    the meter ticks up as long as we're working.\n\nTwo ways of looking at the request
    fee. First, it's a fudge factor representing the aggregate marginal costs of the
    various backends involved in handling the request.\n\nBut if you're an MBA, it's
    also a way to shift to \"[pay-for-value](https://twitter.com/tef_ebooks/status/1339151538917355520)\"
    or [value-based](https://en.wikipedia.org/wiki/Value-based_pricing) pricing, a [founding
    tenet](https://aws.amazon.com/serverless/faqs/) of Lambda. Value pricing says
    that you pay based on how useful the service is; if we figure out ways to deliver
    the service more cheaply, that's gravy for us. Without the surcharge, we're doing
    [cost-plus pricing](https://en.wikipedia.org/wiki/Cost-plus_pricing). You'd just
    pay for the resources we allocated to you.\n\n(Remember, we're up to 10x oversubscribed.
    Customers are, on average, utilizing only 10% of the resources they pay for.)\n\nWe
    combine CPU and memory pricing to simplify duration-based pricing. Simple is good,
    but costs our users flexibility if they have lopsided CPU or memory-heavy functions.
    For that problem, there's Fargate, Lambda's evil twin.\n\nThis pricing seems simple!
    But it's actually a little bit complicated, if you are sensitive to cost.\n\nYour
    image-cruncher function might be making good use of its resources for most of
    its running time. But what if a function process is actually really fast?  It
    might actually skew cheap in resources and expensive in requests. \n\nAnd now,
    you've added a function to periodically scrape the major socials for new pictures
    tagged with any sandwich, artisanal sandwich stockist, or vending machine known
    to your database. Or, better, say you're [Max Rozen, doing uptime checks on every
    endpoint in your database](https://onlineornot.com/on-moving-million-uptime-checks-onto-fly-io).
    Now you're paying full whack for CPU and RAM usage the whole time you wait (up
    to 10s) for a response from each one, to, you know, see if it’s online.\n\nThe
    value-based pricing here hits the sweet spot for functions that a) run long enough
    per request to amortize the request cost, and b) make enough use of the provisioned
    resources, while they run, to justify paying for them that long.\n\nPrioritizing
    nimble scaling, combined with instance-per-request and per-request billing, does
    set up a potential footgun for our customers. Don't [DDoS](https://news.ycombinator.com/item?id=31907374)
    [yourself](https://www.vividbytes.io/lambda_cost_controls/).\n\nWe're counting
    on the product as a whole to add enough value to keep less price-sensitive customers
    coming back, even far from the sweet spot.\n\n## Ramifications for APIs\n\nThe
    public runtime API to a Lambda function is the `Invoke` REST API, which accepts
    a POST method specifying the function name and request \"payload\", and requires
    a signature with appropriate AWS credentials. This conforms to Amazon's monolithic,
    [internally-mandated](https://gist.github.com/chitchcock/1281611) API structure,
    but practically unusable outside the API-wrangling comfort of the AWS SDK.\n\nA
    cottage industry has [sprung up](https://www.serverless.com/blog/releasing-serverless-framework-v1-and-fundraising/)
    around frameworks just to help you hook Lambda up to the web. Amazon built one
    of them into [CloudFormation](https://aws.amazon.com/blogs/compute/introducing-simplified-serverless-application-deplyoment-and-management/).
    Problem: too much YAML. Solution: more YAML.\n\nThe way out is [embarrassingly
    simple](https://twitter.com/tef_ebooks/status/1339151538917355520): the runtime
    API can just pass HTTP requests directly to the function instance. Most of what
    \"API gateways\" do can be built into HTTP proxy layers. For the common case of
    web applications, an HTTP-based API eliminates a layer of indirection and plugs
    in nicely with the mature ecosystem of web utilities.\n\n## Ramifications for
    Resilience\n\nLambda's [execution environment](https://docs.aws.amazon.com/lambda/latest/dg/runtimes-context.html)
    sets strict limits:\n\n-  on function initialization (10 seconds) \n- on `Invoke`
    duration (default 3 seconds; limit originally 60 seconds, later increased to 5
    and then 15 minutes), _and_ \n- zero guarantees around idle-function lifecycle
    (a function instance could get shut down any time it's not handling a request,
    and will shut down once [every 14 hours](https://docs.aws.amazon.com/whitepapers/latest/security-overview-aws-lambda/lambda-executions.html#microvms).)\n\nThis
    tightly-scoped lifecycle is great for the platform provider. It helps workloads
    quickly migrate away from overloaded or unhealthy instances, and makes it easy
    to shuffle functions around during server maintenance and upgrades without impacting
    services. And what's good for the platform is probably good for most customers,
    too!\n\nBut it's not ideal for apps\n\n- with expensive or time-consuming initialization
    steps \n- or that depend heavily on dynamic local caches for performance \n- or
    when you're just not sure how long a response might take. \n\nOne alternative
    is for the platform to try to keep servers up and running forever, but sometimes
    you just [have to reboot servers](https://aws.amazon.com/blogs/aws/ec2-maintenance-update/)
    to patch stuff. Another option to recycling VMs is live migration, sending a snapshot
    of the running VM over the network to the new server with as little downtime as
    possible. Google Compute Engine [supports live migration](https://cloud.google.com/compute/docs/instances/live-migration)
    for its instances and uses the feature to seamlessly conduct maintenance on its
    servers [every few weeks](https://cloud.google.com/compute/docs/instances/host-maintenance-overview#maintenanceevents).\n\n<div
    class=\"callout\">\n\nDespite the simple runtime interface, Lambda functions run
    in a full Linux runtime environment that let you run your own x86 executables
    on the platform, which gives you all of POSIX for your application to play with.\n\nIf
    your apps can do with less, a \"language sandbox\" can offer some isolation without
    the overhead of virtualization. Google App Engine adopted this with tuned language
    runtimes that disabled networking and writing to the filesystem by [disabling/customizing
    Python modules](http://web.archive.org/web/20090410070937/http://code.google.com/appengine/docs/python/runtime.html#Pure_Python)
    and [restricting Java class usage](http://web.archive.org/web/20090411110624/http://code.google.com:80/appengine/docs/java/jrewhitelist.html).
    [CloudFlare Workers](https://developers.cloudflare.com/workers/learning/how-workers-works/)
    adopt a similar approach with the v8 runtime wrapping JavaScript code in 'isolates',
    offering a restricted set of [runtime APIs](https://developers.cloudflare.com/workers/runtime-apis/)
    modeled loosely after JavaScript browser APIs.\n\nWebAssembly extends the language-sandbox
    approach with a virtual instruction set architecture, either embedded within v8
    isolates or run by a dedicated server-side runtime like [wasmtime](https://github.com/bytecodealliance/wasmtime).\n\nFastly
    built its [Compute@Edge](https://docs.fastly.com/products/compute-at-edge) product
    around WebAssembly/WASI. However, WASI is still young and evolving quickly. On
    the serverside, WASM's overhead doesn't pay its freight: there's as much as a
    [50% performance gap](https://www.usenix.org/conference/atc19/presentation/jangda)
    between WASM and native code, which makes virtualization look cheap by comparison.\n\n</div>\n\n##
    How did I do?\n\nI just designed a shameless knockoff of Lambda, the most popular
    specimen of the most serverless of serverless services: a fleeting scrap of compute
    you can will into being, that scales freely (not in the monetary sense) and fades
    into oblivion when it’s no longer needed.\n\nThis article contains no small degree
    of bias! There’s also no small degree of appreciation for the craft that goes
    on behind the curtain at AWS and other purveyors of \"serverless\" services."
- :id: blog-logbook-2022-06-23
  :date: '2022-06-23'
  :category: blog
  :title: Logbook - 2022-06-23
  :author: fly
  :thumbnail: logbook-default-thumbnail.jpg
  :alt: A 1950s style cartoon lady pointing at a vintage flight logbook
  :link: blog/logbook-2022-06-23
  :path: blog/2022-06-23
  :body: "\n\n<div class=\"lead\">\nThis is a Logbook post. It tells you what we’ve
    been up to here at Fly.io, to make running your code close to users better in
    all the ways. The proof of the pudding is in the eating, though, so [you should
    dig in now](https://fly.io/docs/speedrun/); it’ll take you just a minute or two
    to get up and running.\n</div>\n\nWe've had a lot of changelogs about our Phoenix/LiveView-based
    [web UI](https://fly.io/dashboard) in recent weeks. It's pretty rad; we've been
    vocal about being CLI-first, but we love a first-class dashboard. Our dashboard
    has sprouted a lot of new capabilities, and at this point you're missing out if
    you never use it!\n\nThat doesn't change the power and adaptability of the CLI&mdash;we'll
    never not love [flyctl](/docs/flyctl/) (alias `fly`).  Speaking of which: we have
    a few flyctl✨ entries in this here collection. Because flyctl is open-source,
    you can [peek in](https://github.com/superfly/flyctl/releases) and see what that
    means in terms of commits, any updates we may have missed, and, notably, lots
    of fun activity around [Fly Machines](/blog/fly-machines/).\n\nAnd now the changelog:\n\n-
    **[Feature]** We now have a YUL region (Montreal, QC, Canada) \U0001F680\n- **[Feature]**
    Optimized API queries in flyctl to make it a lot faster.\n- **[Feature]**  `fly
    deploy` now displays plain Docker build output if you set `NO_COLOR=1` first.
    `NO_COLOR` now properly disables all colorized output for deployments. The plain
    Docker output can be useful for debugging Docker builds while running in interactive
    terminals.\n- **[Feature/Docs]** We now have a Laravel launcher: `fly launch`
    will now detect and configure Laravel apps straight from source code. Bonus: Also
    handles Laravel Octane projects using Swoole or RoadRunner. ([Updated docs](/docs/getting-started/laravel/))\n-
    **[Feature]** Changed the way we pre-verify DNS propagation to issue certificates
    with Let's Encrypt. We're now using a non-caching unbound alongside our app responsible
    for the pre-verification, and that's what we query for everything related to certificates
    and hostname checks.\n- **[Feature]** Updated some logic to prevent multiple reloads
    of the same data about an app’s current image (used for several GraphQL attributes).
    This should improve API performance on things like getting app status, and parts
    of the deployment workflow.\n- **[Feature]** Updated our app-discarding logic
    to put resource cleanup into a job, which should reduce user-facing issues with
    deleting apps, reduce database locking issues, and make the cleanup more retryable.\n-
    **[Feature]** We periodically come up against data-lock issues with our API’s
    database. We’ve added more context to our SQL calls, in the form of SQL comments,
    to provide information about where the queries are coming from. This will help
    target and debug issues when they come up.\n- **[Feature]** Optimized API calls
    used for first deploys, shaving about 6s off the process. People should see fewer
    weird CLI timeouts when they run `fly launch` or `fly deploy` for the first time
    on a new app.)\n- **[Feature]** Set up source coverage and test analytics for
    fly-proxy's all-Rust codebase before merging 22 new commits. This has already
    caught a handful of bad bugs before they could make it to production.\n- **[Feature]**
    Added additional TLVs to our Rust proxy's PROXYPROTO v2 support (tracking TLS
    ciphers, signature algorithms, hash algorithms, and authority/servername).\n-
    **[Feature]** Broke out a dedicated internal reference doc for diagnostics and
    troubleshooting from our ballooning Support knowledge base. This will help present
    and future Support team members 1) walk customers through solving common issues
    and 2) identify when a support request indicates a bug or issue that needs attention
    on the engineering side.\n- **[Feature]** Harmonized the styling in the Account
    Settings dashboard components with our old UI. Removed still more leftover Tailwind
    syntax.\n- **[Feature]** Shipped a [page](/documents/) to showcase our selection
    of serious documents that go by acronymic names like DPA and HIPAA BAA. Previously,
    only logged-in customers could [see these](/dashboard/personal/documents). Now
    prospective customers can, too!\n\n![Documents page showing Data Processing Agreement
    and HIPAA Business Associate Agreement options.](documents-page.png?3/4&center&card)\n<ul>\n-
    **[Feature]** Made the [Plans](/plans) UI clearer and simpler to use. Quickly
    switch between orgs with a dropdown selector.\n</ul>\n\n![Plans page with a dropdown
    to choose the organization](plans-page.png?3/4&center&card)\n\n\n- **[Fix]** [Fixed](https://github.com/superfly/flyctl/pull/1011)
    a flyctl [bug](https://github.com/superfly/flyctl/issues/560) that wiped fly.toml
    env variables when `--env` (or `-e`) flags were used. Wrong type checks were causing
    existing ones to be overwritten. This was a neglected, annoying bug that Michael
    fixed (probably with glee) with his feet still moving on the hiring treadmill.\n-
    **[Fix]** You can now delete an organization if you’ve deleted your custom domains&mdash;previously
    having had a custom, Fly-managed domain _at any point_ could have prevented you
    from doing this.\n- **[Fix]** Password-reset links from welcome emails were redirecting
    logged-in Oauth users to their dashboard instead. You might have encountered this
    if you were logged in via GitHub. This is fixed.\n- **[Fix]** Changed our GraphQL
    API query for listing app secrets. This should eliminate very rare 404s seen on
    both `fly secrets list` and the dashboard App Secrets tab. (Thanks macwilko [in
    the forum](https://community.fly.io/t/500-error-when-accessing-app-from-dashboard/5624)
    for flagging up the error!)\n- **[Fix]**  `fly apps move` was breaking metrics.
    It will now start sending metrics properly to the new organization.\n- **[Fix]**
    Fixed a bad `if` that was causing a 500 error on the app activity dashboard. (Looks
    like macwilko [got hit](https://community.fly.io/t/404-error-when-accessing-secrets-from-the-app-page/5626)
    with this one, too!)\n- **[Fix]** Fixed TLV values we'd just added to our proxy
    for PROXYPROTO V2 SSL. They were using 20 (decimal) instead of 0x20 (hexadecimal),
    in essence passing on an unknown 0x14 TLV type.\n- **[Fix]** Relaxed some email
    risk checks that were blocking signups from actual humans. For some reason, the
    email check service we use started detecting more emails as “mailbox does not
    exist”, even when we know they do. We don’t listen to that anymore; we just require
    an email confirmation instead.\n- **[Docs]** Updated [buildpacks](https://fly.io/docs/reference/builders/#buildpacks)
    and [NodeJS](/docs/reference/postgres/#connecting-with-node-js-docs) documentation
    to explain that [build arguments](/docs/reference/configuration/#specify-docker-build-arguments)
    are exposed to buildpacks as environment variables. Some buildpacks&mdash;like
    the [Heroku NodeJS](https://devcenter.heroku.com/articles/nodejs-support#only-installing-dependencies)
    one&mdash;use env vars for configuration.\n- **[Blog]** Published [How to Make
    Rust Leak Memory (Also: How to Make It Stop)](https://fly.io/blog/rust-memory-leak/)
    on Rust and tracking down memory leaks.\n\n\n\n"
- :id: blog-rust-memory-leak
  :date: '2022-06-15'
  :category: blog
  :title: 'How to make Rust leak memory (also: how to make it stop)'
  :author: amos
  :thumbnail: fixing-leaks-thumbnail.jpg
  :alt: A network of dripping pipes.
  :link: blog/rust-memory-leak
  :path: blog/2022-06-15
  :body: "\n\n<p class=\"lead\">This is a post about fixing a memory leak in our Rust-based
    proxy, `fly-proxy`. That's the code that gets your users' requests to the nearest
    VM that can fulfill them, on one of our servers in one of [21 regions](/docs/reference/regions/)
    worldwide. Take it for a spin by [deploying an app](/docs/speedrun) in mere minutes.</p>\n\nWe
    have a [Rust-based proxy](/blog/the-tokio-1-x-upgrade/). It was leaking memory.
    We fixed it, and we'll talk about that, but to be really thorough, we'll look
    at how loading a web page works. Starting with hardware interrupts.\n\n<aside
    class=\"right-sidenote\">The downside of Thomas writing job postings is that they
    can turn out to have a [great little primer on `fly-proxy`](/blog/fly-io-is-hiring-rust-developers/)
    that we just have to link to, even though we are NOT hiring Rust devs at the moment.</aside>\n\n##
    Loading a web page — a journey\n\nYou type `https://fly.io` in your browser address
    bar and hit enter. What happens next?\n\nFirst off, are you even using a keyboard?
    Not everyone can use a keyboard: voice input may be a suitable alternative there.
    Soft keyboards like the ones that pop up on mobile devices when you focus an input
    also don't count — they're software, like the name implies.\n\nAs keys get pressed,
    electrical contact is made between two conductive bits, which closes a circuit,
    and the microcontroller inside the keyboard (it's computers all the way down)
    takes note and stuffs it in a buffer.\n\nBack when you plugged in said keyboard,
    or, more likely, when your computer woke up and started enumerating USB devices,
    they entered a negotiation: the keyboard announced that it was HID class (for
    human interface device), and needed an \"interrupt transfer\" at a certain rate,
    and that's the rate at which the computer will poll that device for&hellip;data.\n\n(What
    about Bluetooth, proprietary wireless dongles, or even laptop keyboards? Chances
    are, these all end up being USB anyway, as far as your computer is concerned.
    Yes, even for internal devices. It's just easier that way)\n\nAnd then well, your
    computer does poll the device at the negotiated rate, and processes events in-order.
    So really, there's no hardware interrupts involved.\n\n(To be pedantic, because
    it's that kind of article, your USB keyboard can actually interrupt the CPU, but
    that's only so the BIOS can be driven by your keyboard. By the time a real operating
    system has been started up, that behavior has been overriden.)\n\nYour operating
    system then does translation between scan codes (that depend on where the keys
    are located on the keyboard) and key codes, like \"the letter A\". Then that goes
    through a bunch of event loops in the OS and your browser, and finally, we have
    `https://fly.io` somewhere in RAM.\n\nSo far so good.\n\nAnd then, well, browsers
    are complicated beasts. If your browser is Chrome, then it checks some probabilistic
    data structure for [safe browsing](https://safebrowsing.google.com/)&mdash;if
    the domain is on the Bad List, then you get a scary page that tells you to click
    away! Now!\n\nAfter that, or maybe in parallel, a DNS lookup happens, which translates
    the domain `fly.io` into an IPv4 and/or IPv6 address. This may happen over UDP,
    or it may happen [over HTTPS](https://support.mozilla.org/en-US/kb/firefox-dns-over-https)!
    Or maybe it doesn't happen, because it's in the browser's DNS cache, or the OS's
    DNS cache, or your local router's DNS cache. It's caches all the way down, really.\n\nIf
    that succeeds, your browser tries to establish a TCP connection to that IP address.
    Because it's an [anycast](https://en.wikipedia.org/wiki/Anycast) IP address, packets
    get routed to an edge node nearby. For me that's Paris, France. For my colleagues,
    it's Toronto, Canada. Or South Africa, Brazil, the UK etc. It's really [quite
    the list](/docs/reference/regions).\n\n(That's assuming BGP routes are behaving
    that day. BGP is like DNS in that it's always to blame somehow. It's how different
    AS (autonomous systems), or peers inside the same AS, know where to send a packet
    next, so that it eventually reaches its destination.\n\nWhen it works, it sends
    packets on&hellip;maybe not the best path, but a decent path. When the routes
    are wrong, it can send packets halfway around the world. And when it gets hijacked,
    well, [it makes the headlines](https://en.wikipedia.org/wiki/BGP_hijacking#Public_incidents).
    Take notes, TV writers!)\n\nIt's not like your browser crafts packets itself&mdash;that
    would let it spoof IP addresses, which is generally frowned upon. No, it asks
    the underlying operating system to please establish a TCP connection, and that
    sends a SYN packet.\n\n(Technically it's \"a TCP segment with the SYN flag set,
    wrapped in an IP datagram, wrapped in an Ethernet frame\", but that doesn't exactly
    roll off the tongue).\n\nThat packet goes on quite a journey.\n\nIt goes from
    userland to kernel space_,_ out a NIC, shooting through the air, or through copper,
    then almost definitely fiber, crossing the land, maybe speeding through the ocean
    deep if you live too far from us&mdash;finally it enters a datacenter, a NIC,
    and the Linux kernel networking stack.\n\nTherein lies our first bit of Rust:
    an eBPF program, built with [aya](https://lib.rs/crates/aya).\n\nBecause our edge
    nodes have to listen on entire ranges of IPv4 and IPv6 addresses, and also all
    ports at the same time, we have a small program, loaded in the kernel, that decides
    whether the connection you're trying to establish is allowed or not.\n\nThat's
    all determined by the [app configuration](/docs/reference/configuration/). In
    our fictional scenario, your browser is connecting to `fly.io` on port 443, and
    that's a-ok. The TCP handshake completes, our second bit of Rust is ready to take
    over.\n\nAt first your connection sits in an accept queue, unless someone is flooding
    us with SYN packets, in which case there's [cookies](https://en.wikipedia.org/wiki/SYN_cookies)
    involved, no, not that kind, and unfortunately not the tasty kind either.\n\nWe
    try to process that accept queue as fast as the CPU will let us, asynchronously
    with [tokio](https://lib.rs/crates/tokio), which really actually uses [mio](https://lib.rs/crates/mio),
    which really \"just\" uses a kernel interface, in this case, [epoll](https://docs.rs/mio/latest/mio/struct.Poll.html#implementation-notes).\n\n(In
    practice, this just means we can handle a lot of connections concurrently, with
    only a spoonful of OS-level threads. It's all event-based and all the functions
    are state machines in a trenchcoat. It's a whole thing.)\n\nBecause the Fly.io
    app has a TLS handler, your browser and `fly-proxy` engage in a multi-stage dance
    to establish a secure tunnel. Through that dance, we are able to prove that you
    are visiting the _real_ Fly.io by presenting a valid certificate for it, signed
    by third parties that your OS trusts, and your browser does too.\n\nNow that we've
    negotiated a secure channel, we're ready to move on to more substantial exchanges.\n\nBecause
    you're using a modern browser, it supports HTTP/2, and so do we, thanks to [hyper](https://lib.rs/crates/hyper).
    That means the exchange is all binary, and I can't show what it would look like
    without whipping out some sort of protocol analyzer like [Wireshark](https://www.wireshark.org/).\n\nAny
    HTTP requests you send over that connection are handled by a [tower](https://lib.rs/crates/tower)
    service created especially for you, and of course there's a bunch of concurrency
    limits involved: some set in the app configuration, and some global, just so everything
    stays nice and fast for everyone.\n\nBecause we occasionally need to look into
    how `fly-proxy` operates, or how a certain request was handled, a lot of internals
    are instrumented with [tracing](https://lib.rs/crates/tracing), which generates
    spans and log events that we funnel to a large store we can later query.\n\n(We're
    able to turn up the verbosity for single requests, which is what we do when you
    report a problem and we're trying to reproduce it! There's no special \"debug\"
    build of `fly-proxy`, it's all dynamically configured.)\n\nSo we get your request,
    and if there's an instance of the app running on the same node, we're able to
    serve it directly. And if not, we proxy it to a nearby node that _does_ have a
    running instance: taking into account things like the round-trip time to that
    node, and how busy it is.\n\nEventually, just as the request was, the response
    is proxied all the way back to your computer, your browser gets a bunch of HTML
    (maybe compressed, maybe not), and before it's even done parsing it, it immediately
    fires up a half-dozen new requests, re-using the same connection.\n\nAnd then
    we leak a bunch of memory.\n\n## What do you mean you leak a bunch of memory\n\nWell,
    we used to!\n\nIt wasn't, like, a lot. But it added up. That's the thing with
    leaks: when request volume goes up, you expect resource utilization to go up,
    too. But when request volume goes down, and utilization doesn't go down too, well&hellip;I
    mean that _is_ an incentive to deploy often.\n\nAnd it's surprising! Because in
    languages with manual memory management, you can `malloc` and forget to `free`.
    You can `new`, and forget to `delete`.\n\nBut in Go for example, you can't! Because
    there's a garbage collector, which periodically runs, looks at all the stuff,
    and frees the stuff no one remembers. It's just like in Coco (2017), except less
    sad.\n\nOf course that's a lie. Because it doesn't look at all the things, it's
    [generational](https://go.dev/blog/ismmkeynote). And you can totally leak memory
    with a GC. All you need to do is stuff references to a bunch of big structs in
    a big map, and never take them out. That way they always remain reachable, are
    never collected, and memory usage goes weeeeeeeee.\n\nAs for Rust, it's RAII-ish,
    so when you hold a value, it's fully initialized, and when it falls out of scope,
    the associated memory (and any other resources) gets freed. And if that's not
    enough, you can do reference-counting via `Rc`, or `Arc` if you need to share
    stuff across threads, and then you can have multiple things pointing to a single
    thing, which only gets freed when no things point to it any longer.\n\nThat scheme
    has different performance characteristics than a GC: the cost is more \"spread
    out\" (since stuff gets freed as soon as it's no longer needed), there's no pauses
    to worry about (even micro ones), people like it for real-time processing, high-performance
    network applications, and all sorts of other stuff.\n\nExcept there too, same
    deal: if you _really_ try, you can leak memory in Rust. Don’t believe us? [Lily
    says so too.](https://onesignal.com/blog/solving-memory-leaks-in-rust/)\n\nBut
    we weren't trying. And we didn't really have an explanation we liked.\n\nAnd historically,
    things haven't been so good on that front: there's two problems you _really_ didn't
    want to have when you had a big Rust async application in production:\n\n1. Something
    asynchronous is stuck somewhere\n1. You're leaking some resources (probably memory)\n\nThe
    first got a lot better with the advent of [tracing](https://lib.rs/crates/tracing).
    You're still not able to dump \"all async stack traces\" the way Go lets you,
    for example. Instead you instrument some code: here we're establishing a connection,
    here we're sending HTTP headers, and your downstream crates do too (like hyper),
    and then you're looking at something a litlte more semantic than a stack trace.\n\nBut
    you also, still, can't really dump it all. Something like [tokio-console](https://docs.rs/tokio-console/latest/tokio_console/)
    solves this and more, BUT it's still early days, and not really something you
    can run in production today.\n\nAs for the second (leaking resources), I remember
    struggling with it just a year ago. Because allocations and deallocations are
    very fast and it's all very nice, but there's absolutely no one keeping track
    of _where_ or _why_ anything was allocated.\n\nThat wouldn't be very zero-cost.\n\nAnd
    yet, sometimes RAII fails you. Sometimes you just keep stuffing items into a `Vec`
    or a `HashMap` and never ever clean it up. The question is: where? And how are
    you going to find that out without exporting metrics for _every last container
    type in your codebase_?\n\nLeak detectors have existed as long as we've had leaks.
    The remarkable [Valgrind](https://valgrind.org/docs/manual/quick-start.html) tool
    suite comes with MemCheck, which has a `--leak-check` option. But it checks for
    a _lot_ of errors that simply can't happen in safe Rust code, and makes your program
    run 20-30x slower, also using \"a lot more memory\".\n\nSo, not an option for
    production web services.\n\nIn fact, if you're writing a production network service,
    chances are you've switched away from the system memory allocator (glibc) to something
    like [jemallocator](https://lib.rs/crates/jemallocator), which, on top of being
    (sometimes) faster, is also less prone to fragmentation, and comes with a wealth
    of tools to monitor memory usage.\n\nIncluding a heap profiler, which you can
    use to [check for leaks](https://github.com/jemalloc/jemalloc/wiki/Use-Case:-Leak-Checking).
    It feels like Go's [pprof](https://github.com/google/pprof), not a surprise since
    they're both based on [gperftools](https://github.com/gperftools/gperftools),
    which builds on the ideas in [GNU gprof](https://en.wikipedia.org/wiki/Gprof),
    in turn an extended version of the standard Unix prof tool.\n\nBut at the end
    of the day, you're either looking at extremely long text output that doesn't really
    have any temporal information, or a gigantic file PDF that makes you wish you
    had a trackball mouse handy.\n\n<%= partial \"shared/posts/cta\", locals: {\n
    \ title: \"Come for the Rust esoterica, stay for the hosting\",\n  text: \"It'll
    take less than 10 minutes to get almost any container you've got running globally
    on our Rust-powered anycast proxy network.\",\n  link_url: \"/docs/speedrun/\",\n
    \ link_text: \"Try Fly for free&nbsp;&nbsp;<span class='opacity-50'>&rarr;</span>\"\n}
    %>\n\n## Enter bytehound\n\n[Bytehound](https://github.com/koute/bytehound) is
    a memory profiler written in Rust, which works extremely well for Rust codebases
    (but, I'm assuming, C &amp; C++ codebases too!). I had no hand in it — I just
    think it's extremely cool.\n\nFor me, bytehound is the poster child for \"NIH
    good? sometimes?\". Its custom stack unwinding implementation makes it orders
    of magnitude faster than competing tools. It's designed to stream data to disk
    or over the network to some \"gatherer\" process, which means its impact in production
    is minimal.\n\n(The non-NIH alternatives would be&hellip;something like [heaptrack](https://github.com/KDE/heaptrack),
    [Valgrind](https://valgrind.org/)'s memcheck tool, one of the \"checking\" malloc
    implementations like Dmalloc, or even swapping malloc with an actual GC like [Boehm](https://www.hboehm.info/gc/leak.html).)\n\nIt's
    all runtime instrumentation, too — I was able to inject it straight into ~~my
    veins~~ our production binary with `LD_PRELOAD`: no recompilation needed here.
    And it supports [jemallocator](https://lib.rs/crates/jemallocator)!\n\nIn fact,
    of the whole hour I spent on this, 80% were spent fighting with [the dynamic linker/loader](https://man7.org/linux/man-pages/man8/ld.so.8.html)
    itself. Normally, it lets you inject just about any library into any executable,
    as long as the bitness matches up and all the dependencies are satisfied.\n\nBut,
    in secure-execution mode:\n\n- `LD_DEBUG` is ignored (and so is `LD_SHOW_AUXV`)\n
    \   - Unless the file `/etc/suid-debug` exists! `touch` does the trick.\n- `LD_PRELOAD`
    has several limitations:\n    - arguments with slashes are ignored (can't preload
    `/tmp/libaww.so`)\n    - injected libraries _must be SUID root_\n\nSo, long story
    short, moving `libbytehound.so` to a standard search path like `/lib/x86_64-linux-gnu`
    and invoking `chmod u+s` on it did the trick.\n\nThe last 20% was a breeze: by
    default, bytehound starts profiling immediately, writing to a `.dat` file on disk
    as it goes. I applied a touch of load with [oha](https://lib.rs/crates/oha), another
    of my Rust favs, monitored memory usage in htop, it goes up, it goes up, it don't
    go down, at least it's easy to reproduce.\n\nI then restarted `fly-proxy` (this
    is [zero-downtime](https://martinfowler.com/bliki/BlueGreenDeployment.html), never
    fear) opened the profile, and&hellip;no leaks.\n\nWell, none that matter:\n\n![The
    bytehound UI, with the \"Memory usage\" tab selected, showing memory going up
    up up during the load test, and back down. It shows we're profiling a x86_64 binary
    named fly-proxy, that ran for 1m34s. It captured just below eight thousand backtraces,
    as deep as 53 frames.](rust-memory-01.png)\n\nRestarting `fly-proxy` involves
    spinning up another instance, waiting until it's ready to handle connections,
    then asking the previous instance to stop listening (then leaving it some time
    to handle established connections).\n\nAnd the drop you can see right before 10:33:00
    is when we stop listening: memory usage drops back to the ~40MB we use for internal
    state. According to bytehound, our inventory of nodes and services (and SQLite's
    cache) are the only things we leak:\n\n![A flamegraph generated by bytehound,
    showing where allocations come from.The big chunk (left) comes from AttacheProvider,
    which is responsible for fetching the inventory. The smaller chunk (right) is
    too small to see, but it's SQLite's own memory cache.](rust-memory-02.png)\n\n(Internal
    state on the left, SQLite mem cache on the right)\n\nSo RAII _is_ working. And
    to be clear, I'm not really talking about the \"by the time you hold a value of
    a given type, the associated resource is fully initialized\" part, I'm talking
    about the \"as soon as you let go of that value, associated resources get freed,
    too\" part.\n\nFor my second try, I did&hellip;exactly the same thing, except
    I stopped profiling by sending `SIGUSR1`  _before_ restarting fly-proxy, so that
    bytehound would consider anything that hadn't been freed _at that point_ leaked.\n\n![bytehound
    UI showing memory going up and up and up and which doesn't go back down.](rust-memory-03.png)\n\nThe
    \"only leaked\" flamegraph looks very different this time around:\n\n![A flamegraph,
    that looks very different from the other one. The largest block goes through the
    regular tokio machinery, a tower Service::call, some tracing-subscriber stuff,
    and the very top (actually the leafs) are functions like \"alloc::raw_vec::finish_grow\"
    or \"reserve_rehash\"](rust-memory-04.png)\n\nZooming in a little, we can see
    that almost all of it is allocated in `fly_proxy::trace::honeycomb::HoneycombLayer`:\n\n![The
    same flamegraph after clicking on \"LocalKey::with\", which made it and all its
    children (above it) full-width. We now see more clearly that all the allocations
    go through HoneycombLayer.](rust-memory-05.png)\n\nBut that's not even the best
    view of it: the bytehound UI lets you filter/sort allocations any which way you
    like. For example, we can ask for \"only leaked\", grouped by backtraces, sorted
    by largest leak first, with graphs:\n\n![bytehound's UI, showing a backtrace that
    goes from \"start_thread\" (in libc) all the way to \"HoneycombVisitor::record_str\".
    There's a neat graph to the right, that's a perfect red triangle: the line is
    a diagonal, that goes up linearly with time.](rust-memory-06.png)\n\nAnd seeing
    this, there's no question. And there's no distractions like in the flamegraph:
    the internal state we leak (because of background tasks we don't cancel properly
    before shutting down — that one's on me) looks very different:\n\n![bytehound's
    UI, in the same view, but this time the backtrace goes through \"AttacheProvider::sync_instances\",
    which does a /bunch/ of allocations at startup, and then keeps them around. The
    graph goes up sharply, then remains stable.](rust-memory-07.png)\n\nThat's not
    the last of bytehound's goodies: using just a little bit of [rhai scripting](https://lib.rs/crates/rhai):\n\n```bash\nlet
    leaked = allocations().only_leaked();\n\ngraph()\n    .add(\"HoneycombLayer\",
    leaked.only_passing_through_function(\"honeycomb\"))\n    .add(\"all leaked\",
    leaked)\n    .save();\n```\n\nWe can plot exactly the graph we want to see&mdash;here:
    does our `HoneycombLayer` account for most of the leak?\n\n![A custom graph with
    two series: In green, \"all memory leaked\", and in red, \"HoneycombLayer\". At
    first, \"all leaked\" climbs to 40MB, then stays stable for a bit. Then the load
    test commences, and both \"HoneycombLayer\" and \"all leaked\" climb together,
    showing that the problem comes, in fact, from HoneycombLayer.](rust-memory-08.png)\n\nYes.
    Yes it does.\n\n## Why though? A tracing primer\n\nLet me first say: the leak
    wasn't in any code provided by [Honeycomb](https://www.honeycomb.io/) itself,
    or even any [publicly available crate](https://crates.io/).\n\nA long time ago,
    Fly.io's traffic was low enough that we could afford to send _some_ traces to
    Honeycomb.\n\n(Or at least we thought we could, until Honeycomb emailed saying
    \"hey, wtf!\". They have burst protection now; it was a different time.)\n\nThe
    tracing / OpenTelemetry ecosystem wasn't as fleshed-out as [it is now](https://lib.rs/crates/tracing-opentelemetry).
    So we had to write custom code. And I say we, but I wasn't there.\n\nWhen I started
    getting comfortable with `fly-proxy`'s codebase, and after I was done making CI
    builds faster (switched linkers, split crates, adopted [cargo nextest](https://nexte.st/),
    set up incremental compilation caching, etc.), I noticed that custom `HoneycombLayer`
    code, and wanted to remove it, but after all, it wasn't even exporting traces
    any more, so what harm could it do?\n\nThe way [tracing](https://lib.rs/crates/tracing)
    works is that you can instrument functions: this opens and enters a span when
    the function is called, exits that span when the function returns, and, if nothing
    else refers to that span, the span is closed, and eventually exported somewhere
    nice.\n\nIt works almost the same way for `async` functions in Rust, except async
    functions are state machines that can be polled from any thread. So, the span
    is opened, and it's entered+exited every time the async function is polled.\n\n(Wait,
    polling? Yes, but only when there's work to be done: some socket is ready to be
    read from / written to, some timer has elapsed, some lock has been acquired, etc.)\n\nEach
    span has a name, a target, and a bunch of fields, which a good tracing platform
    like [Honeycomb](https://www.honeycomb.io/) lets you use for filtering, sorting,
    heck, even monitoring. The duration of the span is tracked (from the time it's
    opened to the time it's closed), and when async functions are instrumented, we
    keep track of `busy_ns` too&mdash;how much CPU time we spent on it, and `idle_ns`&mdash;how
    long the task waited for something else to happen.\n\n<aside class=\"right-sidenote\">If
    you want to learn more about what Honeycomb feels like in conjuction with Rust
    tracing, I've written [a whole thing](https://fasterthanli.me/articles/i-won-free-load-testing)
    about it recently.</aside>\n\nThing is, if you instrument your `main` function&hellip;well
    not that, but a similarly high-level function, like `handle_incoming_connections`,
    that span opens, is entered and exited a bunch of times, but never closes until
    you stop listening.\n\n&hellip;sounds familiar?\n\nAnd our custom, should-have-thrown-it-out-along-time-ago
    `HoneycombLayer` had something along the lines of: `parent_span.children.add(span)`
    whenever a span was closed. Which means _every request_ was leaking a tiny bit
    of memory. \n\n<div class=\"callout\">To reiterate: the issue was our own prehistoric
    Honeycomb tracer implementation, not Honeycomb. We love Honeycomb and OpenTelemetry
    at Fly.io.</div>\n\nWhen the proxy was restarted, that top-level task was \"tripped\",
    exited, the associated span closed and freed, on some nodes, tens of gigabytes
    of memory — letting most heap profilers think everything was just peachy.\n\nThe
    PR to fix it was the most satisfying kind: sending old code into retirement.\n\n![GitHub
    PR screenshot: title Remove custom HoneycombLayer. +22, -476.](rust-memory-09.png)\n\nBut
    did it fix the issue?\n\n![bytehound UI, showing two graphs: Memory usage (stable
    at 50MB), and Leaked memory usage (relatively stable at 3.5MB)](rust-memory-10.png)\n\nAbsolutely.
    Memory usage barely went over 50MB during the load test.\n\nWhat's that? \"Leaked
    memory usage\" keeps growing ever so slightly? Let's focus on the part where it
    _seems_ stable:\n\n![Same bytehound UI, but starting slightly later so the initial
    allocations don't get in the way and we can see what \"stable\" operation looks
    like. Leaked memory does go up ever so slightly.](rust-memory-11.png)\n\nWhy yes,
    that line does go up, still.\n\nShowing leaked allocations, grouped by backtraces,
    with graphs, but this time only between two precise timestamps, we can see what's
    happening. And it's not hard to explain!\n\nTake that one:\n\n![bytehound's backtrace
    + graph view, going through the metrics_exporter_prometheus crate. The \"render\"
    method is called, and it ends up pushing into a Queue from the crossbeam-epoch
    crate.](rust-memory-12.png)\n\nThat's a queue. Under load, backing storage is
    grown to accomodate more items being, well, queued. It's never shrunk again, that
    would just be extra work. This is fine as long as it's bounded in some way.\n\nSame
    deal here:\n\n![bytehound's backtrace + graph view again, this time going through
    \"StateReadLock::set_remote_load\", which apparently creates a histogram. That
    involves a bunch of allocations.](rust-memory-13.png)\n\nBecause we keep track
    of latency for all requests, load generates a bunch of data points there — we
    need to keep some data around to compute various percentiles, until an interval
    was elapsed and we send them out to Prometheus.\n\n## That's it\n\n_Of course_
    you can leak memory, even in Rust. For even medium-sized long-running applications,
    lots of graphs from a good memory profiler can make life better. And they'll probably
    help you find the memory leak too.\n"
- :id: blog-logbook-2022-06-10
  :date: '2022-06-10'
  :category: blog
  :title: Logbook - 2022-06-10
  :author: fly
  :thumbnail: logbook-default3-thumbnail.jpg
  :alt: A 1950s style cartoon lady pointing at a vintage flight logbook
  :link: blog/logbook-2022-06-10
  :path: blog/2022-06-10
  :body: |2+


    <div class="lead">Fly.io makes it easy to host applications worldwide the same way a CDN hosts HTML pages. Our users ship us containers, and we transmute them into Firecracker microVMs that run on our hardware in data centers around the world. [The easiest way to learn more is to sign up](https://fly.io/docs/speedrun/); if you’ve got a working container now, it can be running in Sydney, Chennai, or Amsterdam in just a few minutes.</div>

    Here's our latest changelog. This week we're putting the in-browser UI updates a little closer to all the other ones, to see if they'll play nicely together.

    - **[Feature]** Our WireGuard peers sync a lot faster with the kernel's wg state, by adding only peers that have changed. This should make userspace WireGuard features dramatically faster and eliminate API timeout issues some users were seeing&mdash;especially significant for GitHub Actions and other CI processes that may create a new WireGuard peer every time.
    - **[Feature]** Improved proxy lock contention (by getting rid of a global lock) in our tracing instrumentation. The proxy is much faster at tls handshaking and http parsing when it’s under load now. This should reduce response times for almost everyone.
    - **[Feature]**  `fly deploy` now accepts [build-time secrets](https://docs.docker.com/develop/develop-images/build_enhancements/#new-docker-build-secret-information) via the `--build-secret` flag. You should use this instead of [insecurely passing secrets as build arguments](https://pythonspeed.com/articles/docker-build-secrets/). ([flyctl v0.0.333](https://github.com/superfly/flyctl/releases/tag/v0.0.333))
    - **[Feature]** Rolling deploys now update more VMs in each batch. This will make deploys 2-5x faster for apps that run in many regions.
    - **[Feature]** Our [Jupyter launcher](https://fly.io/launch/jupyter) still fits on our free tier, but now it requires a credit card, because we saw significant abuse when it didn't. This is why we can't have nice things!
    - **[Feature]** Dashboard style improvements continue, including such highlights as making the certificates "delete" button not-invisible.
    - **[Feature/Fix]** Shipped an intelligent longpolling fallback in response to the occasional user report of the dashboard failing to load. This also resulted in a new feature in [Phoenix 1.6.10](https://github.com/phoenixframework/phoenix/blob/master/CHANGELOG.md#1610-2022-06-01) to make identifying websocket health easier.
    - **[Feature]** Added permanent redirects to make sure our UI only shows up at the `fly.io` domain where it belongs. Our UI apps are Fly apps, and like all Fly apps, they get `.fly.dev` URLs for free. We don't want Google indexing them at these addresses; it's confusing and dilutes our SEO something something.
    - **[Feature]** Clicking on an app's status badge now takes you to its Monitoring tab. Because it felt like it should work like that.
        ![Screenshot of an app status badge on the dashboard](clickable-badge.png?card&centered&3/4)
    - **[Feature]** Before you can delete an app from the dashboard, you'll now have to type the full app name. This will make it harder to accidentally delete an app, or to delete the wrong one.
    - **[Feature]** Simplified adding a credit card to a new organization. Previously you had to juggle between links on Fly and on Stripe to get this done.
    - **[Fix]** Automatic redeployment of Turboku apps from Heroku deploys should work again now. We had broken our [Turboku](/blog/new-turboku/) webhook with recent code changes.
    - **[Fix]** Fixed a bug that wouldn’t let you delete your user if that user was an admin on a deleted org.
    - **[Fix]** The app metrics page now loads properly, and also hides raw queries by default.
    - **[Fix]** All customers with coupons should now be able to see them on the dashboard. Coupons were not showing for orgs that hadn't incurred charges yet.



- :id: blog-logbook-2022-06-01
  :date: '2022-06-01'
  :category: blog
  :title: Logbook - 2022-06-01
  :author: fly
  :thumbnail: logbook-default2-thumbnail.jpg
  :alt: A 1950s style cartoon lady pointing at a vintage flight logbook
  :link: blog/logbook-2022-06-01
  :path: blog/2022-06-01
  :body: "\n\n<p class=\"lead\"> We’re Fly.io. We take container\nimages and run them
    on our hardware around the world.  It’s pretty\nneat, and you [should check it
    out](https://fly.io/docs/speedrun/); with an already-working Docker container,
    you can be up and running on Fly in well under 10 minutes.</p>\n\nHere's our latest
    changelog. Looking back over the week, our [forum community](https://community.fly.io/)
    has been quite a driver of (logged) change. When you're done here, [head over
    there](https://community.fly.io/) to be a part of it! \n\n- **[Feature]** Created
    an [example Node.js app](https://github.com/fly-apps/fly-app-with-multiple-internal-ports)
    to demonstrate how to expose multiple internal ports on separate public ports.
    (Related [community discussion](https://community.fly.io/t/how-to-use-multiple-ports/5406/)
    with additional helpful info from charsleysa.)\n- **[Feature]** `fly restart`
    now performs a rolling restart (thanks to ryansch [in the forums](https://community.fly.io/t/rolling-restart/5269/)).\n-
    **[Feature]** `fly status --all` now shows failed VMs for 7 days, up from 2 (thanks
    to dhess1 [in forums](https://community.fly.io/t/app-is-dead-restart-does-nothing-no-logs/5401/)).\n-
    **[Feature]** Created a sample Terraform project for orchestrating the Fly API,
    machines and DNSimple to bootstrap an app: [https://github.com/fly-apps/terraformed-machines](https://github.com/fly-apps/terraformed-machines).\n-
    **[Feature/Docs]** We now have a [getting-started guide for Laravel apps](https://fly.io/docs/getting-started/laravel/);
    there's a [hello-world app](https://github.com/fly-apps/fly-hello-laravel) to
    go with it, and a more [in-depth sample app](https://github.com/fly-apps/fly-hello-laravel-db-replay)
    demonstrating the use of the [`fly-replay` header](https://fly.io/blog/globally-distributed-postgres/#the-fly-replay-header)
    to take advantage of multi-region databases.\n- **[Feature]** Released the first
    version of our [official Terraform provider](https://registry.terraform.io/providers/fly-apps/fly/)
    with our awesome new community-member-turned-consultant, [Dov Alperin](https://dov.dev/).
    This provider will keep feature parity with the Machines API and allow users to
    easily orchestrate machines.\n- **[Feature]** Added a 512MB swap for release commands,
    in case they need more memory than normal VM runtime. This allows memory-hungry
    release commands to succeed without increasing RAM on apps that don't otherwise
    need it.\n- **[Feature]** Added three new, direct GraphQL endpoints for fetching
    volumes, IPs and certificates by ID. These records were previously only available
    in the scope of an application. This was added to make our [Terraform provider](https://github.com/fly-apps/terraform-provider-fly)
    \  happy when importing existing apps. But it may be useful to anyone building
    stateful tooling around Fly resources. **Caveat:** Use our [GraphQL API](https://api.fly.io/graphql)
    directly at your own peril; it's in a preview state and may change.\n- **[Feature]**
    Now it’s possible to add a description and a cover image to docs pages, so we
    can make their link previews on social more informative. Added them to the [Fly
    Machines reference](https://fly.io/docs/reference/machines), so at least one link
    will look good.\n- **[Blog]** Published [A Reusable Multi-Select Component for
    Phoenix LiveView](https://fly.io/phoenix-files/liveview-multi-select/), attacking
    the problem of building a component that let users select multiple options from
    a list, and getting the front and back ends to do things with that selection.\n\n\nOur
    [web dashboard](https://fly.io/dashboard) still needs its own section:\n\n\n-
    **[Feature]** Livebook apps created with the Fly.io [web launcher](https://fly.io/launch/livebook)
    can now be upgraded via a button in the app's Image Details section. If the image
    is v0.6.0 or older, the Livebook interface itself will offer a link to our UI
    to update it:\n    ![Screenshot of an update prompt on a running Livebook](livebook-update-prompt.png?card&centered&3/4)\n-
    **[Feature]** Put IP addresses (in the App Overview tab) and the App Secrets table
    in a `monospace` font, to improve readability.\n- **[Feature]** Subscription discounts
    are now visible on the dashboard main page.\n- **[Feature]** Any coupons an organization
    has are now visible on our billing page.\n- **[Fix]** Fixed a [bug](https://community.fly.io/t/bug-in-fly-dashboard-on-certificates-dialog/5357)
    on the certificates page where clicking \"copy\" on a IPv6 DNS entry would actually
    copy the IPv4 entry.\n- **[Fix]** Individual [Machines](https://fly.io/blog/fly-machines/)
    have their own info page, accessible through their parent App's Machines tab,
    with a table to display events (e.g. `start`, `launch`). The actual events weren't
    making it into the table. And now they are.\n- **[Fix]** Users without credit
    cards cannot change organization plans. No point in upgrading to a paid plan without
    a payment method!\n- **[Fix]** Updated the Tailwind classes on the New Organization
    button so you can see it better.\n- **[Fix]** Changed the URL for billing errors
    triggered on our API from `fly.io/organizations/foo` to `fly.io/dashboard/foo/billing`
    to fit our newer URL model.\n- **[Fix]** Logging out now logs you out of all open
    browser tabs. \n- **[Fix]** Removed the \"usage\" column from the App Volumes
    table, until we can plug a useful number into it. It was displaying total data
    ever written&mdash;which is, yes, a kind of usage&mdash;but rather panic-inducing
    when you're expecting to see how much filesystem space you're occupying. \n- **[Fix]**
    Two-factor authentication setup now limits input to 6 characters."
- :id: phoenix-files-liveview-multi-select
  :date: '2022-05-30'
  :category: phoenix-files
  :title: A reusable Multi-Select component for Phoenix LiveView
  :author:
  :thumbnail: multi-select-thumbnail.jpg
  :alt:
  :link: phoenix-files/liveview-multi-select
  :path: phoenix-files/2022-05-30
  :body: "\n\nHave you ever wanted a feature that lets your users select multiple
    items from a list, and performs some action on their selection? This is a really
    common thing to do, and it can be pretty involved to build. What do we do when
    we don't want to write something over and over? We build a component!\n\nToday
    we'll walk through building a reusable Phoenix LiveView multi-select component.
    Then we'll learn how to hook it up to let users filter a book collection by category.
    A single book can be both a &quot;Romance&quot; and a &quot;Thriller&quot; at
    the same time. We want to let users choose any combination of categories using
    the multi-select component we're about to build.\n\n## Roadmap\n\nWe'll [define
    our component](#creating-a-livecomponent) using the [Phoenix.LiveComponent](https://hexdocs.pm/phoenix_live_view/Phoenix.LiveComponent.html)
    module, so it can manage its own state and events. We want this component to:\n\n
    \ - [Display a list of selectable options](#rendering-the-selectable-options-within-a-form):
    To [model the selectable options](#defining-a-data-model) and keep track of the
    state, we'll define a couple of `embedded_schema`s.  Then we'll use the [Phoenix.HTML.Form](https://hexdocs.pm/phoenix_html/Phoenix.HTML.Form.html)
    helpers to render the HTML inputs for selecting and deselecting the options; we'll
    design the component to take a [form](https://hexdocs.pm/phoenix_html/Phoenix.HTML.Form.html#t:t/0)
    as a parameter so that we can create the inputs as part of any external form.\n
    \ - [Show the options we've already chosen](#displaying-the-set-of-selected-options):
    We'll define a  `selected_options`  assign to track and render the component's
    selections.\n  - [Send selection updates to the parent LiveView](#sending-selection-updates):
    every time an option is selected, the multi-select component should inform the
    parent LiveView of any changes to the selection, so that it can perform any needed
    action with them. We’ll make this happen using the new feature that was introduced
    in LiveView [v0.17.8](https://github.com/phoenixframework/phoenix_live_view/blob/master/CHANGELOG.md#0178-2022-04-06):
    The ability to use `phx-change` to emit an event when an individual input has
    changed!\n  - [Update when the selection changes](#multiselectcomponent-in-action):
    We'll take advantage of the [LiveComponent life cycle](https://hexdocs.pm/phoenix_live_view/Phoenix.LiveComponent.html#module-life-cycle).
    Once the parent assigns are updated to keep the updated selection, the updated
    assigns will propagate to the multi-select component so it will render its own
    changes. \n  - [Show/hide the list of selectable options](#showing-and-hiding-the-selectable-options):
    We'll use the `JS.toggle`  command to hide and show the options list.\n\nOnce
    we finish our work, we'll have this component ready to be used:\n\n<%= video_tag
    \"multi_select_00.mp4?card&center?1/3\" %>\n\nLet's start!\n\n## Creating a LiveComponent\n\nWe
    start by defining a skeleton [LiveComponent](https://hexdocs.pm/phoenix_live_view/Phoenix.LiveComponent.html)—which
    we name `MultiSelectComponent`—with placeholder `render/1` and `update/2` callbacks.\n\n```elixir\ndefmodule
    PhoenixFilesWeb.MultiSelectComponent do\n  use PhoenixFilesWeb, :live_component\n\n
    \ alias PhoenixFiles.MultiSelect\n\n  def render(assigns) do\n    ~H\"\"\"\n    <div>\n\n
    \   </div>\n    \"\"\"\n  end\n\n  def update(%{id: id} = params, socket) do\n
    \   {:ok, assign(socket, :id, id)}\n  end\nend\n```\n\nOur `update/2` function
    is called after the `mount/1` function on the parent LiveView, and again every
    time there is an update of the parent LiveView's assigns.\n\nNote that all of
    the component's HTML content must be inside a containing `<div>`. Let's start
    filling it in.\n\n<aside class=\"right-sidenote\">You may have noticed that we're
    interpolating the component's `@id` assign to define the ID of the selected options
    container. This is to ensure that this element (and other HTML elements we'll
    define later) will have a unique ID in case more than one instance of our `MultiSelectComponent`
    is rendered. </aside>\n\n```elixir\ndef render(assigns) do\n    ~H\"\"\"\n    <div
    class=\"multiselect\">\n      <div class=\"fake_select_tag\" \n        id={\"#{@id}-selected-options-container\"\n
    \     >\n        <div class=\"icon\">\n          <svg id={\"#{@id}-down\"} \n
    \           <path ... />\n          </svg>\n        </div>\n      </div>\n    </div>\n
    \   \"\"\"\n  end\n```\n\nWe define a container to display the options, and simulate
    an HTML `<select>` tag with the help of a `<div>` and a _chevron-down_ SVG icon:\n\n![](multi-select-011.png?1/3?center)\n\nWe'll
    add some magic to this later. But first let's get the component working!\n\n##
    Defining a data model\n\nEach option in our list needs a label to display, and
    some way to keep track of whether it is selected or not.\n\n```elixir\ndefmodule
    PhoenixFiles.MultiSelect do\n  use Ecto.Schema\n\n  embedded_schema do\n    embeds_many
    :options, PhoenixFiles.MultiSelect.SelectOption\n  end\n\n  defmodule SelectOption
    do\n    use Ecto.Schema\n\n    embedded_schema do\n      field :selected, :boolean,
    default: false\n      field :label, :string\n    end\n  end\nend\n\n```\n\nThis
    defines an `embedded schema` called `MultiSelect` ; its `:options` field embeds
    a list of `SelectOption` schemas. The `SelectOption` schema, in turn, defines
    the two fields we need for each option: `:selected`  and `:label`.\n\n## Rendering
    the selectable options within a form\n\nOur  `update/2`  function adds  `:selectable_options`
    (a list of `SelectOption` schemas) and a yet-to-be-defined enclosing `:form`  to
    the component's assigns.\n\n```elixir\ndef update(params, socket) do\n%{options:
    options, form: form, id: id} = params\nsocket =\n  socket\n  |> assign(:id, id)\n
    \ |> assign(:selectable_options, options)\n  |> assign(:form, form)\n  \n  {:ok,
    socket}\nend\n```\n\nThen `render/1` uses those assigns to put the pieces together
    in our template:\n\n```elixir\ndef render(assigns) do\n  ~H\"\"\"\n  <div class=\"multiselect\">\n
    \   <div class=\"fake_select_tag\" \n      id={\"#{@id}-selected-options-container\"}\n
    \   >\n      ...\n    </div>\n    <div id={\"#{@id}-options-container\"}>\n      <%%=
    inputs_for @form, :options, fn opt -> %>\n        <div class=\"form-check\">\n
    \         <div class=\"selectable-option\">\n            <%%= checkbox opt, :selected,
    \n              value: opt.data.selected \n            %>\n            <%%= label
    opt, :label, opt.data.label %>\n          </div>\n        </div>\n      <%% end
    %>\n    </div>\n  </div>\n  \"\"\"\nend\n```\n\nThe main feature here is our [inputs_for/4](https://hexdocs.pm/phoenix_html/Phoenix.HTML.Form.html#inputs_for/4)
    function. It attaches our nested `:options` data to the form and iterates over
    the options, invoking the [checkbox/3](https://hexdocs.pm/phoenix_html/Phoenix.HTML.Form.html#checkbox/3)
    and [label/3](https://hexdocs.pm/phoenix_html/Phoenix.HTML.Form.html#label/3)
    \ functions to render those elements for each option.\n\nWith this code, our component
    looks like this:\n\n![](multi-select-01.png?1/3?center)\n\n## Displaying the set
    of selected options\n\nInside our _select_, we want to display a list of selected
    options, like this:\n\n![](multi-select-02.png?1/3?center)\n\nWe write a private
    function `filter_selected_options` to find all the `SelectOptions`  with `selected
    == true` , and `update/2` adds these to our assigns:\n\n```elixir\ndef update(params,
    socket) do\n  %{options: options, form: form, id: id} = params\n  socket =\n    socket\n
    \   |> assign(:id, id)\n    |> assign(:selectable_options, options)\n    |> assign(:form,
    form)\n    |> assign(:selected_options, filter_selected_options(options))\n\n
    \ {:ok, socket}\nend\n\ndefp filter_selected_options(options) do\n  Enum.filter(options,
    fn opt -> \n    opt.selected in [true, \"true\"] \n  end)\nend\n```\n\nNow we
    can iterate over `@selected_options` and display their labels in their own `<div>`s:\n\n```xml\n<div
    class=\"fake_select_tag\" \n  id={\"#{@id}-selected-options-container\"}\n>\n
    \ <%%= for option <- @selected_options do %>\n    <div class=\"selected_option\">\n
    \     <%%= option.label %>\n    </div>\n  <%% end %>\n  <div class=\"icon\">\n
    \   ...\n  </div>\n</div>\n```\n\n## Sending selection updates\n\nSo far we can
    render selected options, but the checkboxes aren't set up to change the selection.\n\nWe
    need to tell the server (and update our component) every time items have been
    selected or deselected, which means emitting an event.\n\nThanks to [LiveView
    v0.17.8](https://github.com/phoenixframework/phoenix_live_view/blob/master/CHANGELOG.md#0178-2022-04-06),
    we have the `phx_change` option available to emit events when there are changes
    to individual inputs in a form. For our example, we use this option on each of
    the checkboxes to emit the `checked` event. This tells us when a selection change
    was made.\n\n```elixir\n<%%= checkbox value, :selected, \n  value: value.data.selected\n
    \ phx_change: \"checked\",\n  phx_target: @myself\n%>\n```\n\nSince our `LiveComponent`
    lives in the same process as the parent LiveView, we must specify that the `checked`
    event is handled by the `MultiSelectComponent` and not by the parent LiveView.
    To do this, we add the `phx-target: @myself` option to our form. The `@myself`
    here tells LiveView [to route the event to our component](https://hexdocs.pm/phoenix_live_view/Phoenix.LiveComponent.html#module-targeting-component-events).\n\nNow
    let's see how to handle the `checked` event:\n\n```elixir\n#values = %{\"2\" =>
    %{\"selected\" => \"true\"}}\ndef handle_event(\n      \"checked\", \n      %{\"multi_select\"
    => %{\"options\" => values}}, \n      socket\n    ) do\n\n  [{index, %{\"selected\"
    => selected?}}] = Map.to_list(values)\n  index = String.to_integer(index)\n  selectable_options
    = socket.assigns.selectable_options\n  current_option = Enum.at(selectable_options,
    index)\n  \n  updated_options =\n    List.replace_at(selectable_options, \n      index,
    \n      %{current_option | selected: selected?}\n    )\n\n  send(self(), {:updated_options,
    updated_options})\n\n  {:noreply, socket}\nend\n```\n\nThere's a lot going on
    in that code. Let's go through it in more detail.\n\nThe event is emitted with
    a payload with the following structure:\n\n```elixir\n%{\"multi_select\" => \n
    \ %{\"options\" => \n    %{\"2\" => %{\"selected\" => \"true\"}}\n  }\n}\n```\n\nFirst
    we extract the checkbox `index` and the `selected?` value from that payload, with
    the following line:\n\n```elixir\n#[{\"2\", %{\"selected\" => \"true\"}}]\n[{index,
    %{\"selected\" => selected?}}] = Map.to_list(values)\n```\n\nWe used the `:selectable_options`
    assign to store the options list in our component; now we have to update the _checked_
    option value inside that list. For that, we get the `SelectOption` found at the
    `index` we got earlier from the `:selectable_options` assign, and  change the
    `:selected` value within the list of options.\n\n```elixir\nindex = String.to_integer(index)\nselectable_options
    = socket.assigns.selectable_options\ncurrent_option = Enum.at(selectable_options,
    index)\n\nupdated_options =\n  List.replace_at(selectable_options, \n    index,
    \n    %{current_option | selected: selected?}\n  )\n```\n\nThen, we send the updated
    options to the parent LiveView, so it can decide what to do with them.\n\n```elixir\nsend(self(),
    {:updated_options, updated_options})\n```\n\n## Showing and hiding the selectable
    options\n\nNow that our component essentially works, we can add life to our simulated
    HTML `<select>` tag.\n\nWe add a chevron-up icon to match the chevron-down we
    already have, so that both icons have the same location. When the component loads,
    the `#up-` icon and the  `#options_container-` element containing the whole list
    have class `hidden`, which is defined in our CSS (we use Tailwind) with the property
    `display: none;`.\n\nA `phx-click` binding on each `<svg>` invokes [JS.toggle](https://hexdocs.pm/phoenix_live_view/Phoenix.LiveView.JS.html#toggle/1)
    to toggle visibility of both icons and the options list on click.\n\n```elixir\n<div
    class=\"icon\">\n  <svg id={\"#{@id}-down-icon\"}\n    phx-click={\n      JS.toggle()
    \n      |> JS.toggle(to: \"##{@id}-up-icon\")\n      |> JS.toggle(to: \"##{@id}-options-container\")\n
    \   }>\n    <path ... />\n  </svg>\n  <svg id={\"#{@id}-up-icon\" class=\"hidden\"
    \n    phx-click={\n      JS.toggle() \n      |> JS.toggle(to: \"##{@id}-down-icon\")
    \n      |> JS.toggle(to: \"##{@id}-options-container\")\n    }>\n    <path ....
    />\n  </svg>\n</div>\n```\n\nTada! Our `render` function is finished!\n\n<%= video_tag
    \"multi_select_03.mp4?card&center?1/3\" %>\n\n## MultiSelectComponent in action\n\nYou'll
    remember we were going to let users choose multiple categories to filter our books
    by. We saw the categories in our examples, but we haven't shown where they come
    from yet. We need to plug our categories into `MultiSelectComponent`.\n\nHere
    they come! We define the assigns we use in the parent LiveView:\n\n```elixir\ndef
    mount(_params, _session, socket) do\n  categories =\n    [\n      %SelectOption{id:
    1, label: \"Fantasy\", selected: false}\n      %SelectOption{id: 2, label: \"Horror\",
    selected: true},\n      %SelectOption{id: 3, label: \"Literary Fiction\", selected:
    false},\n    ]\n\n  {:ok, set_assigns(socket, categories)}\nend\n```\n\n`categories`
    contains our category options in the shape of the `SelectOption` schema we defined
    earlier as part of `MultiSelectComponent`.\n\nOur `set_assigns/2` function sets
    three assigns we'll need: `:changeset`, `:books`, and `:categories`:\n\n```elixir\ndefp
    set_assigns(socket, categories) do\n  socket\n  |> assign(:changeset, build_changeset(categories))\n
    \ |> assign(:books, filter_books(categories))\n  |> assign(:categories, categories)\nend\n```\n\nWe've
    discussed `:categories`, and you can guess what `:books` and `filter_books/2`
    are. What about `:changeset`?\n\nLiveView provides the [form](https://hexdocs.pm/phoenix_live_view/Phoenix.LiveView.Helpers.html#form/1)
    function component to create HTML forms. This component receives an [Ecto.Changeset](https://hexdocs.pm/ecto/Ecto.Changeset.html)
    to convert it to a form, and then, the resulting form is used to generate input
    fields.\n\nWe create a [MultiSelect](#defining-a-data-model) changeset with the
    `build_changeset/1` function:\n\n```elixir\ndefp build_changeset(options) do\n
    \ %MultiSelect{}\n  |> Ecto.Changeset.change()\n  |> Ecto.Changeset.put_embed(:options,
    options)\nend\n```\n\nOnce the changeset is created, we can use it to create our
    `form`:\n\n```elixir\n<.form let={f} for={@changeset} id=\"multiselect-form\">\n</.form>\n```\n\nAn
    important detail here is that our form is accessible in the `f`  variable. We
    need to  pass this to our inputs so it all gets linked together inside our `MultiSelectComponent`.
    \ For that, we just render our `MultiSelectComponent` inside our form as follows:\n\n```elixir\n<.form
    let={f} for={@changeset}  id=\"multiselect-form\">\n  <.live_component\n    id=\"multi\"\n
    \   module={MultiSelectComponent}\n    options={@categories}\n    form={f}\n  />\n</.form>\n\n```\n\nWhen
    a category is selected in the `MultiSelectComponent`, an update is sent to the
    parent LiveView. The parent LiveView needs to handle the event and update the
    books list:\n\n```elixir\ndef handle_info({:updated_options, options}, socket)
    do\n  # update books list, the selected categories and the changeset\n  {:noreply,
    set_assigns(socket, options)}\nend\n```\n\nWhen the update is received, the parent
    assigns are updated and the `update/2` function inside our `MultiSelectComponent`
    is called. This means both parent and multi-select component `:categories/:selected_options`
    are updated, and both the parent LiveView and the `MultiSelectComponent` now have
    the same information.\n\nThe parent LiveView [is the source of truth](https://hexdocs.pm/phoenix_live_view/Phoenix.LiveComponent.html#module-liveview-as-the-source-of-truth);
    it keeps the selected categories and is responsible for updating the `MultiSelectComponent`
    assigns.\n\n<%= video_tag \"multi_select_04.mp4?card&center\" %>\n\n<%= partial
    \"shared/posts/cta\", locals: {\n  title: \"Fly.io ❤️ Elixir\",\n  text: \"Fly.io
    is a great way to run your Phoenix LiveView app close to your users. It's really
    easy to get started. You can be running in minutes.\",\n  link_url: \"https://fly.io/docs/elixir/\",\n
    \ link_text: \"Deploy a Phoenix app today!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>\"\n}
    %>\n\n\nWow, we covered a lot! We built a highly responsive fancy filter feature,
    without writing any JavaScript!\n\nOur component is great, but we can still improve
    it. Let's think about the future: what would happen to the selected options if
    we lost the internet connection for a few seconds and the page was reloaded? What
    if, in addition to sending updates to the parent LiveView, we want to perform
    another action?\n\n## Bonus: Form recovery\n\nWhat happens to the component if
    we lose the connection and then recover it?\n\nLet's try selecting some values,
    stopping the app and starting it again to see what happens:\n\n<%= video_tag \"multi_select_05.mp4?card&center\"
    %>\n\nThe selected options are lost!\n\nTo avoid losing the selected options,
    we make use of [form recovery](https://hexdocs.pm/phoenix_live_view/0.7.0/Phoenix.LiveView.html#module-form-recovery-following-crashes-or-disconnects)
    after reconnections. We mark our form with the `phx-change` binding, so that the
    `validate` event is triggered to the server with the selected values right after
    the connection is re-established.\n\n```elixir\n<.form \n  let={f} \n  for={@changeset}
    \n  id=\"multiselect-form\" \n  phx-change=\"validate\"\n>\n  ...\n</.form>\n```\n\nNow
    we have to set all our assigns inside the `handle_event`  callback:\n\n```elixir\n#multi_form
    = %{ \"options\" => \n#  %{\n#    \"0\" => \n#      %{\"id\" => \"1\", \"label\"
    => \"Fantasy\", \"selected\" => \"true\"},\n#    \"1\" => \n#      %{\"id\" =>
    \"2\", \"label\" => \"Horror\", \"selected\" => \"true\"},\n#    \"2\" => \n#
    \     %{\"id\" => \"3\", \"label\" => \"Liter...\", \"selected\" => \"true\"}\n#
    \  }\n#}\ndef handle_event(\n      \"validate\", \n      %{\"multi_select\" =>
    multi_form}, \n      socket\n    ) do\n  options = build_options(multi_form[\"options\"])\n
    \ \n  {:noreply, set_assigns(socket, options)}\nend\n```\n\nWe receive all the
    values that had been selected and were saved thanks to our form. The `build_options/1`
    function is just a helper to create a list of `SelectOption` with the form values.\n\n```elixir\ndefp
    build_options(options) do\n    Enum.map(options, fn {_idx, data} -> \n      %SelectOption{\n
    \       id: data[\"id\"], \n        label: data[\"label\"], \n        selected:
    data[\"selected\"]\n      }\n    end)\n  end\n```\n\nWith these changes, we ensure
    that the form can recover the values after reconnecting:\n\n<%= video_tag \"multi_select_06.mp4?card&center\"
    %>\n\n## Bonus 2: Customized behavior when options are selected.\n\nRight now,
    every time an element is selected, we send the updates to the parent LiveView.
    However, we are creating a reusable component; wouldn't it be better to be able
    to customize what action to take when there is a new selection?\n\nTo do this,
    we apply a little trick; we send a function in the component's parameters to be
    executed when an element is selected.\n\n```elixir\n<.live_component\n  id=\"multi\"\n
    \ module={MultiSelectComponent}\n  options={@categories}\n  form={f}\n  selected={fn
    opts -> send(self(), {:updated_options, opts}) end}\n/>\n```\n\nThen we keep the
    `:selected` function in the component assigns:\n\n```elixir\ndef update(params,
    socket) do\n  %{options: options, form: form, id: id, selected: selected} = params\n
    \ socket =\n    socket\n    |> assign(:id, id)\n    |> assign(:selectable_options,
    options)\n    |> assign(:form, form)\n    |> assign(:selected_options, filter_selected_options(options))\n
    \   |> assign(:selected, selected)\n\n  {:ok, socket}\nend\n```\n\nThat way we
    can execute any function with an arity 1 when we handle the `checked` event:\n\n```elixir\ndef
    handle_event(\n      \"checked\", \n      %{\"multi_select\" => %{\"options\"
    => values}}, \n      socket\n    ) do\n  .\n  .\n  .\n  socket.assigns.selected.(updated_options)\n\n
    \ {:noreply, socket}\nend\n```\n\nPretty slick! It lets the caller of the function
    decide what action to apply.\n\n## Wrap-Up\n\nIn our example we used the `Phoenix.LiveComponent`
    behavior to separate markup, state and events into a component that we can reuse
    as many times as needed. We designed a data model to keep the selectable options
    independent of any specific use case, we designed our component to be used as
    part of any custom `Phoenix.Form`, and we used a trick to execute a callback function
    each time an option is selected.\n\nAll these design decisions to create a solid
    base to reuse our component and customize it to  fit our needs!\n\nWhere else
    would you use this multi-select component?\n\nYou can find this example here:
    [bemesa21/phoenix_files](https://github.com/bemesa21/phoenix_files/)"
- :id: blog-logbook-2022-05-26
  :date: '2022-05-26'
  :category: blog
  :title: Logbook - 2022-05-26
  :author: fly
  :thumbnail: logbook-default-thumbnail.jpg
  :alt: A 1950s style cartoon lady pointing at a vintage flight logbook
  :link: blog/logbook-2022-05-26
  :path: blog/2022-05-26
  :body: "\n\n\n<p class=\"lead\">The other day, we [unleashed the (Fly) Machines](https://fly.io/blog/fly-machines/).
    Which is to say: we now officially have an API for starting and stopping [fast-booting
    Firecracker VMs](https://fly.io/docs/reference/machines/) directly, bypassing
    orchestration. Normal Fly.io apps will carry on, oblivious to the fuss. [You can
    spin up an app](/docs/speedrun/) in a matter of minutes.</p>\n\nWork leading up
    to the Fly Machines launch involved a multitude of changes by many of the cogs
    in this corporate machine, but that's not to say the other production lines have
    been idle. For one thing, our web UI has been transforming before our eyes. We've
    grouped its updates at the end, for easy digestion (and so we don't have to type
    \"web UI\" so many times). \n\nHere's our latest changelog:\n\n- **[Feature]**
    \ You can now specify `proxy_proto_options = { version = \"v2\" }` to have our
    proxy send a proxy protocol v2 header line when establishing connections to your
    app.\n- **[Feature]** [Fly HA Postgres](https://fly.io/docs/reference/postgres/)
    instances will turn read-only, and thus fail a health check, when their volume
    is 90% full. This gives users whose databases are filling up (with WAL files or
    genuine data growth) a chance to react before they crash with full disks, which
    is much more of a PITA to recover from. This is a quick fix until we can make
    user-facing volume metrics/alerts and expand/shrink our volumes. ([postgres-ha
    v0.0.22](https://github.com/fly-apps/postgres-ha/releases/tag/v0.0.22))\n- **[Feature]**
    The GraphQL API now says `Could not find <some-app-name>` when it can't find the
    specified app, instead of the somewhat mystifying `Could not resolve App` when
    it can't find an app, which most commonly happens when a) there's a typo in the
    app name or b) credentials used are for the wrong user and so the app isn't accessible.
    This should make it more intuitive to find the issue.\n- **[Feature]** Made our
    API Telemetry pickier to better track meaningful API operations and diagnose performance
    issues.\n- **[Fix]** As some people have started using our APIs in ways that we
    don't, it's uncovered some edge cases. We've addressed what we've found so far;
    in particular:\n  1. Selecting data on the response for deleting App Certificates
    could cause errors because the data changed while rendering.\n  2. Selecting data
    on the response for deleting a Volume could cause errors for a similar case to
    above.\n  3. Volume creation has some specific expectations on character format
    for region codes. We were mutating a string in place, which our runtime has recently
    gotten more strict about preventing in certain cases.\n  4. Sometimes we receive
    opaque validation errors on DNS updates to our DNS provider. We've added some
    more context to these errors so we can better understand and respond to these
    in the future.\n- **[Fix]** Sometimes when users sent invitations for folks to
    join their organization, we would initiate the invitation before the data was
    available in our database, resulting in a noticeable delay before a retry would
    successfully send the invitation. This should be drastically less likely now.\n-
    **[Docs]** Released a [blog post](/blog/fly-machines/) announcing the Fly Machines
    API.\n- **[Docs]** Added a [Machines reference doc](/docs/reference/machines/)
    and a [FaaS guide](/docs/app-guides/functions-with-machines/) using the machines
    API.\n- **[Docs]** Improved the [`fly.toml` documentation](/docs/reference/configuration/):
    documented `PRIMARY_REGION` for targeting release commands, clarified some details
    about deployment strategies, and reworded `statics` to more accurately reflect
    its behavior and caveats.\n- **[Docs]** Added a [guide](https://fly.io/docs/app-guides/planetscale/),
    with [sample Node.js/Express app](https://github.com/fly-apps/nodejs-planetscale-read-replicas/),
    for taking advantage of PlanetScale's new [Portals](https://planetscale.com/blog/introducing-planetscale-portals-read-only-regions)
    read-only regions feature with your Fly.io apps.\n\nWeb UI improvements! We have
    a revamped [dashboard](https://fly.io/dashboard) and [app page](https://community.fly.io/t/feature-improved-app-page/5213),
    and more:\n\n- **[Feature]**  Shipped the new [Apps dashboard](https://fly.io/dashboard)
    with a redesign, new pages and more information for users.\n- **[Feature]** Shipped
    a redesigned app page for convenient access to information and some administration
    tasks for individual apps; [posted about it](https://community.fly.io/t/feature-improved-app-page/5213)
    on the community forum. More to come!\n- **[Feature]** Shipped our new organization
    UI to bundle together all organization-related things into the new Dashboard.
    Bonus that it also has a list of latest invoices, which were difficult to find
    before. [Community forum post](https://community.fly.io/t/new-feature-improved-organization-pages/5296).\n-
    **[Feature]** Shipped our new [paid plans UI](/plans), complete with in-depth
    info, chill design and spot illos, to help potential customers decide what works
    for them.\n- **[Feature]** Released our new [documents UI](https://fly.io/dashboard/personal/documents)
    for self-service DPA and BAA document requests.\n- **[Feature]** Added region
    information to the app Volumes tab.\n- **[Feature]** In the app Overview tab,
    the Image Details block is hidden if the app doesn't yet have an image associated
    with it, and now only shows Image Version if there is one.\n- **[Feature]**  Shipped
    the Machines ✨ tab for apps.\n- **[Fix]** Fixed some messed-up markup on the Logs
    tab.\n- **[Fix]** Added back a deleted template that was causing a 500 error on
    the organization page.\n- **[Fix]** Removed a redundant logo on our CLI login.\n\n\n<%=
    partial \"shared/posts/cta\", locals: {\n  title: \"The Fly Machines API is the
    latest thing\",\n  text: \"If you've got a Fly.io account, you can play with our
    new API for fast-booting Firecracker VMs.\",\n  link_url: \"/docs/reference/machines/\",\n
    \ link_text: \"Try Machines&nbsp;&nbsp;<span class='opacity-50'>&rarr;</span>\"\n}
    %>"
- :id: blog-fly-machines
  :date: '2022-05-24'
  :category: blog
  :title: 'Fly Machines: an API for fast-booting VMs'
  :author: kurt
  :thumbnail: machine-whack-a-mole-thumbnail.jpg
  :alt: Whack-a-mole-type game with Fly.io bird characters where the moles would be.
  :link: blog/fly-machines
  :path: blog/2022-05-24
  :body: "\n\n<p class=\"lead\">[Fly.io](http://Fly.io) turns Docker images into running
    VMs on physical servers all over the world. We built [an API for booting VMs very
    quickly](https://fly.io/docs/reference/machines/). Here's what you need to know.</p>\n\nFly
    Machines are VMs with a fast REST API that can boot instances in about 300ms.
    \n\nOur proxy can boot Fly Machines for you, and you can shut them down when they're
    idle. Which means you can cost-effectively create VMs and keep them standing by
    to handle new requests.\n\n<aside class=\"callout\">If you're already running
    an app on Fly.io, here's what you need to know. Machines are basically the same
    as the VMs we manage with our orchestration system (Nomad). When you deploy an
    app, we create new VMs and destroy the old ones with your preferred image. Fly
    Machines are one level lower, and let developers manage individual VMs.</aside>\n\nWe
    built Machines for us. Our customers want their apps to scale to zero, mostly
    to save money. Also because it feels right. An app that isn't doing anything shouldn't
    be running. Fly Machines will help us ship apps that scale to zero sometime this
    year.\n\nFly Machines may be useful to you, too. A lot of y'all want to build
    your own functions-as-a-service. [You can build a FaaS with Fly Machines](https://fly.io/docs/app-guides/functions-with-machines/).\n\n##
    How to boot VMs in a hurry\n\nWe said we want our VMs to boot fast. They already
    do; [Firecracker is pretty darn fast](https://jvns.ca/blog/2021/01/23/firecracker--start-a-vm-in-less-than-a-second/)
    to boot a given executable on a given host. Our job is to get our own plumbing
    out of your way, and get you close to local-Firecracker speeds.\n\nSpinning up
    a VM as fast as possible _on a server somewhere_ is an exercise in reducing infrastructure
    latency. We need to play latency whack-a-mole.\n\nWhen you ask for a VM, you wait
    for network packets to travel to an API somewhere. Then you wait for them to come
    back. The API is also, at minimum, talking to the host your job will run on. If
    all your users are in Virginia and your API is in Virginia and the hardware running
    Firecrackers is in Virginia, this might take 20-50ms.\n\nIf your users are in
    Sydney and the hardware for the Firecrackers are in Sydney and the API is in Virginia,
    \"boot me a VM\" takes more like 300ms. Three tenths of a second just waiting
    for light to move around is not fast.\n\nWe're not done. You need something to
    run, right? Firecracker needs a root filesystem. For this, we download Docker
    images from our [repository](https://fly.io/blog/docker-without-docker/#multi-tenant-repositories),
    which is backed by S3. This can be done in a few seconds if you're near S3 and
    the image is smol. It might take several minutes (minutes!) if you're far away
    and the image is chonk.\n\nWe solve this by making you create machines ahead of
    time. Accountants (not the TikTok kind; actual accountants) call this \"amortization\"
    – pay the cost up front, yield the benefit over time.\n\n## The slow part\n\nHere's
    what happens when you call the [create machine endpoint](https://fly.io/docs/reference/machines/#create-and-start-a-machine):\n\n![The
    92 step process our infrastructure uses to create a machine](./machine-reservation-process.png)\n\nIf
    you're an app developer in Los Angeles and you want a machine in São Paulo, your
    request gets routed to your friendly local API server. We run API servers in every
    region, so this part of the process is fast.\n\nThe API server makes a preflight
    request to our centralized database in Virginia, which gives back a yay (or nay!)
    and an immutable Docker image URL.\n\nOur database in Virginia has to be looped
    in on machine creation. We want a strongly-consistent record that machines exist.
    We also want to make sure you can't create a machine if you're 8 months behind
    on bills or got banned for mining cryptocurrency with a stolen credit card.\n\nThe
    Los Angeles API instance then broadcasts a NATS message to the available hosts
    in São Paulo saying \"hey, reserve me a machine with these specs\". Hosts with
    resources to spare reserve a slice of their capacity and reply with information
    about what they have to offer.\n\nThe API server evaluates each host's offer,
    picks one, and says \"OK, host, create a machine for reservation X\". The API
    server then records a machine record in our centralized database. The other hosts
    garbage-collect the unused reservations a few seconds later.\n\nYou might be thinking
    \"if I'm in Los Angeles and I request a machine in São Paulo, won't it take like
    a second for that whole dance to happen?\" It would, yes. \n\nYou might also be
    thinking \"pulling that image from a remote repository was probably soul-crushingly
    slow, right?\" Also true! You don't want to do that any more times than you need
    to.\n\nWe made machines really cheap to create and keep stopped. In fact, right
    now, you pay for image storage; that's it. What we want you do is: create machines
    ahead of time and start them when you need them.\n\n## The fast part\n\nYou should
    create machines just before you need them. Slightly earlier than just-in-time.
    All the stuff I just told you about is necessary to get to this point, but the
    protein is here: we designed Fly Machines for fast _starts_.\n\nWhen you're ready,
    you start a machine in São Paulo with a request to the nearest API server. This
    time, though, there's no need to wait on our database in Virginia. The central
    accounting is done and the API server knows exactly which host it needs to talk
    to. And the OCI image for the VM's filesystem is ready to go.\n\nHere's what the
    [start machine endpoint](https://fly.io/docs/reference/machines/#start-a-machine)
    does:\n\n![The one step process our infrastructure uses to start a machine](./machine-start-api.png?3/4&centered)\n\n_Now_ the
    start is fast. How fast?\n\nWhen you run `fly machine start e21781960b2896`, the
    API server knows that `e21781960b2896` is owned by a host in São Paulo. It then
    sends a message directly to that host saying \"[start it up](https://www.youtube.com/watch?v=7XL84zQZ1nw)\".
    This message travels as fast as physics allows.\n\nThe host receives the start
    message&hellip;and starts the machine. It already has the image stored locally,
    so it doesn't need to wait on an image pull.\n\nIf you're in Los Angeles and start
    your machine in São Paulo, the \"start\" message gets where it needs to go in
    ~150ms. But if you're in Los Angeles and start a machine in Los Angeles, the \"start\"
    message might arrive in ~10ms.\n\nThe lesson here is \"start machines close to
    your users\"; the operation is very fast. Here's something cool about this, though:
    _You_ don't necessarily start the machine from where you are; an _app_ can do
    it for you. In fact, this is kind of the point. Your application logic should
    be close to your users' machines.\n\nOr, you can forego the app and let `fly-proxy`
    boot machines when HTTP requests arrive. It can do all this for you. \n\nI should
    clarify: our infrastructure is fast to run start operations. Your application
    boot time is something you should optimize. We can't help with that (yet!)\n\n###
    Stopping and scaling to zero\n\nStop commands are fast too. You may not want to
    issue stop commands, though. If your machine should stop when it's idle, there's
    a better way.\n\nFly.io users have been requesting \"scale to zero\" since January
    1st, 1970 at 00:00:00 UTC. Scaling up is pretty easy; it's usually safe to boot
    a new VM and add it to a load balancer. Scaling down is harder&mdash;stop a VM
    at the wrong time and shit breaks.\n\nSo here's how we modeled this: when you
    use Fly.io machines to run apps that need to scale down, make your process exit
    when it's idle. That's it. You've exited the container, effectively stopping the
    machine, but it's intact to pick up a future start request from a clean slate.\n\nThis
    works because your in-machine process has a strongly-consistent view of local
    activity and can confidently detect \"idle\". \n\n<%= partial \"shared/posts/cta\",
    locals: {\n  title: \"Play with the Machines API\",\n  text: \"If you've got a
    Fly.io account, you can play with the Fly Machines API right now&mdash;even if
    you're not ready to [build your own FaaS](https://fly.io/docs/app-guides/functions-with-machines).\",\n
    \ link_url: \"/docs/reference/machines/\",\n  link_text: \"Try Machines&nbsp;&nbsp;<span
    class='opacity-50'>&rarr;</span>\"\n} %>\n\n\n## How Fly Machines will frustrate
    you (the emotional cost of simplicity)\n\nOne thing you may have noticed about
    our design: machines are pinned to specific hardware in our datacenters. This
    is a tradeoff that buys simplicity at the risk of your patience.\n\nPinning machines
    to specific hardware means that if the PSU on that host goes pop, your machine
    won't work (kind of; we run redundant PSUs). Capacity issues will create more
    surprising failures. If you create a biggish 64GB RAM machine and leave it stopped,
    we might be out of capacity on that specific host when you attempt to start it
    back up. \n\nWe will mostly shield you from capacity issues, but you should _definitely_
    be prepared for the eventuality that your first-choice hardware is indisposed.
    Which really just means: plan to have two machines for redundancy.\n\nThe good
    news is that our API is pretty fast. Creating a machine is relatively slow, but
    you can do it in a pinch. If a machine fails to start, you can usually get another
    one running in a few seconds.\n\nThe best way to use machines is to think of a
    list of operations in priority order. If you're trying to run some user code,
    come up with a list like this:\n\n1. Start machine X in Chicago. If that fails,\n1.
    Start machine Z in New Jersey. If that fails,\n1. Launch new machine in Chicago.
    If that fails,\n1. Launch new machine in New Jersey. If that fails,\n1. Launch
    new machine somewhere in the US. If that fails,\n1. Check your internet connection
    (or our status page). That should not fail.\n\nThis cycle will account for all
    the predictable failures and should get you a machine any time you want one.\n\n##
    Pricing (the monetary cost of simplicity)\n\nRunning machines costs the same as
    [running normal VM instances](https://fly.io/docs/about/pricing/#compute). The
    same goes for bandwidth, RAM, and persistent disks.\n\nStopped machines, though,
    are something we could use your feedback on. There's a cost to keeping these things
    around. Right now, we just charge you for storage when a machine isn't running.
    Like $0.15/mo for a 1GB Docker image. \n\n---\n\nQuestions? Comments? Pricing
    ideas? Vitriol? Comment [in our forum thread](https://community.fly.io/t/were-launching-machines-today/5305)."
- :id: blog-fly-io-is-hiring-rails-specialists
  :date: '2022-05-17'
  :category: blog
  :title: Fly.io is Hiring Rails Specialists
  :author: mark
  :thumbnail: ruby-job-thumbnail.png
  :alt:
  :link: blog/fly-io-is-hiring-rails-specialists
  :path: blog/2022-05-17
  :body: |2


    <div class="lead">Fly.io takes container images and converts them into fleets of Firecracker VMs running on our own hardware around the world. We make it easy to run applications near users, whether they’re in Singapore, Seattle, or São Paulo. [Try it out](https://fly.io/docs/speedrun/); if you’ve got a working container already, it can be running here in less than 10 minutes.</div>

    We want Fly.io to be the best place on the Internet to run Ruby on Rails apps, and especially Hotwire. If you're a Rails developer that's enthusiastic about Hotwire, we need your help.

    Here's Fly.io's not-so-secret evil plan. We make it easy to run full-stack apps—any app, in any framework—close to your users. We're a simple and powerful way to run any application, with modern dev UX, advanced Postgres deployments with replication, and app scaling knobs that don't require you to learn Terraform to use. But we're especially shiny for frameworks that benefit from geographic distribution, like Elixir's [LiveView](https://fly.io/blog/how-we-got-to-liveview/), Laravel's [Livewire](https://laravel-livewire.com/) and Ruby on Rail's [Hotwire](https://hotwired.dev/). We want those kinds of frameworks to succeed, because the better they do, the more valuable we are.

    The business goal for this Rails job is, of course, to make Fly.io more attractive to Rails developers. But the job isn't simply advocacy; we're not just looking for cheerleaders. Rather, we're looking for people who can make substantive contributions both to Fly.io and, more importantly, to Rails and Hotwire itself. If you can develop Rails features that are useful to everybody, including people who don't use Fly.io and never will, that's great for us! Anything that makes Rails more effective makes Fly.io more valuable.

    We think Fly.io is a pretty great place to deploy a Rails app today. But it could be much better. So we're also interested in contributions you can make to the dev UX at Fly.io. Some of that might just be expert feedback on what Fly.io should be doing differently to make things more pleasant for Rails developers. Some of it might be new feature ideas. And a lot of it will probably just be helping Rails/Hotwire developers better understand what we're about at Fly.io.

    We don't expect you to be an expert on Fly.io, it just so happens that we already are that! We need your help to understand the needs of the Rails' Hotwire community and framework. You will help pave the way to make Fly.io an even better platform for Rails and Hotwire developers.

    We've been hiring people to do this kind of work for Elixir and LiveView for about a year now, and it's been a huge success for us (so much so that people are starting to think of us as an Elixir company!) Here's your chance to get people to start thinking of Fly.io as a Rails company, too. Represent!

    ## About us and about the job

    This is a mid to senior level job. The salary ranges from $120k to $200k USD. We also offer competitive equity grants. Hopefully that's enough to keep you intrigued, here's what you should _really_ care about:

    - We're a small team, almost entirely technical.
    - We are active in developer communities, including our own at [community.fly.io](https://community.fly.io/). Part of this role is helping to answer support questions specific to Rails running on Fly. Providing meaningful troubleshooting and diagnosis information in most posts.
    - Virtually all customer communication, documentation, and blog posts are in writing. We are a global company, but most of our communication is in English. Clear writing in English is essential.
    - We are remote, with team members in Colorado, Quebec, Chicago, London, Mexico, Spain, Virginia, Brazil, Utah, and more! Most internal communication is also written, and often asynchronous. You'll want to be comfortable with not getting an immediate response for everything, but also know when you need to get an immediate response for something.
    - We are an unusually public team; you'd want to be comfortable working in open channels rather than secretively over in a dark corner.
    - This could be a great job for someone who loves Rails and Hotwire, thinks it's the best thing ever, and wants other people to discover how awesome it gets when you serve pages physically closer to your users.
    - We’re a real company – hopefully that goes without saying – and this is a real, according-to-Hoyle full-time job with health care for US employees, flexible vacation time, hardware/phone allowances, the standard stuff. The senior level compensation for this role is $160k-$200k USD plus equity. We are also looking for some junior level members to join the team. So we'd like to hear from you too!

    ## What you'll do

    - Help maintain and improve the initial "Run a Rails App" [Getting Started Guide](https://fly.io/docs/rails/). We'll help answer questions about the Fly.io side.
    - Be a public presence in the Rails community. This just means that you post about the cool things you're doing and engage with others in the community; in particular, it means being helpful to Rails developers everywhere, regardless of where they deploy their applications.
    - Create a sample Rails Hotwire app that demonstrates how to take advantage of what the Fly.io platform provides. You won't be alone on this either, we can help you with ideas and Fly.io platform features. We need someone who can handle the Rails side.
    - Help guide other Rails developers as they come the platform. That may include engaging in the [community.fly.io](https://community.fly.io/) forums where people have Rails or Hotwire specific questions. For Fly.io platform questions, you won't be able to answer those right away and that's expected, we have platform focused people who handle that. We need help with the easy "Oh, you just need to configure X!" stuff specific to Rails.
    - Write articles for [Ruby Dispatch](https://fly.io/ruby-dispatch/) that are genuinely helpful for other Rails Hotwire developers. That might be a how-to, coverage of an upcoming Rails or Hotwire feature, or an article about how to build Turbo Native Hotwire apps.

    ## You might enjoy this job if you

    - Enjoy using Hotwire and want more people to see the benefits you see. You enjoy sharing and showing others.
    - Want more people to understand Hotwire and start using it.
    - Have good instincts for balancing competing customer demands.
    - Are comfortable with software development. Our customers are developers, so knowing how to build apps is important. Bonus points if you are familiar with how to make it run on Fly.io.
    - Have general knowledge about deploying and supporting Rails apps in production.
    - Work on Turbo Native or Electron apps and want to make them feel faster and snappier by running the Rails server closer to users on Fly.io.
    - Like some structure, but are comfortable with not having a standard playbook for most things. You also like putting some structure in place where there isn't any, and are open to trying new things if something isn't working.

    ## You'll know you're succeeding in this job if

    - Other Rails Hotwire developers can follow your guides to successfully deploy their apps on Fly.io.
    - As you identify shortcomings, you update documentation or work with other team members to help improve Fly tooling to make deployment a smoother process.
    - Awareness of Fly.io in the Rails and Hotwire community is going up. People are talking about it without you starting the conversation.
    - You are influencing Rails developer behavior.

    ## How We Hire People

    We are weird about hiring. We’re skeptical of resumes and we don’t trust interviews (we’re happy to talk, though). We respect career experience but we aren’t hypnotized by it, and we’re thrilled at the prospect of discovering new talent.

    The premise of our hiring process is that we’re going to show you the kind of work we’re doing and then see if you enjoy actually doing it; “work-sample challenges”. Unlike a lot of places that assign “take-home problems”, our challenges are the backbone of our whole process; they’re not pre-screeners for an interview gauntlet.

    For this role, we’re going to ask you to create a "recipe" and a sample project. We'll give you the problem to solve and provide the recipe format to use. A recipe is a simple "problem and solution" format for writing a blog post.

    If you're interested, mail jobs+rails@fly.io. You can tell us a bit about yourself, if you like.
- :id: blog-logbook-2022-05-13
  :date: '2022-05-13'
  :category: blog
  :title: Logbook - 2022-05-13
  :author: fly
  :thumbnail: logbook-default-thumbnail.jpg
  :alt:
  :link: blog/logbook-2022-05-13
  :path: blog/2022-05-13
  :body: "\n<p class=\"lead\" >Here's absolutely everything people have made or changed
    here at Fly.io in the past week or so. Nothing else. Pretty sure. [Spin up an
    app](/docs/speedrun/) and see for yourself how it's going!</p>\n\n- **[Feature]**
    Provisioned new servers in `syd`, `iad`, and `dfw` which were very full. Added
    capacity should mean customers should no longer get provisioning issues when trying
    to deploy to these regions.\n- **[Feature]** The account deactivation page now
    asks users to leave any non-personal organizations they’re an admin of, instead
    of immediately sending them to support. (Being the only admin of a non-personal
    org blocks account deletion.) This change puts a little more power in users&#39;
    hands for self-service account deletion.\n- **[Feature]** Deployed our API with
    Ruby 3.1 &#127881; which should hopefully come with some performance improvements
    as well as allow us to better keep up with security patches going forward. This
    also opens up some better concurrency patterns for some key pieces of our API
    that could help us improve performance.\n- **[Feature]** Published a first draft
    of an internal doc with tips and tricks for working with our Elixir UI code. This
    should save team members headaches and help us keep our codebase sane.\n- **[Feature]**
    Published Ben Johnson&#39;s [blog post](https:///blog/all-in-on-sqlite-litestream/)
    on SQLite and Litestream, to show off  the minimalistic beauty of SQLite in a
    new Lite.\n- **[Feature/Fix]** Permanently disabled `consul-templaterb` on every
    node that runs fly-proxy. ([Related](https://fly.io/blog/a-foolish-consistency/).)
    Also found, and fixed, a memory leak in fly-proxy. Nodes now have nice CPU/RAM
    headroom they didn’t have before. \n- **[Feature]** Improved error messaging from
    our API for app and machine name validation, to save debugging time for users
    and support.\n- **[Feature]** When paid plans roll out, and folks start subscribing,
    their organization should now receive a personalized email address to send support
    inquiries to.\n- **[Feature]** Added three new workers to `ord`, as we had run
    out of capacity there. Customers should now be able to deploy without errors returning
    or VMs appearing in undesirable locations.\n- **[Fix]** Added a missing space
    on our app activity web UI that would otherwise show `X daysago`. We know our
    users have enough stress in their lives without seeing that.\n- **[Fix]** Fixed
    a bug with the [community forum](https://community.fly.io/) that wouldn't let
    some users with older Fly.io accounts get past the registration page.\n- **[Fix]**
    Put in a workaround for an MTU issue causing IPv6 connection errors for people
    on tunnels, DSL, etc. The bigger fix is going to take a little more work, but
    this should improve connectivity immediately.\n- **[Fix]** Fixed a bug causing
    us to not bill some customers. This is good for everyone because we need to make
    some money, goshdangit.\n- **[Fix]** Fixed volume metrics which had been broken
    months.\n"
- :id: blog-all-in-on-sqlite-litestream
  :date: '2022-05-09'
  :category: blog
  :title: I'm All-In on Server-Side SQLite
  :author: ben
  :thumbnail: litestream-thumbnail.jpg
  :alt:
  :link: blog/all-in-on-sqlite-litestream
  :path: blog/2022-05-09
  :body: "\n\n\n<p class=\"lead\">I'm Ben Johnson. I wrote BoltDB, an embedded database
    that is the backend for systems like etcd. Now I work at [Fly.io](https://fly.io),
    on [Litestream](https://litestream.io/). Litestream is an open-source project
    that makes SQLite tenable for full-stack applications through the power of ✨replication✨.
    If you can set up a SQLite database, [you can get Litestream working in less than
    10 minutes](https://fly.io/docs/speedrun/).</p>\n\nThe conventional wisdom of
    full-stack applications is the n-tier architecture, which is now so common that
    it's easy to forget it even has a name. It's what you're doing when you run an
    \"application server\" like Rails, Django, or Remix alongside a \"database server\"
    like Postgres. According to the conventional wisdom, SQLite has a place in this
    architecture: as a place to run unit tests.\n\nThe conventional wisdom could use
    some updating. I think that for many applications – production applications, with
    large numbers of users and high availability requirements – SQLite has a better
    place, in the center of the stack, as the core of your data and persistence layer.\n\nIt's
    a big claim. It may not hold for your application. But you should consider it,
    and I'm here to tell you why.\n\n## A Brief History Of Application Databases \n\n50
    years is not a long time. In that time, we've seen a staggering amount of change
    in how our software manages data.\n\nIn the beginning of our story, back in the
    '70s, there were [Codd's rules,](https://www.oreilly.com/library/view/sql-in-a/9780596155322/ch01s01s01.html)
    defining what we now call \"[relational databases](https://en.wikipedia.org/wiki/Relational_database)\",
    also known today as \"databases\". You know them, even if you don't: all data
    lives in tables; tables have columns, and rows are addressable with keys; C.R.U.D.;
    schemas; a textual language to convey these concepts. The language, of course,
    is SQL, which prompted a Cambrian explosion of SQL databases, from Oracle to DB2
    to Postgres to MySQL, throughout the '80s and '90s.\n\nIt hasn't all been good.
    The 2000s got us XML databases. But our industry atoned  by building some [great
    columnar databases](https://www.vertica.com/secrets-behind-verticas-performance/)
    during the same time. By the 2010s, we saw dozens of large-scale, open-source
    distributed database projects come to market.  Now anyone can spin up a cluster
    and query terabytes of data.\n\nAs databases evolved, so too did the strategies
    we use to plug them in to our applications. Almost since Codd, we've divided those
    apps into tiers. First came the database tier. Later, with [memcached](https://memcached.org/)
    and [Redis](https://redis.io), we got the caching tier. We've got [background
    job tiers](https://sidekiq.org/) and we've got [routing tiers](https://www.pgbouncer.org/)
    and distribution tiers. The tutorials pretend that there are 3 tiers, but we all
    know it's called \"n-tier\" because nobody can predict how many tiers we're going
    to end up with.\n\nYou know where we're going with this. Our scientists were so
    preoccupied with whether or not they could, and so on.\n\nSee, over these same
    five decades, we've also seen CPUs, memory, &amp; disks become hundreds of times
    faster and cheaper. A term that practically defines database innovation in the
    2010s is \"big data\". But hardware improvements have made that concept slippery
    in the 2020s. Managing a 1 GB database in 1996? A big deal. In 2022? Run it on
    your laptop, or a t3.micro.\n\nWhen we think about new database architectures,
    we're hypnotized by scaling limits. If it can't handle petabytes, or at least
    terabytes, it's not in the conversation. But most applications will never see
    a terabyte of data, even if they're successful. We're using jackhammers to drive
    finish nails.\n\n## The Sweet Release of SQLite\n\nThere's a database that bucks
    a lot of these trends. It's one of the most popular SQL databases in the world,
    so standardized it's an [official archival format of the Library of Congress](https://www.sqlite.org/locrsf.html),
    it's renowned for its reliability and its [unfathomably encompassing test suite](https://www.sqlite.org/testing.html),
    and its performance is so good that citing its metrics on a message board invariably
    starts an argument about whether it should be disqualified. I probably don't have
    to name it for you, but, for the one person in the back with their hand raised,
    I'm talking about [SQLite](https://www.sqlite.org).\n\nSQLite is an embedded database.
    It doesn't live in a conventional architectural tier; it's just a library, linked
    into your application server's process. It's the standard bearer of the \"[single
    process application](https://crawshaw.io/blog/one-process-programming-notes)\":
    the server that runs on its own, without relying on nine other sidecar servers
    to function.\n\nI got interested in these kinds of applications because I build
    databases. I wrote [BoltDB](https://github.com/boltdb/bolt), which is a popular
    embedded K/V store in the Go ecosystem. BoltDB is reliable and, as you'd expect
    from an in-process database, it performs like a nitro-burning funny car. But BoltDB
    has limitations: its schema is defined in Go code, and so it's hard to migrate
    databases. You have to build your own tooling for it; there isn't even a REPL.\n\nIf
    you're careful, using this kind of database can get you a lot of performance.
    But for general-purpose use, you don't want to run your database off the open
    headers like a funny car. I thought about the kind of work I'd have to do to make
    BoltDB viable for more applications, and the conclusion I quickly reached was:
    that's what SQLite is for.\n\nSQLite, as you are no doubt already typing into
    the message board comment, is not without its own limitations. The biggest of
    them is that a single-process application has a single point of failure: if you
    lose the server, you've lost the database. That's not a flaw in SQLite; it's just
    inherent to the design.\n\n## Enter Litestream\n\nThere are two big reasons everyone
    doesn't default to SQLite. The first is resilience to storage failures, and the
    second is concurrency at scale. Litestream has something to say about both concerns.\n\nHow
    Litestream works is that it takes control of SQLite's [WAL-mode journaling](https://sqlite.org/wal.html).
    In WAL mode, write operations append to a log file stored alongside SQLite's main
    database file. Readers check both the WAL file and the main database to satisfy
    queries. Normally, SQLite automatically checkpoints pages from the WAL back to
    the main database. Litestream steps in the middle of this: we open an indefinite
    read transaction that prevents automatic checkpoints. We then capture WAL updates
    ourselves, replicate them, and trigger the checkpointing ourselves.\n\n<div class=\"callout\">
    The most important thing you should understand about Litestream is that it's just
    SQLite. Your application uses standard SQLite, with whatever your standard SQLite
    libraries are. We're not parsing your queries or proxying your transactions, or
    even adding a new library dependency. We're just taking advantage of the journaling
    and concurrency features SQLite already has, in a tool that runs alongside your
    application. For the most part, your code can be oblivious to Litestream's existence.\n\nOr,
    think of it this way: you can build a Remix application backed by Litestream-replicated
    SQLite, and, while it's running, crack open the database using the standard `sqlite3`
    REPL and make some changes. It'll just work.\n\n  You can read more about [how
    this works here](https://litestream.io/how-it-works/).</div>\n\nIt sounds complicated,
    but it's incredibly simple in practice, and [if you play with it](https://litestream.io/getting-started/)
    you'll see that it \"just works\". You run the Litestream binary on the server
    your database lives on in \"replicate\" mode:\n\n```cmd\nlitestream replicate
    fruits.db s3://my-bukkit:9000/fruits.db\n```\n\nAnd then you can \"restore\" it
    to another location:\n\n```cmd\nlitestream restore -o fruits-replica.db s3://my-bukkit:9000/fruits.db\n```\n\nNow
    commit a change to your database; if you restore again then you'll see the change
    on your new copy.\n\n<aside class=\"right-sidenote\">We'll replicate almost anywhere:
    to S3, or Minio; to Azure, or Backblaze B2, or Digital Ocean or Google Cloud,
    or an SFTP server.</aside>\n\nThe ordinary way people use Litestream today is
    to replicate their SQLite database to S3 (it's remarkably cheap for most SQLite
    databases to live-replicate to S3). That, by itself, is a huge operational win:
    your database is as resilient as you ask it to be, and easily moved, migrated,
    or mucked with.\n\nBut you can do more than that with Litestream. The upcoming
    release of Litestream will let you live-replicate SQLite directly between databases,
    which means you can set up a write-leader database with distributed read replicas.
    Read replicas can [catch writes and redirect them to the leader](https://fly.io/blog/globally-distributed-postgres/);
    most applications are read-heavy, and this setup gives those applications a globally
    scalable database.\n\n<%= partial \"shared/posts/cta\", locals: {\n  title: \"Litestream
    SQLite, Postgres, CockroachDB, or any other database\",\n  text: \"They all work
    on Fly.io; we do built-in persistent storage and private networking for painless
    clustering, so it's easy to try new stuff out.\",\n  link_url: \"/docs/speedrun/\",\n
    \ link_text: \"Try Fly&nbsp;&nbsp;<span class='opacity-50'>&rarr;</span>\"\n}
    %>\n\n## You Should Take This Option More Seriously\n\nOne of my first jobs in
    tech in the early 2000s was as an Oracle Database Administrator (DBA) for an Oracle9i
    database. I remember spending hours poring over books and documentation to learn
    the ins  and  outs of the Oracle database. And there were a lot. The [administration
    guide](https://docs.oracle.com/cd/A91034_01/DOC/server.901/a90117.pdf) was almost
    a thousand pages—and that was just one of over [a hundred documentation guides](https://docs.oracle.com/cd/A91034_01/DOC/nav/docindex.htm).\n\nLearning
    what knobs to turn to optimize queries or to improve writes could make a big difference
    back then. We had disk drives that could only read tens of megabytes per second
    so utilizing a better index could change a 5-minute query into a 30 second query.\n\nBut
    database optimization has become less important for typical applications. If you
    have a 1 GB database, an NVMe disk can slurp the whole thing into memory in under
    a second. As much as I love tuning SQL queries, it's becoming a dying art for
    most application developers. Even poorly tuned queries can execute in under a
    second for ordinary databases.\n\nModern Postgres is a miracle. I've learned a
    ton by reading its code over the years. It includes a slew of features like a
    genetic query optimizer, row-level security policies, and a half dozen different
    types of indexes. If you need those features, you need them. But most of you probably
    don't.\n\nAnd if you don't need the Postgres features, they're a liability. For
    example, even if you don't use multiple user accounts, you'll still need to configure
    and debug host-based authentication. You have to firewall off your Postgres server.
    And more features mean more documentation, which makes it difficult to understand
    the software you're running. The documentation for Postgres 14 is nearly [3,000
    pages](https://www.postgresql.org/files/documentation/pdf/14/postgresql-14-US.pdf).\n\nSQLite
    has a subset of the Postgres feature set. But that subset is 99.9% of what I typically
    need. Great SQL support, [windowing](https://www.sqlite.org/windowfunctions.html),
    [CTEs](https://www.sqlite.org/lang_with.html), [full-text search](https://www.sqlite.org/fts5.html),
    [JSON](https://www.sqlite.org/json1.html). And when it lacks a feature, the data
    is already next to my application. So there's little overhead to pull it in and
    process it in my code.\n\nMeanwhile, the complicated problems I really need to
    solve aren't really addressed by core database functions. Instead, I want to optimize
    for just two things: latency &amp; developer experience.\n\nSo one reason to take
    SQLite seriously is that it's operationally much simpler. You spend your time
    writing application code, not designing intricate database tiers. But then there's
    the other problem.\n\n\n\n## The light is too damn slow\n\nWe're beginning to
    hit theoretical limits. In a vacuum, light travels about 186 miles in 1 millisecond.
    That's the distance from Philadelphia to New York City and back. Add in layers
    of network switches, firewalls, and application protocols and the latency increases
    further.\n\nThe per-query latency overhead for a Postgres query within a single
    AWS region can be up to a millisecond. That's not Postgres being slow—it's you
    hitting the limits of how fast data can travel. Now, handle an HTTP request in
    a modern application. A dozen database queries and you've burned over 10ms before
    business logic or rendering.\n\nThere's a magic number for application latency:
    **responses in 100ms or less feel instantaneous**. Snappy applications make happy
    users. 100ms seems like a lot, but it's easy to carelessly chew it up. The 100ms
    threshold is so important that people  [pre-render their pages and post them on
    CDNs](https://jamstack.org/) just to reduce latency.\n\nWe'd rather just move
    our data close to our application.  How much closer? Really close.\n\nSQLite isn't
    just on the same machine as your application, but actually built into your application
    process. When you put your data right next to your application, you can see per-query
    latency drop to 10-20 microseconds. That's micro, with a μ. A 50-100x improvement
    over an intra-region Postgres query.\n\nBut wait, there's more. We've effectively
    eliminated per-query latency. Our application is fast, but it's also simpler.
    We can break up larger queries into many smaller, more manageable queries, and
    spend the time we've been using to hunt down corner-casey N+1 patterns building
    new features.\n\nMinimizing latency isn't just for production either. Running
    integration tests with a traditional client/server database easily grows to take
    minutes locally and the pain continues once you push to CI. Reducing the feedback
    loop from code change to test completion doesn't just save time but also preserves
    our focus while developing. A one-line change to SQLite will let you run it in-memory
    so you can run integration tests in seconds or less.\n\n## Small, Fast, Reliable,
    Globally Distributed: Choose Any Four \n\nLitestream is distributed and replicated
    and, most importantly, still easy to get your head around. Seriously, [go try
    it](https://litestream.io/getting-started/). There's just not much to know.\n\nMy
    claim is this: by building reliable, easy-to-use replication for SQLite, we make
    it attractive for all kinds of full-stack applications to run entirely on SQLite.
    It was reasonable to overlook this option 170 years ago, when [the Rails Blog
    Tutorial](https://guides.rubyonrails.org/getting_started.html) was first written.
    But SQLite today can keep up with the write load of most applications, and replicas
    can scale reads out to as many instances as you choose to load-balance across.\n\nLitestream
    has limitations. I built it for single-node applications, so it won't work well
    on ephemeral, serverless platforms or when using rolling deployments. It needs
    to restore all changes sequentially which can make database restores take minutes
    to complete. We're [rolling out live replication](https://github.com/benbjohnson/litestream/issues/8),
    but the separate-process model restricts us to course-grained control over replication
    guarantees.\n\nWe can do better. For the past year, what I've been doing is nailing
    down the core of Litestream and keeping a focus on correctness. I'm happy with
    where we've landed. It started as a simple, streaming back up tool but it's slowly
    evolving into a reliable, distributed database. Now it's time to make it faster
    and more seamless, which is my whole job at Fly.io. There are improvements coming
    to Litestream — improvements that aren't at all tied to Fly.io! — that I'm psyched
    to share.\n\nLitestream has a new home at Fly.io, but it is and always will be
    an open-source project. My plan for the next several years is to keep making it
    more useful, no matter where your application runs, and see just how far we can
    take the SQLite model of how databases can work.\n"
- :id: blog-logbook-2022-05-05
  :date: '2022-05-05'
  :category: blog
  :title: Logbook - 2022-05-05
  :author: chris-n
  :thumbnail: logbook-default3-thumbnail.jpg
  :alt:
  :link: blog/logbook-2022-05-05
  :path: blog/2022-05-05
  :body: "\n\n<p class=\"lead\">Here's some of what we've been doing behind the scenes
    at Fly.io, to run your apps ever faster and better in their own VMs close to your
    users worldwide. If you haven't already, [deploy an app](/docs/speedrun/) (it'll
    only take a few minutes) and get to know us better; you'll be more invested in
    the plot.<p>\n\nFeatures and fixes are flying like dodgeballs in a school gym,
    and the Fly.io Changelog Enforcer could probably have done a better job patrolling&mdash;but
    let's have a look at our haul of updates since our [first Logbook post](/blog/logbook-2022-04-20/).
    \n\nThere's a fair amount of protein in this week's mix. Let's kick it off with
    improved remote builders you can activate for your organization!\n\n- **[Feature]**
    We updated remote builders for faster first deployments and fewer full disks.
    Builders will now:\n<ol>\n<li> Use regional Docker Hub mirrors, which especially
    improved initial Buildpack deployments</li>\n<li> Clean up unused build caches
    created by `RUN --mount=type=cache` along with standard image cleanup</li>\n<li>
    Run cleanup tasks on startup and shutdown.</li>\n</ol>\nThis behavior is the default
    for new builders. Existing builders must be destroyed with `fly apps destroy builder-app-name`
    to get the new behavior.\n- **[Feature]** Deployed some new security logging for
    our API that should both help us check a box for SOC2, and also start to give
    us a way to track down key security-related events for customers. More to come
    there. \n- **[Feature]** Shipped a self-service payment retry feature on our dashboard.
    [Official announcement](https://community.fly.io/t/new-feature-self-service-payment-retry/4990)\n-
    **[Feature]** Shipped our new /dashboard page. [Here’s the official announcement.](https://community.fly.io/t/new-feature-improved-dashboard-page/4985)\n-
    **[Feature]**  Thomas wrote about 10,000 words of security policy for SOC2 and
    policy will make us all safer, shut up, it will.\n- **[Feature]**  Improved performance
    of our existing WireGuard gateways (by fixing load issues) and added more capacity
    in each region. This does two things: 1) `fly deploy` and `ssh` commands will
    work more reliably with less timeouts and 2) it lays the groundwork for gateway
    redundancy. gateway redundancy is important for people who want to peer with other
    private networks for server communications.\n- **[Feature]** Spun up some new
    gateways so that customers have a better experience creating new peers.\n- **[Feature]**
    Merged some internal dev-setup Quality of Life things, to make it easier for new
    and old team members to jump into our API and be productive quickly.\n- **[Feature]**
    Changed the default size for new persistent volumes from 10GB to 3GB. This makes
    it less likely that new users will accidentally provision (and be charged for)
    more storage than they need for small projects.\n- **[Feature]** Re-enabled [OpenTelemetry](https://opentelemetry.io/),
    which should give us deeper visibility into how our API is running, allow us to
    better target problem areas, and catch issues earlier, with higher confidence.\n-
    **[Feature]** Major speedup to our proxy build pipeline, enabling faster features
    and fixes deployment.\n- **[Feature]**  Stood up new servers to increase capacity
    in Hong Kong.\n- **[Feature]** Updated the reset password page styling to make
    it easier to use.\n- **[Feature]** Added `fly.toml` validation and inline documentation
    to the VSCode [Even Better TOML](https://marketplace.visualstudio.com/items?itemName=tamasfe.even-better-toml)
    extension, to help users read and edit their `fly.toml` more easily.\n- **[Feature]**
    \ Added another wireguard gateway in IAD. Customers connect through gateways to
    their 6pn networks, IAD is our busiest region for customer network peers because
    most CI tools run there. This should make CI (github actions especially) more
    reliable.\n- **[Feature]** Deployed some initial changes to how we collect our
    database models for our GraphQL APIs that should help us manage API performance
    as GraphQL queries increase in complexity and depth. Also removed some unnecessary
    metric collection that should reduce our baseline database load.\n- **[Fix]**
    Deployed some fixes to the GraphQL API for edge cases folks were hitting, mostly
    with some of our older nodeproxy apps.\n- **[Fix]** Fixed organization dashboard
    not showing the latest invoice total. It only showed “-”.\n- **[Fix]** Fixed [fly
    monitor: Ctrl-C sets off an infinite loop of error messages](https://github.com/superfly/flyctl/issues/934)
    in `fly monitor` by porting it to the new CLI format and reusing the new deployment
    monitoring code.\n- **[Fix]** Personal Organizations are a subclass of Organization.
    Both of these models automatically encrypt/decrypt SSH key information. Last week
    we noticed that something that wasn&#39;t accessing those encrypted attributes
    was still throwing errors due to Vault connection issues. This is due to a mix
    of a discrepancy with how Ruby does inheritance/class attributes and an issue
    in our use of the library that does encryption/decryption for us. This should
    prevent us from loading those attributes unless they&#39;re actually used going
    forward.\n- **[Fix]** Updated an expired token that would have broken automatic
    bumps of `flyctl` in Homebrew. We didn't have any releases to bump while this
    was broken, so this should be an invisible fix. [PR](https://github.com/superfly/flyctl/issues/938)\n-
    **[Docs]** Updated our list of built-in metrics for completeness and included
    details about the more complex ones. [https://fly.io/docs/reference/metrics/#built-in-metrics](https://fly.io/docs/reference/metrics/#built-in-metrics)\n-
    **[Docs]** Updated regions doc, linking to other related doc pages. It’s now easier
    to find the commands for working with regions. Added missing Madrid region. [https://fly.io/docs/reference/regions/](https://fly.io/docs/reference/regions/)\n</ul>\n\n<%=
    partial \"shared/posts/cta\", locals: {\n  title: \"Shipping so fast it's bananas!\",\n
    \ text: \"We're working hard to make Fly.io the place to run all the stacks.\",\n
    \ link_url: \"https://fly.io/docs/speedrun/\",\n  link_text: \"Try Fly.io for
    free&nbsp;&nbsp;<span class='opacity-50'>&rarr;</span>\"\n} %>\n \n"
- :id: phoenix-files-recognize-digits-using-ml-in-elixir
  :date: '2022-05-05'
  :category: phoenix-files
  :title: Recognize digits using ML in Elixir
  :author: philip-brown
  :thumbnail: training-thumbnail.jpg
  :alt:
  :link: phoenix-files/recognize-digits-using-ml-in-elixir
  :path: phoenix-files/2022-05-05
  :body: |2


    <p class="lead">Here's Philip Brown, Elixir engineer [Yellow Flag](https://yflag.com/), with a tutorial on building a fullstack machine learning application using Nx, Axon, and LiveView. Coincidentally, Fly.io is the perfect place to run your LiveView apps. [Get started](https://fly.io/docs/elixir/).</p>

    (**Updated** March 2023 by [Tom Berman](https://twitter.com/tjcberman) for the official Axon releases.)

    Machine learning allows you to solve problems that were once totally unimaginable. The ability for a computer to take an image and tell you what it sees was once only possible in science fiction.

    Now, it's possible to build machine learning models that can do amazing things. However, part of the challenge of machine learning is that there are a lot of moving parts to learn. This means that solving a problem with machine learning can be a difficult task for an individual engineer.

    One of the big advantages Elixir has over similar programming languages is the integrated nature of what you have available to you. You can do a lot in Elixir without ever leaving the comfort of the language you love.

    ## What are we going to build?

    In this tutorial we're going to look at building out an end-to-end machine learning project using only Elixir. Boom! As if that wasn't enough, we're going to build a machine learning model that can recognize a handwritten digit. We'll train the model so that it will predict the digit from an image. We'll also build an application that can accept new handwritten digits from the user, and then display the prediction.

    <aside class="right-sidenote">
    **HOT TIP!**
    Be sure to grab the full code for this tutorial [here](https://github.com/philipbrown/handwritten-digits)!
    </aside>
    Here's a preview of what it looks like:

    <%= video_tag "handwritten-predictions.mp4?center&2/3&card", title: "Handwritten prediction animation" %>

    Let's get started!

    ## Setting up the project

    We're going to build this project using [Phoenix](https://www.phoenixframework.org), so the first thing we need is to create a new Phoenix project.

    If you don't already have Elixir installed on your computer, you can find instructions for your operating system on the [Elixir Website](https://elixir-lang.org/install.html).

    Once you have Phoenix installed, you can run the following command in terminal:

    ```bash
    $ mix archive.install hex phx_new
    ```

    With Elixir and Phoenix installed, we can create a new Phoenix project:

    ```
    $ mix phx.new digits --no-ecto
    ```

    I'm including the `--no-ecto` flag because we don't need a database for this project. This command should prompt you to install the project's dependencies. Hit `Y` on that prompt and wait for the dependencies to be installed.

    Once the dependencies are installed, follow the onscreen instructions to run your new Phoenix application and verify that everything was set up correctly.

    I'm also going to add the [Tailwind](https://github.com/phoenixframework/tailwind) package for styling the application. If you want to add Tailwind to your project add the following to the list of dependencies in your `mix.exs` file:

    ```elixir
    {:tailwind, "~> 0.1", runtime: Mix.env() == :dev}
    ```

    Then follow the configuration instructions listed [here](https://github.com/phoenixframework/tailwind#installation).

    ## Where will get our training data?

    One of the most important aspects of machine learning is having good, quality data to train on. When working on real life machine learning projects, expect to spend the majority of your time on the data.

    Fortunately for us, there is already a ready made dataset we can use. The [MNIST Database](https://en.wikipedia.org/wiki/MNIST_database) is a large dataset of handwritten digits that have already been prepared and labeled. This dataset is commonly used for training image recognition machine learning models. The dataset consists of images of handwritten digits from 0 - 9 that are already labeled.

    ## Prepare the project for machine learning

    Next, we need to set up the machine learning model. The Elixir ecosystem has a number of exciting packages that can be used for training machine learning models.

    The [Nx](https://github.com/elixir-nx/nx/tree/main/nx#readme) package is the foundation of machine learning in Elixir. Nx allows us to manipulate our data using tensors, which are essentially efficient multi-dimensional arrays. When we say "tensor" below, just think "multi-dimensional array".

    Next, we have [EXLA](https://github.com/elixir-nx/nx/tree/main/exla#readme), which provides hardware acceleration for training our models. Crunching the numbers of machine learning is a very intensive process, but EXLA makes that much faster.

    [Axon](https://github.com/elixir-nx/axon) builds on top of Nx and makes it possible for us to create neural networks in Elixir.

    Finally we have the [Scidata](https://github.com/elixir-nx/scidata) package, which provides conveniences for working with machine learning datasets, including MNIST.

    So, the first thing we need to do is to add those dependencies to our `mix.exs` file:

    ```elixir
    {:axon, "~> 0.5.1"},
    {:exla, "~> 0.5.1"},
    {:nx, "~> 0.5.1"},
    {:scidata, "~> 0.1.5"}
    ```

    Then we can install our new dependencies from a terminal:

    ```bash
    $ mix deps.get
    ```

    We also need to set the default backend in `config.exs`

    ```elixir
    import Config

    # Set the backend for Nx
    config :nx, :default_backend, EXLA.Backend
    ```

    ## Working with our training data

    There's a couple of steps required for getting and transforming the training data, so we'll start building out a module that can encapsulate everything that we're building:

    ```elixir
    defmodule Digits.Model do
      @moduledoc """
      The Digits Machine Learning model
      """
    end
    ```

    First up we'll add a `download/0` function that downloads the training data for us. We're just delegating to the `Scidata` package for that.

    ```elixir
    def download do
      Scidata.MNIST.download()
    end
    ```

    This function returns a tuple of `{images, labels}`. However, we want to transform the images and labels so we can use them in our model.

    First, we'll use the following function to transform the images:

    ```elixir
    def transform_images({binary, type, shape}) do
      binary
      |> Nx.from_binary(type)
      |> Nx.reshape(shape)
      |> Nx.divide(255)
    end
    ```

    The image data from the download includes the following:

    - Binary data - This is the image data as a binary.
    - The type of the data - In this example the type is `{:u, 8}` unsigned integer.
    - The shape of the data - In this example the shape is `{60000, 1, 28, 28}`. This means there are 60000 images, which all have 1 channel (ie they're black and white) and have a dimension of 28x28.

    We can convert the binary into a tensor using Nx.

    If we open up `iex` we can visualize the image data. Run the following command in a terminal to open up `iex` with our project loaded:

    ```bash
    $ iex -S mix
    ```

    Next, we run the following code:

    ```elixir
    {images, labels} = Digits.Model.download()

    images
    |> Digits.Model.transform_images()
    |> Nx.slice_axis(0, 1, 0)
    |> Nx.reshape({1, 1, 28, 28})
    |> Nx.to_heatmap()
    ```

    You should see the first handwritten digit of the dataset. This is what it looks like:

    ![Sample image of number 5 digit heatmap](./example-digit-no-5.png?centered&1/3&card)

    We can see the corresponding label for the image too. Let's see how to do that.

    First, we pattern match the binary data and type from the downloaded label data.

    ```elixir
    {binary, type, _} = labels
    ```

    Then we convert the binary to a tensor and "slice" off the first item as our example.

    ```elixir
    binary
    |> Nx.from_binary(type)
    |> Nx.new_axis(-1)
    |> Nx.slice_axis(0, 1, 0)
    ```

    The first label should be a `5`. We'll refactor that code in our transform function to get the labels.

    ```elixir
    def transform_labels({binary, type, _}) do
      binary
      |> Nx.from_binary(type)
      |> Nx.new_axis(-1)
      |> Nx.equal(Nx.tensor(Enum.to_list(0..9)))
    end
    ```

    The labels of the training data are used as targets for the model's predictions. For each image, we know how it was labelled. During training, the model uses the labels to compare it's predictions with the actual correct result. The guessing is adjusted to give better results in the future.

    Currently, the labels are integers from 0 - 9. You can think of them as 10 different categories. In our case, the categories are integers, but when training a machine learning model, you might have categories such as colors, sizes, types of animals, etc.

    So we need to convert our categories into something that the machine learning model can understand. The way we do this is to convert the label into a tensor of size `{1, 10}`, where `10` is the number of categories you have.

    For example:

    ```elixir
    #Nx.Tensor<
      u8[1][10]
      [
        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
      ]
    >
    ```

    In this example, the long list of numbers has a `1` is in the first position. This represents the first category. In our case, that is the number "0", but it could also be the color "red", the size "small", or the type of animal "dog".

    The second category would be:

    ```elixir
    #Nx.Tensor<
      u8[1][10]
      [
        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]
      ]
    >
    ```

    And so on.

    This process is called one-hot encoding.

    You can see what the first label of the training data is when it's been one-hot encoded using the following chunk of code. (Still in `iex`):

    ```elixir
    labels
    |> Digits.Model.transform_labels()
    |> Nx.slice_axis(0, 1, 0)
    ```

    This should output the following tensor:

    ```elixir
    #Nx.Tensor<
      u8[1][10]
      [
        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]
      ]
    >
    ```

    Remember, we're working with the number `5` right now. This tensor is an array  of zeros with a `1` in the index for the 5. Counting from 0, the `1` is in the 5th spot.

    Next, we convert the images and labels into batches. During training, we feed the data into the model in batches rather all at once. In this example we're using a batch size of 32. This means each batch will include 32 examples.

    ```elixir
    batch_size = 32

    images =
      images
      |> Digits.Model.transform_images()
      |> Nx.to_batched(batch_size)
      |> Enum.to_list()

    labels =
      labels
      |> Digits.Model.transform_labels()
      |> Nx.to_batched(batch_size)
      |> Enum.to_list()
    ```

    Next, we zip the images and labels together using `Enum.zip`. Then  we split the dataset into training, testing, and validation datasets. We need to use the majority of the data for training, and then a portion of the data to use to test the accuracy of the model. In this example we're using 80% of the data for training and validation, and the remaining 20% unseen data will be used for testing.

    ```elixir
    data = Enum.zip(images, labels)

    training_count = floor(0.8 * Enum.count(data))
    validation_count = floor(0.2 * training_count)

    {training_data, test_data} = Enum.split(data, training_count)
    {validation_data, training_data} = Enum.split(train, validation_count)
    ```

    Phew! That may seem pretty heavy but we've already achieved a lot! We've downloaded our training data, preprocessed it, and got it ready for building the model. During a real-life machine learning project you will likely spend a lot of time at acquiring, cleaning, and manipulating the data. We're now in a great position to build and train the model!

    ## Building the model

    Next up we'll use Axon to build the machine learning model. Add a new function to the `Digits.Model` module with the following code:

    ```elixir
    def new({channels, height, width}) do
      Axon.input("input_0", shape: {nil, channels, height, width})
      |> Axon.flatten()
      |> Axon.dense(128, activation: :relu)
      |> Axon.dense(10, activation: :softmax)
    end
    ```

    First we need to set the input shape of the model to fit our training data. Next we flatten the previous layer and add a dense layer that uses relu as the activation function. Finally the output layer returns one of 10 labels (because our labels are 0 - 9).

    You can experiment with different model configurations to get different results.

    ## Training the model

    Now that we have the data and the model, we can start training. Add another function to `Digits.Model` to train the model:

    ```elixir
    def train(model, training_data, validation_data) do
      model
      |> Axon.Loop.trainer(:categorical_cross_entropy, Axon.Optimizers.adam(0.01))
      |> Axon.Loop.metric(:accuracy, "Accuracy")
      |> Axon.Loop.validate(model, validation_data)
      |> Axon.Loop.run(training_data, %{}, compiler: EXLA, epochs: 10)
    end
    ```

    We're using categorical cross entropy because we're matching multiple labels and the "adam" optimizer because it gives fairly good results. We'll track a single accuracy metric, and we'll also validate the model with our validation data from earlier to ensure the model is not over-fitting on the training data.

    Finally we'll use EXLA as the compiler and we'll train for 10 epochs. An epoch is one cycle through the data, so this means we'll cycle through the data 10 times during training.

    ## Testing our model

    We can also test our model after training to get an idea of how well it performs. Add the following function to `Digits.Model`:

    ```elixir
    def test(model, state, test_data) do
      model
      |> Axon.Loop.evaluator()
      |> Axon.Loop.metric(:accuracy, "Accuracy")
      |> Axon.Loop.run(test_data, state)
    end
    ```

    This tests the model using previously unseen data to check the accuracy of the predictions.

    ## Saving and loading our model

    The final thing to do is to add the ability to save and load the model. Our model is just an Elixir struct, so saving and loading it is simply a case of using Erlang's `binary_to_term/1` and `term_to_binary/1` functions:

    ```elixir
    def save!(model, state) do
      contents = Axon.serialize(model, state)

      File.write!(path(), contents)
    end

    def load! do
      path()
      |> File.read!()
      |> Axon.deserialize()
    end

    def path do
      Path.join(Application.app_dir(:digits, "priv"), "model.axon")
    end
    ```

    ## Running the model

    Now that we've written all the code to transform the data, train, and test our machine learning model, we'll write a mix command to put it all together:

    <aside class="right-sidenote">
    **NOTE:** The mix task locally caches the downloaded Minst data set so it doesn't download it every time it's run.
    </aside>
    ```elixir
    defmodule Mix.Tasks.Train do
      use Mix.Task

      @requirements ["app.start"]

      alias Digits

      def run(_) do
        {images, labels} = load_mnist()

        images =
          images
          |> Digits.Model.transform_images()
          |> Nx.to_batched(32)
          |> Enum.to_list()

        labels =
          labels
          |> Digits.Model.transform_labels()
          |> Nx.to_batched(32)
          |> Enum.to_list()

        data = Enum.zip(images, labels)

        training_count = floor(0.8 * Enum.count(data))
        validation_count = floor(0.2 * training_count)

        {training_data, test_data} = Enum.split(data, training_count)
        {validation_data, training_data} = Enum.split(training_data, validation_count)

        model = Digits.Model.new({1, 28, 28})

        Mix.Shell.IO.info("training...")

        state = Digits.Model.train(model, training_data, validation_data)

        Mix.Shell.IO.info("testing...")

        Digits.Model.test(model, state, test_data)

        Digits.Model.save!(model, state)

        :ok
      end

      defp load_mnist() do
        if !File.exists?(path()) do
          save_mnist()
        end

        load!()
      end

      defp save_mnist do
        Digits.Model.download()
        |> save!()
      end

      defp save!(data) do
        contents = :erlang.term_to_binary(data)

        File.write!(path(), contents)
      end

      defp load! do
        path()
        |> File.read!()
        |> :erlang.binary_to_term()
      end

      defp path do
        Path.join(Application.app_dir(:digits, "priv"), "mnist.axon")
      end
    end
    ```

    We can run the training with the following command:

    ```bash
    mix train
    ```

    ## Setting up the LiveView

    Now that we have a trained machine learning model, we can set up a LiveView to accept new handwritten digits, and then display the predicted results.

    First, we add a new live route to our router file in `lib/digits_web/router.ex` :

    ```elixir
    scope "/", DigitsWeb do
      pipe_through :browser

      live "/", PageLive, :index
    end
    ```

    Next, we create a new file under `lib/digits_web/live` called `page_live.ex`. This is our LiveView module where all the interactivity happens:

    ```elixir
    defmodule DigitsWeb.PageLive do
      @moduledoc """
      PageLive LiveView
      """

      use DigitsWeb, :live_view
    end
    ```

    When a user submits a new handwritten digit, the machine learning model makes a prediction on what digit was written and then the LiveView displays the prediction to the user. However, when the LiveView is first loaded, there isn't a prediction to display. So, first, we need to initiate the `prediction` assign value as `nil` inside the `mount/3` callback:

    ```elixir
    def mount(_params, _session, socket) do
      {:ok, assign(socket, %{prediction: nil})}
    end
    ```

    Next, the `render/1` function is responsible for rendering the LiveView:

    ```elixir
    def render(assigns) do
      ~H"""
      <div id="wrapper" phx-update="ignore">
        <div id="canvas" phx-hook="Draw"></div>
      </div>

      <div>
        <button phx-click="reset">Reset</button>
        <button phx-click="predict">Predict</button>
      </div>

      <%%= if @prediction do %>
      <div>
        <div>
          Prediction:
        </div>
        <div>
          <%%= @prediction %>
        </div>
      </div>
      <%% end %>
      """
    end
    ```

    Notice above that we have a `div` with the `id` of "canvas". This will have an HTML canvas attached.  The  `phx-hook`  uses Javascript to let us interact with the canvas. The canvas `div` is wrapped in another `div` with the `phx-update="ignore"`  because we don't want Phoenix to update it.

    Next are two buttons, one to reset the canvas and one to make a prediction from what the user drew. Each of these buttons are wired up to `phx-click` triggers.

    Finally, if we have a `prediction`, it is displayed.

    <%= partial "shared/posts/cta", locals: {
      title: "Fly.io ❤️ Elixir",
      text: "Fly.io is a great way to run your Phoenix LiveView app close to your users. It's really easy to get started. You can be running in minutes.",
      link_url: "https://fly.io/docs/elixir/",
      link_text: "Deploy a Phoenix app today!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>"
    } %>

    ## Adding the canvas

    Next, we need some input from the user. We could let the user upload images using Phoenix's LiveView upload functionality, but a better (and way cooler) experience is to let the user draw new examples directly into the LiveView.

    There's a handy NPM package called [draw-on-canvas](https://www.npmjs.com/package/draw-on-canvas) that make this part easy.

    To install it, `cd` into the `assets` directory and run the following command in a terminal:

    ```bash
    $ npm i draw-on-canvas
    ```

    This installs the `draw-on-canvas` into the project.

    Now we connect the `draw-on-canvas` package to our LiveView via a hook. Open up `assets/js/app.js` and import the `draw-on-canvas` package:

    ```javascript
    import Draw from 'draw-on-canvas'
    ```

    Let's create a new `Hooks` object:

    ```javascript
    let Hooks = {}
    ```

    Remember to register the hook object in the `LiveSocket`:

    ```javascript
    let liveSocket = new LiveSocket("/live", Socket, {
      params: {_csrf_token: csrfToken},
      hooks: Hooks
    })
    ```

    Next we add a new `Draw` hook:

    ```javascript
    Hooks.Draw = {}
    ```

    We need to implement the `mounted` function, which is called when the hook is mounted. This is where we set up the canvas:

    ```javascript
    Hooks.Draw = {
      mounted() {
        this.draw = new Draw(this.el, 384, 384, {
          backgroundColor: "black",
          strokeColor: "white",
          strokeWeight: 10
        })
      }
    }
    ```

    When we open the app in a browser, we should see a black square canvas that we can draw on!

    ## Interacting with the canvas

    Remember back in our `PageLive` module, we added two buttons for interacting with the canvas.

    The first button is used to reset the canvas. When the button is pressed we send a message to the client to reset the canvas. The `push_event` function makes this easy.

    Our new "reset" event handler in `PageLive` looks like this:

    ```elixir
    def handle_event("reset", _params, socket) do
      {:noreply,
        socket
        |> assign(prediction: nil)
        |> push_event("reset", %{})
      }
    end
    ```

    When the reset button is clicked, the `phx-click` trigger sends the `reset` event to the server. We then push an event called `reset` to the client. We also set the prediction to `nil` in the socket assigns.

    On the Javascript side, we add a `handleEvent`, that listens for the `reset` event, and resets the canvas:

    ```javascript
    this.handleEvent("reset", () => {
      this.draw.reset()
    })
    ```

    Next, let's make our "predict" button work. We want to grab the contents of the canvas as an image. Again, we send a message to the client from the `PageLive` LiveView module:

    ```elixir
    def handle_event("predict", _params, socket) do
      {:noreply, push_event(socket, "predict", %{})}
    end
    ```

    In the `mounted` callback, we add another `handleEvent`. This grabs the contents of the canvas as a data URL and sends it to the server using `pushEvent`:

    ```javascript
    this.handleEvent("predict", () => {
      this.pushEvent("image", this.draw.canvas.toDataURL('image/png'))
    })
    ```

    ## Making predictions

    Now that we hooked up the buttons to reset the canvas and send up the canvas contents to make a prediction, we will use the image from the canvas as a new input to our machine learning model.

    We can accept the image data URL from the client using another `handle_event/3` callback function:

    ```elixir
    def handle_event("image", "data:image/png;base64," <> raw, socket) do
      name = Base.url_encode64(:crypto.strong_rand_bytes(10), padding: false)
      path = Path.join(System.tmp_dir!(), "#{name}.png")

      File.write!(path, Base.decode64!(raw))

      prediction = Digits.Model.predict(path)

      File.rm!(path)

      {:noreply, assign(socket, prediction: prediction)}
    end
    ```

    In this function, we use a binary pattern matching on the `params` to get the image data. Next, we generate a random file name and create a path to a temporary directory for storing the image. Then we decode the image data and write it to the path.

    Next we pass the path into the `Digits.Model.predict/1` function and return a prediction. The prediction result is a number between 0 and 9. We'll write that function next.

    Finally, we delete the image file and assign the prediction to the socket for display in our LiveView.

    Before we can use the user's drawing with our model, we need to prepare the image. We need to:

    - Convert it to grayscale to reduce the number channels from 3 to 1
    - Resize it to 28 x 28

    The `Evision` library can do these changes for us. Let's add it as a dependency in our `mix.exs`  file now:

    ```elixir
      {:evision, "~> 0.1.28"}
    ```

    Install the dependency using:

    ```
    mix deps.get
    ```

    In the `Digits.Model` module, let's add a new function for making a prediction.

    ```elixir
    def predict(path) do
      {:ok, mat} = Evision.imread(path, flags: Evision.Constant.cv_IMREAD_GRAYSCALE())
      {:ok, mat} = Evision.resize(mat, [28, 28])

      data =
        Evision.Nx.to_nx(mat)
        |> Nx.reshape({1, 28, 28})
        |> List.wrap()
        |> Nx.stack()
        |> Nx.backend_transfer()

      {model, state} = load!()

      model
      |> Axon.predict(state, data)
      |> Nx.argmax()
      |> Nx.to_number()
    end
    ```

    First, we read the image path and convert it to grayscale. This reduces the number of channels from 3 to 1. Then we resize the image to `28` x `28`.

    We also need to convert the image data to an Nx tensor and reshape it to an expected correct shape. Our machine learning model expects a "batch" of inputs, and so we'll wrap the tensor using `List.wrap/1` and then stack it using `Nx.stack/1`.

    Next, we  load the `model` and the `state`, using the `load!/0` function from earlier. Ideally you wouldn't be loading the `model` and `state` for each prediction, but it's fine for our basic example.

    We pass the `model`, `state` and `data` into the `Axon.predict/4` function. One thing to note is, you will need to add `require Axon` to the `Digits.Model` module because `Axon.predict/4` is actually a macro.

    The `Axon.predict/4` function returns a prediction in the form of a one-hot encoded tensor. We use the `Nx.argmax/1` function to convert it to a tensor that contains a single scalar value between 0 and 9, and then we use `Nx.to_number/1` to return the value as a number.

    Our predicted number is set as the `prediction` is the LiveView assigns, displaying it to the user.

    ## We built an end-to-end machine learning application in Elixir!

    Wow! Check out what we just did!

    We built an end-to-end machine learning application using Elixir! We trained a model from scratch. We used LiveView for interactive, real-time application input from the user. We ran predictions and displayed the results interactively.

    One of the most amazing things here was that we did it all using Elixir and didn't need external machine learning tools or languages. Machine learning in Elixir is still maturing, but I hope this inspires you to try something new in your own project.

    Full code for this tutorial is found at [philipbrown/handwritten-digits-elixir](https://github.com/philipbrown/handwritten-digits).
- :id: blog-accessibility-clearing-the-fog
  :date: '2022-04-25'
  :category: blog
  :title: 'Accessibility and Real-time Apps: Clearing Fog and Picking Fruit'
  :author: nolan
  :thumbnail: accessibility_02-thumbnail.png
  :alt: A murky scene, clouds parting to reveal a full moon with the word 'labels'
    misspelled 'lables.'
  :link: blog/accessibility-clearing-the-fog
  :path: blog/2022-04-25
  :body: "\n\n<p class=\"lead\">Here's Nolan Darilek on making sure [LiveBeats](https://fly.io/blog/livebeats/)&mdash;or
    any web app&mdash;has a solid, accessible foundation to build on. If your app
    is already solid, [deploy it in minutes](/docs/speedrun/) on Fly.io's global server
    network.\n</p>\n\nHey, everyone. [Last time](/blog/intro-to-accessibility/) we
    talked a bit about what accessibility is, why it's important, and how you can
    incorporate it into your process. Today, using the time-travel superpowers of
    Git, I'll take you along as I start making LiveBeats more accessible for screen
    reader users.\n\n[LiveBeats](https://fly.io/blog/livebeats/) is a reference Phoenix
    LiveView app with real-time social features. We'd be reckless not to make sure
    all the parts are in good working order before real-time updates start moving
    them around on us. Let's set our time machines to [LiveBeats commit `fad3706`](https://github.com/fly-apps/live_beats/commit/fad37064db742375add78a8c85531d9dde5028d3)&mdash;or
    mid-November. Thanksgiving is on the horizon, winter is coming, and I'm starting
    to dig into this little app called LiveBeats *(cue dreamlike harp glissandos)*...\n
    \n## Low-hanging fruit\n \nSometimes, setting out to make an app accessible feels
    like surveying fog-shrouded terrain. Is that supposed to be a button? What's all
    that clutter over there? Fortunately, we can clear away a lot of the murk with
    some easy early wins.\n \nLabels are a great way to give some definition to the
    landscape. You've got a few tools to help with this, each of which has its own
    use cases:\n* The faithful `alt` attribute is essential for images that have meaning,
    whether they're beautiful photos or actionable controls.\n* Embedded SVGs are
    a separate beast entirely. An SVG image doesn't use `alt` at all, but instead
    has a `<desc/>` child tag. Also, since SVGs are their own unique element, there's
    a [*lot* you can do with child tags and attributes to make these more accessible](https://www.deque.com/blog/creating-accessible-svgs/).\n*
    For elements that aren't images, and for which there is no easy way to add text,
    the `aria-label` attribute adds a label that is only visible to assistive technologies.\n
    \n\nSo, back to [`fad3706`](https://github.com/fly-apps/live_beats/commit/fad37064db742375add78a8c85531d9dde5028d3).
    We use `aria-label` here to add accessible labels to controls; for example, this
    button that skips to the previous track in the playlist:\n\n```erb\n<button type=\"button\"
    class=\"sm:block xl:block mx-auto scale-75\" phx-click={js_prev(@own_profile?)}
    aria-label=\"Previous\">\n```\n\n![\"Previous\", \"play\", and \"next\" buttons;
    all have SVG icons and no visible text.](prev.png)\n\n<aside class=\"right-sidenote\">\nBecause
    this is all in a HEEx template, our `aria-label` can be conditional, e.g. `aria-label={if
    @playing do \"Pause\" else \"Play\" end}`\n</aside>\n\nIdeally, you'd just add
    text as a child of the button, particularly since users with cognitive disabilities
    may struggle with what a given icon means. If you're using an image for your button,
    add an `alt` attribute. Where neither is the case, `aria-label` is the ticket.\n
    \nLabeling meaningful elements is only part of the story, however. *Hiding* irrelevant
    images can be just as important. If my screen reader presents a thing, then it
    should be relevant to me. Decorations and placeholders usually aren't, and should
    be hidden, either by applying `aria-hidden=\"true\"` to the element, or by adding
    a blank `alt` attribute to images.\n \nYou can see an example of hiding icons
    in [`f7db67f`](https://github.com/fly-apps/live_beats/commit/f7db67f636441a398c5ce0aa3e3b62893e85e96f):\n
    \n```erb\n<span class=\"mt-1\"><.icon name={:user_circle} aria-hidden=\"true\"/></span>\n```\n\n![A
    generic head-and-shoulders icon next to the username of the playlist owner](user_circle.png)\n\nHiding
    a decorative icon saves my screen reader from reading what appears to be an empty
    element. It's a small thing, but half a dozen small things add up.\n \n## Role
    with it, but not too far\n \nWe've labeled some things and hidden others. The
    fog is burning away, and we have a slightly clearer view of the land around us.
    It's now time to fill in the details.\n \nRoles are powerful tools that make one
    thing appear to be another. Say you have a `<div/>` tag that needs to work as
    a button. You can make it *seem* like a button like so:\n \n```html\n<div role=\"button\">No
    really, click me!</div>\n```\n \nUnfortunately, the above is the equivalent of
    slapping on a fake mustache and glasses. To my screen reader, it now *looks* like
    a button. But the role alone doesn't make it *act* like a button; it doesn't focus
    when I tab, and doesn't click when I press `Space` or `Enter`. \n\nIt's better
    to use a `<button/>` and get this button behavior built in. But if you can't,
    or if you're building a widget like a dropdown menu or [tree](https://www.w3.org/wiki/TreeView),
    roles are crucial. \n\nRoles are great for signposting semantic regions on pages.
    Here are the most important:\n\n* `role=\"navigation\"`: Use this, or a `<nav/>`
    element, for application menus.\n* `role=\"main\"`: Use this, or the `<main/>`
    element, for the main area of your page. There should only be one `main` element
    or role per page.\n* `role=\"article\"`: Use this, or the `<article/>` element,
    for self-contained page elements representing posts in a blog or forum.\n \nAll
    of the above roles have semantic HTML equivalents. This isn't universally true&mdash;there
    isn't a semantic HTML equivalent of a tree control, for instance&mdash;but you
    should prefer HTML where possible.\n\nThere's a lot to unpack there. But as an
    example, [commit `5cf58b2`](https://github.com/fly-apps/live_beats/commit/5cf58b2b77843a653fc0a4a403ecfbe6b8a2245f)
    combines some of what we've discussed to achieve more intuitive navigation within
    LiveBeats. \n\nWe use the `<main>` element to surround the page content that changes
    when new routes are visited. Using `role=\"main\"` would serve the same purpose,
    though less elegantly.\n\nThen, we use another technique to associate names with
    areas of the page:\n\n```erb\n    <!-- player -->                                                                                                                \n
    \   ...\n    <div id=\"audio-player\" phx-hook=\"AudioPlayer\" class=\"w-full\"
    role=\"region\" aria-label=\"Player\" >                                \n      <div
    phx-update=\"ignore\">                                                                                                    \n
    \       <audio></audio>                                                                                                            \n
    \     </div>                                                                                                                        \n
    \   </div>\n```\n \nThis last snippet does a couple of things. Adding a new region
    landmark to the page with `role=\"region\"` gives us easier navigation into and
    out of it, via hotkey. In [NVDA](https://www.nvaccess.org/about-nvda/), I can
    jump between this and other interesting regions of the page by pressing `d`.  Then,
    the `aria-label` attribute ties the label \"Player\" with that region, such that
    navigating into or out of the player explicitly announces the focus entering or
    leaving the player. \n\nIf you find yourself writing documentation with phrases
    like \"In the player, click *Pause*\" or \"Find this in the messages section of
    the dashboard,\" named regions help make those instructions more relevant to screen
    reader users. Styling may make the player region visually obvious, but the `region`
    role and \"Player\" label makes it very apparent when the caret enters the audio
    player. \n\nRoles are powerful in large part because they're promises. If you
    promise your user that a thing is a button, then it needs a `tabindex` for keyboard
    focus, as well as handlers to activate it when `Enter` or `Space` is pressed.
    If you're curious about what these promises are, the [WAI-ARIA Authoring Practices](https://www.w3.org/TR/wai-aria-practices-1.1/)
    document is an exhaustive source of all commonly expected keyboard behaviors for
    a bunch of widget types. Want to know how a list box should behave when arrowing
    down past the last element? This resource has you covered.\n \nThat said, it is
    very possible to overuse roles in your application. Here are two examples of role
    misuse I often find in the wild.\n \n### Menus aren't what you think\n \n`role=\"menu\"`
    is not intended for every list of links in your app that might possibly be a menu
    if you tilt your head and the sunlight lands just so. These are either application
    menus like you'd find in a menu bar, or standalone dropdown menus that capture
    keyboard focus and expand when you click on them. Misusing `role=\"menu\"` won't
    necessarily make a given control unusable, but it does cause confusion by presenting
    an element as something it isn't.\n\n\n<%= partial \"shared/posts/cta\", locals:
    {\n  title: \"Fly.io is not just for Elixir/Phoenix/LiveView!\",\n  text: \"Any
    app that benefits from low-latency connections to users worldwide will love Fly.io.\",\n
    \ link_url: \"https://fly.io/docs/speedrun/\",\n  link_text: \"Try Fly.io for
    free&nbsp;&nbsp;<span class='opacity-50'>&rarr;</span>\"\n} %>\n \n## This one
    weird trick makes your entire app inaccessible in 30 seconds!\n \nThis one gets
    its own section because it's *that* bad. If you want your app to be so inaccessible
    that most screen reader users turn away in the first few seconds, slap `role=\"application\"`
    on one of the top-level elements. Explaining just why this is takes a bit of effort,
    so please bear with me.\n \nBroadly speaking, screen reader users browse the web
    in one of two modes. Caret browsing mode presents pages as if they're documents.
    We can arrow through their contents line by line or character by character, select
    and copy text, etc. You can experience a limited version of this by pressing `F7`
    in most browsers, though screen readers enable this mode by default. They also
    add conveniences like jumping between headings with `h`, buttons with `b`, landmarks
    with `d`, etc.\n \nFocus mode, sometimes called \"forms mode,” because it enables
    automatically when form fields are highlighted, passes keys directly through to
    the application. This is generally what you want when typing into forms, or when
    using keyboard commands that you don't want filtered out by caret browsing. Incidentally,
    focus mode is closest to how native applications behave; you don't normally read
    application screens as if they were documents, or jump between controls by element
    type.\n \nThat, more or less, is what `role=\"application\"` enforces. It enables
    focus mode by default, meaning your screen reader users can't explore via caret
    browsing and its associated commands. It also changes the way the site presents
    itself to screen reader users, such that it appears to be a native app, and not
    a web document with a built-in interaction model. It's a promise that you'll supply
    it all; you've gone through the extraordinary effort to ensure all your controls
    are focusable, they all have sensible keyboard behaviors, and that your users
    won't be struggling to read text that changes.\n\nYou might feel you have to use
    `role=\"application\"` just because, well, you're making an application. But this
    is often not the right choice. If you've ever been annoyed by an Electron app's
    non-native behavior, multiply that by about 11 and you're in the ballpark of how
    frustrating `role=\"application\"` can be when it's not backed up by thorough
    and consistent handling of every interaction. While I've got this podium: Slack,
    you're one of the biggest perpetrators of this, and need to cut it out. Most of
    my usability issues with Slack spring from its use of `role=\"application\"` everywhere,
    with haphazard and nonstandard workarounds in place to patch in what HTML provides
    for free. \n \n## Closing thoughts\n\nWhile this post has been light on the real-time
    aspects of LiveBeats, harvesting this low-hanging fruit is an important step to
    making any web app accessible. We certainly don't want it in the way next time,
    when we'll start going live, exploring the challenges and methods to accessibly
    presenting changes as they roll in over the wire. Meanwhile, if you have questions
    or topics you'd like covered, drop a line [here](https://community.fly.io/t/accessibility-for-real-time-web-apps/4395)
    and I'll try to work them in. Thanks for reading!\n\n"
- :id: blog-logbook-2022-04-20
  :date: '2022-04-20'
  :category: blog
  :title: Logbook - 2022-04-20
  :author: chris-n
  :thumbnail: logbook-default2-thumbnail.jpg
  :alt:
  :link: blog/logbook-2022-04-20
  :path: blog/2022-04-20
  :body: "\n\n<p class=lead> This post is a changelog, and it's also about the challenges
    of generating a changelog at Fly.io&mdash;where we run your apps in VMs on our
    hardware around the world. It only takes a few minutes to [try out](https://fly.io/docs/speedrun/)
    the latest and greatest Fly.io!</p>\n\nHere's a changelog covering our most recent
    activity (i.e. since we started compiling updates, a bit under two weeks ago):\n\n-
    **[Feature]** Added extra capacity in IAD.\n- **[Feature]** Backup regions (by
    far our most confusing misfeature) are now disabled by default. This should mean
    less people getting apps deployed in weird regions.\n- **[Feature]** Released
    [flyctl 0.0.318](https://github.com/superfly/flyctl/releases/tag/v0.0.318) with
    [NuxtJS](https://nuxtjs.org/) app launch support.\n- **[Feature]** Created our
    [NuxtJS launcher](https://fly.io/docs/getting-started/nuxtjs/). [NuxtJS](https://nuxtjs.org/)
    is a huge framework that wraps VueJS with server side rendering for SEO and faster
    initial load times. Those developers can really benefit from deploying apps closer
    to their users. This launcher uses a recommended Dockerfile from their docs so
    no buildpack needed.\n- **[Feature]** Completed a first version flyctl support
    and [documentation](https://fly.io/docs/getting-started/redwood/) for RedwoodJS
    app deployment on Fly.io. This is a new framework with a lot of momentum behind
    it.\n- **[Feature]** Released [flyctl v0.0.319](https://github.com/superfly/flyctl/releases/tag/v0.0.319)
    which updates some [Fly.io](http://fly.io/) API response handling to allow us
    to provide more informative errors without breaking functionality.\n- **[Feature]**
    Moved all the postgres operations in flyctl (except `connect`) from going through
    `ssh` to going through `http`, which increases their reliability since we had
    people reporting issues with commands like `fly pg attach`. ([PR](https://github.com/superfly/flyctl/issues/893
    )) ([postgres-ha release v0.0.19](https://github.com/fly-apps/postgres-ha/releases/tag/v0.0.19))\n-
    **[Feature]** Took some time to clarify internal logic around timeouts and error
    handling. This is the first step towards documenting more errors (that currently
    appear as code &quot;Undocumented&quot;) and making it easier to diagnose the
    performance and behavior of requests going through fly-proxy.\n- **[Feature]**
    Refactored  `Trigger Failover`, `Restart`, `View PG Settings` operations in the
    latest releases of postgres-ha and flyctl. This continues the move of [Fly.io](http://fly.io/)
    \ `postgres` operations from ssh to http.  ([postgres-ha v0.0.20](https://github.com/fly-apps/postgres-ha/releases/tag/v0.0.20)),
    ([flyctl v0.0.319](https://github.com/superfly/flyctl/releases/tag/v0.0.319))\n-
    **[Feature]** Just shipped a `changelog` fizz update type so we can start getting
    interesting stuff in front of users\n- **[Fix]** Merged a fix for IP collision
    errors, once this is deployed users will see far fewer of these errors preventing
    their VMs from starting. Probably 0 errors. \n- **[Fix]** Fixed a state machine
    bug that broke builders in weird ways. Builder machines were occasionally getting
    in a state that didn't allow a “start” to be issued. This caused fly deploy to
    hang for some organizations.\n- **[Fix]** Fixed \"recent logs\" on `flyctl vm
    status.` \n- **[Fix]** Deployed updates to make the flyctl release process more
    reliable. Released flyctl versions were getting stuck such that certain platforms
    weren't always notified about most up-to-date flyctl version. \n- **[Docs]** Documented
    the `restart_limit` option for TCP and HTTP health checks configured in `fly.toml`.
    This was missing from the docs, and there were two different plausible behaviours.
    ([Docs](https://fly.io/docs/reference/configuration/))\n\n## How Do We Make a
    Changelog Happen at Fly.io?\n\nWe want to tell you about every interesting thing
    we're doing, from adding a new option to a flyctl command, to speeding up Docker
    image pulls, to generating more enlightening error messages. How do we collect
    updates like this from a distributed company that's grown from 7 people to 26
    in the past 8 months? Turns out, it's not easy. Here's why it's hard for us.\n\nThe
    job is basically:\n\n1. Capture all the work that should get an entry.\n2. Formulate
    entries so users see what we did and why it's interesting.\n\nIn the spirit of
    exploration, we tried having one person compile a changelog by looking at all
    the status updates and git commits over the span of a few days. This helped crystallize
    some challenges for us; specifically: \n\n* Commits don't always mean something
    interesting got finished. \n* Something interesting getting done didn't always
    mean anybody wrote a status update about it. \n* We tend to write commit messages
    and status updates from a technical point of view, not always including the context
    needed to judge how the work impacts users.\n\nSince it's not practical to delegate
    discovery and translation of everyone's work to a team of changelog artisans,
    we'd better all just write good updates for our own work.\n\nThis means we *all*
    have to get good at the following:\n\n1. Writing changelog items that (a) make
    it obvious what we did, and (b) explain why it matters to users. \n2. Recognizing
    that the thing we just finished _does_ matter to users, so we should write an
    update (if it _really_ doesn't, why did we do it?). This can be really tricky!\n\nOn
    that second point: we do a tremendous amount of work to improve apps that already
    work on Fly.io, and most of this work is practically invisible.\n\nYou may have
    noticed that in today's list we have new deployment support for two frameworks.
    We are definitely pedal-to-the-metal on making it easy to deploy all kinds of
    apps on Fly.io, and it's easy to write an update for new features. But there's
    also furious activity on improving the platform for _all_ the apps running on
    us, and this is important and interesting, and we should showcase it, even if
    it's a harder blurb to write.\n\n<%= partial \"shared/posts/cta\", locals: {\n
    \ title: \"Shiny new things\",\n  text: \"Trying RedwoodJS? Check out how easy
    we made it to deploy on Fly.io.\",\n  link_url: \"https://fly.io/docs/getting-started/redwood/\",\n
    \ link_text: \"Get started for free&nbsp;&nbsp;<span class='opacity-50'>&rarr;</span>\"\n}
    %>\n\nIf everyone's writing their own updates, this brings up its own minor issue:
    the mechanics of collecting all the changelog entries in one place. Fortunately,
    we're nerds (or at least Kurt is), so we've automated this part. When we have
    an update to emit, we tell it to a Slack bot that passes it to an app (that Kurt
    wrote), which in turn collates our updates into feeds ripe for the copypasting.
    It doesn't solve the hardest problems, but it's pretty damn cool.\n\nWe'll be
    making a special effort to cultivate good changelog habits so we can bring you
    all the interesting things."
- :id: phoenix-files-dates-formatting-with-hooks
  :date: '2022-04-19'
  :category: phoenix-files
  :title: Formatting the user's local date-times using Hooks
  :author: berenice
  :thumbnail: date-times-thumbnail.jpg
  :alt:
  :link: phoenix-files/dates-formatting-with-hooks
  :path: phoenix-files/2022-04-19
  :body: "\n\n## Problem\n\nWhen we're developing an application for users around
    the world, we are bound to hit problems with timezones.\n\nOften we decide to
    store the dates in UTC format to avoid storing the timezone of each user. This
    brings us to the real problem, \"How can we display the UTC time in the user's
    local timezone?\"\n\n## Solution\n\nWe need to convert the date to the user's
    local timezone and  display it. We can use the user's locale information in the
    browser, add a few lines of Javascript in a [Client Hook](https://hexdocs.pm/phoenix_live_view/js-interop.html#client-hooks-via-phx-hook)
    and update the DOM to display the right time!\n\n### Converting a UTC date to
    a local date in Javascript\n\nFirst we'll create a `Date` object, passing our
    UTC date as a parameter to the constructor.  This lets us use the `Date` class
    methods to manipulate dates.\n\n```javascript\nlet dt = new Date(\"2016-05-24T13:26:08.003Z\");\n```\n\nThrough
    a `Date` class object we can use the `toLocaleString` method. Based on the user's
    default locale and timezone, this method   returns the local date as a string.\n\n```javascript\ndt.toLocaleString()\n//
    \  5/24/2016, 8:26:08 AM\n```\n\nWe can also extract and display the timezone
    that is used internally for the conversion as follows:\n\n```javascript\nIntl.DateTimeFormat().resolvedOptions().timeZone;\n//
    \  America/Mexico_City\n```\n\nConcatenating the results of the previous functions,
    we put our string into a more friendly format:\n\n```javascript\nlet dt = new
    Date(\"2016-05-24T13:26:08.003Z\");\nlet dateString = dt.toLocaleString() +\n
    \                \" \" + \n                 Intl.DateTimeFormat().resolvedOptions().timeZone;\n\n//
    \   5/24/2016, 8:26:08 AM America/Mexico_City\n\n```\n\n### Configuring and defining
    a Hook\n\nWe've figured out how to get a new representation of a date from the
    user locale with JavaScript. Now we want to show the date in its new format from
    our LiveView application.\n\nWe've stored the [date](https://hexdocs.pm/elixir/1.13/DateTime.html)
    \ we want to show in the `@utc` assign, and we render its content inside a `time`
    tag:\n\n```elixir\n<time><%%= @utc %></time>\n```\n\nTo change this to the new
    format, we'll use a client hook to execute a variant of the lines of Javascript
    we used above.\n\nFirst we need to define our `Hooks` object inside `assets/js/app.js`
    and add the hook, which we name  `LocalTime` . This hook is responsible for taking
    the content of the `time` tag, reformatting it and updating its content.\n\n```javascript\nlet
    Hooks = {}\n\nHooks.LocalTime = {\n  mounted() {\n    let dt = new Date(this.el.textContent);\n
    \   this.el.textContent = \n      dt.toLocaleString() + \n      \" \" + \n      Intl.DateTimeFormat().resolvedOptions().timeZone;\n
    \ }\n}\n```\n\nA hook can be executed at different life stages of an HTML element.
    Above, we defined the `mounted` callback, so the transformation will happen when
    the `time` tag is added to the DOM and the component has finished mounting.\n\nAll
    callbacks in a hook object have in-scope access to the `el` attribute, which is
    a reference to the DOM element on which the hook is running: here, the `time`
    tag. We get the UTC date from its `textContent` attribute, and store it in a Date
    object called `dt`:\n\n```javascript\nlet dt = new Date(this.el.textContent);\n```\n\nThen
    we replace the content of the HTML element with the new string we've created.\n\n```javascript\nthis.el.textContent
    = dt.toLocaleString() + \n                      \" \" +\n                      Intl.DateTimeFormat().resolvedOptions().timeZone;\n```\n\nThe
    conversion  also needs to be redone whenever the server updates the element, so
    we add the `updated` callback, with a small refactor to avoid duplicating code:\n\n```javascript\nHooks.LocalTime
    = {\n  mounted(){\n    this.updated()\n  },\n  updated() {\n    let dt = new Date(this.el.textContent);\n
    \   this.el.textContent = \n      dt.toLocaleString() + \n      \" \" + \n      Intl.DateTimeFormat().resolvedOptions().timeZone;\n
    \ }\n}\n```\n\nNow that our `LocalTime` hook is defined, we pass the `Hooks` object
    to the socket:\n\n```javascript\nlet liveSocket = new LiveSocket(\"/live\", Socket,
    {hooks: Hooks})\n```\n\n\n<%= partial \"shared/posts/cta\", locals: {\n  title:
    \"Fly.io ❤️ Elixir\",\n  text: \"Fly.io is a great way to run your Phoenix LiveView
    app close to your users. It's really easy to get started. You can be running in
    minutes.\",\n  link_url: \"https://fly.io/docs/elixir/\",\n  link_text: \"Deploy
    a Phoenix app today!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>\"\n} %>\n\n###
    Executing a Hook from an HTML element\n\nThe only missing piece is to tie our
    `LocalTime` hook to the `time` tag we defined. For this, we'll use the `phx-hook`
    option, passing the name of our `LocalTime` hook.\n\n```elixir\n<time phx-hook=\"LocalTime\"
    id=\"my-local-time\">\n  <%%= @utc %>\n</time>\n```\n\nNote that, when using `phx-hook`,
    we must always define a unique DOM ID for the HTML element.\n\nLet's see our work
    in action (slowed down for visibility):\n\n<%= video_tag \"date-times-01.mp4?card&center\",
    title: \"The date in utc format is rendered inside a button; a second later the
    content of the button changes to the new format.\" %>\n\nWhen the `time` tag is
    first rendered, the date is in UTC format, but then our hook is executed and the
    content of the tag is replaced.\n\nWe can make a little improvement and hide the
    initial content of the time tag using CSS.  Here we're using the TailwindCSS  `invisible`
    class.\n\n```elixir\n<time phx-hook=\"LocalTime\" id=\"my-local-time\" class=\"invisible\">\n
    \ <%%= @utc %>\n</time>\n```\n\nThis way the time tag still exists in the DOM,
    but its content will be hidden.\n\nOnce we've modified the date format, we remove
    the class with JavaScript to display the content of the time tag:\n\n```javascript\nupdated()
    {\n    let dt = new Date(this.el.textContent);\n    this.el.textContent = \n      dt.toLocaleString()
    +\n      \" \" +\n      Intl.DateTimeFormat().resolvedOptions().timeZone;\n    this.el.classList.remove(\"invisible\")\n
    \ }\n```\n\nLet's see what has changed:\n\n<%= video_tag \"date-times-02.mp4?card&center\",
    title: \"The button is rendered without visible content until the formatted date
    appears\" %>\n\nThe content of the time tag is not displayed until the date is
    reformatted and replaced.\n\n### Creating a reusable component\n\nIf this  is
    not the only date we'll show in our application, we can go one step further and
    define a function component that takes a unique DOM ID and a date as part of its
    assigns:\n\n```elixir\ndef local_time(%{date: date, id: id} = assigns) do\n  ~H\"\"\"\n
    \   <time phx-hook=\"LocalTime\" id={@id} class=\"invisible\">\n      <%%= @date
    %>\n    </time>\n  \"\"\"\nend\n```\n\nWe use our component as follows:\n\n```elixir\n<.local_time
    id=\"my-date\" date=\"2016-05-24T13:26:08.003Z\"/>\n```\n\nOr passing the content
    of an assign:\n\n```elixir\n<.local_time id=\"my-date\" date={@date}/>\n```\n\n##
    Discussion\n\nWe could get the user's locale and timezone from the browser and
    use server-side libraries like [Timex](https://hexdocs.pm/timex/Timex.html) to
    manipulate datetimes, which would involve writing lines of code on both, the client
    and server side, and storing those values in some part of our application.\n\nWith
    the Phoenix client Hooks, we can format dates using a few lines of Javascript
    and entirely client-side.\n"
- :id: jobs-platform-product-engineer
  :date: '2022-04-11'
  :category: jobs
  :title: Platform Product Engineer
  :author: michael
  :thumbnail:
  :alt:
  :link: jobs/platform-product-engineer
  :path: jobs/2022-04-11
  :body: "\n\n<div class=\"lead\">\n  Fly.io takes Docker containers from users and
    [converts them](https://fly.io/blog/docker-without-docker/) to [Firecracker micro-VMs](https://fly.io/blog/sandboxing-and-workload-isolation/)
    running on our hardware around the world, hooked via [WireGuard](https://fly.io/blog/ipv6-wireguard-peering/)
    to a global [Anycast network](https://fly.io/blog/32-bit-real-estate/), [managed
    Postgres databases](https://fly.io/blog/free-postgres/), [LVM2 volumes](https://fly.io/blog/persistent-storage-and-fast-remote-builds/),
    an [industrial-scale metrics cluster](https://fly.io/blog/measuring-fly/), a high-performance
    messaging-based logging system, all coordinated by a [global orchestration system](https://fly.io/blog/a-foolish-consistency/).
    You don't have to know any of this to use Fly.io: if you have a working container,
    it can be running in Singapore, São Paolo, and Sunnyvale [in just a couple minutes](https://fly.io/docs/speedrun/).\n</div>\n\nWe're
    usually more concise when describing Fly.io. But we're not just trying to convince
    you to boot your apps up on Fly.io. If that last paragraph sounded interesting
    to you, what we really want to do is talk to you about working here.\n\nThe job
    is Platform Product Engineer. It's software development for the core engine of
    our product.\n\nThe team at Fly.io includes a bunch of different roles and there's
    great things about all of them. Full-stack developers get to work up-close and
    personal with our users, shipping immediately-visible features. The networking
    team is building an ambitious high-speed Rust-based proxy network. We've got a
    Kurt. We've got developers pushing the state of the art on platforms like Elixir,
    and SREs rolling out a fleet of servers around the world.\n\nPlatform Product
    Engineer, though: it's the kernel of the platform. And not just because there's
    kernel code involved. This job is all about sculpting infrastructure into a product
    customers want. The work is deeply technical, but UX is the forcing function and
    it goes all the way down to the metal. You gotta know what customers need and
    what makes a good UX.\n\nHere's some of what Platform is working on right now:\n\n-
    A new orchestration system that makes it easy, predictable, and fast to spin up
    instances of apps anywhere in the world in response to scaling signals and user
    requests, one flexible enough to be the compute engine for everything from managed
    databases to remote developer environments.\n- Volume and state management systems
    that can seamlessly move compute loads, along with their data, around different
    machines in our fleet.\n- Private networking that just works, without anyone writing
    a line of Terraform code, to connect any Docker image (and any libc) to any service
    anybody thinks to run on our platform, and makes peering between companies trivial
    and secure.\n- Always-on metrics, logging, and visibility features that scale
    to hundreds of thousands of apps and can answer questions about app behavior our
    users didn't think of before deploying their apps.\n\nThis is sort of a golden
    moment to come work with us at Fly.io. We've hammered out our basic service and
    have a base of enthusiastic users [shipping](https://community.fly.io/t/git-limo-a-git-source-code-management-tool-powered-by-elixir/1420)
    \ [super](https://messwithdns.net/)  [cool](https://multiplayer.dev/)  [stuff](https://minesweeper.fly.dev/sessions/new)
    \ [on](https://twitter.com/benbjohnson/status/1494800752333647872)  [it](https://semantleless.fly.dev/).
    Our team is growing but still at a point where everyone knows who everyone else
    is and what they're working on. Nobody's off in a corner on a solitary death march.
    It's still easy to have a good idea here, float it to the team, and have it take
    off. We are having fun. For some of us, this kind of environment is why we work
    in startups.\n\nIt's not all sunshine and marshmallows. Platform Engineering is
    a not-messing-around serious role. It doesn't do anybody any good to sell you
    a bill of goods about what the work is like. So, if you're going to be comfortable
    working in this role, here are some messy things we want you to know:\n\n- We're
    a small team working on something ambitious. There's a lot going on. There is
    some chaos. We work hard to tame it, but we don't let its existence paralyze us
    or keep us from shipping. It's a whole thing, and you'd want to be on board with
    it.\n- We've got a lightweight management style. At times, we feel more like a
    large open source project than an industrial software development team. We do
    1:1's and we keep track of what's being worked on, but there isn't a board you
    can go look at to know exactly what you're going to be working on next week.\n-
    We're ruthless about working on stuff that our users will see and care about.
    We are not ruthless about shaping and polishing our code into a radiant-cut gem
    of perfection. We have a \"no refactoring for your first several months\" rule,
    and you'll remember that rule is there as you bounce around our code; there's
    a lot that could be refactored.\n- We're on call, 24/7. Everyone shares a rotation.
    We've chosen a cortisol-intensive domain to work in: when our stuff breaks, our
    users notice. We're a chill bunch of people (many of us with families; nobody's
    pulling 80 hour weeks), but our problem space is unmerciful.\n- We're a helpful
    bunch, but all of us are learning stuff as we go along and we expect you to do
    the same. You'd want to be comfortable diving deep into the details of complicated
    systems and teaching yourself enough to solve problems. There's a scary amount
    of technical complexity (and some of own-goal complexity we brought along for
    the ride), and we need people who won't freeze up.\n- We're not running Kubernetes,
    or whichever database the cool kids are using. We're addicted to code that works,
    right away, with minimal ceremony. We like SQLite, and we get nervous when people
    talk about Raft. The engineering culture here is pragmatic to what HN would consider
    a fault.\n- Your enjoyment of a job here will hinge less on your technical skills,
    and more on your ability to make decisions that benefit end users. We don't have
    a rigid product roadmap, we don't have detailed issues for you to implement, and
    it's gonna take some persistence to figure out what's important to build. \n\nWe
    like this project very much and it's hard to write negatively about it. We could
    take another 3-4 editing passes on those last bullets and make them more honest
    and clear about how these things might creep up and annoy you. But instead, you
    can do that for yourself: give them another read, and just extrapolate all the
    bad implications you can from them. Then ask us about them, and we'll be candid.\n\nIt's
    such a cool job, though.\n\n## More Details\n\nThis is a mid to senior level job.
    The salary ranges from $120k to $200k USD. We also offer competitive equity grants.
    Hopefully that's enough to keep you intrigued, here's what you should _really_
    care about:\n\n- We're a small team, almost entirely technical.\n- Most of our
    platform code is in Go. Our networking code is in Rust. We like both languages
    and you very much need to be on board with both of them yourself (you don't need
    to be a fluent Rust programmer for this gig, but you can't be allergic to the
    idea of picking it up - or, for that matter, allergic to Go).\n- We are active
    in developer communities, including our own at [community.fly.io](https://community.fly.io/).\n-
    Virtually all customer communication, documentation and blog posts are in writing.
    We are a global company, but most of our communication is in English. Clear writing
    in English is essential.\n- We are remote, with team members in Colorado, Quebec,
    Chicago, London, Mexico, Spain, Virginia, Brazil, and Utah. Most internal communication
    is written, and often asynchronous. You'll want to be comfortable with not getting
    an immediate response for everything, but also know when you need to get an immediate
    response for something.\n- We are an unusually public team; you'd want to be comfortable
    working in open channels rather than secretively over in a dark corner.\n- We're
    a real company - hopefully that goes without saying - and this is a real, according-to-Hoyle
    full-time job with health care for US employees, flexible vacation time, hardware/phone
    allowances, the standard stuff.  \n\n## How We Hire People\n\nWe're weird about
    hiring. We're skeptical of resumes and we don't trust interviews (we're happy
    to talk, though). We respect career experience but we aren't hypnotized by it,
    and we're thrilled at the prospect of discovering new talent.\n\nThe premise of
    our hiring process is that we're going to show you the kind of work we're doing
    and then see if you enjoy actually doing it; “work-sample challenges”. Unlike
    a lot of places that assign “take-home problems”, our challenges are the backbone
    of our whole process; they're not pre-screeners for an interview gauntlet.\n\nFor
    this role, we're asking people to write us a small proxy that does just a couple
    of interesting things (we'll tell you more). We're looking for people who are
    super-comfortable with Go and network programming in general, but we're happy
    to bring people up to speed with the domain-specific stuff in Fly.io.\n\n<div
    class=\"instructions\">\n  **If you're interested, mail [jobs+platform@fly.io](mailto:jobs+platform@fly.io).
    You can tell us a bit about yourself, if you like. Please also include your GitHub
    username and a sentence (yes, just one) about your least favorite Linux system
    call. `ioctl` doesn't count.**\n</div>\n"
- :id: phoenix-files-sdeb-toggling-element
  :date: '2022-04-11'
  :category: phoenix-files
  :title: Easy UI Toggling with LiveView JS Commands
  :author: sophie
  :thumbnail: toggle-thumbnail.jpg
  :alt:
  :link: phoenix-files/sdeb-toggling-element
  :path: phoenix-files/2022-04-11
  :body: "\n\n<p class=\"lead\">Here's Sophie DeBenedetto, co-author of [*Programming
    Phoenix LiveView*](https://pragprog.com/titles/liveview/programming-phoenix-liveview/),
    with a deep dive into building a toggling UI element with LiveView's `JS.toggle`
    command. Coincidentally, you can run Phoenix LiveView apps here on Fly.io! [Get
    started](/docs/elixir/).</p>\n\nLiveView empowers developers to be more productive
    than ever before by keeping your mind firmly focused on the server, even while
    you build out rich interactive UIs. But until recently, we were somewhat limited
    by LiveView's reliance on server-side state to manage those interactions. With
    LiveView's new [JS commands](https://hexdocs.pm/phoenix_live_view/Phoenix.LiveView.JS.html),
    you can manage common UI interactions purely on the client, while still maintaining
    accurate and up-to-date state on the server. In this post, we'll use JS commands
    to build a toggle feature. When we're done, you'll have a solid understanding
    of how to use JS commands to build common UI interactions on the client in LiveView.\n\n##
    The Problem\n\nYou want to toggle the appearance of some content on your live
    view page in response to a user's interaction. For example, you want to show/hide
    a search form when the user clicks the &quot;search&quot; icon, like this:\n\n<%=
    video_tag \"sdeb-2-1.mp4?card&center\", title: \"Screen capture of a search bar
    appearing and disappearing when an icon is clicked.\" %>\n\nWithout LiveView's
    JS commands, you'd be forced to send an event to the server to update socket state
    to indicate that the form should be shown/hidden. Your template would have some
    conditional logic to show/hide the search from depending on some key in socket
    assigns. The live view's template would re-render when you updated that socket
    assigns key in response to the user clicking the search icon, thereby showing
    or hiding the form appropriately. This means a round-trip to the server to do
    something that a simple JavaScript snippet can do for you. But we're LiveView
    developers; we like to let the LiveView framework handle our JavaScript for us.
    Well, now we can.\n\n## The Solution\n\nThe `Phoenix.LiveView.JS` module provides
    functionality to invoke client-side operations in response to LiveView events
    like the `phx-click` event. It abstracts away some of the most common JS interactions,
    like showing/hiding content, adding/removing classes, setting CSS attributes,
    and transitioning CSS classes. It builds these interactions directly into the
    LiveView JS framework and exposes an easy-to-use API you can use to execute these
    JS commands in response to user events on your live view page.\n\n## Example:
    Toggling a Form\n\nFirst, we'll take a look at a simple example. Then, we'll break
    down how it works under the hood before making our example slightly more advanced.
    Let's turn our attention back to the search form that we want to toggle. We'd
    like the form to be hidden when the user loads the page, so we'll give the form
    an ID of `#book-search-form` and add some custom css to `app.scss` to set the
    display to `none`.\n\n```elixir\n# lib/live_library_web/live/book_live/search_component.html.heex\n\n<.form\n
    \ let={f}\n  for={@search_changeset}\n  id=\"book-search-form\"\n  phx-submit=\"search\"\n
    \ phx-target={@myself}\n  class={@search_class} >\n\n<!-- ... -->\n\n/* assets/css/app.css
    */\n\n#book-search-form {\n display: none;\n}\n```\n\nGreat. Now when the page
    loads, the form will be hidden. Next up, we'll use the `Phoenix.LiveView.JS.toggle/1`
    function to toggle the display attribute between `none` and `block`. Before we
    do though, let's take a look at the code that renders the search icon into the
    search form template.\n\nWe have a simple function component, `Search.icon/1`
    that display the search icon. Here's a look at our function component:\n\n```elixir\n#
    lib/live_library_web/live/book_live/search.ex\n\ndefmodule LiveLibraryWeb.Search
    do\n  use Phoenix.Component\n \n  def icon(assigns) do\n    ~H\"\"\"\n    <i class=\"bi
    bi-search\"></i>\n    \"\"\"\n  end\nend\n```\n\nOur `Search` module implements
    one function, `icon/1`, that returns some HEEx markup with the search Bootstrap
    icon. Assuming we've aliased the `LiveLibraryWeb.Search` module in the live view
    or live component rendering the search form template, we can render our function
    component in that template like this:\n\n```elixir\n<Search.icon />\n```\n\nWith
    that in place, we're ready to add our JS binding. Add a `phx-click` binding to
    the icon element with a value of the `JS.toggle/1` function call:\n\n```elixir\ndef
    icon(assigns) do\n  ~H\"\"\"\n  <i phx-click={JS.toggle(to: \"#book-search-form\")}
    class=\"bi bi-search\"></i>\n  \"\"\"\nend\n```\n\nAnd that's it! Now, when the
    user clicks the search icon, the `phx-click` event will fire which will trigger
    LiveView's JavaScript to toggle the value of the CSS `display` attribute of the
    specified element between `none` and `block`. This one small line of code is incredibly
    powerful, and it's all we need in order to implement this purely client-side interaction.\n\nLet's
    take a brief look at how it works under the hood. Then, we'll add some more advanced
    options to our toggling example.\n\n## LiveView JS Commands Under the Hood\n\nWe'll
    examine how the `Phoenix.LiveView.JS` module and the front-end `JS` object work
    together to implement the JS toggle command. First up, let's revisit our toggle-adding
    code here:\n\n```elixir\n<i phx-click={JS.toggle(to: \"#book-search-form\")} class=\"bi
    bi-search\"></i>\n```\n\nHere's where the magic happens:\n\n```elixir\nphx-click={JS.toggle(to:
    \"#book-search-form\")}\n```\n\nThis call to `JS.toggle/1` returns a `Phoenix.LiveView.JS`
    struct that looks like this:\n\n```elixir\n%Phoenix.LiveView.JS{\n  ops: [\n    [\n
    \   \"toggle\",\n      %{\n        display: nil,\n        ins: [[], [], []],\n
    \       outs: [[], [], []],\n        time: 200,\n        to: \"#book-search-form\"\n
    \     }\n    ]\n  ]\n}\n```\n\nYou can see the `:to` attribute is populated with
    our `\"#book-search-form\"` CSS selector, and the remaining attributes are set
    to their default values. We're not taking advantage of the additional attributes
    at this time, but you can optionally specify a `:display` value to set when the
    element is toggled (defaults to &quot;block&quot; on the front-end), as well as
    classes to apply/remove when the element is transitioning in and out of visibility,
    and the time duration for which to apply those transition classes.\n\nWhen LiveView
    renders our template, this struct is rendered by `Phoenix.HTML.Safe` and appears
    as the following markup in the UI:\n\n![A selected element and browser devtools
    with the phx-click binding described in the text.](sdeb-2-2.png?card)\n\nYou can
    see that all this has done is add a `phx-click` event that specifies an event
    that looks like this:\n\n```js\n[[\"toggle\",{\"display\":null,\"ins\":[[],[],[]],\"outs\":[[],[],[]],\"time\":200,\"to\":\"#book-search-form\"}]]\n```\n\nRecall
    that when the page loads, the `app.js` file loads with it. The `app.js` file that
    is generated when you generate your Phoenix LiveView app contains these lines:\n\n```js\nlet
    liveSocket = new LiveSocket(\"/live\", Socket, {params: {_csrf_token: csrfToken},
    hooks: Hooks})\n// ...\nliveSocket.connect()\n```\n\nUnder the hood, `liveSocket.connect()`
    will call the `bindClick()` function that adds an event listener for every `phx-click`
    event. At a high level, the code looks like this:\n\n```js\nwindow.addEventListener(eventName,
    e => {\n  //  ...\n  JS.exec(\"click\", phxEvent, view, target, [\"push\", {data:
    this.eventMeta(\"click\", e,target)}])\n}, capture)\n```\n\nThat brings us to
    the `JS.exec` function. This function pulls the event &quot;kind&quot; out of
    the name (in our case `\"toggle\"`) and dynamically invokes `exec_${kind}`, as
    you can see here:\n\n```js\nthis[`exec_${kind}`](eventType, phxEvent, view, sourceEl,
    el, args);\n```\n\nFinally, the `exec_toggle` function is invoked, which implements
    the [logic](https://github.com/phoenixframework/phoenix_live_view/blob/30ee942b3a18a9e2e1f222a76a707bfba7bd94f7/priv/static/phoenix_live_view.js#L2062)
    to toggle the value of the `display` attribute on the target element and apply
    any transition classes.\n\nAll of this happens within the LiveView framework.
    The only thing we had to do to trigger the toggling JavaScript was add one `phx-click`
    binding to our HTML element.\n\nNow that we have a basic understanding of how
    LiveView JS commands work, let's revisit our toggle command make it a little more
    sophisticated.\n\n<%= partial \"shared/posts/cta\", locals: {\n  title: \"Fly.io
    ❤️ Elixir\",\n  text: \"Fly.io is a great way to run your Phoenix LiveView app
    close to your users. It's really easy to get started. You can be running in minutes.\",\n
    \ link_url: \"https://fly.io/docs/elixir/\",\n  link_text: \"Deploy a Phoenix
    app today!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>\"\n} %>\n\n## Trigger
    Client- and Server-Side Interactions Together\n\nJS commands let us execute common
    UI interactions entirely on the client, without round-tripping to the server.
    But what if we _also_ want to push some message to the server in response to the
    same `phx-click` event? We can chain a call to the `JS.push/1` function to push
    a message to the server while still toggling our search form entirely on the client-side.
    We want our push event to target the live component that is rendering the search
    icon function component. So, when we call `Search.icon/1` in that component's
    template, we'll pass in a `target` assignment, like this:\n\n```erb\n<!-- lib/live_library_web/book_live/search_component.html.heex
    -->\n\n<Search.icon target={@myself} />\n```\n\nNow, add the following to your
    `phx-click` event:\n\n```elixir\ndef icon(assigns) do\n  ~H\"\"\"\n  <i phx-click={JS.push(\"toggle_book_search\",
    target: @target) |> JS.toggle(to: \"#book-search-form\")} class=\"bi bi-search\"></i>\n
    \ \"\"\"\nend\n```\n\nFirst, we call `JS.push(\"toggle_book_search\", target:
    @target)`, then we pipe the resulting `Phoenix.LiveView.JS` struct to a call to
    `JS.toggle/1`. This nice neat reducer pipeline is eloquent and easy to read. We
    can even wrap it up in a helper function like this:\n\n```elixir\ndef icon(assigns)
    do\n  ~H\"\"\"\n  <i phx-click={toggle(@target)} class=\"bi bi-search\"></i>\n
    \ \"\"\"\nend\n\ndef toggle(target) do\n  JS.push(\"toggle_book_search\", target:
    target)\n  |> JS.toggle(to: \"#book-search-form\")\nend\n```\n\nBefore we build
    an event handler, let's think about what we want to happen when the user clicks
    the icon. We know we want the search form to be shown/hidden, but let's say we
    _also_ want to update the URL to `/books?search` and ensure that if a user navigates
    directly to `/books?search`, the search form is shown. We'll take a step back
    and tackle that second behavior first.\n\nWhen a user visits `/books?search`,
    the search form should be shown. Let's implement a new CSS rule that sets `display:
    block;` for a CSS selector with an ID of `#book-search-form`  _and_ a class of
    `.show`:\n\n```scss\n/* app.scss */\n\n#book-search-form.show {\n  display: block;\n}\n```\n\nNext
    up, let's implement a `handle_params/3` function in the parent live view that
    stores the appropriate CSS class name in state depending on the URI:\n\n```elixir\n#
    lib/live_library_web/live/book_live/index.dex\n\ndef handle_params(_params, uri,
    socket) do\n  if String.match?(uri, ~r/search/) do\n    {:noreply, assign_search_class(socket,
    \"show\")}\n  else\n    {:noreply, assign_search_class(socket)}\n  end\nend\n\ndef
    assign_search_class(socket) do\n  assign(socket, :search_class, \"\")\nend\n\ndef
    assign_search_class(socket, class) do\n  assign(socket, :search_class, class)\nend\n```\n\nThe
    index template renders the search live component, and the search component renders
    the form. We'll update the form markup in the search live component now to apply
    a class of the `@search_class` assignment:\n\n```elixir\n<!-- lib/live_library_web/book_live/search_component.html.heex
    -->\n\n<.form\n  let={f}\n  for={@search_changeset}\n  id=\"book-search-form\"\n
    \ phx-submit=\"search\"\n  phx-change=\"update\"\n  phx-target={@myself}\n  class={@search_class}
    > <!-- add this line -->\n```\n\nNow, when a user points their browser at `/books?search`,
    the live view will set `:search_class` in socket assigns to `\"show\"`, and our
    CSS rule will kick-in and set `display: block;`. If the user points their browser
    at `/books`, the live view socket assigns will set `:search_class` to `\"\"`,
    and our CSS rule will _not_ kick-in. This will will the `#book-search-form` CSS
    rule in place, setting `display: none;`.\n\nWith this logic in place, we're finally
    ready to implement the event handler for the `\"toggle_book_search\"` event that
    we're pushing to the live component when the user clicks the search icon. Let's
    do that now:\n\n```elixir\n# lib/live_library_web/live/book_live/search_component.ex\n\ndef
    handle_event(\"toggle_book_search\", _value, socket) do\n  if socket.assigns.search_class
    == \"show\" do\n    {:noreply, push_patch(socket, to: \"/books\")}\n  else\n    {:noreply,
    push_patch(socket, to: \"/books?search\")}\n  end\nend\n```\n\nOur event handler
    is fairly simple. If the socket assigns `:search_class` is already set to `\"show\"`,
    then push a patch to `/books`. Otherwise, push a patch to `/books?search`. This
    will invoke the parent LiveView's `handle_params/3` callback. This will update
    the URL and change the value of `socket.assigns.search_class` accordingly. But,
    the page _won't_ re-render. This is because the `JS.toggle` client-side code already
    fired and updated the `display` CSS value of the target element, so there will
    be no changes to display. No changes means no re-rendering. With this, we achieve
    the following:\n\n- A user can navigate directly to `/books` and see a search
    icon with no form.\n- A user can navigate directly to `/books?search` and see
    a search icon and a search form.\n- If the form is hidden, a user can click the
    icon and see the URL update to `/books?search` and see the form appear, all without
    reloading any portion of the page.\n\nThanks to our ability to chain JS commands,
    and push events to the server even while we continue to trigger client-side code,
    we get all of this for just a few lines of Elixir code.\n\n## JS Commands Give
    Us Simple and Powerful Client-Side Interactions\n\nYou've seen how the LiveView
    framework abstracts away more and more of the commonly reached-for pieces of functionality
    that back the average modern web page. Now, you get even more JavaScript-powered
    features without writing any of your own JavaScript.\n\nCheck out the full functionality
    in the [docs](https://hexdocs.pm/phoenix_live_view/Phoenix.LiveView.JS.html) and
    check out [this post](https://fly.io/phoenix-files/tabs-with-js-commands/) to
    play around with even more JS commands functionality.\n"
- :id: phoenix-files-js-push-loading-options
  :date: '2022-04-06'
  :category: phoenix-files
  :title: Loading indicators for events with JS.push
  :author:
  :thumbnail: loading-indicators-thumbnail.jpg
  :alt:
  :link: phoenix-files/js-push-loading-options
  :path: phoenix-files/2022-04-06
  :body: "\n\n## Problem\n\nPhoenix/LiveView apps involve a lot of interaction between
    the client and the server. We want to customize how we indicate to users that
    our UI is waiting for a server response to some event. Specifically, we'd like
    to see a general-purpose loading animation triggered, or we'd like to apply a
    visual cue to a specific part of our app.\n\n## Solution\n\nDepending on the desired
    effect, we can use the JS.push command to customize one (or both) of two approaches
    built into LiveView: a loading animation, such as [LiveView's default progress
    bar](https://hexdocs.pm/phoenix_live_view/installation.html#progress-animation),
    or the temporary [loading classes](https://hexdocs.pm/phoenix_live_view/bindings.html#loading-states-and-errors)
    applied to elements when a `phx-` event is pushed.\n\nLet's explore how, using
    [LiveBeats](https://livebeats.fly.dev)!\n\n### Setup\n\nOur real-life example
    uses the `JS.push` API to send an event with a payload and a target, much as we
    did in an [earlier post](https://fly.io/phoenix-files/pushing-events-with-js-push/).\n\nIn
    LiveBeats we can listen to our favorite playlist and browse  different friends'
    profiles at the same time, without interrupting the music.  The secret behind
    the scenes is that we have two different LiveViews (`PlayerLive` and `ProfileLive`)
    that work independently and communicate by pushing events.\n\n![](loading-indicators-01.png?2/3&centered)\n\n`ProfileLive`
    shows the profile and the playlist; `PlayerLive` takes care of playing whichever
    playlist we tell it to play.\n\nThe \"Listen\" button is part of `ProfileLive`.
    When we click it, we want  the _player_, `PlayerLive`_,_  to start playing the
    playlist currently displayed in the profile.\n\nHere's how we achieve that with
    `JS.push`:\n\n```elixir\n<.button phx-click={JS.push(\n  \"switch_profile\", \n
    \ value: %{user_id: @profile.user_id}, \n  target: \"#player\")}\n>\n  Listen\n</.button>\n```\n\nThe
    \"Listen\" button sends an event called  `switch_profile`  to the server. We define
    a payload that contains  the  user ID of the profile owner, and finally, we specify
    that the event should be handled by the component whose identifier is `#player`.\n\nOn
    the `PlayerLive` side, we need a  `handle_event` callback to execute all the logic
    to switch to the current playlist:\n\n```elixir\ndef handle_event(\"switch_profile\",
    %{\"user_id\" => user_id}, socket) do\n  {:noreply, switch_profile(socket, user_id)}\nend\n```\n\nWe're
    ready to play music!\n\n<%= video_tag \"loading-indicators-02.mp4?card?center\",
    title: \"When a user clicks the \\\"listen\\\" button, the player displays the
    profile playlist name\" %>\n\n\n\n### Triggering a page loading animation\n\nWe've
    set up our profile-player communication. Now we want some visual feedback on the
    page while the server applies its changes.\n\nBy default, LiveView displays the
    `topbar`  progress indicator at the top of the page when we navigate across our
    application, as well as on form submits. Can we trigger  `topbar` when our custom
    events are being processed, too?\n\nThe secret is in the `phx:page-loading-start`
    and `phx:page-loading-stop` events. If we take a look in our `app.js` file, we
    find a listener  for each of those events:\n\n```javascript\nwindow.addEventListener(\"phx:page-loading-start\",
    info => topbar.show())\nwindow.addEventListener(\"phx:page-loading-stop\", info
    => topbar.hide())\n```\n\nThe top bar is shown and hidden when events `phx:page-loading-start`
    and `phx:page-loading-stop` are emitted, respectively.\n\n<aside class=\"right-sidenote\">
    You can replace the default `topbar` animation with your own actions to be applied
    when these events are triggered.</aside>\n\nWith this in mind: if we can emit
    those events, we can trigger the `topbar` animation. And that's what the `page_loading`
    option of `JS.push` is for!\n\nThis option takes a boolean value. If `true`,  `phx:page-loading-start`
    is emitted when the event is pushed, and `phx:page-loading-stop` when the server
    has finished all its processing and responded with an acknowledgement.\n\nWe can
    extend the LiveBeats \"Listen\" button to use this and see the `topbar`:\n\n```elixir\nJS.push(\n
    \     \"switch_profile\", \n      value: %{user_id: @profile.user_id}, \n      target:
    \"#player\",\n      page_loading: true\n)\n```\n\n<aside class=\"right-sidenote\">You
    may be thinking that you could do the same with `phx-page-loading` annotation,
    but it's always good to have new options, right? </aside>\n\nThis is an artificial
    example, because in general the LiveBeats profile change is so fast that adding
    a loading indicator [would make it feel slower](https://fly.io/phoenix-files/make-your-liveview-feel-faster/)!\n\n<%=
    video_tag \"loading-indicators-03.mp4?card?center\", title: \"When a user clicks
    the \\\"listen\\\" button, a topbar is showed and after that the player displays
    the profile playlist name\" %>\n\n### Using temporary loading classes\n\nThe `topbar`
    \ animation makes our playlist change look the same as a full page reload.  What
    if we want to indicate that _just_ the player is waiting for an action to finish?\n\nWhenever
    an event is sent using a LiveView `phx-` binding, the emitting element is given
    a temporary [loading class](https://hexdocs.pm/phoenix_live_view/bindings.html#loading-states-and-errors)
    (e.g. `phx-click-loading`) that can be used to apply some effect in CSS.\n\nThe
    `JS.push`  `loading` option specifies further elements to receive this loading
    class.\n\n```elixir\nJS.push(\n      \"switch_profile\", \n      value:%{user_id:
    @profile.user_id}, \n      target: \"#player\",\n      loading: \"#player\"\n)\n```\n\nNow
    we can apply a fade effect to the `#player` that ends when the server responds
    with an acknowledgement for the `switch_profile` event.\n\n<%= video_tag \"loading-indicators-04.mp4?card?center\",
    title: \"When a user clicks the \\\"listen\\\" button, the player fades out until
    the profile playlist name is displayed\" %>\n\nWe are indicating that the player
    is executing an action, but there isn't an entire page reloading.\n\n<%= partial
    \"shared/posts/cta\", locals: {\n  title: \"Fly ❤️ Elixir\",\n  text: \"Fly is
    an awesome place to run your Elixir apps. It's really easy to get started. You
    can be running in minutes.\",\n  link_url: \"https://fly.io/docs/elixir/\",\n
    \ link_text: \"Deploy your Elixir app today!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>\"\n}
    %>\n\n\n## Discussion\n\n`JS.push` is a difficult utility to summarize.\n\nWe
    _can_ simply use its API as an [alternative syntax to send events to the back
    end](https://fly.io/phoenix-files/pushing-events-with-js-push/). We can [compose
    it with the client-side LiveView `JS commands`](https://fly.io/phoenix-files/pushing-events-with-js-push/#composing-loading-indicators-0with-other-js-commands),
    to coordinate optimistic client-side effects. Today, we explored the unique loading-state
    options of JS.push, which allow us more control over how an app, or some part
    of it, indicates that it's awaiting a server response to a given event.\n\nPut
    together, `JS.push` is a toolkit to push events and extend our creative options
    for how the front-end interacts and communicates with the user."
- :id: phoenix-files-exploring-options-for-storing-custom-data-in-ecto
  :date: '2022-04-04'
  :category: phoenix-files
  :title: Exploring Options for Storing Custom Data in Ecto
  :author: mark
  :thumbnail: storing-maps-with-2-options-thumbnail.jpg
  :alt:
  :link: phoenix-files/exploring-options-for-storing-custom-data-in-ecto
  :path: phoenix-files/2022-04-04
  :body: |2


    <p class="lead">[Fly.io](http://Fly.io) runs applications by transmogrifying Docker containers into Firecracker micro-VMs running on our hardware around the world, connected with WireGuard to a global Anycast network. Your Elixir app could be one of them! [Check us out](https://fly.io/docs/speedrun/): you can deploy in minutes.</p>

    Ever wanted to store a blob of custom data on a database record? The data we want to store might be complex too, made up of multiple fields and even lists. When the data is stored using [Ecto](https://hexdocs.pm/ecto/Ecto.html) there are several ways we can do it. This article explores two paths and discusses some of the pros and cons for each approach.

    Recently I was confronted with this choice again. This choice doesn't come along that often for me and I had to re-discover the pros and cons for each option. This article is intended to be a sign posted at the fork in the road to let travelers know a bit more about what lies beyond in the paths before them.

    Both in life and in programming, we can be presented with a choice of two directions to go. If you are familiar with Robert Frost's poem [The Road Not Taken](https://www.poetryfoundation.org/poems/44272/the-road-not-taken), then you may know that it's actually about [indecision](https://www.poetryfoundation.org/articles/89511/robert-frost-the-road-not-taken), the desire to take _both_ paths because deciding things can be scary! In this article, we will venture a little way down each path before finally deciding which to take.

    Before we discuss the choice of paths, we need to cover the problem being solved. Let's talk about the kind of data being stored.

    ## Our custom data

    In our scenario, we have a service that offers many games that members can play. The games can be saved in various states of play and be resumed later. An `Account` has many `GameSave` records.

    We didn't want to create a database table for every type of game that _could_ be saved. We are continually adding new games and occasionally retiring old ones that didn't catch on.

    Instead of creating a table for each game type, we created a single table that records a saved game of _any_ game type. It looks something like this:

    ![Account has many GameSave. GameSave has one GameData. ERD diagram](./store_custom_data_erd.png?center)

    Every game needs to store its data in a different way. For example, our saved [Tic-Tac-Toe](https://fly.io/blog/building-a-distributed-turn-based-game-system-in-elixir/) game will be stored very differently from a chess game or a memory game. The structure and details of the `GameData` are defined by the game itself.

    How can we do this using [Postgres](https://www.postgresql.org/) and [Ecto](https://hexdocs.pm/ecto/Ecto.html)?

    ## Path One: Store data as a `map`

    The easiest and first thought might be to use an Ecto `:map` field type. In Postgres, this is stored as a `jsonb` field. In MSSQL it is stored as text. Other databases will vary, so we need to understand how it works in our particular database. In this situation, it's Postgres.

    The [`Ecto.Schema` docs on the `:map` field type](https://hexdocs.pm/ecto/Ecto.Schema.html#module-the-map-type) are very helpful if you plan to go in this direction.

    Let's see what it would look like for our `GameSave` to use a `:map` field.

    ### Migration

    It's easy to create this in our migration:

    ```elixir
    defmodule Core.Repo.Migrations.CreateGameSaves do
      use Ecto.Migration

      def change do
        create table(:game_saves) do
          add :account_id, references(:accounts, on_delete: :delete_all), null: false
          # ...
          # Game data stored as a map
          add :data, :map, null: false, default: %{}

          timestamps()
        end

      end
    end
    ```

    The data type used is `:map`. We can even assign a default value of an empty map. Cool!

    ### Ecto Schema

    Our schema also makes easy use of the `:map` field type.

    ```elixir
    defmodule Core.GameSaves.GameSave do
      use Ecto.Schema

      schema "game_saves" do
        # ...
        field :data, :map, default: %{}, required: true

        belongs_to :account, Core.Accounts.Account
        timestamps()
      end
    end
    ```

    With our storage and data structures ready, let's look at how we use it.

    ### Writing data

    When writing the data, a common approach is to use a normal map. By "normal", we mean that it uses atoms for keys and uses atoms for field values as well. Like this:

    ```elixir
    data = %{
      turn: 1,
      group_types: [:hero, :mage, :elf],
      characters: [
        %{name: "Herman the Hero", type: :hero, health: 130},
        %{name: "Morgan the Mage", type: :mage, health: 100},
        %{name: "Edward the Elf", type: :elf, health: 100},
      ]
    }

    {:ok, save} = Core.GameSaves.create_game_save(player_account.id, data)
    save
    ```

    After inserting the data, our struct is returned looking like this:

    ```elixir
    %GameSave{
      id: 1,
      data: %{
        turn: 1,
        group_types: [:hero, :mage, :elf],
        characters: [
          %{name: "Herman the Hero", type: :hero, health: 130},
          %{name: "Morgan the Mage", type: :mage, health: 100},
          %{name: "Edward the Elf", type: :elf, health: 100},
        ]
      }
    }
    ```

    The thing to note here is the `data` field. As you would expect, it looks like the data we assigned.

    However, when we load that same record back from the database, it looks very different.

    ```elixir
    %GameSave{
      id: 1,
      data: %{
        "turn" => 1,
        "group_types" => ["hero", "mage", "elf"],
        "characters" => [
          %{"name" => "Herman the Hero", "type" => "hero", "health" => 130},
          %{"name" => "Morgan the Mage", "type" => "mage", "health" => 100},
          %{"name" => "Edward the Elf", "type" => "elf", "health" => 100},
        ]
      }
    }
    ```

    Everything in `data` was serialized through the [Jason](https://hex.pm/packages/jason) library. We lost all the atoms, both as keys and values.

    This behavior _is_ expected and [documented](https://hexdocs.pm/ecto/Ecto.Schema.html#module-the-map-type), but it still catches people because at first it looks like it works just how you'd expect.

    From the documentation:

    > Keep in mind that we advise the map keys to be strings or integers instead of atoms. Atoms may be accepted depending on how maps are serialized but the database will always convert atom keys to strings due to security reasons.

    The inconsistency of assigning a map with atoms and later restoring with string keys can create accidental bugs. If we write code that uses the data right after creation when it still has atom keys, those same functions will not work when used on data after it's read from the database.

    Take a look at this function. It works fine right after we create the record. When we load it back, the `characters` key will be a string and this function won't match!

    ```elixir
    def get_group_size(%GameSave{data: %{characters: characters}}) do
      length(characters)
    end
    ```

    Hopefully our tests would catch this early.

    So let's assume that we are careful and only create maps with string keys and no atoms. What are some pros and cons to this approach?

    ### Pros to map approach

    - [Postgres allows us to query](https://www.postgresql.org/docs/current/datatype-json.html) data in the JSON map. This is perhaps the biggest benefit! We can actually query for data based on the contents inside the Postgres `jsonb` field!
    - Postgres tools like [pgAdmin](https://www.pgadmin.org/) know how to display the data and support reading and writing it.
    - This approach is officially encouraged.

    ### Cons to map approach

    - Data needs to be created using string keys and strings for atoms. Depending on where the data is coming from, it may need to be converted manually.
    - Limited to only store what JSON can represent.
    - The potential for accidental bugs when setting data using atoms.

    ## Path Two: Store data as an Erlang encoded binary

    There is another less used approach available to us. We can natively serialize our Elixir data structures and store them directly in the database. Doing this has a different set of trade-offs which we'll explore further.

    First, it's important to know that you can do this:

    ```elixir
    data = %{custom_data: "Yes!", valid_states: [:on, :off]}

    term = :erlang.term_to_binary(data)
    #=> <<131, 116, 0, 0, 0, 2, 100, 0, 11, 99, 117, 115, 116, 111, 109, 95, 100, 97,
    #=>   116, 97, 109, 0, 0, 0, 4, 89, 101, 115, 33, 100, 0, 12, 118, 97, 108, 105,
    #=>   100, 95, 115, 116, 97, 116, 101, 115, 108, 0, 0, 0, 2, 100, ...>>

    restored = :erlang.binary_to_term(term)
    #=> %{custom_data: "Yes!", valid_states: [:on, :off]}
    ```

    In the above code, we create an Elixir map and convert it to a binary term format. Then we convert from the binary format back to the our original data.

    Using [`:erlang.term_to_binary/1`](https://www.erlang.org/doc/man/erlang.html#term_to_binary-1) we can convert _any_ Elixir term to a binary format. The binary data is restored using [`:erlang.binary_to_term/1`](https://www.erlang.org/doc/man/erlang.html#binary_to_term-1).

    Serializing through the [External Term Format](https://www.erlang.org/doc/apps/erts/erl_ext_dist.html) is a documented official feature. It is used for transferring data outside of Elixir/Erlang systems and back. Hey! That's what we're doing!

    If we wanted to try this out on an `Ecto.Schema`, how would we do it?

    ### Migration

    Postgres and Ecto both support `binary` column types.

    ```elixir
    defmodule Core.Repo.Migrations.CreateGameSaves do
      use Ecto.Migration

      def change do
        create table(:game_saves) do
          add :account_id, references(:accounts, on_delete: :delete_all), null: false
          # ...
          # Game data stored as binary
          add :data, :binary, null: false

          timestamps()
        end

      end
    end
    ```

    Notice that our `data` field has the type `:binary`.

    ### Ecto Schema

    To make it easier to store our binary data in our schema, we can create a custom [Ecto.Type](https://hexdocs.pm/ecto/Ecto.Type.html). To do what we want, this is what ours would look like:

    ```elixir
    defmodule Core.EctoErlangBinary do
      @moduledoc """
      A custom Ecto type for handling the serialization of arbitrary
      data types stored as binary data in the database. Requires the
      underlying DB field to be a binary.
      """
      use Ecto.Type
      def type, do: :binary

      @doc """
      Provides custom casting rules for params. Nothing changes here.
      We only need to handle deserialization.
      """
      def cast(:any, term), do: {:ok, term}
      def cast(term), do: {:ok, term}

      @doc """
      Convert the raw binary value from the database back to
      the desired term.
      """
      def load(raw_binary) when is_binary(raw_binary),
        do: {:ok, :erlang.binary_to_term(raw_binary)}

      @doc """
      Converting the data structure to binary for storage.
      """
      def dump(term), do: {:ok, :erlang.term_to_binary(term)}
    end
    ```

    This implements the callbacks for the [Ecto.Type](https://hexdocs.pm/ecto/Ecto.Type.html) behaviour. The important bits here are the `load` and `dump` functions.

    With our custom `Ecto.Type` defined, we can use it in our schema.

    ```elixir
    defmodule Core.GameSaves.GameSave do
      use Ecto.Schema
      alias Core.EctoErlangBinary

      schema "game_saves" do
        # ...
        field :data, EctoErlangBinary, required: true

        belongs_to :account, Core.Accounts.Account
        timestamps()
      end
    end
    ```

    The field `data` uses the type `EctoErlangBinary`. Now, what does it look like to use this?

    ### Writing data

    What happens when we use our same map example?

    ```elixir
    data = %{
      turn: 1,
      group_types: [:hero, :mage, :elf],
      characters: [
        %{name: "Herman the Hero", type: :hero, health: 130},
        %{name: "Morgan the Mage", type: :mage, health: 100},
        %{name: "Edward the Elf", type: :elf, health: 100},
      ]
    }

    {:ok, save} = Core.GameSaves.create_game_save(player_account.id, data)
    save
    ```

    After inserting the data, our struct is returned looking like this:

    ```elixir
    %GameSave{
      id: 1,
      data: %{
        turn: 1,
        group_types: [:hero, :mage, :elf],
        characters: [
          %{name: "Herman the Hero", type: :hero, health: 130},
          %{name: "Morgan the Mage", type: :mage, health: 100},
          %{name: "Edward the Elf", type: :elf, health: 100},
        ]
      }
    }
    ```

    That's what we would expect. The `data` field has the Elixir map we assigned it.

    What happens when we load it from the database?

    ```elixir
    Core.GameSaves.get_game_save!(1)

    %GameSave{
      id: 1,
      data: %{
        turn: 1,
        group_types: [:hero, :mage, :elf],
        characters: [
          %{name: "Herman the Hero", type: :hero, health: 130},
          %{name: "Morgan the Mage", type: :mage, health: 100},
          %{name: "Edward the Elf", type: :elf, health: 100},
        ]
      }
    }
    ```

    After reading back from the database, it stayed the same! Our atom keys stayed atoms and so did our atom values.

    ### Another Example

    For another concrete example, let's talk about modeling a standard card game with face cards. We start by creating the game logic. During the design of the game, we find it makes the most sense to model the cards like this:

    - `{:club, 4}` for a 4 of Clubs
    - `{:heart, "K"}` for a King of Hearts
    - `{:spade, "A"}` for an Ace of Spades

    With this data structure, a players hand can be expressed as a simple keyword list!

    ```elixir
    player_1 = [club: 2, club: 8, spade: 8, heart: 4, heart: "J"]
    ```

    For writing the game logic, a data structure like this makes pattern matching easy. After our game logic is implemented, we turn to serializing the data so it can be recovered when it crashes or a node is shutdown as part of a new deploy.

    If we store the data as a `map` in a `jsonb` field, then we can't serialize a keyword list. That doesn't translate automatically to JSON. We can convert it to a JSON friendly map like this:

    ```elixir
    card = %{
      "suit" => "spade",
      "value" => 8
    }
    ```

    While this data structure works well for JSON, we have to handle transforming our data into this map for storage and then later transform it back to the keyword list format.

    If we store the data as a `binary` using `:erlang.term_to_binary`, we can directly store our keyword list.

    Storing our data in binary is a lot simpler!

    ### Security considerations

    The Ecto [documentation](https://hexdocs.pm/ecto/Ecto.Schema.html#module-the-map-type) explained that it goes through JSON for security reasons. It can be dangerous to deserialize malicious data back into Elixir code.

    It comes down to the risk of converting unknown strings into atoms and possibly exhausting the atom table and crashing the system. So we really need to think about where the stored data comes from. We don't want to store data returned from an external service or anything supplied by a user that we don't explicitly control.

    In this situation, the data is generated by our game code and we define what the values can be. With the current design, a user can't mess with the data stored in the system. However, it's still important to keep this potential risk in mind going forward!

    ### Don't store that!

    Because it's now _possible_ to store any Elixir data structure doesn't mean you should. It may be tempting to store a full Elixir struct in the database but that would be a mistake. The problem comes after we've deployed new versions of our application and the struct in our project no longer looks like the struct stored in the database. This can create unexpected problems for our code!

    Imagine how much our data structures can change over 2 years of active development! What kinds of things can go wrong when restoring an old structure? Lots!

    Here are a few tips when deciding what to store:

    - Instead of storing structs, define a map that stores only enough data to restore things later.
    - Instead of storing fully nested records, store IDs that can be used to reload current versions of those records.
    - Avoid storing extra _runtime_ state that isn't needed for _restoring_ a game. An example of this is something like a timer reference.

    We don't want to just store the whole blob of game state as-is! We still want to be thoughtful and selective about what we store.

    ### Version the data!

    This tip is also true when storing data as a `:map`. The key is to add this to our data from the very beginning so we can always count on it being there.

    The idea is, store a `version` along with our data. It might look like this:

    ```elixir
    data = %{
      version: 1,
      turn: 10,
      # ...
    }
    ```

    This lets us do on-the-fly data migration from older versions.

    The `version: 1` data can easily be pattern matched when we are restoring old data in the future. Our code may now be on version 3, but when we see `version: 1`, we know how to migrate that data to `version: 2` and from `version: 2` we can migrate to `version: 3`.


    Let's review some of pros and cons we've identified for storing our data as a binary field.

    ### Pros to the binary approach

    - Atoms stay atoms.
    - Whatever Elixir data we store is returned. This includes tuples, keyword lists, atoms, binaries, floats, etc.
    - More options on what we want to store and how to store it.
    - Once we add a custom Ecto type, it's very easy to do.

    ### Cons to the binary approach

    - Postgres doesn't understand the binary format. Tools like [pgAdmin](https://www.pgadmin.org/) show a meaningless binary blob.
    - Postgres is unable to search and query within the data.
    - Presents a potential security problem if you can't trust the source of the data.
    - May be tempted to store data we shouldn't.

    <%= partial "shared/posts/cta", locals: {
      title: "Fly ❤️ Elixir",
      text: "Fly is an awesome place to run your Phoenix apps. It's really easy to get started. You can be running in minutes.",
      link_url: "https://fly.io/docs/elixir/",
      link_text: "Try Fly for free&nbsp;&nbsp;<span class='opacity-50'>&rarr;</span>"
    } %>

    ## Closing thoughts

    The longer we are in tech, the more we realize that the answer to a question like, "Which is better, X or Y?" is often, "It depends."

    Here we explored two options for storing custom complex data on a our `Ecto.Schema`. It's like when "Two paths diverged in a wood", the path you choose depends on things like:

    - Where do you want to end up?
    - What features do you value?

    **Path One**: Use the fully supported `:map` data type. It's a great option. It's actually the recommended approach to take by default. It has some gotchas to be aware of but with a little extra work those can be handled.

    **Path Two**: Encode Elixir data structures as `:binary` and store it on the record. This makes it easy to store anything we want with less work. This comes with a number of cautions and conditions.

    Together we've peered down the two paths a bit and hopefully now we can quickly determine which path we want next time we come to this fork in the road.

    As for me, I chose the binary field approach. It fit my needs and had the features I valued. The path you choose to take is up to you!

    Safe travels!

    > Two paths diverged in a project, and I— <br>
    > I took the one less traveled by, <br>
    > And that has made all the difference.
- :id: phoenix-files-pushing-events-with-js-push
  :date: '2022-03-31'
  :category: phoenix-files
  :title: 'Pushing Events: with and without JS.push'
  :author:
  :thumbnail: pushing-events-thumbnail.jpg
  :alt:
  :link: phoenix-files/pushing-events-with-js-push
  :path: phoenix-files/2022-03-31
  :body: "\n\nLiveView [DOM element bindings](https://hexdocs.pm/phoenix_live_view/bindings.html)
    can be used to send events to the server, as well as issue [LiveView JS commands](https://hexdocs.pm/phoenix_live_view/bindings.html#js-commands)
    on the client.\n\n[In another post](https://fly.io/phoenix-files/tabs-with-js-commands/),
    we used client-side JS commands to show and hide content in a set of tabs, just
    by manipulating DOM element attributes.\n\n`JS.push` is a bit different; it has
    one foot on the client side and one on the server side. On its own, `JS.push`
    provides a combined API for pushing events to the server, specifying targets and
    payloads, and customizing loading states. As a LiveView JS command, it's composable
    with the other JS commands to coordinate more complex, optimistic client-side
    effects.\n\nIn its basic event-pushing functionality, `JS.push` provides an alternative
    syntax for some things you can already do with `phx-*` bindings alone.\n\nLet's
    take a closer look at how we can migrate between pure `phx-*` pushes and `JS.push`.
    Then we'll take it one step further and combine a push with an asynchronous transition
    effect, by composing `JS.push` with `JS.transition`.\n\n## Sending a simple click
    event to the server\n\nIf all you want to do is send an event called `clicked`
    to the server, you can use the `phx-click` binding and call it a day.\n\n```elixir\n<button
    phx-click=\"clicked\">Don't panic!</button>\n```\n\nHere's how we can write the
    exact same thing using the `JS.push` API:\n\n```elixir\n<button phx-click={JS.push(\"clicked\")}>Don't
    panic!</button>\n```\n\nThis looks a _little_ bit silly. But it will work! Either
    way, we're sending the `clicked` event to the server, where we'd have a `handle_event`
    callback defined that knows what to do with `clicked` events.\n\nHere's a super-simple
    callback that can receive our event and print the string `Handling clicked event`
    to the iex console.\n\n```elixir\ndef handle_event(\"clicked\", _values, socket)
    do\n  IO.inspect(\"Handling clicked event\")\n  {:noreply, socket}\nend\n```\n\n##
    A click with a payload\n\nSay we want to send an event called `clicked`, with
    a payload of parameter values: `val1`, `val2`, and `val3`. We can do it with `phx-click`
    and `phx-value-*`:\n\n```elixir\n<button \n    phx-click=\"clicked\" \n    phx-value-val1=\"At\"
    \n    phx-value-val2=\"the\" \n    phx-value-val3=\"disco!\">\n   Panic!\n</button>\n```\n\nOr
    we can use `JS.push`, passing a map of values using its `value` option:\n\n```elixir\n<button
    \n    phx-click={ \n        JS.push(\"clicked\", \n          value: %{val1: \"At\",
    val2: \"the\", val3: \"disco!\"})\n    }\n>\n  Panic!\n</button>\n```\n\nEither
    way, our `handle_event` callback receives a map as its `values` parameter.\n\nThe
    following example callback assigns values for variables `val1`, `val2`, and `val3`
    from the `values` map, and concatenates them into a string called `content`. Finally,
    it uses this string to update the value of the `content`  _assign_ and returns
    the updated socket.\n\n```elixir\ndef handle_event(\"clicked\", values, socket)
    do\n  %{\"val1\" => val1, \"val2\" => val2, \"val3\" => val3} = values\n  content
    = val1 <> \" \" <> val2 <> \" \" <> val3\n  {:noreply, assign(socket, :content,
    content)}\nend\n```\n\nSome part of our LiveComponent would render the contents
    of the `content` assign; for example:\n\n<%= video_tag \"pushing-events-01.mp4?card?center\",
    title: \"When a user clicks on a button, the string \\\"At the disco!\\\" is rendered
    inside a div.\" %>\n\nWhen the button is clicked, `content` gets updated and our
    concatenated string appears in the rectangle.\n\n## Pushing the event to a specific
    target\n\nWe can send events to a particular LiveView or LiveComponent, by specifying
    a DOM selector that matches  an element or elements inside it. The owner of each
    matching DOM element will automatically receive the event.\n\nImagine we've defined
    a LiveView named `ContentLive` and we render two  `ContentLive`s with DOM IDs
    `content1` and `content2`.\n\n```elixir\n<%%= live_render(@socket, ContentLive,
    id: \"content1\", session: %{}})%>\n<%%= live_render(@socket, ContentLive, id:
    \"content2\", session: %{}})%>\n```\n\nAnd the HTML content rendered inside them
    is the following:\n\n```elixir\n~H\"\"\"\n<div class=\"container_content\">\n
    \ <h1><%%= @content  %></h1>\n</div>\n\"\"\"\n```\n\nEach `ContentLive` will render
    one div with the class `container_content`.  We can target an event to both divs
    using the class selector as follows.\n\nWith `phx-click` and `phx-target`:\n\n```elixir\n<button
    phx-target=\".container_content\" phx-click=\"clicked\">\n  Panic!\n</button>\n```\n\nWith
    `JS.push`:\n\n```elixir\n<button phx-click={JS.push(\"clicked\", target: \".container_content\")}>\n
    \ Panic!\n</button>\n```\n\nEither way, both of our `ContentLive`  LiveViews will
    receive and handle the `clicked` event, because they each contain an element with
    class `container_content`. We didn't need to specify the `ContentLive`s by id.\n\n<%=
    video_tag \"pushing-events-02.mp4?card?center\", title: \"When a user clicks on
    a button, the string \\\"At the disco!\\\" is rendered inside two different divs.\"
    %>\n\nWe  _can_ target a LiveView or LiveComponent  directly by id if we want
    to:\n\n```elixir\n# pure phx-bindings way\n<button phx-target=\"#content1\" phx-click=\"clicked\">\n
    \ Panic!\n</button>\n\n# JS.push way\n<button phx-click={JS.push(\"clicked\",
    target: \"#content1\")}>\n  Panic!\n</button>\n```\n\n<%= partial \"shared/posts/cta\",
    locals: {\n  title: \"Fly ❤️ Elixir\",\n  text: \"Fly is an awesome place to run
    your Elixir apps. It's really easy to get started. You can be running in minutes.\",\n
    \ link_url: \"https://fly.io/docs/elixir/\",\n  link_text: \"Deploy your Elixir
    app today!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>\"\n} %>\n\n## Composing
    JS.push with other JS commands\n\nIn all the above scenarios, `JS.push` is doing
    nothing more than what the `phx-*` bindings are doing. We haven't seen any compelling
    reason to migrate to the `JS.push` syntax.\n\n<aside class=\"right-sidenote\">For
    the moment, we're putting aside the loading-state tweaks that `JS.push` enables
    through its `loading` and `page_loading` options. </aside>\n\nBut here's a cool
    trick! If we're using `JS.push` to send our event, we can orchestrate client-side
    transition effects along with our push, simply by composing it with other  [LiveView
    JS commands](https://hexdocs.pm/phoenix_live_view/Phoenix.LiveView.JS.html)!\n\nLet's
    try with the `JS.transition` command:\n\n```elixir\n<button phx-click={\n    JS.push(\"clicked\",
    value: %{val1: \"At\", val2: \"the\", val3: \"disco!\"})\n    |> JS.transition(\"shake\",
    to: \"#content\")\n  }\n>\n  Panic! \n</button>\n```\n\nThis applies a transition
    named `shake` to the div with id `content`, totally client-side, at the same time
    the event is being processed and the `content` assign is changed.\n\n<%= video_tag
    \"pushing-events-03.mp4?card&center\", title: \"When a user clicks on a button,
    the string \\\"At the disco!\\\" is rendered inside a div and a shake transition
    is executed\" %>\n\n\nEven when `JS.push`  _itself_ is replicating push behavior
    that can be achieved with `phx-*` bindings alone, we've found a use-case for preferring
    the `JS.push` syntax, and one of its _raisons d'être_ as a LiveView JS command."
- :id: blog-a-foolish-consistency
  :date: '2022-03-29'
  :category: blog
  :title: 'A Foolish Consistency: Consul at Fly.io'
  :author: thomas
  :thumbnail: default-books-thumbnail.png
  :alt:
  :link: blog/a-foolish-consistency
  :path: blog/2022-03-29
  :body: "\n\n<p class=\"lead\">[Fly.io](http://Fly.io) runs applications by transmogrifying
    Docker containers into Firecracker micro-VMs running on our hardware around the
    world, connected with WireGuard to a global Anycast network. Yours could be one
    of them! [Check us out](https://fly.io/docs/speedrun/): from a working container,
    your app can be running worldwide in minutes.</p>\n\nWe set the scene, as usual,
    with sandwiches. Dig if you will the picture: a global Sandwich Bracket application,
    ascertaining once and for all the [greatest sandwich on the planet](https://jpgraziano.com/).\n\n[Fly.io](http://Fly.io)
    wants our app, `sandwich-bracket`, deployed close to users around the world. Chicago
    users vote for [Italian beefs](https://www.tripadvisor.com/Restaurant_Review-g35956-d502500-Reviews-Johnnie_s_Beef-Elmwood_Park_Illinois.html)
    on an instance of `sandwich-bracket` in Chicago; people who love bánh mì are probably
    voting on a Sydney instance, egg salad on white bread, Tokyo.\n\nTo run a platform
    that makes this kind of thing work, we need a way to route incoming traffic to
    instances. The way we do that is with service discovery: a distributed catalog
    of all services running at Fly.io. The Fly.io service catalog lives in [Consul](https://www.consul.io/).
    The catalog expands consciousness. The catalog is vital to space travel.\n\nThe
    catalog occupies more of our mental energy than just about anything at Fly.io.
    We've sunk a huge amount of energy into keeping São Paulo, Sydney, Singapore,
    and points between consistent in their view of what's running on Fly.io, scaling
    a single global Consul cluster. What we think we've learned is that keeping São
    Paulo and Sydney on exactly the same page about what's running in Mumbai is a
    mug's game, and we shouldn't be playing it.\n\nAnd so, to begin, it is my privilege
    to inflict Consul on you.\n\n### What the hell is Consul?\n\n<aside class=\"right-sidenote\">a
    “service” is whatever you say it is, but the classic example would be an HTTP
    micro-service server</aside>\n\nConsul is a distributed database that attempts
    to be a source of truth for which services are currently running. It’s [one of
    several](https://aphyr.com/posts/316-jepsen-etcd-and-consul) “service coordination”
    or “service discovery” databases; the other popular ones are [Etcd](https://etcd.io/),
    which once powered Kubernetes, and [Zookeeper](https://zookeeper.apache.org/),
    the original service coordinator, important in the Java/Hadoop ecosystem.\n\nThe
    challenge of these databases, the reason they’re not just trivial MySQL instances,
    is that you can’t just have one of them. Once you start relying on service discovery,
    it can't go down, or your applications all break. So you end up with a cluster
    of databases, which have to agree with each other, even as services come and go.
    These systems all expend a lot of effort, and make a lot of compromises, in order
    to cough up consistent answers on flappy networks with fallible servers where
    individual components can fail.\n\nHow Consul works is that you have a cluster
    of “Consul Servers” — maybe 3, 5, or 7 — and then all the rest of your machines
    run a “Consul Agent” that talks to the Servers. The Servers execute the [Raft
    consensus protocol](https://raft.github.io/), maintaining a log of updates that
    form the basis for the database. Agents, in turn, inform the servers about events,
    such as an instance of a service terminating. An Agent can talk to any Server,
    but the Servers elect a leader, and updates are routed to the leader, which coordinates
    the Raft update to the log.\n\nWith me so far? Neither am I. But the specifics
    don’t matter much, as long as you understand that every machine in our fleet runs
    a lightweight Consul Agent that relays events to Consul Servers, which we only
    have a few of, locked in an unending and arcane ritual of consensus-tracking,
    producing: a map of every service running on [Fly.io](http://Fly.io), and exactly
    where it’s running. [Plus some other stuff](#other-consul-uses).\n\nLet's see
    it in action.\n\n### Consul at Fly.io\n\nA Consul \"service\" at Fly.io, in the
    main, is an exposed port on an app a user deployed here. An \"instance\" of  a
    service is a VM exposing that port. A \"node\" is one of Fly.io's own servers.\n\nWe
    run a couple different kinds of servers, among them lightweight “edges” handling
    Internet traffic, and chonky “workers” running customer VMs. Both run [`fly-proxy`,
    our Rust+Tokio+Hyper proxy server](https://fly.io/blog/the-tokio-1-x-upgrade/).\n\nEvery
    app running on [Fly.io](http://Fly.io) gets a [unique, routable IPv4 address](https://fly.io/blog/32-bit-real-estate/).\n\nThat’s
    how our CDN works: we advertise these addresses, the same addresses, from dozens
    of data centers around the world, with BGP4 (this is “Anycast”). Backbone routing
    takes you to the closest one. Say `sandwich-bracket` is currently deployed in
    Frankfurt and Sydney. Anycast means the votes for doner land on a [Fly.io](http://Fly.io)
    edge in Frankfurt, right next to a worker running `sandwich-bracket`; A bánh mì
    vote lands on a Sydney edge, and is routed to a Sydney worker.  We're not deployed
    in Tokyo, so a vote for egg salad hits a Tokyo edge, and gets routed… out of Japan.\n\nThe
    problem facing `fly-proxy` is, “where do I send this egg salad vote”. “The garbage”
    being, unfortunately, not a valid answer, `fly-proxy` needs to know which workers
    your `sandwich-bracket` app is running on, and then it needs to pick one to route
    to.\n\nHere's the data we're working with:\n\n<div id=\"pricing-1\">\n| data |
    stability | what |\n| --- | --- | --- |\n| nodes | very stable | network locations
    of workers and edges |\n| app IPs | stable | match incoming traffic to apps |\n|
    services | changing | pinpoint instances to route to |\n| health | flappy |  pinpoint
    instances to avoid routing to |\n| load | untenably flappy | enable load balancing
    |\n</div>\n\nEnter Consul. When you first created `sandwich-bracket` with our
    API, we:\n\n1. allocated an IPv4 address for it \n1. wrote it to Consul’s KV store\n1.
    deployed it to, say, a worker node in Frankfurt\n1. when that instance came up,
    our orchestration code registered the service instance with Consul. \n\n<aside
    class=\"right-sidenote\">`consul-templaterb` is [Pierre Souchay's](https://medium.com/@pierresouchay)
    Ruby rewrite of `consul-template`, which comes with Consul. Track Consul, write
    a template, and then run a program that eats the file.</aside>\n\nThe simplest
    way to integrate all that information, and what we did until a couple months ago,
    is: we’d run `consul-templaterb` and ask it to track every service in Consul and
    sync a JSON file with the data; when the file is updated, `fly-proxy` gets a signal
    and re-reads it into memory.\n\nTheoretically, Consul can tell us how “close”
    each of those services are [– Consul puts a bunch of work into network telemetry](https://fly.io/blog/building-clusters-with-serf/)
    — but we do that bit ourselves, and so there’s another JSON file that `fly-proxy`
    watches to track network distance to every server in our fleet.\n\n<aside class=\"right-sidenote\">Consul
    experts: avert your eyes</aside>\nConsul doesn't give us the load (in concurrent
    requests) on all the services. For a long time, we abused Consul for this, too:
    we tracked load in Consul KV. Never do this! Today, we use a messaging system
    to gossip load across our fleet.\n\n<div class=\"callout\">\nSpecifically, we
    use [NATS](https://nats.io/), a simple \"brokered\" asynchronous messaging system,
    the pretentious way to say \"almost exactly like IRC, but for programs instead
    of people\".\n\nUnlike Consul, NATS is neither consistent nor reliable. That's
    what we like about it. We can get our heads around it. That's a big deal: it's
    easy to get billed for a lot of complexity by systems that solve problems 90%
    similar to yours. It seems like a win, but that 10% is murder. So our service
    discovery will likely never involve an event-streaming platform like Kafka.\n</div>\n\nPutting
    it all together, you have a sense of how our control plane works. Say the World
    Sandwich Authority declares doner is no longer a sandwich, and Japanese biochemists
    invent an even fluffier white bread. Traffic plummets in Frankfurt and skyrockets
    in Tokyo. We move our Frankfurt instance to Tokyo (`flyctl regions set syd nrt`).
    This kills the Frankfurt instance, and Frankfurt's Consul Agent deregisters it.
    JSON files update across the fleet. The Tokyo instance comes up and gets registered;
    more JSON churn.\n\n<a name=\"other-consul-uses\">We use Consul for other stuff!\n\n-
    Apps on [Fly.io](http://Fly.io) belong to “organizations”, and apps within an
    organization, all over the world, [can talk to each other on 6PN private networks](https://fly.io/blog/incoming-6pn-private-networks/).
    The DNS records for those 6PN addresses are derived (mostly) from Consul.\n- One
    of Consul’s big features is “health checks” — it’ll report the health of a service
    based on configurable checks, so we don’t route traffic to instances that are
    having problems.\n- Consul used to be [how we propagated WireGuard information
    for our users](https://fly.io/blog/our-user-mode-wireguard-year/), which is how
    `flyctl` works. \n- Consul is also the source of truth for the WireGuard mesh
    network that links every machine in our fleet.\n\n### It Burns\n\nIt looks like
    textbook Consul. But it's not, really.\n\nConsul is designed to make it easy to
    manage a single engineering team's applications. We're managing deployments for
    thousands of teams. It's led us to a somewhat dysfunctional relationship.\n\nTo
    start with, we have a single Consul namespace, and a single global Consul cluster.
    This seems nuts. You can federate Consul. But every Fly.io data center needs  details
    \ for every app running on the planet! Federating costs us the global KV store.
    We can engineer around that, but then we might as well not use Consul at all.\n\n<aside
    class=\"right-sidenote\">“Tracking” data in Consul generally involves [long-polling
    an HTTP endpoint](https://www.consul.io/api-docs/features/blocking). Each HTTP
    request bears an index, which in turn tracks the Raft log; as a user, you don’t
    care about what this value means, only that if it changed, there’s new data. This
    is fine.</aside>\n\nConsul's API was also not designed with our needs in mind
    (nor should it have been). It's got a reasonable API for tracking some things,
    but not quite the things we need. So, for instance, there’s an HTTP endpoint we
    can long-poll to [track the catalog of services](https://www.consul.io/api-docs/catalog).
    But:\n\n1. For any endpoint in the Consul HTTP API, if something changes, we get
    a refresh of all the data for that endpoint, which means incremental changes,
    which happen every few seconds, are expensive. \n1. What’s worse (and not Consul’s
    fault), tooling like `consul-templaterb` means we’re constantly rewriting and
    rereading large JSON files to register those small changes.\n1. The “[catalog
    of services](https://www.consul.io/api-docs/catalog#list-services)” endpoint returns
    information about the services (the `sandwich-brackets` app), but not the metadata
    for individual instances of those services (the instances of `sandwich-brackets`
    in `nrt` and `syd`).\n\nThat second problem is kind of a nightmare. We have tens
    of thousands of  distinct services. We need per-instance metadata for every instance
    of those services.\n\nConsul can give us that metadata in two ways: by asking
    it about individual services, one-by-one, or by asking for service catalogs from
    each of our servers. We can't long poll tens of thousands of endpoints. So, the
    way we get instance metadata from Consul is to ask it about servers, not services.
    We long-poll [an API endpoint for each individual server](https://www.consul.io/api-docs/catalog#retrieve-map-of-services-for-a-node).
    There’s no one endpoint that we can long-poll for all the nodes.\n\n<div class=\"callout\">\nYou
    might at this point ask why we’re storing this kind of stuff in instance metadata
    at all. That’s a good question with a complicated answer. Some of it has to do
    with [Nomad](https://www.nomadproject.io/), the orchestration service we use (and
    are gradually moving away from) to actually run VM jobs. Information about, say,
    what ports an app listens on percolates from a Nomad task description into its
    Consul service registration; that’s [just how Nomad works.](https://www.nomadproject.io/docs/integrations/consul-integration)\n\nYou
    also can’t just factor the information out into, say, a Consul KV tree, because
    apps have versions, and different versions of apps listen on different ports,
    and `fly-proxy` needs to track them.\n\nThis stuff is all solvable! But, like,
    are you going to solve it by using Consul more carefully, or are you going to
    solve it by using Consul less?\n</div>\n\nWhich is how it came to be that we found
    ourselves driving over 10 (t-e-n) gb/sec of Consul traffic across our fleet. Meanwhile,
    and I haven’t done the math, but it’s possible that the underlying data, carefully
    formatted and compressed, might fit on a dialup modem.\n\nThis, it turns out,
    was Not Entirely Our Fault. Long-suffering SRE Will Jordan, his brain shattered
    by ten gigabits of sustained Consul traffic, dove into the Consul codebase and
    [discovered a bug](https://github.com/hashicorp/consul/issues/12398): updates
    anywhere in Consul un-blocked every long-polling query. We had tens of thousands,
    N^2 (don’t email me!) in the number of nodes, all of which return a full refresh
    of the data they’re tracking when they unblock. Anyways, [Will wrote a couple
    dozen lines of Go,](https://github.com/hashicorp/consul/pull/12399) and:\n\n![A
    gruesome bandwidth graph](hashi-graph.png?&centered)\n\n## Extricating Ourselves\n\nSo,
    `consul-templaterb`is easy, but rough at the scale we work at. It runs Ruby code
    to to track updates that happen multiple times per second, each time writing giant
    JSON blobs to disk.\n\n<aside class=\"right-sidenote\">One of Consul's big-ticket
    features is a built-in DNS server. We don't use it; we need our server to do silly
    stuff like `top3.nearest.of.my-app.internal`.</aside>\n\nWe felt this acutely
    with private DNS. Consul propagates the data that our DNS servers use (it's similar
    to the data that `fly-proxy` uses). [Fly.io Postgres](https://fly.io/blog/free-postgres/)
    depends on these DNS records, so it needs to work.\n\nOur DNS server was originally
    written in Rust, and used `consul-templaterb` the way `fly-proxy` did, but wrote
    its updates to a sqlite database. At certain times, for certain workers, we’d
    experience double-digit second delays after instances came up — or worse, after
    they terminated. This is a big deal: it's a window of many seconds during which
    internal requests get routed to nonexistent hosts; worse, the requests aren't
    being handled by our smart proxy, but by people's random app networking code.\n\nWe
    blamed Consul and `consul-templaterb`.\n\nTo fix this, we rewrote the DNS server
    (in Go), so that it tracked Consul directly, using Consul’s Go API, rather than
    relying on `consul-templaterb`. We also had it take “hints” directly from our
    orchestration code, via NATS messages, for instances starting and stopping.\n\n<div
    class=\"callout\">\nRewriting a Rust program in Go is sacrilege, we know, but
    Go had the Consul libraries we needed, and, long term, that Go server is going
    to end up baked into our orchestration code, which is already in Go.\n</div>\n\nIt
    turns out that what we really want (if not our dream Consul API) is a local sqlite
    cache of all of Consul’s state. That way our proxy, WireGuard code, DNS servers,
    and everything else can track updates across our fleet without a lot of complicated
    SRE work to make sure we’re interfacing with Consul properly.\n\nBy rewriting
    our DNS server, we'd inadvertently built most of that. So we extracted its Consul-tracking
    code and gave it an identity of its own, `attache`. `attache` runs on all our
    hosts and tracks most of Consul in sqlite. In theory, infra services at [Fly.io](http://Fly.io)
    don’t need to know anything about Consul anymore, just the schema for that database.\n\nNew
    architectural possibilities are becoming apparent.\n\nTake that horrible N^2 polling
    problem. Since we've abstracted Consul out, we really don't need a better Consul
    API; we just need an `attache` API, so a \"follower\" `attache` can sync from
    a \"leader\". Then we could run a small number of leaders around the world — maybe
    just alongside Consul Servers, so that almost all our Consul read traffic would
    be from machines local to the Consul Servers. We'd chain lots of followers from
    them, and possibly scale Consul indefinitely.\n\nWe can get clever about things,
    too. What we'd really be doing with \"leader\" and \"follower\" `attache` is replicating
    a sqlite database. That's already a solved problem! There's an amazing project
    called [Litestream](https://litestream.io/) that [hooks](https://litestream.io/how-it-works/)
    sqlite's [WAL checkpointing](https://sqlite.org/wal.html) process and [ships WAL
    frames to storage services](https://litestream.io/how-it-works/). Instead of building
    a new `attache` event streaming system, we could just set up \"followers\" to
    use Litestream to replicate the \"leader\" database.\n\n<%= partial \"shared/posts/cta\",
    locals: {\n  title: \"You don't really have to think about any of this.\",\n  text:
    \"It's just a couple commands to get an app deployed on Fly.io; no service discovery
    required.\",\n  link_url: \"https://fly.io/docs/speedrun/\",\n  link_text: \"Try
    Fly for free&nbsp;&nbsp;<span class='opacity-50'>&rarr;</span>\"\n} %>\n\n###
    We Don't Want Raft\n\nWe're probably not going to do any of that, though, because
    we're increasingly convinced that's sinking engineering work into the wrong problem.\n\nFirst,
    if you haven't read the Google Research paper \"[The Tail At Scale](https://research.google/pubs/pub40801/)\",
    drop everything and remedy that. It's amazing, an easy read, and influential at
    Fly.io.\n\nThen: the problem we're solving with `attache` is \"creating a globally
    consistent map of all the apps running across our fleet\". But that's not our
    real problem. Our real problem is \"generate fast valid responses from incoming
    requests\". Solving the former problem is hard. It's one answer to the real problem,
    but not necessarily the optimal one.\n\nNo matter how we synchronize service catalogs,
    we're still beclowned by the speed of light. Our routing code always races our
    synchronization code. Routing in Tokyo is impacted by events happening in  São
    Paolo (sandwich: mortadella on a roll), and events in São Paolo have a 260ms head
    start.\n\nA different strategy for our real problem is request routing that's
    resilient to variability. That means a distributing enough information to make
    smart first decisions, and smart routing to handle the stale data. We've already
    had to build some of that. For instance, if we route a request from an edge to
    a worker whose instances have died, `fly-proxy` replays it elsewhere.\n\nYou can
    finesse this stuff. \"The Tail At Scale\" discusses \"hedged\" requests, which
    are sometimes forwarded to multiple servers (after cleverly waiting for the 95th
    percentile latency); you take the first response. Google also uses \"tied\" requests,
    which fan out to multiple servers that can each call dibs, canceling the other
    handlers.\n\nWe'd do all of this stuff if we could. Google works on harder problems
    than we do, but they have an advantage: they own their applications. So, for instance,
    they can engineer their protocols to break large, highly-variable requests into
    smaller units of work, allowing them to interleave heavyweight tasks with latency-sensitive
    interactive ones to eliminate head-of-line blocking. We haven't figured out how
    to do that with your Django POST and GET requests. But we're working on it!\n\nA
    lot of our problems are also just simpler than Raft makes them out to be. We used
    to use Consul to synchronize load information. But that's kind of silly: Consul
    is slower than the feed of load events, and with events sourced globally, we never
    could have had a picture that was both fresh and accurate. What we needed were
    hints, not databases, and that's what we have now: we use NATS (which isn't necessarily
    even reliably delivered, let alone consensus-based) to gossip load.\n\nSame goes
    for health checks. We already have to be resilient to stale health check information
    (it can and does change during request routing). Consul keeps a picture of health
    status, and we do still use it to restart ailing instances of apps and report
    status to our users. But `fly-proxy` doesn't use it anymore, and shouldn't: it
    does a fine job generating and gossiping health hints on its own.\n\n### But,
    As Always: DNS\n\nWe can make our routing resilient to stale health and load information,
    and push orchestration decisions closer to workers to be less reliant on distributed
    health events. We have a lot of flexibility with our own infrastructure.\n\nThat
    flexibility stops at the doors of the VM running your Django app. We don't own
    your app; we don't really even know what it does. We're [at the mercy of your
    socket code.](https://christoph.luppri.ch/fixing-dns-resolution-for-ruby-on-alpine-linux)\n\nSo
    one place we're kind of stuck with Consul-style strongly consistent service maps
    is DNS. Normal apps simply aren't built to assume that DNS can be a moving target.
    If your app looks up `sandwich-postgres.internal`, it needs to get a valid address;
    if it gets nothing, or, worse, the address of a VM that terminated 750ms ago,
    it'll probably break, and break in hyper-annoying ways that we don't yet have
    clever ways to detect.\n\nWe've spent the better part of 9 months improving the
    performance of our `.internal` DNS, which is a lot for a feature that was an afterthought
    when I threw it together. We're in a sane place right now, but we can't stay here
    forever.\n\nWhat we're going to do instead – you'll see it soon on our platform
    – is play the ultimate CS trump card, and `add another layer of indirection`.
    Apps on Fly.io are getting, in addition to the DNS names they have today, \"internal
    Anycast\": a stable address that routes over `fly-proxy`, so we can use the same
    routing smarts we're using for Internet traffic for Postgres and Redis.\n\nYou'll
    still get to be fussy about connectivity between your internal apps! Internal
    Anycast is optional. But it's where our heads are at with making internal connectivity
    resilient and efficient.\n\n## And So\n\nWe mostly like Consul and would use it
    again in new designs. It’s easy to stand up. It’s incredibly useful to deploy
    infrastructure configurations. For example: we write blog posts like this and
    people invariably comment about how cool it is that we have a WireGuard mesh network
    between all of our machines. But, not to diminish Steve’s work on flywire, that
    system falls straight out of us using Consul. It’s great!\n\nBut we probably wouldn’t
    use Consul as the backing store for a global app platform again, in part because
    a global app platform might not even want a single globally consistent backing
    store. Our trajectory is away from it.\n\n"
- :id: phoenix-files-liveview-bootstrap-card
  :date: '2022-03-23'
  :category: phoenix-files
  :title: LiveView card components with Bootstrap
  :author: sophie
  :thumbnail: components-thumbnail.jpg
  :alt:
  :link: phoenix-files/liveview-bootstrap-card
  :path: phoenix-files/2022-03-23
  :body: "\n\n<p class=\"lead\">This is a guest post from Sophie DeBenedetto, co-author
    of [*Programming Phoenix LiveView*](https://pragprog.com/titles/liveview/programming-phoenix-liveview/).
    You can [get started](/docs/elixir/) with your own Elixir Phoenix application
    in a jiffy!</p>\n\nYou may already be reaching for LiveView components to wrap
    up the behavior and markup of distinct portions of your LiveView UI.\n\nIn this
    post, we’re going to take a single-purpose component that displays book review
    data in Bootstrap-style card format, and transform it into a highly-reusable component
    that you can use throughout your application, using LiveView component slots.\n\nWe&#39;ll
    start with a simple component that uses the default slot, then we&#39;ll use named
    slots and teach our component to yield variables back up to the caller. Finally,
    we&#39;ll use iteration and named slots to dynamically render nested function
    components.\n\n## The Goal\n\nWe want to be able to dynamically render content
    within the markup of a [Bootstrap card](https://getbootstrap.com/docs/4.0/components/card/).
    When we’re done, we’ll have a set of nested function components that iterates
    over a list of items, and uses named slots to render a card for each one. Our
    function components will be generic and reusable—they won’t have awareness of
    what content they’re rendering, so you’ll be able to use them again and again
    throughout your app to display Bootstrap cards.\n\n## The Feature: Display Book
    Reviews\n\nIn this post, we&#39;ll build a Bootstrap-style card component and
    use it to render a set of book reviews like this:\n\n![Screenshot of a list of
    three book reviews, each with a star rating and review text.](components-01.png)\n\nWe&#39;ll
    use our reusable card component to render each book review&#39;s details, including
    the number of stars, the email of the user who left the review, and the details
    of the review.\n\nWe&#39;ll start with a simple component that iterates over the
    list of a book&#39;s reviews and renders each one in a card. Let&#39;s begin.\n\n##
    The Simple Card Component\n\nFirst up, we&#39;ll define a module that implements
    a `cards/1` function component to iterate over book reviews and display each one
    in a Bootstrap card.\n\n```erb\ndefmodule LiveLibrary.CardComponent do\n  use
    Phoenix.Component\n \n  def cards(assigns) do\n    ~H\"\"\"\n    <%%= for review
    <- @reviews do %>\n      <div class=\"card review-card\">\n        <div class=\"card-header\">\n
    \         <%%= for _ <- 0..review.stars do %>\n            <i class=\"bi bi-star-fill\"></i>\n
    \         <%% end %>\n        </div>\n        <div class=\"card-body\">\n          <h5
    class=\"card-title\">\n            <strong>by: <%%= review.user.email %></strong>\n
    \         </h5>\n          <p class=\"card-text\">\n            <%%= review.notes
    %>\n          </p>\n        </div>\n      </div>\n    <%% end %>\n    \"\"\"\n
    \ end\n\nend\n```\n\nOur function component is simple enough&mdash;it expects
    the provided assigns to include a `@reviews` assignment. It iterates over those
    reviews and renders a Bootstrap card to display the details of each one. Now,
    we can call on our function component from another template like this:\n\n```elixir\n<CardComponent.cards
    reviews={@book.reviews} />\n```\n\nAnd the browser should display the following:\n\n![Screenshot
    of a browser window, including the review \"cards\" seen in the first figure.](components-02.png)\n\nOur
    simple function component is a clean way for us to encapsulate the markup used
    for rendering a book review. \n\nThis may be perfect for our needs. But we can
    make our Bootstrap card more reusable. Right now the `CardComponent.cards/1` function
    component contains markup for rendering content in a card, but that content is
    hard-coded to display book review details. We can imagine reusing this card markup
    elsewhere in our app to display different types of content. Let&#39;s refactor
    our component to make it a little more reusable.\n\n## Dynamic Components with
    the Default Slot\n\nWe&#39;ll start by taking advantage of a component feature
    called &quot;slots&quot;. Slots allow us to provide our component with blocks
    of HTML to render via a simple syntax:\n\n```elixir\n<CardComponent.cards reviews={@book.reviews}
    >\n<!-- some content -->\n</CardComponent.cards >\n```\n\nFirst, we&#39;ll take
    a look at how we want to be able to call our function component, then we&#39;ll
    build out that functionality with slots.\n\nWe want our card component to only
    be responsible for rendering card markup&mdash;it shouldn&#39;t know anything
    about the content it&#39;s rendering within that markup. In other words, it shouldn&#39;t
    need awareness that the content we&#39;re rendering describes a book review.\n\nAs
    an intermediate step, we&#39;ll start with a simplified version of our function
    component that _only_ renders the card header:\n\n```erb\ndef cards(assigns) do\n
    \ ~H\"\"\"\n  <%%= for review <- @reviews do %>\n    <div class=\"card review-card\">\n
    \     <div class=\"card-header\">\n        <%%= for _ <- 0..review.stars do %>\n
    \         <i class=\"bi bi-star-fill\"></i>\n        <%% end %>\n      </div>\n
    \   </div>\n  <%% end %>\n  \"\"\"\nend\n```\n\nNow, if we check out our component
    in the browser, we should see this:\n\n![Just the star ratings from three reviews](components-03.png)\n\nNext
    up, we&#39;ll make the `cards/1` function component a little less smart. We&#39;ll
    refactor the `@reviews` assignment to an `@items` one and we&#39;ll use the default
    slot to dynamically render content into the card header:\n\n```erb\ndef cards(assigns)
    do\n  ~H\"\"\"\n  <%%= for item <- @items do %>\n    <div class=\"card review-card\">\n
    \     <div class=\"card-header\">\n        <%%= render_slot(@inner_block, item)\n
    \     </div>\n    </div>\n  <%% end %>\n  \"\"\"\nend\n```\n\nWe&#39;ll break
    down how this works in a bit. First, let&#39;s take a look at how we&#39;ll render
    this component:\n\n```erb\n<CardComponent.cards let={review} items={@book.reviews}
    >\n  <%%= for _ <- 0..review.stars do %>\n    <i class=\"bi bi-star-fill\"></i>\n
    \ <%% end %>\n</CardComponent.cards >\n```\n\nIn between the opening and closing
    component tags, we&#39;re telling the component to render a Bootstrap star icon
    for each of the stars in a given review. This content is made available to the
    component as the `@inner_block` assigns, and the component renders it by invoking
    `render_slot/2` with a first argument of `@inner_block`. Let&#39;s take a closer
    look at the second argument that we&#39;re passing to `render_slot/2` now.\n\nWe&#39;re
    able to refer to the `review` variable in the content between our component tags
    because we&#39;re telling the function component to yield a variable back up to
    the caller by passing a second argument to `render_slot/2`. So, in the function
    component, we&#39;re iterating through the list of items in the `@items` assigns.
    Then, when we call `render_slot/2`, we pass it a second argument of an individual
    `item`. This passes the value of the `item` variable back up to the caller and
    sets it equal to a variable, `review`, that we specified in the `let` assignment
    of our opening component tag.\n\nWith this, we&#39;re left with a highly dynamic
    function component that wraps up some card markup and leaves the awareness of
    the content to render in that markup entirely up to the caller.\n\nOur usage of
    the default slot that can render the `@inner_block` assignment is a little limiting
    though. We can only render one segment of content as the `@inner_block`, and we
    need to inject not only card header content, but also the card title and body
    content. We can do exactly that with the help of named slots.\n\n## Extending
    The Component With Named Slots\n\nWe&#39;ll implement a named slot for each of
    the sections of the card component we need to inject content into. Let&#39;s begin
    once again by writing out how we want to be able to call our updated component:\n\n```erb\n<CardComponent.cards
    items={@book.reviews} >\n  <:header let={review}>\n    <%%= for _ <- 0..review.stars
    do %>\n      <i class=\"bi bi-star-fill\"></i>\n    <%% end %>\n  </:header>\n
    \ \n  <:title let={review}>\n    by: <%%= review.user.email %>\n  </:title>\n
    \ \n  <:body let={review}>\n    <%%= review.notes %>\n  </:body>\n</CardComponent.cards
    >\n```\n\nHere, we&#39;re using an opening and closing `<:header>`, `<:title>`,
    and `<:body>` tag placing some content to render in between those tags. This content
    will be made available in the component as the `@header`, `@title`, and `@body`
    assignment respectively. We&#39;ve also moved the `let={review}` variable assignment
    to each individual slot tag. Now, let&#39;s update our component to render the
    header, title, and body content now.\n\n```erb\ndef cards(assigns) do\n~H\"\"\"\n
    <%%= for item <- @items do %>\n   <div class=\"card review-card\">\n     <div
    class=\"card-header\">\n       <%%= render_slot(@header, item) %>\n     </div>\n
    \    <div class=\"card-body\">\n       <h5 class=\"card-title\">\n         <strong><%%=
    render_slot(@title, item) %></strong>\n       </h5>\n       <p class=\"card-text\">\n
    \        <%%= render_slot(@body, item) %>\n       </p>\n     </div>\n   </div>\n
    <%% end %>\n\"\"\"\nend\n```\n\nWe use `render_slot/2` to render each piece of
    dynamic content, and we pass the value of the `item` variable, which evaluates
    to an individual review struct, back up to the caller where it is made available
    as the `review` variable thanks to our usage of `let`. Now, if we go back to the
    browser, we’ll see all of the review details rendered in an individual card:\n\n![A
    single review card, showing star rating in one area and review text in the other.](components-04.png)\n\nWith
    that, we&#39;ve built a reusable `cards/1` component that can render a list of
    cards. But what about if we only want to render a single card? We can refactor
    our component module to implement a new function component, `card/1`. Then, we&#39;ll
    update the `cards/1` function component to iterate over the list of items in the
    assigns and render a card component for each one. Let&#39;s do it.\n\n## Iteratively
    Render Nested Function Components\n\nFirst up, we&#39;ll define a `card/1` function
    component like this:\n\n```erb\ndef card(assigns) do\n  ~H\"\"\"\n  <div class=\"card
    review-card\">\n    <div class=\"card-header\">\n      <%%= render_slot(@header,
    @item) %>\n    </div>\n    <div class=\"card-body\">\n      <h5 class=\"card-title\">\n
    \       <strong><%%= render_slot(@title, @item) %></strong>\n      </h5>\n      <p
    class=\"card-text\">\n        <%%= render_slot(@body, @item) %>\n      </p>\n
    \   </div>\n  </div>\n  \"\"\"\nend\n```\n\nOur new function component is simple
    enough&mdash;all we did was move the markup that represents an individual card
    into its own component. This component also assumes that it will receive an `assigns`
    that contains a `@item`, `@header`, `@title`, and `@body` assignment. Next up,
    let&#39;s refactor the `cards/1` component to iteratively render a `card` for
    each item:\n\n```erb\ndef cards(assigns) do\n  ~H\"\"\"\n  <%%= for item <- @items
    do %>\n    <.card header={@header} title={@title} body={@body} item={item} />\n
    \ <%% end %>\n  \"\"\"\nend\n```\n\nHere, we start our iteration in the `for`
    loop. Then, we call the `card/1` function component for each item, passing through
    the `@header`, `@title`, and `@body` assignment. The manner in which we call `cards/1`
    doesn&#39;t change. It still looks like this:\n\n```erb\n<CardComponent.cards
    items={@reviews}>\n  <:header let={review}>\n    <%%= for _ <- 0..review.stars
    do %>\n      <i class=\"bi bi-star-fill\"></i>\n    <%% end %>\n  </:header>\n
    \n  <:title let={review}>\n    by: <%%= review.user.email %>\n  </:title>\n \n
    \ <:body let={review}>\n    <%%= review.notes %>\n  </:body>\n</CardComponent.cards>\n```\n\nSo,
    the `cards/1` function uses the `:header`, `:title`, and `:body` named slots and
    its assigns contains the `@header`, `@title`, and `@body` assignments. Then, it
    simply passes those assignments through to the `card/1` component, which uses
    `render_slot/1` to render the content in the original caller&#39;s named slots.\n\nPutting
    it all together, we end up with a highly dynamic and reusable function component
    that can render many different types of content into a list of Bootstrap cards.
    The `cards/1` function component wraps up all of the card markup without having
    any awareness of what content will be rendered into that markup. We leave it entirely
    up to the caller to specify what content will be rendered within the card&#39;s
    header, title, and body.\n\n<%= partial \"shared/posts/cta\", locals: {\n  title:
    \"Fly ❤️ Elixir\",\n  text: \"Fly is an awesome place to run your Elixir apps.
    It's really easy to get started. You can be running in minutes.\",\n  link_url:
    \"https://fly.io/docs/elixir/\",\n  link_text: \"Deploy your Elixir app today!&nbsp;&nbsp;<span
    class='opacity:50'>&rarr;</span>\"\n} %>\n\n## Wrap Up\n\nIn our example, we began
    with a single-purpose Bootstrap card LiveView component, and converted it into
    a versatile function component with the help of component slots. We used more
    advanced techniques like named slots and yielding variables up to the function
    component caller to make the function component even more sophisticated and dynamic.
    And, we used named slots and nested function components to iteratively render
    a list of components. You&#39;re ready to go out and build some reusable function
    components of your own.\n"
- :id: phoenix-files-build-simple-reusable-widgets-using-slots
  :date: '2022-03-08'
  :category: phoenix-files
  :title: Build Simple Reusable Widgets Using Slots
  :author: mark
  :thumbnail: phoenix-insert-thumbnail.jpg
  :alt:
  :link: phoenix-files/build-simple-reusable-widgets-using-slots
  :path: phoenix-files/2022-03-08
  :body: |2


    A new feature in [LiveView](https://hexdocs.pm/phoenix_live_view/Phoenix.LiveView.html) called "slots" can help make your components more composable and reusable. This post is about getting started with slots to build a simple component.

    ## Problem

    You have a design element that is used repeatedly in your site. Rather than copy and paste the markup everywhere, you want to make a component that handles the layout part for you. With the contents removed, it looks like this:

    ![Show card layout graphically](basic-card-1.png?center&card)

    How can we make a component that supports inserting content in multiple places? Something like this:

    ![Show card insert areas](basic-card-pointing.png?center)

    And what if the inserted content is complex, like HTML markup and not just simple strings?

    ## Solution

    A feature was introduced in LiveView v0.17 called "slots". The idea and inspiration for this comes from Javascript front-end frameworks like [Vue.js](https://vuejs.org/).

    The idea is pretty simple. We want to "slot in" content at different places in a component. If we have multiple places that take content, then we need to _name_ the slot so we can specify which content is destined for which slot. This becomes clearer as we work through an example.

    Let's start with the basic default slot that we've had for some time. Here we have a custom component called `basic_card` and pass content into it.

    ```html
    <.basic_card>
      Default content.
    </.basic_card>
    ```

    For this example, it should render like this:

    ![Show card with default content](basic-card-3.png?center&card)

    Let's see what the `basic_card` function looks like that makes this work. The CSS styling comes from [TailwindCSS](https://tailwindcss.com/docs/installation).

    ```erb
    def basic_card(assigns) do
      ~H"""
      <div class="rounded-lg border border-gray-300 w-full shadow-sm">
        <div class="px-3 py-2">
          <%%= render_slot(@inner_block) %>
        </div>
      </div>
      """
    end
    ```

    The key part here is the [`render_slot(@inner_block)`](https://hexdocs.pm/phoenix_live_view/Phoenix.LiveView.Helpers.html#render_slot/2) function call. This renders the string `"Default content."` because it's passed in as the `@inner_block`. Meaning it's the blob-of-stuff-we-included-but-didn't-specify-a-slot-name that ends up as the default `@inner_block`.

    But our design element includes a header, what should the template look like to pass in content for the header?

    ```html
    <.basic_card>
      <:header>
        Header Content
      </:header>

      Default content.
    </.basic_card>
    ```

    Here we use a colon (`:`) specify a "named slot" called `header`. Writing that in a template looks like this `<:header></:header>`

    Whatever content we put between those tags is available in the function assigns as `@header`. Let's look at how the `basic_card` function changes to render the header.

    ```erb
    def basic_card(assigns) do
      ~H"""
      <div class="rounded-lg border border-gray-300 w-full shadow-sm">
        <div class="rounded-t-lg bg-gray-100 px-3 py-2 font-medium border-b border-gray-300">
          <%%= render_slot(@header) %>
        </div>
        <div class="px-3 py-2">
          <%%= render_slot(@inner_block) %>
        </div>
      </div>
      """
    end
    ```

    This now renders like this:

    ![Show card with header and default content](basic-card-2.png?center&card)

    Yay! Looking good!

    But if we play around with it, we'll see that if we _don't_ include a `<:header></:header>` slot in the template, it blows up!

    We can fix this! We can make the `<:header></:header>` slot optional. Doing this also reveals an interesting thing... **a slot's content is actually a list**.

    We'll make a couple changes and talk through it next.

    ```erb
    def basic_card(assigns) do
      assigns = assign_new(assigns, :header, fn -> [] end)

      ~H"""
      <div class="rounded-lg border border-gray-300 w-full shadow-sm ">
        <%%= for header <- @header do %>
          <div class="rounded-t-lg bg-gray-100 px-3 py-2 font-medium border-b border-gray-300">
            <%%= render_slot(header) %>
          </div>
        <%% end %>
        <div class="px-3 py-2">
          <%%= render_slot(@inner_block) %>
        </div>
      </div>
      """
    end
    ```

    We made the following two changes:

    **1)** We used `assign_new/3` to add `:header` to the assigns if it's not already there. If the key is missing, the 3rd argument, an anonymous function, is executed and the result is used for the value. In this case, it gets set to an empty list `[]`.

    This means our assigns will always have a `@header` and it is a list.

    **2)** We use a `for` comprehension to render the header: `for header <- @header do`. If the `:header` list is empty, nothing gets rendered!

    Setting a default empty list for the header and using a `for` comprehension to render it effectively makes our `header` slot optional!

    Our simple header example could have just passed the contents as an attribute. The template would look like this:

    ```html
    <.basic_card header="Header Content">
      Default content.
    </.basic_card>
    ```

    Why is it better to use a slot? Because we can pass in complex markup instead of simple strings. Here's an example of more complex header content:

    ```html
    <.basic_card>
     <:header>
       <div class="flex items-center space-x-3">
        <div class="flex-shrink-0">
         <i class="fas fa-star text-2xl text-center text-gray-600 h-8 w-8" />
        </div>
        <div class="flex-1 min-w-0">
          <p class="text-sm font-medium text-gray-900">
            Header Text
          </p>
        </div>
       </div>
     </:header>

      <p>
        Lorem ipsum dolor sit amet, consectetur adipiscing elit,
        sed do eiusmod tempor incididunt ut labore et dolore magna
        aliqua. Ut enim ad minim veniam, quis nostrud exercitation
        ullamco laboris nisi ut aliquip ex ea commodo consequat.
      </p>
    </.basic_card>
    ```

    This is using [FontAwesome](https://fontawesome.com/) for the icon. It looks like this:

    ![Show card with complex header](basic-card-4.png?center&card)

    All that extra header markup starts to look messy, but if we find we keep using that complex header pattern in our app, then it can become a component as well. Even still, notice that all the markup noise is the _exception_ instead of the norm.

    This is where slots become really powerful. Slots let us insert complex content into multiple places inside of a component.

    ## Discussion

    Slots works really well when creating common components for our application. The components don't have to be 100% configurable, they just need to meet the our needs today. That's a great place to start and we can grow from there.

    Slots can do even more than we looked at here. This was an example of how slots make common UI patterns easy to reuse in our applications. Check out [this more advanced post](https://fly.io/phoenix-files/function-components/) that even covers passing arguments into a named slot as well.
- :id: blog-intro-to-accessibility
  :date: '2022-03-03'
  :category: blog
  :title: Accessibility for real-time web apps
  :author: nolan
  :thumbnail: accessibility-thumbnail.png
  :alt: A composition of assistive devices and musical notes, centered around a phone
    and a laptop, with a botanical motif at the bottom.
  :link: blog/intro-to-accessibility
  :path: blog/2022-03-03
  :body: "\n\n<p class=\"lead\">Hi, I'm Nolan, a totally blind software developer
    working on accessibility at Fly.io. I've worked on website remediation, I've written
    screen readers from scratch, and I also create [2-D audio-only arcade games](https://www.lightsout.games).
    Here's a screenshot of my latest game: </p>\n\n![Blank image because, like I said,
    they’re audio only games.](blank.png?centered)\n\nLike every developer, I rely
    on a number of products and services to manage and promote my many side projects.
    But I've had more than one neat idea go down in flames because I simply couldn't
    use the amazing, must-have service that would have made that project feasible
    to complete.\n\nA few weeks back, Chris McCord [launched LiveBeats](https://fly.io/blog/livebeats/),
    a real-time collaborative MP3 player, inspired by  [turntable.fm](https://turntable.fm/).
    LiveBeats is cool on its own merits ([check it out](https://livebeats.fly.dev/chrismccord)),
    but, just as importantly, it's designed to be a model real-time Phoenix LiveView
    application, a blueprint other people can start with for their own apps. And,
    well, the model real-time Phoenix LiveView application needs to be more accessible.
    Today, I'm going to get you into the right mindset by talking about what that
    means, and why it matters.\n\n<div class=\"callout\">Reading about accessibility
    is good, but engaging is better. For obvious reasons, I'm serious about this stuff.
    If you've got questions, or accessibility thoughts of your own, [hit me up here](https://community.fly.io).
    I'm more than willing to chat about this stuff.</div>\n\nDoing accessibility well
    isn't difficult, especially if you plan ahead. So I'd like to talk a bit about
    what accessibility is, why you should care, and a few common missteps teams make
    on their journey.\n\n## What Is Accessibility, Really?\n\nBroadly speaking, accessibility
    refers to whether someone can use your product or service regardless of how they
    interact with it. \n\nAs a blind developer, I naturally focus on screen reader
    accessibility. Now, it's important not to fall into the trap of assuming that
    accessibility is one-size-fits-all. But improvement in one area generally helps
    across the board. It's OK to start somewhere and branch out. \n\nAccessibility
    is not a destination, but a process. It's meeting your users where they are, and
    \ working to improve their experiences. As your product evolves, it gets faster.
    More reliable. More featureful. You should expect it to get more accessible too.\n\n##
    Why Should I Care?\n\nFirst, you care because it makes your app better. Accessibility
    isn't just about whether your software or service is usable by the [15% of the
    world's population that identify as having a disability](https://www.who.int/teams/noncommunicable-diseases/sensory-functions-disability-and-rehabilitation/world-report-on-disability).\n\nServices
    with fewer interaction limitations are almost certainly easier and more pleasant
    to use for everybody. Common sense, right? Take mobile accessibility. For me,
    labeled controls and clearly-defined focus areas are essential. But those same
    \"accessibility features\" could just as easily serve you with reliable voice
    operation while driving, or just with your phone safely tucked away. And if you're
    already investing in good UX, many of those same practices transfer to building
    solid accessibility.\n\nNext, you care because any of us might acquire a disability
    at any time. One day, I stepped off a curb, twisted my ankle, and had a hard time
    supporting myself on that foot for several months. I don't normally need physical
    supports, so my home wasn't built with them. I had a lot of trouble doing things
    myself.\n\nMy partner has a mobility-related disability, however, and her apartment
    has a number of grab rails and transfer bars. My life was so much easier while
    I was there. Most of the time for me, accessibility is primarily about whether
    and to what extent I can achieve something non-visually. But for those few months
    when I had a new temporary disability, every painful struggle in my home was a
    moment when I desperately wished we designed more universally, not waiting for
    someone to _need_ accessibility before haphazardly slapping it on.\n\nEventually
    age will likely impair your hearing, eyesight, mobility, or cognition. But even
    if age treats you well, injury or illness may temporarily render you unable to
    use products and services on which you've come to rely. If you'd like to keep
    doing the things you enjoy, then you owe it to yourself and others to care about
    accessibility before it becomes something you depend on.\n\n## Build Accessibility
    in From the Start\n\nAccessibility is an ongoing conversation, not a single step.
    Putting off an important chat until the last minute is usually a recipe for disaster.
    So is deferring accessibility planning until your customers squawk about it.\n\n<aside
    class=\"right-sidenote\">[Deque University's accessibility courses](https://dequeuniversity.com/)
    are a thorough foundation for any team wanting to learn lots about web accessibility.</aside>\n\nImagine
    planning an industry event that you'd like to be accessible. You'll need to select
    a venue, line up presenters, and arrange printed material for distribution to
    attendees. Treating accessibility as something you can retrofit _might_ work out,
    if you're lucky. More likely, you'll miss the fact that the venue is atop a flight
    of stairs. Deaf participants won't be able to participate in presentations. You'll
    dump a pile of paper on people who can't even read it. And inevitably someone
    will write an angry blog post about it. I’ve been there. Trust me, the person
    writing the angry blog post wishes they didn't have to write it.\n\nAll of these
    missteps are significantly more difficult to fix at the last minute. Incidentally,
    they're also mistakes I see people in our industry make, over and over again,
    when they host events – even events they describe as \"accessible\".\n\nCompared
    to an accessibility checklist, an accessibility-focused _process_ asks what barriers
    each choice might impose, then works to either address or eliminate those before
    they're set in stone. Doing this requires knowing which questions to ask, and
    asking them before they become costly mistakes that are hard to fix.\n\n### Gather
    Your Tools\n\nModern tools can identify many of the most common access pitfalls.
    Though imperfect, automated testing can usually catch enough issues to make an
    inaccessible app at least usable. A minimally-usable app can then be audited \"live\",
    which provides far more effective feedback.\n\n<aside class=\"right-sidenote\">Deque's
    [Axe DevTools extension](https://www.deque.com/axe/devtools/chrome-browser-extension/)
    runs a number of automated in-browser accessibility checks. They're no replacement
    for live testing, but the extension can catch a number of issues that you can
    fix before bringing in outside help.</aside>\n\nOn the web, lots of useful checks
    are baked directly into modern browsers' development tools. Even more advanced
    tests can be installed via extensions. Mobile apps have similar tooling, either
    run directly on-device or included in your IDE.\n\n### Test With Real Users\n\nIf
    you're serious about a product, you test it for usability. In the same way, you
    should test your product and its overall design to see how it works for users
    with specific access needs.\n\n<aside class=\"right-sidenote\">[Knowbility](https://knowbility.org/)
    is an excellent provider of accessibility consulting, training, and testing.</aside>\n\nSome
    teams learn the basics of the most popular screen readers and magnifiers. That's
    a great way to build empathy. But unless you use these tools daily, you're not
    using them the way people who rely on them do, and so you're probably not using
    them correctly or effectively. This is a solvable problem! Accessibility testing
    services help you identify specific access needs, scope tests to address them,
    and recruit testers to provide actionable feedback.\n\n### Hire Lived Experience\n\nOne
    of the best ways to be better at accessibility is to recruit and hire people with
    disabilities. You don't need someone with every disability. A decent dose of empathy
    goes a long way! But we're usually (by necessity) the experts on our access needs,
    or are better equipped to ask the right questions and identify the most important
    next steps. Having people with disabilities on your team is one of the easiest
    ways to sort out the \"must haves\" from the accessibility \"nice to haves\".\n\nAnd
    there’s a large pool of experts out there. The unemployment rate among disabled
    folks is staggering&mdash;[around 60% in the US blind population](https://www.afb.org/research-and-initiatives/statistics/archived-statistics/key-employment-statistics),
    for example. That's sixty. A six, followed by a zero. And the numbers are similarly
    high across other disabilities.\n\nA word of caution, though: if your group goes
    this route, please don't make the disabled person your company's \"accessibility
    czar\". Hiring lived experience should be a part of your accessibility strategy,
    but not the whole thing. We're already expending extra energy advocating for our
    access needs alone outside of work. Making accessibility entirely our responsibility
    at work is a sure path to burn-out.\n\n## Back to LiveBeats, For A Moment\n\nA
    big part of what I'm doing at Fly.io over the coming weeks is taking LiveBeats,
    the model Phoenix LiveView application, and making it accessible. I want people
    to learn not just the right way to stand up a Phoenix application, but also some
    of the most important tools to make all their applications accessible.\n\nSo,
    [LiveBeats](https://github.com/fly-apps/live_beats) lets you play and listen to
    music with friends. All web apps have a bunch of generic accessibility problems.
    But LiveView posed a few additional unique challenges:\n\n1. New routes are loaded
    dynamically, with no indication to screen reader users that a page transition
    occurred.\n1. Live updates to specific regions of the page should be made accessible
    in ways that make sense, and don't negatively impact user experience.\n1. Dropdown
    menus and modals are not keyboard-accessible by default, and require some clever
    tricks to make the accessible experience seamless.\n\nThese are all fixable issues,
    and I'll post articles about solving them in turn. But I'm interested in hearing
    from you as well. If you have questions about what I've shared, or are curious
    about other aspects of making a real-time web app accessible, please let me know!
    I'll be reading and responding to comments in the [Fly.io community forum](https://community.fly.io/).\n\n"
- :id: blog-fly-io-is-hiring-laravel-specialists
  :date: '2022-02-25'
  :category: blog
  :title: Fly.io is Hiring Laravel Specialists
  :author: mark
  :thumbnail: jobs-cover-02-thumbnail.png
  :alt:
  :link: blog/fly-io-is-hiring-laravel-specialists
  :path: blog/2022-02-25
  :body: |2+


    <div class="lead">Fly.io takes container images and converts them into fleets of Firecracker VMs running on our own hardware around the world. We make it easy to run applications near users, whether they’re in Singapore, Seattle, or São Paulo. [Try it out](https://fly.io/docs/speedrun/); if you’ve got a working container already, it can be running here in less than 10 minutes.</div>

    Fly.io sponsors the Laravel [Livewire](https://laravel-livewire.com/) project, and now we're hiring Laravel people.

    Here's Fly.io's not-so-secret evil plan. We make it easy to run full-stack apps&mdash;any app, in any framework&mdash;close to your users. We're a simple and powerful way to run any application, with modern dev UX, advanced Postgres deployments with replication, and app scaling knobs that don't require you to learn Terraform to use. But we're especially shiny for frameworks that benefit from geographic distribution, like Elixir's [LiveView](https://fly.io/blog/how-we-got-to-liveview/) and Laravel's Livewire. We want those kinds of frameworks to succeed, because the better they do, the more valuable we are.

    We want Fly.io to be the best place on the Internet to run Laravel apps, and especially Livewire. If you're a Laravel developer that's enthusiastic about Livewire, we need your help.

    The business goal for this Laravel job is, of course, to make Fly.io more attractive to Laravel developers. But the job isn't simply advocacy; we're not just looking for cheerleaders. Rather, we're looking for people who can make substantive contributions both to Fly.io and, more importantly, to Laravel and Livewire itself. If you can develop Livewire features that are useful to everybody, including people who don't use Fly.io and never will, that's great for us! Anything that makes Livewire more effective makes Fly.io more valuable.

    We think Fly.io is a pretty great place to deploy a Laravel app today. But it could be much better. So we're also interested in contributions you can make to the dev UX at Fly.io. Some of that might just be expert feedback on what Fly.io should be doing differently to make things more pleasant for Laravel developers. Some of it might be new feature ideas. And a lot of it will probably just be helping Laravel/Livewire developers better understand what we're about at Fly.io.

    We don't expect you to be an expert on Fly.io, it just so happens that we already are that! We need your help to understand the needs of the Laravel Livewire community and framework. You will help pave the way to make Fly.io an even better platform for Laravel Livewire developers.

    We've been hiring people to do this kind of work for Elixir and LiveView for about a year now, and it's been a huge success for us (so much so that people are starting to think of us as an Elixir company!) Here's your chance to get people to start thinking of Fly.io as a Laravel company, too. Represent!

    ## About us and about the job

    This is a mid to senior level job. The salary ranges from $120k to $200k USD. We also offer competitive equity grants. Hopefully that's enough to keep you intrigued, here's what you should _really_ care about:

    - We're a small team, almost entirely technical.
    - We are active in developer communities, including our own at [community.fly.io](https://community.fly.io/). Part of this role is helping to answer support questions specific to Laravel running on Fly. Providing meaningful troubleshooting and diagnosis information in most posts.
    - Virtually all customer communication, documentation and blog posts are in writing. We are a global company, but most of our communication is in English. Clear writing in English is essential.
    - We are remote, with team members in Colorado, Quebec, Chicago, London, Mexico, Spain, Virginia, Brazil, and Utah. Most internal communication is also written, and often asynchronous. You'll want to be comfortable with not getting an immediate response for everything, but also know when you need to get an immediate response for something.
    - We are an unusually public team; you'd want to be comfortable working in open channels rather than secretively over in a dark corner.
    - This could be a great job for someone who loves Livewire, thinks it's the best thing ever, and wants other people to discover how awesome it gets when you are physically closer to your users.
    - We’re a real company – hopefully that goes without saying – and this is a real, according-to-Hoyle full-time job with health care for US employees, flexible vacation time, hardware/phone allowances, the standard stuff. The senior level compensation for this role is $160k-$200k USD plus equity. We are also looking for some junior level members to join the team. So we'd like to hear from you too!

    ## What you'll do

    - Help create the initial "Run a Laravel App" [Getting Started Guide](https://fly.io/docs/getting-started/). We'll help answer questions about the Fly.io side.
    - Be a public presence in the Laravel community. This just means that you post about the cool things you're doing and engage with others in the community; in particular, it means being helpful to Laravel developers everywhere, regardless of where they deploy their applications.
    - Create a sample Livewire app that demonstrates how to take advantage of what the Fly.io platform provides. You won't be alone on this either, we can help you with ideas and Fly.io platform features. We need someone who can handle the Laravel side.
    - Help guide other Laravel developers as they come the platform. That may include engaging in the [community.fly.io](https://community.fly.io/) forums where people have Laravel or Livewire specific questions. For Fly.io platform questions, you won't be able to answer those right away and that's expected, we have platform focused people who handle that. We need help with the easy "Oh, you just need to configure X!" stuff that we don't know for Laravel.

    ## You might enjoy this job if you

    - Enjoy using Laravel Livewire and want more people to see the benefits you see. You enjoy sharing and showing others.
    - Want more people to understand Livewire and start using it.
    - Have good instincts for balancing competing customer demands.
    - Are comfortable with software development. Our customers are developers, so knowing how to build apps is important. Bonus points if you are familiar with how to make it run on Fly.io.
    - Have general knowledge about deploying and supporting Laravel apps in production.
    - Like some structure, but are comfortable with not having a standard playbook for most things. You also like putting some structure in place where there isn't any, and are open to trying new things if something isn't working.

    ## You'll know you're succeeding in this job if

    - Other Laravel developers can follow your guides to successfully deploy their apps on Fly.io.
    - As you identify shortcomings, you update documentation or work with other team members to help improve Fly tooling to make deployment a smoother process.
    - Awareness of Fly.io in the Laravel community is going up. People are talking about it without you starting the conversation.
    - You are influencing Laravel developer behavior.

    ## How We Hire People

    We are weird about hiring. We’re skeptical of resumes and we don’t trust interviews (we’re happy to talk, though). We respect career experience but we aren’t hypnotized by it, and we’re thrilled at the prospect of discovering new talent.

    The premise of our hiring process is that we’re going to show you the kind of work we’re doing and then see if you enjoy actually doing it; “work-sample challenges”. Unlike a lot of places that assign “take-home problems”, our challenges are the backbone of our whole process; they’re not pre-screeners for an interview gauntlet.

    For this role, we’re going to ask you to create a "recipe" and a sample project. We'll give you the problem to solve and provide the recipe format to use. A recipe is a simple "problem and solution" format for writing a blog post. A companion Laravel project is created to demonstrate the solution.

    If you're interested, mail jobs+laravel@fly.io. You can tell us a bit about yourself, if you like. Please also include a short description of the Laravel feature that you think a Rails or Elixir developer would find most surprising.

- :id: phoenix-files-tabs-with-js-commands
  :date: '2022-02-17'
  :category: phoenix-files
  :title: Client-Side Tabs in LiveView with JS Commands
  :author:
  :thumbnail: liveview-js-thumbnail.jpg
  :alt:
  :link: phoenix-files/tabs-with-js-commands
  :path: phoenix-files/2022-02-17
  :body: "\n\nThere are some things it really does make sense for our LiveView to
    do without calling home. Simple things that the browser doesn't need help with.
    Things we'd like to see happen instantly, like hiding a modal—maybe even with
    a transition animation.\n\nWith the [Phoenix.LiveView.JS](https://hexdocs.pm/phoenix_live_view/Phoenix.LiveView.JS.html)
    module, we have an assortment of tidy, composable JS utility commands to easily
    carry out common client-side tasks without breaking into custom JavaScript.\n\nMost
    LiveView JS commands do what can be summed up as manipulating HTML element attributes,
    with two exceptions. We can dispatch an event to a DOM element using `JS.dispatch`.
    And beyond purely client-side operations, `JS.push` pushes an event to the server,
    but with extended options to \"[customize targets, loading states, and additional
    payload values](https://hexdocs.pm/phoenix_live_view/Phoenix.LiveView.JS.html).\"\n\nThe
    most excellent thing about LiveView JS commands may be that their work persists
    across DOM patches: when an update comes down the line from the server, the JS
    commands are replayed locally so our local changes don't get wiped out.\n\nToday
    we're going to use a practical example to demonstrate a selection of these: `JS.show`,
    `JS.hide`, `JS.remove_class`, and `JS.add_class`.\n\n## The problem\n\nWe're building
    a set of tabs, each with its own HTML content to display. Making a tabbed UI work
    boils down to showing stuff, hiding other stuff, and making one of the tabs look
    different so we know which one the displayed content belongs to.\n\nNone of that
    needs any information from the server, so it would be great if we didn't have
    to push events to it and wait for it to respond.\n\n## The solution\n\nWe can
    build our tab functionality fully client-side, with a few LiveView JS commands.
    When a user clicks on a tab, we will:\n\n- Show the content that corresponds to
    that tab, using `JS.show`  \n- Hide the content belonging to the other tabs, using
    `JS.hide`\n- Indicate which tab is now active by setting classes on the tab elements
    with `JS.remove_class` and `JS.add_class` (and styling them accordingly)\n\n###
    Laying the foundation\n\nPlanning ahead a bit: the utility functions we want to
    use all work by changing attributes on elements. `remove_class` and `add_class`
    are self-explanatory.  `JS.hide` and `JS.show` give an element an inline style
    property of `display: none;` and (by default) `display: block;`  respectively.\n\nSo
    let's build the basis for the UI with that in mind.\n\n```xml\n<div class=\"container\">\n
    \ <div class=\"tab_header\">\n    <ul>\n      <li class=\"tab_option\">\n        <a
    id=\"tab1\" class=\"tab active-tab\"> Tab one </a>\n      </li>\n      <li class=\"tab_option\">\n
    \       <a id=\"tab2\" class=\"tab\"> Tab two </a>\n      </li>\n      <li class=\"tab_option\">\n
    \       <a id=\"tab3\" class=\"tab\"> Tab three </a>\n      </li>\n    </ul>\n
    \ </div>\n  <div id=\"content\" class=\"tab_body\">\n    <div id=\"content_tab_1\"
    class=\"tab-content\">\n      <img src={ Routes.static_path(@socket, \"/images/1.png\")
    }>\n    </div>\n\n    <div id=\"content_tab_2\" class=\"tab-content hidden\">\n
    \     <img src={ Routes.static_path(@socket, \"/images/2.png\") }>\n    </div>\n\n
    \   <div id=\"content_tab_3\" class=\"tab-content hidden\">\n      <img src={
    Routes.static_path(@socket, \"/images/3.png\") }>\n    </div>\n  </div>\n</div>\n\n```\n\nThe
    tabs at the top are a styled unordered list. In the `#content` div below the tabs,
    we have the content for _all_ the tab options, in elements with `id` attributes
    `\"content_tab_1\"`, `\"content_tab_2\"`, and  `\"content_tab_3\"`. We're using
    the class `\"hidden\"` to hide two of the three.\n\nWe're fans of [Tailwind CSS](https://fly.io/phoenix-files/tailwind-standalone/),
    where an element with `class=\"hidden\"` gets the property `display: none;`. If
    we're not using Tailwind, we can add that CSS rule ourselves. This sets the _default_
    visibility of each piece of content.\n\nHere's what the result could look like:\n\n![A
    row of tabs labeled \"Tab one\" through \"Tab three\", with an image beneath;
    \"Tab one\" has different styling.](liveview-js-01.jpg?2/3&center?card)\n\nYay!
    We have constructed something that _looks_ like a tabbed interface when the page
    loads. It looks like one tab is active, thanks to some styling on the `active-tab`
    class. Only one of the content options is displayed.\n\nBut the content doesn't
    actually change if a user clicks on a different tab.\n\n### JS.show\n\nTo change
    that, we'll add a `phx-click` binding to each tab, invoking `JS.show` for the
    corresponding content-containing element.\n\n```elixir\n<li class=\"tab_option\">\n
    \ <a id=\"tab1\" class=\"tab active-tab\" \n    phx-click={JS.show(\n      to:\"#content_tab_1\",\n
    \     transition: {\"ease-out duration-300\", \"opacity-0\", \"opacity-100\"},\n
    \     time: 300\n    )}\n  > Tab one </a>\n</li>\n```\n\nThe `to` option takes
    a DOM identifier for the element we want to target. Above, we're targeting the
    `#content_tab_1` element when the `#tab1` element is clicked.\n\nSince JS commands
    and DOM patches are coordinated, we can use CSS transition animations and not
    worry about them being interrupted if the server happens to send an update. (Fun
    fact: there is also a `JS.transition` command in case you want a transition animation
    by itself. )\n\nWe're giving the `transition` option a 3-tuple of classes: `\"ease-out\"`
    and `\"duration-300\"` have CSS styling to provide the properties for the transition
    (in this case `transition-timing-function` and `transition-duration` ). Class
    `\"opacity-0\"` provides styling for the start of the transition, and `\"opacity-100\"`
    \ for the end.\n\nYes, we're using Tailwind classes again. You can also customize
    the transition by using your own classes, as long as the app's stylesheet provides
    rules for them.\n\nFinally, there's a `time` option. It limits the time taken
    for the transition, and it defaults to 200ms. Here we're using a transition that
    takes 300ms, so we have to adjust `time` to 300ms to ensure we don't get any weird
    behaviour.\n\nIf we do this for each tab, clicking on a tab makes its corresponding
    content div visible.\n\n\n\n<%= video_tag \"liveview-js-02.mp4?card?center\",
    title: \"When the user clicks on a tab, it adds another image to the content area.\"
    %>\n\nClearly we're not done yet. We still need to hide the content of the other
    tab(s). This is where `JS.hide` comes into play.\n\n### JS.hide\n\n`JS.hide`    hides
    the HTML elements we choose by giving them the style property  `display: none;`.
    Let's make some changes to our code.\n\nWe could just stick a `JS.hide` command
    before `JS.show` in each tab's `phx-click` binding. But we need the same pattern
    in all three tabs, so we may as well make a private function to both show the
    active content and hide the content of the remaining tabs.\n\nWe'll call it   `show_active_content/2`:\n\n```elixir\ndefp
    show_active_content(js \\\\ %JS{}, to) do\n  js\n  |> JS.hide(to: \"div.tab_content\")\n
    \ |> JS.show(to: to)\nend\n```\n\nFirst, `show_active_content`  does a `JS.hide`
    \ on all the divs that have class  `tab_content`; then it  `JS.show`s the element
    whose DOM identifier is specified in the `to` argument.\n\nFor simplicity, this
    snippet doesn't show the `transition` option, but we would use it in just the
    same way as we did with `JS.show` above. We can also add a transition to `JS.hide`
    in the same way.\n\nA note here about the `js` argument to this function. The
    JS commands inside are composable: that is, we can apply them in a series, [piping](https://hexdocs.pm/elixir/Kernel.html#%7C%3E/2)
    the output of one into the input of the next. Our optional `js` argument makes
    `show_active_content` composable as well.  If there's no input from a previous
    command, `js` is just an empty struct `%JS{}` .\n\nLet's see how to use our function:\n\n```elixir\n<li
    class=\"tab_option\">\n  <a id=\"tab1\" class=\"tab active-tab\" \n    phx-click={show_active_content(\"#content_tab_1\")}\n
    \ > Tab one </a>\n</li>\n<li class=\"tab_option\">\n  <a id=\"tab2\" class=\"tab\"
    \n    phx-click={show_active_content(\"#content_tab_2\")}\n  > Tab two </a>\n</li>\n<li
    class=\"tab_option\">\n  <a id=\"tab3\" class=\"tab\" \n    phx-click={show_active_content(\"#content_tab_3\")}\n
    \ > Tab three </a>\n</li>\n```\n\nEach of our tab options will pass the identifier
    for its own content in the `to` argument of `show_active_content`, so that after
    `JS.hide` hides all the `content` components, the content for just the desired
    tab gets unhidden.\n\n<%= video_tag \"liveview-js-03.mp4?card?center\", title:
    \"When a user clicks on a tab, the previous content is replaced by a different
    image.\" %>\n\n### JS.remove\\_class and JS.add\\_class\n\nSo far we can show
    the content of any tab with a click, but right now, the first tab always looks
    like it's the active one. That's because it has the class `\"active-tab\"` and
    we have some CSS behind the scenes to make it look special.\n\nSo all we have
    to do is remove the class `\"active-tab\"`  from the tab element that has it,
    and add it to the tab that's been clicked. For this we define a new composable
    function:\n\n```elixir\ndefp set_active_tab(js \\\\ %JS{}, tab) do\n  js\n  |>
    JS.remove_class(\"active-tab\", to: \"a.active-tab\")\n  |> JS.add_class(\"active-tab\",
    to: tab)\nend\n```\n\nHere, we're literally using the class that we want to remove
    in the DOM identifier to pass to `JS.remove_class`. Then `JS.add_class` adds  the
    `\"active-tab\"` class to the tab whose identifier was sent in the `tab` parameter.\n\nIncidentally,
    you can add a transition effect to both `JS.remove_class` and `JS.add_class` just
    as with `JS.show` and `JS.hide`.\n\nNow let's see how to use our function:\n\n```elixir\n<li
    class=\"tab_option\">\n  <a id=\"tab1\" class=\"tab active-tab\" \n    phx-click={\n
    \     set_active_tab(\"#tab1\") |> show_active_content(\"#content_tab_1\")\n      }\n
    \ > Tab one </a>\n</li>\n<li class=\"tab_option\">\n  <a id=\"tab2\" class=\"tab\"
    \n    phx-click={\n      set_active_tab(\"#tab2\") |> show_active_content(\"#content_tab_2\")\n
    \     }\n  > Tab two </a>\n</li>\n<li class=\"tab_option\">\n  <a id=\"tab3\"
    class=\"tab\" \n    phx-click={\n      set_active_tab(\"#tab3\") |> show_active_content(\"#content_tab_3\")\n
    \     }\n  > Tab three </a>\n</li>\n```\n\nSince we made both functions composable,
    we can string them together with the pipe operator.\n\n<%= video_tag \"liveview-js-04.mp4?card?center\",
    title: \"The active-tab class now gets given to the active tab option, to distinguish
    it from the other tabs.\" %>\n\n\n\nThere we have it: a working tab bar.\n"
- :id: phoenix-files-make-your-liveview-feel-faster
  :date: '2022-02-14'
  :category: phoenix-files
  :title: LiveView feels faster with a delayed loading indicator
  :author: mark
  :thumbnail: live-view-faster-thumbnail.jpg
  :alt:
  :link: phoenix-files/make-your-liveview-feel-faster
  :path: phoenix-files/2022-02-14
  :body: |2-


    <div class="lead">This is a recipe about making your LiveView app feel faster. You can also make your app faster by [running it on Fly.io](https://fly.io/docs/elixir/). It only takes a few minutes.</div>

    LiveView is already fast. Couple that with hosting it on Fly.io where the server can be physically closer to your users and you've already got a great experience. However, the default setup for a new LiveView application displays the `topbar` progress indicator across the top of the page making the whole page _feel_ like it's not ready. Here's a quick tip to make this so much better.

    ## _Problem_

    By default, LiveView ships with [topbar](https://www.npmjs.com/package/topbar) to visually show that it's loading and setting up. The problem is, when I see a progress bar moving, my brain tells me "the page isn't ready yet". Even if that progress bar only takes 100 msec to quickly flash across the top, my brain still says, "the page isn't ready yet".

    Notice the progress animation on the top of the page as we navigate around...

    ![Before version with progress bar flashing](1-before-with-topbar.gif?card)

    The page is fully rendered and I could start using the page, but it _feels_ like I can't. Just because there's a quick flash of a progress bar on the top.

    Why not just get rid of the progress display all together? Well, it's actually really helpful for those times when a page is taking longer to load for a user. In those situations, we _want_ to let the user know there's a bit of work left to do.

    So, if it's worth keeping the progress display, is there a way to make the topbar only display when it actually needs to? Can it only show if it's taking more time to setup the page?

    ## _Solution_

    Luckily, [José Valim](https://twitter.com/josevalim/status/1489234959592214531) shared this tip and now it's documented in the [LiveView Installation Guide](https://hexdocs.pm/phoenix_live_view/installation.html#progress-animation). However, if you are like me, you already have a LiveView project that doesn't have this change. Fortunately, it's easy for us to do right now.

    In less than 5 minutes I made the change and deployed my app. I now have a LiveView page that _feels_ so much more responsive! Let's look at how to add this to an existing app.

    In the project's `app.js` file, it might have a section that looks like this.

    ```javascript
    // Show progress bar on live navigation and form submits
    topbar.config({barColors: {0: "#29d"}, shadowColor: "rgba(0, 0, 0, .3)"})
    window.addEventListener("phx:page-loading-start", info => topbar.show())
    window.addEventListener("phx:page-loading-stop", info => topbar.hide())
    ```

    Replace that code with something like this:

    ```javascript
    // Show progress bar on live navigation and form submits. Only displays if still
    // loading after 120 msec
    topbar.config({barColors: {0: "#29d"}, shadowColor: "rgba(0, 0, 0, .3)"})

    let topBarScheduled = undefined;
    window.addEventListener("phx:page-loading-start", () => {
      if(!topBarScheduled) {
        topBarScheduled = setTimeout(() => topbar.show(), 120);
      };
    });
    window.addEventListener("phx:page-loading-stop", () => {
      clearTimeout(topBarScheduled);
      topBarScheduled = undefined;
      topbar.hide();
    });
    ```

    Basically, this only _shows_ the topbar at all if it's taking longer than 120 msec to receive the `phx:page-loading-stop` event. So the progress bar is still shown if it takes more than 120 msec. You can play with the exact time delay to use. You could use 200 msec or other times as well.

    We make the change, deploy the app, and see that the progress bar doesn't flash across as we navigate between different LiveView pages!

    ![After version using delay before activating progress bar](2-after-topbar-delay.gif?card)

    Nice! It worked! That just _feels_ so much better!

    ## _Discussion_

    The beauty of this solution is that my brain now has no progress bar movement making it _feel_ like it's not ready.

    The funny thing is, the page is quickly rendered and displayed to the user. Even if it takes 100 msec to setup the websocket and receive the `phx:page-loading-stop` event, as a visitor, I have just been presented with a whole new page.

    It takes a moment for my brain to adjust and orient itself to the new page. Additionally, it takes a moment to move my mouse or my finger to actually interact with the page.

    By the time I actually react, it's fully loaded and ready for me. This change makes it _feel_ instantaneous. It's only perception, but perception is what makes it _feel_ slow or fast.

    If the page takes more time to setup, the topbar progress is displayed letting the user know. Most of the time though, the user will never see it.

    Awesome! An easy win that makes my deployed, server-rendered, LiveView app feel like it's running locally on the user's machine.

    That's a quick win I'll take any day!

    <%= partial "shared/posts/cta", locals: {
      title: "Fly ❤️ Elixir",
      text: "Fly is an awesome place to run your LiveView apps. It's really easy to get started. You can be running in minutes.",
      link_url: "https://fly.io/docs/elixir/",
      link_text: "Deploy your Elixir app today!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>"
    } %>
- :id: blog-our-user-mode-wireguard-year
  :date: '2022-02-09'
  :category: blog
  :title: Our User-Mode WireGuard Year
  :author: thomas
  :thumbnail: danger!-thumbnail.jpg
  :alt:
  :link: blog/our-user-mode-wireguard-year
  :path: blog/2022-02-09
  :body: "\n\n<p class=\"lead\">We’re [Fly.io](http://Fly.io). We run container images
    on our hardware around the world, linked to our Anycast network with a WireGuard
    mesh. It’s pretty neat, and you [should check it out](https://fly.io/docs/speedrun/).
    You can be up and running on Fly.io in single-digit minutes. That's the last thing
    we're going to say about how great Fly.io is in this post.</p>\n  \nWireGuard
    is fundamental to how [Fly.io](http://Fly.io) works. <aside class=\"right-sidenote\">
    Along with [Firecracker](https://fly.io/blog/sandboxing-and-workload-isolation/)
    and [Rust](https://fly.io/blog/the-tokio-1-x-upgrade/), it’s one of the three
    “interesting” technical choices we’ve made.\n</aside> Practically everything that
    talks to anything else at [Fly.io](http://Fly.io) does so over WireGuard.\n\n\nThat
    goes for users, too. F’rinstance: to SSH into an instance of an app running on
    [Fly.io](http://Fly.io), you bring up a WireGuard peer to one of our gateways
    and SSH to an [IPv6 private network address](https://fly.io/blog/ipv6-wireguard-peering/)
    reachable only over that WireGuard session.\n\nThis kind of stuff is mostly hidden
    by `flyctl`, our command-line interface, which is how users interact with their
    apps on [Fly.io](http://Fly.io).  On most platforms, “how do you SSH to an instance”
    is a boring detail. But `flyctl` isn’t boring.\n\nI’m about to explain why, for
    better and worser, `flyctl` is exciting. But first, a disclaimer. In the ordinary
    course, a company writes a blog post such as this to crow about some achievement
    or “unique selling proposition”. This post is not that. This is an exercise in
    radical candor.\n\n## A Houdini With The Manacles Of Sound Engineering\n\nRecap:
    It’s February 2021, and you can deploy a Docker container to [Fly.io](http://Fly.io),
    with `flyctl`, and our platform [will dutifully convert it to a Firecracker VM
    running on a worker in our network](https://fly.io/blog/docker-without-docker/)
    and connected (again, via WireGuard) to our Anycast proxy network so that your
    customers can reach it on the Internet.\n\nWhat you couldn’t do is easily log
    into that VM to `monkey around with it`.\n\nThat wouldn’t do; what’s the point
    of having a fleet of VMs if you can’t `monkey with them`? The team agreed, being
    able to pop a shell on a running app was table-stakes for the platform.\n\nWe
    noodled a bit about how to do it. For awhile, we thought about building a remote-access
    channel into our Rust Anycast proxies. But we’d just rolled out [6PN private networking,](https://fly.io/docs/reference/private-networking/)
    making it easy for [Fly.io](http://Fly.io) apps to talk to each other. SSH seemed
    like an obvious example of a service you might run over a private network.\n\nThe
    trick was how to get users access to 6PN networks from their laptops. It was pretty
    easy to build an SSH server to run on our VMs, and APIs for [certificate-based
    access control](https://engineering.fb.com/2016/09/12/security/scalable-and-secure-access-with-ssh/)
    and building WireGuard peers for 6PN networks. But the client side is tricky:
    WireGuard changes your network configuration, and mainstream operating systems
    won’t let you do that without privileges. It wouldn’t do to require root access
    to run `flyctl`, and having to do a bunch of system administration to set up system
    WireGuard before you could run `flyctl` wasn’t attractive either.\n\nSo we came
    up with a goofy idea. You have to be root to add a network interface, but any
    old user can speak the WireGuard protocol, and once you’ve got a WireGuard session,
    TCP/IP networking is just building and shuttling a bunch of packet buffers around.
    Your operating system doesn’t even have to know you’re doing it. All you need
    is a TCP stack that can run in userland.\n\nI said this within earshot of [Jason
    Donenfeld](https://twitter.com/zx2c4?lang=en). The next morning, he had a [working
    demo.](https://github.com/WireGuard/wireguard-go/tree/master/tun/netstack) As
    it turns out, the [gVisor project](https://fly.io/blog/sandboxing-and-workload-isolation/#emulation)
    had exactly the same user-mode TCP/IP problem, and [built a pretty excellent TCP
    stack in Golang](https://pkg.go.dev/inet.af/netstack). Jason added bindings to
    [wireguard-go](https://git.zx2c4.com/wireguard-go), and an enterprising soul ([Ben
    Burkert](https://benburkert.com/), pay him all your moneys, he’s fantastic) offered
    to build the feature into `flyctl`. We were off to the races: you could `flyctl
    ssh console` into any [Fly.io](http://Fly.io) app, with no client configuration
    beyond just installing `flyctl`.\n\nI want to pause a second here and make sure
    [what we ended up doing](https://fly.io/blog/ssh-and-user-mode-ip-wireguard/)
    makes sense to you, because it still seems batshit to me a year later. Want to
    SSH into a [Fly.io](http://Fly.io) app instance you started? Sure. Just run `flyctl
    ssh console`, and it will:\n\n1. Kick off a WireGuard VPN session from within
    `flyctl`, using the [wireguard-go](https://git.zx2c4.com/wireguard-go) library.\n1.
    Run an entire TCP/IP stack, in userland, inside `flyctl`, to make an IPv6 TCP
    connection over that WireGuard connection.\n1. Using the [golang.org/x/crypto/ssh](https://pkg.go.dev/golang.org/x/crypto/ssh)
    library, run an SSH session over that synthetic TCP connection.\n\nAnd this works!
    You can do it right now! Why am I the only person that thinks it’s bananas that
    this works?\n\n**It Is Bananas That This (Mostly) Works**\n\nAlright, radical
    candor.\n\nThe nerd quotient on `flyctl ssh console` is extreme, which is a strong
    argument in favor of it. But there are countervailing reasons, and we ran into
    them.\n\nHere’s a simple problem. When you tell `flyctl ssh console` to bring
    up a WireGuard session like this, that running instance of `flyctl` on your machine
    — you know, the one that shows up in `ps` — is effectively another computer on
    the Internet. It has an IPv6 address. It is the only machine on the Internet that
    can have that IPv6 address. So what happens when you open up an SSH session in
    one window, and then another session in a different window?\n\nIn March of 2021,
    the answer was “it knocked the first SSH session off the Internet”. That’s how
    WireGuard works! Your peer keeps track of the source socket address that’s talking
    to it, and when a new source appears, that’s the host it starts talking to. It’s
    one of the great things about WireGuard, and why you can bring up a WireGuard
    connection, close your Macbook, walk to the coffee shop, open your Macbook back
    up, and still be connected to WireGuard.\n\nI tried to rationalize this “one SSH
    session at a time” behavior for a couple weeks, but, come on.\n\nThere were two
    paths we could have taken to address this problem. The easy-seeming way would
    be to have each `flyctl` instance make a new WireGuard peer, each with its own
    IPv6 address and public key pair. There were things I didn’t like about that,
    like the fact that it would crud our WireGuard gateways up with zillions of random
    ephemeral WireGuard sessions. But the dealbreaker in Spring 2021 was that creating
    a new WireGuard peer configuration was slow. We will return to this point shortly.\n\nThe
    other way forward was to not have multiple instances of `flyctl` speaking WireGuard.
    Instead, when you made a WireGuard connection, we’d spawn a background process
    — the `flyctl agent`. `flyctl ssh console` runs would come and go, each talking
    to the agent, which would stick around holding open WireGuard sessions. Sure,
    why not!\n\nI know how much you all love random background agent processes. I’m
    here to tell you that my Spring 2021  `flyctl agent` was all you could have imagined
    it would be. It only worked on Unix. Concurrency management? Try to start a new
    agent, and it’ll just ask the old one to die over that same socket, and then take
    over. Configuration changes? I’m just a simple unfrozen caveman agent, what changes
    would I need to know about?\n\n<aside class=\"right-sidenote\">Custom Resolver
    Dialers don't work on Windows in Go?</aside>\nFortunately for everyone else, I'm
    not the only developer on this team, and the agent got some professional help.
    The team got Unix domain sockets working on Windows. They wrote a new DNS resolver
    that worked on Windows as well. The agent will only run one of itself at a time.
    It notices configuration changes after it starts, and doesn't get out of sync
    and stale. If you use `flyctl` today, you're missing a whole lot of debugging
    fun.\n\n## Doubling Down On Banana Futures\n\nUser-mode WireGuard and TCP/IP via
    IPC with a background agent is an awful lot of mechanism just to run an SSH session.
    A lesser engineer might look at this and say “the thing to do here is to get rid
    of some of this mechanism”. We chose instead “do more stuff with that mechanism”.
    I mean, I say “we chose”. But I was the last to know; I arose from a long slumber
    at some point in the middle of the year to find that our deploys were running
    over user-mode WireGuard.\n\nHere’s another challenge users run into when deploying
    Docker apps on [Fly.io](http://Fly.io): they’re often not running Docker. An engineer
    of limited imagination such as myself would look at this as a documentation problem,
    the solution to which would be an instruction to users to “install Docker”. But
    we’re a clever lot, we is.\n\nFlip back to that 1-2-3 process for popping a shell
    over user-mode WireGuard. Here’s a new problem: “from a directory with a Dockerfile,
    deploy a Docker image on [Fly.io](http://Fly.io) if you’re not running Docker
    locally”. Here’s what you do:\n\n1. Use our GraphQL API to tell [Fly.io](http://Fly.io)
    to boot up a “builder” instance that does almost nothing but run Docker, because,
    hammer, nail, only tool, &amp;c. \n1. Kick off a WireGuard VPN session from within
    `flyctl`, using the wireguard-go library.\n1. Run an entire TCP/IP stack, in userland,
    inside `flyctl`, to make an IPv6 TCP connection over that WireGuard connection.\n1.
    Using the [github.com/docker/docker/client](https://pkg.go.dev/github.com/docker/docker/client)
    libraries, build a Docker container on the remote builder instance (which really
    just means connecting to a random IPv6 address rather than `127.0.0.1`). \n1.
    Tell the builder to push the image to our Docker registry, and our API to deploy
    it.\n\nIt’s just 5 steps! It all happens in the background; you just run `flyctl
    deploy`! What could go wrong?\n\n<div class=\"callout\">This pattern repeats.
    The horror of user-mode WireGuard and TCP/IP is that it is a whole lot of mechanism.
    But the beauty of it is that it’s mind-bogglingly flexible. A little later in
    the year, we launched [Fly.io Postgres](https://fly.io/blog/globally-distributed-postgres/).
    Want to bring up a `psql` shell on your database? `flyctl pg connect`. Do I need
    to rattle off the 1-2-3 of that? For that matter, what if you have a cool client
    like [Postico](https://eggerapps.at/postico/) you want to use? No problem! `flyctl
    proxy 5432:5432`. A proxy isn’t even 3 whole steps!</div>\n\n**Here’s where shit
    gets real.** One can rationalize the occasional SSH connection janking out. SSH
    wasn’t even a feature we had at the beginning of the year. But deploys? Deploys
    have to work.\n\n## The Deploys, They Were Not Always Working\n\nMore radical
    candor.\n\nWe have some good ideas on what made remote builds over WireGuard shaky,
    and builds have gotten a lot better. But I can’t tell you we’ve nailed down every
    failure mode. Here are two big ones.\n\nFirst: bringing up new WireGuard peers
    was slow. Real, real slow.\n\nIt’s Fall of 2021 and here’s what happened when
    you asked us to create a new WireGuard peer:\n\n1. You’d trigger a mutation in
    our [GraphQL API](https://api.fly.io/graphql) to add a WireGuard peer.\n1. Our
    API would generate a WireGuard configuration and send it back to you.\n1. Meanwhile,
    it’d trigger a [Consul KV](https://www.consul.io/docs/dynamic-app-config/kv) write,
    adding the configuration to a KV tree that I did not expect to get as big as it
    got.\n1. The Consul cluster would hold an Entmoot.\n1. 45-95 seconds later, `consul-templaterb`
    on our gateway would get wind of a KV change, and download every single peer that
    had ever been configured for the gateway.\n1. About 10 seconds earlier, your `flyctl`
    command gave up trying to bring up a connection to a WireGuard peer that did not
    yet exist on the gateway.\n1. `consul-templaterb` would write `wg1.conf` and then
    run a shell program that would resync the WireGuard configuration and then re-install
    routes for each one of the tens of thousands of WireGuard peers for that gateway.\n1.
    10 seconds later, you’re good to go! Wait, where’d you go?\n\n**This is very bad.**
    It only happens the first time you use WireGuard on a machine; the next time you
    go light up that WireGuard peer, it’ll come right up, because it’s already installed.
    But guess who’s making a WireGuard connection with `flyctl` for the first time?
    That’s right: someone who just decided to try out [Fly.io](http://Fly.io) and
    followed our [Speedrun instructions](https://fly.io/docs/speedrun/). No fair!
    It looks like all of [Fly.io](http://Fly.io) isn’t working, when in fact the only
    part of [Fly.io](http://Fly.io) that isn’t working is the part that allows you
    to use it.\n\n<aside class=\"right-sidenote\">That's Will Jordan, on our SRE team.</aside>\n\n
    \ There was low-hanging fruit to pick here. For instance, Will took one look at
    our WireGuard resync shell script and cut its runtime from 10 seconds to a few
    dozen milliseconds. But `consul-templaterb` — well, it is what it is. “Things
    will go as they will, and there’s no need to hurry to meet them”, it says. “I
    am on nobody’s side, because nobody is on my side, little orc.”\n\nWe have, in
    our prod infrastructure, two basic ways of communicating state changes: Consul
    and [NATS](https://nats.io/). Consul is slow and “reliable”; NATS is fast, but
    doesn't guarantee delivery. A few weeks ago, we switched from `consul-templaterb`
    to a system we call `attache`, which, among other things, does NATS transactions
    to update WireGuard peers. In the new system, creating a new WireGuard peer looks
    like this:\n\n1. You trigger a mutation in our GraphQL API to add a WireGuard
    peer.\n1. Our API generates a WireGuard configuration and sends it to `attache`
    on the gateway.\n1. A couple dozen milliseconds later, the gateway has installed
    the new WireGuard peer, and acknowledges the update to our API.\n1. The API replies
    to your GraphQL request with the WireGuard configuration\n1. Your `flyctl` connects
    to the WireGuard peer, which works, because you receiving the configuration means
    it’s installed on the gateway.\n\nThe whole process might take a second or two.
    It’s fast enough that you could imagine revisiting the decision to have a `flyctl`
    agent; with a little bit of caching, you could just make new WireGuard peers whenever
    you need them, and we could garbage collect the old ones.\n\n<div class=\"callout\">\n**There
    Is A More Ominous Problem I Don’t Like Talking About**\n\nThe remote builds, they’re
    way better. You can stick `flyctl` in your Github actions for CI and it’ll work.\n\nBut
    I have a nagging feeling it can’t be working perfectly for everybody, because
    it involves running WireGuard, and, as you may already know, WireGuard doesn’t
    run over 443/tcp.\n\nIf you’re on a corporate network with a proxy firewall, or
    on a janky VPN, or some random CI VM, 51820/udp might be blocked. In fact, for
    all we know, all UDP might be blocked. I’ve tried telling Kurt “that’s their problem”,
    but I haven’t won the argument.\n\nThere is, in the Github project for `flyctl`,
    [a branch that addresses this problem](https://github.com/superfly/flyctl/pull/566).
    Our WireGuard gateways all run a program called `wgtcpd`. It is as elegant as
    it is easy to pronounce. It runs an HTTPS server (with a self-signed certificate,
    natch!) with a single endpoint that upgrades to WebSockets and proxies WireGuard.
    The `flyctl` `tcp-proxy` branch will run WireGuard over that, instead of UDP.\n\nI’m
    here to tell you that for all the nattering about how problematic UDP-only WireGuard
    is, it turns out not to involve a lot of code to fix; the WebSockets protocol
    for this is just “send a length, then send a packet, read a length, then read
    a packet”.\n\n<aside class=\"right-sidenote\">I originally called DERP \"WebRTC\"
    and Tailscale took offense, saying that the proper term would be \"not-WebRTC
    not-ICE not-TURN but-kinda-similarish DERP NAT Traversal thingy\".</aside>\nWe
    could do something even more clever here; for instance, our friends/archnemeses
    at Tailscale run a global network of something they call “[DERP](https://pkg.go.dev/tailscale.com/derp)”,
    which is part of their [NAT-traversal proxy system](https://tailscale.com/blog/how-nat-traversal-works/);
    we could have our gateways connect to their DERP servers and register our public
    keys, and then you’d be able to connect to the same DERP servers and talk to us,
    and that seems like a fun project because there’s apparently nothing they can
    do to stop us.\n\nBut we’re still in denial about this problem and waiting for
    it to smack us in the face; we haven’t even merged the WebSockets branch of `flyctl`,
    because maybe it’s just not an issue? We only just solved the peer creation lag
    problem, and we’re waiting for things to even out. But if you needed to, you could
    run a WebSockets build of `flyctl` today.\n</div>\n  \n## Where This Leaves Us\n\nI’ve
    painted a picture here, and you might infer from it that I regret user-mode WireGuard
    and TCP/IP. But the truth is, I love it very much; it is one of those [Fly.io](http://Fly.io)
    architectural features that makes me happy to work here. I’d say that for the
    first half of 2021, it probably wasn’t paying its way in complexity and operational
    cost, but that it’s opened up a bunch of possibilities for us that will let us
    build other bananas features without having to change anything in our prod infrastructure.\n\nThere’s
    a fun side to `flyctl` WireGuard. For instance, it has [its own dig command](https://fly.io/docs/flyctl/dig/),
    which talks directly to our private `.internal` nameservers. What’s that, you
    say? The same feature would be a couple dozen lines of Ruby in a GraphQL API?
    Shut up!\n\nOr, how about this: [you can ping things now.](https://git.zx2c4.com/wireguard-go/commit/?id=b9669b734e30e717835ed44ea01f7ee7cdce5563)
    Ping! Of all things! You can `flyctl ping` `my-app.internal` and we’ll ping each
    instance of my-app for you. I know how much you love pinging things. And what’s
    the fun of using a hosting platform if you don’t get to pilot Howl’s Moving Castle
    Of Weird Network Engineering to check the latency on your app instances?\n\n<aside
    class=\"right-sidenote\">\nAlso, I blame Jason Donenfeld and Ben Burkert for all
    of this.\n</aside>\n    \nAs I said at the top, this is one of those posts that
    isn’t trying to sell [Fly.io](http://Fly.io), but just provide a somewhat honest
    accounting of the experience of building it, and a peek into the way we think
    about stuff. Having said all that: you should take [Fly.io](http://Fly.io) for
    a spin, because when `flyctl ssh console` is working — and it’s working pretty
    much all the time now — it is slick as hell.\n\n"
- :id: blog-new-turboku
  :date: '2022-02-08'
  :category: blog
  :title: Turbocharge your Heroku Apps with New Turboku
  :author:
  :thumbnail: turbocharge-heroku-thumbnail.png
  :alt:
  :link: blog/new-turboku
  :path: blog/2022-02-08
  :body: "\n\n<p class=\"lead\">No need to choose between us! Effortlessly clone your
    Heroku app to Fly.io with our new, faster [Turboku launcher](https://fly.io/launch/heroku).
    You can even talk to your Heroku Postgres, no data migration required.</p>\n\nIn
    early 2020, before we launched our VM platform, we made a little landing page
    called Turboku. It was a one-click launcher to deploy a Heroku app on Fly.io.
    It let people try us out without spending time porting an app.\n\nMany of our
    first customers found us through this page. Sadly, we neglected it for some time,
    but we've come back to give it some love. Today we're excited to introduce you
    to [New Turboku](https://fly.io/launch/heroku).\n\n## Where we're coming from\n\n[![Old
    Turboku UI screenshot](turboku-1.jpg?card)](turboku-1.jpg)\n\nHere's how Turboku
    worked: you'd visit the landing page in your browser, authenticate with Fly.io
    and Heroku, select your coffee-bean recommender from a list of your Heroku apps,
    and click &quot;Launch app on Fly.&quot; At that point the build was farmed out
    to AWS CodeBuild.\n\nCodeBuild is Amazon's fully-managed build service. You specify
    (or provide) a build environment, source code and commands, and CodeBuild runs
    it on AWS hardware. Our CodeBuild builders were stock Linux CI VMs with scripts
    and a sequence of commands to run. We could have run all that on our hardware,
    except for one small detail: at the time, we didn't have storage attached to our
    VMs.\n\nWhen the build was done, the CodeBuild VM would deploy your convoluted
    bitter bean chooser on Fly.io.\n\n## Bringing it home\n\nYou may recall that,
    like Old Turboku, _all_ our remote builds used to use CodeBuild. This worked,
    but CodeBuild builds were so slow that we hated subjecting our users to them.
    [Fly Volumes](https://fly.io/docs/reference/volumes/) changed the game, letting
    us run builders on our own hardware, which in turn [turbocharged our remote builds](https://fly.io/blog/persistent-storage-and-fast-remote-builds/#in-other-news-remote-docker-builds-got-way-faster).\n\nWe
    want Turboku builds to be fast, too.  So we rebuilt Turboku with `flyctl` integration,
    skipping third-party services in favor of our speedy remote builders. `flyctl`
    lives on your local machine and talks to our API. It's your CLI command center,
    and now it can deploy your Heroku app!\n\n\nHere's how that goes:\n\n1. You call
    `flyctl turboku` with an app name and a token, so `flyctl` can query Heroku's
    API for all the parts it needs to build and deploy a Fly-ified clone.\n1. It decides
    where to deploy. If you specify a region with the  `--region` option, it uses
    that. Otherwise it chooses the region nearest to your app on Heroku.\n1. At this
    point your app gets created. It has a name and an organization and a preferred
    region - that's about it so far.\n1. The environment variables from your app's
    Heroku config become `fly secrets`.\n1. The Heroku slug gives us the rest of the
    ingredients we need to concoct and configure a Fly.io app. `flyctl` generates
    three files:\n    * a `fly.toml` configuration file\n    * an ephemeral Dockerfile,
    with your app's Heroku stack (Heroku-20 or Heroku-18) for the base image, and
    the slug tarball to extract for your app's source\n    * a Procfile, reconstituted
    from the Heroku process map \n1. With your confirmation, the deployment process
    starts:\n    * The Docker image gets built, either locally or on a Fly.io remote
    builder.\n    * We tell a worker host in the chosen region to spin up a VM running
    your coffee bean thing.\n\n\n<div class=\"callout\">\n## Peek inside\n\nDid you
    know `flyctl` is open source? You can [check out the Turboku integration here](https://github.com/superfly/flyctl/blob/master/cmd/turboku.go).
    We promise Golang won't hurt you.\n</div>\n\n## Turboku launcher joins the battle\n\nNow
    Turboku is turbofast, and we love that you can do it all with `flyctl`, but we
    know that some people might see this as (gasp!) a step backwards from a web launcher.\n\nWe're
    not offended. That's fine! Because we also love [Phoenix](https://www.phoenixframework.org/)
    \ [LiveView](https://hexdocs.pm/phoenix_live_view/Phoenix.LiveView.html) for building
    Fly.io web launchers.\n\nWe've already [launched](https://fly.io/blog/creating-the-livebook-launcher-in-liveview/)
    a launcher for  [Livebook](https://fly.io/launch/livebook), and one for [Jupyter
    Notebooks](https://fly.io/launch/jupyter), and users seem to dig them. Now we
    have a [matching one](https://fly.io/launch/heroku) for Heroku migration too.\n\nWhen
    you connect to Heroku and Fly.io through the [Turboku launcher](https://fly.io/launch/heroku),
    you don't have to copy-paste your token, and we can give you a nice list of your
    Heroku apps to choose from. We'll ask you for some minimal configuration, and
    then it's time to smash that GUI deploy button!\n\n\n[![New Turboku UI screenshot
    showing configuration step (selecting organization, app name, region, and Heroku
    app)](turboku-2.jpg?card)](turboku-2.jpg)\n\nHere's where we're pleased with our
    `flyctl`-loving selves. We could have rewritten the deployment code in Elixir,
    but that would be wasting good `flyctl` magic (and hard work!).\n\nInstead, we
    spawn a Fly.io machine that runs `flyctl` for us! You can sit back and watch as
    it massages your Heroku app into shape for deployment on Fly.io. Turboku handles
    everything from creating the Dockerfile to building your app on our [remote Docker
    builders](https://fly.io/blog/persistent-storage-and-fast-remote-builds/).\n\n\n<%=
    video_tag \"fly-logs.mp4?card\", title: \"Video of log output in launcher as a
    new VM starts up\" %>\n\nThe video above is in real time. It's hard to catch without
    pausing, but it takes about two seconds for the launcher to spin up a new VM running
    `flyctl`. By about eight seconds in, it's created and configured the app, and
    started the deploy process. It calls out to a remote builder and starts transferring
    Docker build context.\n\nGranted, we're not done yet. The actual build and the
    creation of a VM running your app on a worker take a further finite time.\n\nA
    byproduct of this process is an app with a name like `flyctl-host-personal-1234`
    . When the initial Turboku deploy is finished, this app sits around (for free)
    waiting for webhook updates or another Turboku deploy.\n\n[![New Turboku UI finishing
    screenshot with embedded terminal output on the left and app connection info on
    the right.](turboku-logs.jpg?card)](turboku-logs.jpg)\n\n## Taking stock\n\nNew
    Turboku makes migrating from Heroku faster and, if we say so ourselves, more elegant.\n\n**Turboku
    (still) doesn't delete your Heroku app from Heroku**. That part is up to you.
    You can keep it up in both places, if you really want to!  And if you're using
    Heroku's Postgres with your app, or an add-on like Redis, its Fly.io incarnation
    [should be able to use it too](https://devcenter.heroku.com/articles/connecting-to-heroku-postgres-databases-from-outside-of-heroku),
    because Turboku brings your app's environment variables over with it.\n\nAs with
    Old Turboku, the new Turboku launcher sets up webhooks on the Heroku app for automatic
    updates on new Heroku deploys. We also watch for changes to environment variables
    like `DATABASE_URL`, which Heroku [promise](https://devcenter.heroku.com/articles/connecting-to-heroku-postgres-databases-from-outside-of-heroku)
    that they will change on you. \n\n<div class=\"callout\">Webhook integration is
    one feature that `flyctl turboku` doesn't have, so if this is important to you,
    the web launcher is the way to go.</div>\n\nWhile the connection to your Heroku
    database has to happen over the internet, it should be very fast - we measured
    about a millisecond of added latency for a US-based app. The special sauce for
    this is that non-enterprise Heroku apps all live either in Dublin or Virginia,
    so Turboku will deploy in our closest region (London or Virginia) to be next door,
    unless you specify a different region.\n\n\n<%= partial \"shared/posts/cta\",
    locals: {\n  title: \"You've got a parachute.\",\n  text: \"Turboku makes it a
    breeze to add your Heroku app to Fly.io's Rust-powered Anycast network--and it
    stays up on Heroku, so you don't have to commit to try it.\",\n  link_url: \"https://fly.io/launch/heroku\",\n
    \ link_text: \"Turbocharge Heroku for free&nbsp;&nbsp;<span class='opacity-50'>&rarr;</span>\"\n}
    %>\n\n\nBut let's face it: if your Postgres is still stuck in one region, it's
    hard to take advantage of our global edge network to suggest sophisticated beans
    in Singapore. We don't have a similarly painless solution for cloning and syncing
    databases! \U0001F640\n\nObviously, we hope that once you've tried Fly.io, you'll
    be convinced it's worthwhile to make the jump over and run [Globally Distributed
    Postgres](https://fly.io/blog/globally-distributed-postgres/)!\n\nIf you have
    a Heroku app you'd like to bring closer to your users, give the [Fly Turboku Launcher](https://fly.io/launch/heroku)
    a spin.\n"
- :id: blog-livebeats
  :date: '2022-02-01'
  :category: blog
  :title: 'LiveBeats: Building a social music app with Phoenix LiveView'
  :author: chris
  :thumbnail: livebeats-thumbnail.jpg
  :alt:
  :link: blog/livebeats
  :path: blog/2022-02-01
  :body: "\n\n<div class=\"lead\">Fly.io is now free for small Phoenix projects. This
    post is about building a wicked LiveView app with realtime collaboration features.
    If you already have a Phoenix app to deploy, [try us out](https://fly.io/docs/elixir/).
    You can be up and running in just a few minutes.</div>\n\nWe decided that 2022
    was a good year to ship a full-stack Phoenix reference app.\n\nThe &quot;full
    stack&quot; metaphor has progressed beyond its humble beginnings of some REST
    endpoints and sprinkles of JS and CSS. Showing off a todo app is also no longer
    state of the art. A reference app should really stress a framework and match the
    needs of apps being built today. Remember [turntable.fm](https://turntable.fm)?
    That's a more interesting challenge.\n\nA good full-stack framework should help
    you solve ALL the problems you need to build something like turntable.fm quickly,
    and then iteratively make it more powerful. Live updates are no longer optional,
    and a solo full stack developer should be able to deliver on these  features with
    the same productivity of a CRUD Rails app in 2010.\n\n[Meet LiveBeats](https://livebeats.fly.dev),
    a social music application we wrote to show off the LiveView UX, while serving
    as a learning example and a test-bed for new LiveView features. As such, it is,
    of course, open source — follow the development [here](https://github.com/fly-apps/live_beats)!\n\n<div
    class=\"callout text-center\">\n  <h3 class=\"font-heading\"><span class=\"whitespace-nowrap
    mr-2\">\U0001F525\U0001F3B6\U0001F525</span> [Try LiveBeats Now!](https://livebeats.fly.dev)
    <span class=\"whitespace-nowrap ml-2\">\U0001F525\U0001F3B6\U0001F525</span></h3>\n</div>\n\nIf
    you're not familiar with LiveView, our [overview](https://fly.io/blog/how-we-got-to-liveview/)
    states:\n\n> LiveView strips away layers of abstraction, because it solves both
    the client and server in a single abstraction. HTTP almost entirely falls away.
    No more REST. No more JSON. No GraphQL APIs, controllers, serializers, or resolvers.
    You just write HTML templates, and a stateful process synchronizes it with the
    browser, updating it only when needed. And there's no JavaScript to write.\n\nHere
    are some of the things LiveBeats demonstrates:\n\n1. A &quot;live&quot;, shared
    UI. What one person does is visible to everyone else who's connected.\n\n2. File
    management. Uploads should be quick, with a live UI. And the framework should
    make it easy to use different storage backends. Third party object storage is
    one way to do this, but sometimes development is simpler with just a filesystem.
    LiveView uploads is great for both!\n\n3. Presence! Apps are more interesting
    when your friends show up.\n\nWe can accomplish all this with just Phoenix and
    LiveView, in a shockingly small amount of code. It's also super fast.\n\nTo really
    see what's special about LiveView and its new features, you need to [experience
    it for yourself](https://livebeats.fly.dev). Sign in with GitHub OAuth, upload
    a playlist of MP3's, and listen to music with your friends. Playback syncs in
    real-time, presence shows who is currently listening, and file uploads are processed
    concurrently as they are uploaded.\n\nHere's a two-minute demo to see it in action:\n\n<%=
    youtube \"https://www.youtube.com/watch?v=w3xq-t2hpHY\" %>\n\n## Playlist Sync
    and Presence with Phoenix PubSub\n\nThe hallmark of any &quot;live&quot; or social
    app is seeing your friends' activity as it happens. In our case, we want to show
    who is currently listening to a given playlist, and sync the playback of songs
    as the owner drives the song selection. This is what it looks like:\n\n<%= video_tag
    \"livebeats-1.mp4?card\", title: \"Screencast showing user presence and song selection
    change reflected in other user's UI\" %>\n\nPhoenix PubSub makes this trivial.
    In a few lines of code in our business logic, we broadcast updates, then with
    a few LOC in the LiveViews we listen for the events we care about. When they come
    in, we update the UI. Everyone connected sees the pages update, even if they are
    on different horizontally scaled servers.\n\nWith friction-free PubSub at your
    fingertips, any feature that makes sense to be real-time gets to be real-time—even
    changing URLs on the fly.\n\nFor example, user profiles are served at their username,
    such as `livebeats.fly.dev/chrismccord`. But we allow users to update their username
    in the app, which changes that URL. We don't want other users listening to their
    profile to be stuck with an invalid URL that fails on refresh, sharing, or with
    a click on the profile link.\n\nHandling the URL change took a whopping six lines
    of code in our Profile LiveView!\n\n```elixir\n  def handle_info({MediaLibrary,
    %PublicProfileUpdated{} = update}, socket) do\n    {:noreply,\n     socket\n     |>
    assign(profile: update.profile)\n     |> push_patch(to: profile_path(update.profile))}\n
    \ end\n```\n\nWe are already subscribed to profile notifications, so we   just
    have to handle the `PublicProfileUpdated` event, update the template profile state,
    and then `push_patch` to the client to trigger a `pushState` browser URL change.\n\nFor
    a traditional application, this kind of feature would take standing up WebSocket
    connections, ad-hoc HTTP protocols, polling the server for changes, and other
    complexities.\n\nLet's see it in action:\n\n<%= video_tag \"livebeats-2.mp4?card\",
    title: \"Screencast showing LiveBeats UI updating in response to a username change
    in Profile Settings\" %>\n\n## Concurrent Upload Processing\n\nLiveView uploads
    are handled over the existing WebSocket connection, providing interactive file
    uploads with file progress out-of-the-box - with no user-land JavaScript.\n\nIt
    also allows other neat features like processing the file on the server before
    writing it to its final location. You might be thinking handling files on your
    server is sooo mid 2000's, but hear me out! Servers can have volumes… and you
    can like write files to them!\n\nThink about it. There aren't any Lambdas to wire
    up (and pay for per invocation), no webhooks to wire up, and no external message
    queues to configure, because it all happens over the existing LiveView connection.
    Compared to a typical cloud upload solution, we can cut out probably a few paid
    products and just as many failure modes.\n\nLiveView also guarantees the temporary
    uploaded file is on the same load-balanced instance of the LiveView processing
    the page. There's no external state to jump around between servers. So you write
    regular code that takes the file, post-processes or verifies the bits on disk,
    then writes it to its final location – all the while reporting to the UI the progress
    of each step.\n\nAnd we do need to extract data out of those binary blobs one
    way or another, because before we write that MP3 to storage, we want to know that
    (a) it's not a malicious file masquerading as an MP3 and (b) it's less than 20
    minutes long.\n\nSo the user drags and drops a handful of MP3s into the app, we
    upload them concurrently over the WebSocket connection, then we concurrently parse
    the binary MP3 data in a temporary file to verify it's a valid MP3 of acceptable
    duration. Once complete, we show the calculated duration on the UI and the user
    can save their playlist.\n\n<%= video_tag \"livebeats-3.mp4?card\", title: \"Screencast
    showing drag-and-drop multiple-file upload with live upload progress bar\" %>\n\nAn
    aside: It turns out calculating MP3 duration from file content is actually a pain.\n\nMP3s
    can contain ID3 tag metadata about the file, but answering the simple question
    of &quot;how long is this song?&quot; is surprisingly difficult. To properly calculate
    the duration of an MP3, you must walk all the frames and take bitrate encodings
    into consideration. There's a great write-up [here](https://shadowfacts.net/2021/mp3-duration/)
    on what's involved with step-by-step Elixir code to make it happen.\n\n## Components\n\nLiveView
    recently shipped a new HEEx template engine that supports React-style template
    syntax with _function components_. Function components are reusable functions
    that encapsulate a bit of markup to be used throughout your UI. Think dropdowns,
    modals, tables, etc. We have a [post all about function components](https://fly.io/phoenix-files/function-components/),
    if you want to dive deeper.\n\nFor example, one of our components is a TailwindUI
    dropdown, which looks like this:\n\n![A Tailwind UI dropdown under Chris McCord's
    name, username, and photo, with options \"View Profile,\" \"Settings,\" and \"Sign
    out\"](livebeats-4.jpg?center&1/2&card&border)\n\nAnd this is what it looks like
    in code to use anywhere you'd like a dropdown:\n\n```elixir\n~H\"\"\"\n<.dropdown
    id={@id}>\n  <:img src={@current_user.avatar_url}/>\n  <:title><%%= @current_user.name
    %></:title>\n  <:subtitle>@<%%= @current_user.username %></:subtitle>\n\n  <:link
    navigate={profile_path(@current_user)}>View Profile</:link>\n  <:link navigate={Routes.settings_path(Endpoint,
    :edit)}>Settings</:link>\n  <:link href={Routes.session_path(Endpoint, :sign_out)}
    method={:delete}>\n    Sign out\n  </:link>\n</.dropdown>\n\"\"\"\n```\n\nAlong
    with function components, LiveView includes _slots,_ which allows a component
    to specify a named area of the component where arbitrary content can be placed
    by the caller. The actual dropdown component is just a simple function that encapsulates
    all the markup and classes of our Tailwind dropdown:\n\n```elixir\ndef dropdown(assigns)
    do\n  assigns =\n    assigns\n    |> assign_new(:img, fn -> nil end)\n    |> assign_new(:title,
    fn -> nil end)\n    |> assign_new(:subtitle, fn -> nil end)\n\n  ~H\"\"\"\n  <div
    class=\"px-3 mt-6 relative inline-block text-left\">\n    ...\n  </div>\n  \"\"\"\nend\n```\n\nThere's
    a lot going on inside the function with Tailwind classes, SVGs, list item building,
    accessibility attributes, etc, but the internal details aren't important. The
    idea is: we define our dropdown in a single place in our application, complete
    with how it's styled and accessible, and then we use `<.dropdown>` throughout
    our UI.\n\n## Going Global\n\nElixir is a distributed programming language and
    we exploit this fully. LiveBeats is deployed on five continents and the servers
    cluster together automatically over a private network to broker updates. This
    is what the future of full-stack looks like.\n\nYou should serve your full-stack
    app close to users for the same reason we all agree CDNs are necessary for serving
    assets close to users. Less latency and more responsiveness improves the experience,
    and conversions are increased across the board. Folks know this intuitively, but
    historically we've only applied it to assets. With Elixir, our entire stack can
    take advantage of regional access.\n\n<%= partial \"shared/posts/cta\", locals:
    {\n  title: \"Go global with Fly.io.\",\n  text: \"Distributed by design, Elixir
    Phoenix apps are a perfect fit with the globe-spanning Fly.io network.\",\n  link_url:
    \"https://fly.io/docs/elixir/\",\n  link_text: \"Deploy an Elixir app&nbsp;&nbsp;<span
    class='opacity-50'>&rarr;</span>\"\n} %>\n\nFor LiveBeats, we went global with
    only minor changes. First, we set up file proxying between servers, where we stream
    data from one region to another. Next, we set up Postgres read replicas in each
    region and we perform standard replica reads against mostly static data.\n\nWe
    even set up a ping tracker for each user. You can view your own pings, along with
    the locations and pings of any other visitors, to visualize where your friends
    are connected across the globe. It's also neat to see what kind of speedy UX they
    have on the app.\n\nCheck it out:\n\n<%= video_tag \"livebeats-5.mp4?card\", title:
    \"Screencast of LiveBeats UI during song playback. We see the avatars, usernames,
    country flags, and repeatedly-refreshing ping times of 'chrismccord' in the USA
    and 'chriswford' in Australia. 'chrismccord' gets ping times between 29 and 86
    milliseconds; 'chriswford' down under gets between 65 and 96 milliseconds.\" %>\n\nHere
    we have a user connecting from the US (iad) and one from Australia (syd), both
    with [fast](https://fly.io/blog/last-mile-redis/) pings to the regional LiveBeats
    instances.\n\nHere's a bonus video giving a window into the process of scaling
    LiveBeats across regions on Fly.io.\n\n<%= youtube(\"https://www.youtube.com/watch?v=JrqBudJd2YM\")
    %>\n\n## Client-side interactions\n\nThe LiveView paradigm necessarily requires
    a server to be connected, but this doesn't mean all interactions should go to
    the server. Operations that can immediately happen on the client should stay on
    the client.\n\nLiveView has a `JS` command interface that allows you to declare
    client-side effects that work seamlessly with server-issued UI updates. Things
    like opening modal dialogs, toggling menu visibility, etc happen instantly on
    the client without the server needing to be aware - just as it should.\n\nLet's
    see it in action using the LiveBeats modal:\n\n```elixir\n<.modal\n  id={@id}\n
    \ on_confirm={\n    JS.push(\"delete\", value: %{id: @song.id})\n    |> hide_modal(@id)\n
    \   |> hide(\"#song-#{@song.id}\")\n  }\n>\n  Are you sure you want to delete
    \"<%%= @song.title %>\"?\n  <:cancel>Cancel</:cancel>\n  <:confirm>Delete</:confirm>\n</.modal>\n```\n\nWhen
    a user attempts to delete a song, we show a modal component asking for confirmation.
    The key aspect is this: when &quot;Delete&quot; is clicked, our `on_confirm` attribute
    asynchronously pushes a delete event to the server, but _immediately_ hides the
    modal and song in the playlist.\n\nSo we get instant user interaction without
    any latency, while the song is deleted on the server, just like traditional optimistic
    UI patterns in a JavaScript framework.\n\nSince the `JS` command interface is
    regular Elixir code, we can compose functions to handle client-UI operations,
    such as `hide` above:\n\n```elixir\ndef hide(js \\\\ %JS{}, selector) do\n  JS.hide(js,\n
    \   to: selector,\n    time: 300,\n    transition:\n      {\"transition ease-in
    duration-300\", \"transform opacity-100 scale-100\",\n       \"transform opacity-0
    scale-95\"}\n  )\nend\n```\n\nThis wraps `JS.hide` and uses `:transition` to give
    it a slick Tailwind-based CSS transition when hiding elements.\n\n## What's Next\n\nWith
    LiveBeats fleshed out, we'll continue to add features and use it as a test-bed
    for in-progress LiveView development. Expect future content deep diving into approaches
    and considerations when bringing your LiveView application close to users around
    the world!\n"
- :id: phoenix-files-function-components
  :date: '2022-01-26'
  :category: phoenix-files
  :title: Reuse markup with function components and slots
  :author:
  :thumbnail: function-components-thumbnail.jpg
  :alt:
  :link: phoenix-files/function-components
  :path: phoenix-files/2022-01-26
  :body: "\n\n## The problem\n\nWe'd like a way to reuse code for UI components that
    are very similar in structure, but carry different content.\n\nImagine we're writing
    a [Phoenix](https://www.phoenixframework.org) [LiveView](https://hexdocs.pm/phoenix_live_view/Phoenix.LiveView.html)
    app that frequently uses modals to display or save information. For a consistent
    experience, they might all have HTML elements for header, body, and footer regions.
    We'd prefer not to repeat this markup for all possible variations!\n\n## The solution\n\nThis
    is a job for LiveView's [function components](https://hexdocs.pm/phoenix_live_view/Phoenix.Component.html)
    (`Phoenix.Component`).\n\nA function component is basically a wrapper for a [`~H`
    sigil](https://hexdocs.pm/phoenix_live_view/0.17.0/Phoenix.LiveView.Helpers.html#sigil_H/2)
    that provides a template for customized content. It doesn't have state of its
    own.\n\nThe `~H` sigil lets us inject [HEEx](https://hexdocs.pm/phoenix_live_view/assigns-eex.html)
    templating code into our source, to be interpreted and rendered into our LiveView.
    It's used not only in defining a template for a component, but also in rendering
    it.\n\nWhen we call the function component, we pass our unique content to it,
    either through its `assigns` parameter, or, if we need to pass whole blocks of
    HTML, using the [slots](https://hexdocs.pm/phoenix_live_view/Phoenix.Component.html#module-slots)
    mechanism. Or both.\n\n## Example: multi-part modals\n\nLet's go back to our modal
    example. We'll lay some groundwork with a basic function component we'll call
    `modal`, and morph it to demonstrate the following powers:\n\n- [displaying strings](#custom-text-using-assigns)
    from an assigns map we pass to it\n- displaying custom HTML we insert in its [default
    slot](#custom-html-using-slots)\n- placing multiple blocks of custom HTML through
    [*named* slots](#named-slots)\n- [dealing gracefully](#optional-slots) with slots
    that are not always defined\n- passing values into the component through a [named
    slot's *attributes*](#slot-attributes)\n- passing a value from the component's
    assigns map [into one of its slots](#passing-assign-values-to-the-caller-s-slot)\n\n###
    Custom text using assigns\n\nWe call `use Phoenix.Component` at the top of our
    module to import the functions defined in `Phoenix.LiveView` and `Phoenix.LiveView.Helpers`.
    Then we can go ahead and define our function component.\n\n```elixir\ndefmodule
    MyAppWeb.UI do\n  use Phoenix.Component\n\n  def modal(assigns) do\n    ~H\"\"\"\n
    \     <div class=\"modal-container\">\n        <div class=\"header\">\n          <h1><%%=
    @title %></h1>\n        </div>\n\n        <div class=\"modal-body\">\n          <p
    class=\"text-sm text-gray-500\">\n            <%%= @body %>\n          </p>\n
    \       </div>\n\n        <div class=\"modal-footer\">\n          <button>Ok</button>\n
    \         <button>Cancel</button>\n        </div>\n      </div>\n    \"\"\"\n
    \ end\nend\n```\n\nThis defines a function, called `modal/1`, that renders an
    HTML div element with class `\"modal-container\"`, enclosing more divs for the
    three distinct parts of our modal: header, body, and footer.\n\nWithin the `\"header\"`
    div, `<%%= @title %>` will substitute the value of the assign `@title` on render
    (we could equally well write it `<%%= assigns.title %>`; it's just longer). In
    the div with class `\"modal-body\"`, `<%%= @body %>` will give us the value of
    `@body`. The component will expect us to supply it with a map of assigns that
    include these, when we call it.\n\nThe \"modal-footer\" div contains two `buttons`,
    \"Ok\" and \"Cancel,\" that for the moment don't do anything.\n\nNow we can call
    `.modal` inside a `sigil_H` to render it, providing the `@title` and `@body` assigns
    in the opening tag:\n\n```elixir\nalias MyAppWeb.UI\n~H\"\"\"\n<UI.modal \n  title=\"My
    basic modal\" \n  body=\"My modal content\">\n</UI.modal>\n\"\"\"\n```\n\nIf we
    import the module into the view where we'll render it, we don't have to use the
    full module name:\n\n```elixir\nimport MyAppWeb.UI\n~H\"\"\"\n<.modal title=\"My
    basic modal\" body=\"My modal content\"></.modal>\n\"\"\"\n```\n\nWith some CSS
    magic, we'll get a modal like this:\n\n![A modal window with title \"My basic
    modal,\" body text \"My modal content\", and plain buttons labelled \"Cancel\"
    and \"Ok\"](basic-modal.png)\n\n### Custom HTML using slots\n\nWhat if we want
    to put some custom HTML into the body of the modal? We can't fit a block of HTML
    into an assign. That's where slots come in.\n\nWhen you include content inside
    a function component—that is, between its opening and closing tags—it gets assigned
    to the component's _default slot,_ `@inner_block`. This assign can be rendered
    in our component with a [`render_slot/2`](https://hexdocs.pm/phoenix_live_view/Phoenix.LiveView.Helpers.html#render_slot/2)
    macro in the template like this:\n\n```elixir\n<%%= render_slot(@inner_block)
    %>\n```\n\nYou could even use other components within the slot. We'll keep our
    demonstration simple: just a bit of custom HTML and some graphics.\n\nLet's imagine
    that we want to display a modal to confirm that the user settings have been saved
    successfully.  We tell the `modal` function to render the `@inner_block` in the
    `\"modal-body\"` div:\n\n```elixir\ndef modal(assigns) do\n  ~H\"\"\"\n  <div
    class=\"modal-container\">\n    <div class=\"header\">\n      <h1><%%= @title
    %></h1>\n    </div>\n\n    <div class=\"modal-body\">\n      <p class=\"text-sm
    text-gray-500\">\n        <%%= render_slot(@inner_block) %>.   <!-- HERE  -->\n
    \     </p>\n    </div>\n\n    <div class=\"modal-footer\">\n      <button>Ok</button>\n
    \     <button>Cancel</button>\n    </div>\n  </div>\n  \"\"\"\n end\n```\n\nAnd
    put our custom HTML content inside the call to `modal` like so:\n\n```elixir\n~H\"\"\"\n<.modal
    title=\"My basic modal\">        \n  \n  <!-- body content  -->\n  <div class=\"text-center
    justify-center items-center\">\n    <h1 class=\"text-green-600\">Great!</h1>\n
    \   <p>Your settings have been <strong>successfully</strong> saved</p>\n    <div
    class=\"flex items-center justify-center\">\n      <img class=\"h-20 w-20 rounded-full
    flex items-center\"\n      src={Routes.static_path(@socket, \"/images/check.png\")}\n
    \     alt=\"\">\n    </div>\n  </div>\n  <!-- body content  -->\n\n</.modal>\n\"\"\"\n```\n\nOur
    fancy custom message gets rendered in our modal.\n\n![A modal window with title
    \"My basic modal\" from an assign, and a custom message and checkmark image in
    the body, supplied by the component's default slot.](slot-modal-1.png)\n\n###
    Named slots\n\nWe've modified the body of the modal with `@inner_block`, but we
    might want custom markup in the header and footer too. But wait—if `@inner_block`
    is the _default_ slot, that implies there can be others. It's true! They need
    different names, so you can refer to them (a named slot can also take an argument,
    as we'll see below).\n\nNamed slots are defined with the following syntax, when
    we call our function component:\n\n```xml\n<:name_of_my_slot> \n  My slot content\n</:name_of_my_slot>\n```\n\nLet's
    prep the template to customize the header and the buttons in the footer using
    slot assigns named `@header` , `@confirm`, and `@cancel`:\n\n```elixir\ndef modal(assigns)
    do\n  ~H\"\"\"\n  <div class=\"modal-container\">\n    <div class=\"header\">\n
    \     <%%= render_slot(@header) %>                 <!-- HERE  -->\n    </div>\n\n
    \   <div class=\"modal-body\">\n      <p class=\"text-sm text-gray-500\">\n        <%%=
    render_slot(@inner_block) %>          <!-- HERE  -->\n      </p>\n    </div>\n\n
    \   <div class=\"modal-footer\">                    <!-- HERE  -->\n      <button><%%=
    render_slot(@confirm) %></button>\n      <button><%%= render_slot(@cancel) %></button>
    \             \n    </div>\n  </div>\n  \"\"\"\n end\n```\n\nAnd here's how we
    specify the contents of those slots for this variant of the modal, along with
    the default slot we already filled:\n\n```elixir\n~H\"\"\"\n<.modal title=\"My
    basic modal\">        \n  <!-- named slot: header  -->\n  <:header>\n    <div
    class=\"border-b-4 border-green-600\">\n      Success modal\n    </div>\n  </:header>\n\n
    \ <!-- inner_block slot  -->\n  <div class=\"text-center justify-center items-center\">\n
    \   <h1 class=\"text-green-600\">Great!</h1>\n    <p>Your settings have been <strong>successfully</strong>
    saved</p>\n    <div class=\"flex items-center justify-center\">\n      <img class=\"h-20
    w-20 rounded-full flex items-center justify-center\"\n      src={Routes.static_path(@socket,
    \"/images/check.png\")}\n      alt=\"\">\n    </div>\n  </div>\n\n  <!-- footer
    named slots  -->\n  <:confirm>\n    Return to profile\n  </:confirm>\n\n  <:cancel>\n
    \   Back to index\n  </:cancel>\n\n</.modal>\n\"\"\"\n```\n\nNote that everything
    in the body of our function component that is not inside a _named slot_ is inside
    the default slot `@inner_block`.\n\n![A modal window with title \"Success modal,\"
    body message \"Great! Your settings have been successfully saved\" (with a green
    checkmark image), and footer buttons labelled \"Back to index\" and \"Return to
    profile\"](named-slots-modal.png)\n\nGreat! The title, the body, and the button
    text have all been customized!\n\n### Optional slots\n\nSome modals might not
    need all the slots we've told `modal` to render. Our function component will choke
    on any undefined assigns, so we start our definition by making sure `@header`,
    `@confirm`, or `@cancel` exist:\n\n```elixir\ndef modal(assigns) do\n  assigns
    =\n    assigns\n    |> assign_new(:header, fn -> [] end)\n    |> assign_new(:confirm,
    fn -> [] end)\n    |> assign_new(:cancel, fn -> [] end)\n\n  ~H\"\"\"\n  <div
    class=\"modal-container\">\n    <div class=\"header\">\n      <%%= render_slot(@header)
    %>\n    </div>\n    ...\n```\n\nHere, the `assign_new/2` function initializes
    any of those that's missing a value with an empty list. \n\nCall `modal` without
    any of the named slots:\n\n```elixir\n~H\"\"\"\n<.modal title=\"My basic modal\">
    \       \n  \n  <!-- this modal has no footer or header  -->\n\n  <div class=\"text-center
    justify-center items-center\">\n    <h1 class=\"text-green-600\">Great!</h1>\n
    \   <p>Your settings have been <strong>successfully</strong> saved</p>\n    <div
    class=\"flex items-center justify-center\">\n      <img class=\"h-20 w-20 rounded-full
    flex items-center\"\n      src={Routes.static_path(@socket, \"/images/check.png\")}\n
    \     alt=\"\">\n    </div>\n  </div>\n\n</.modal>\n\"\"\"\n```\n\n![A modal with
    no header and no footer; only the custom body message and checkmark image supplied
    by the default slot](optional-slots-modal.png)\n\nNo problem! The unused slots
    are empty, but not error-raisingly undefined.\n\n### Slot attributes\n\nWe can
    give named slots _attributes_ in much the same way that we pass assigns to regular
    function components, and these attributes can be accessed from inside the function
    component.\n\nWe're going to use slot attributes to pass custom CSS classes to
    each of the footer buttons. Here we give the `@confirm` and `@cancel` slots each
    an attribute called `classes`:\n\n```elixir\n<!-- named slot: confirm 1 -->\n<:confirm
    classes=\"bg-green-400 rounded-full text-slate-50 text-sm p-2\">\n  Return to
    profile\n</:confirm>\n\n<!-- named slot: cancel 1 -->\n<:cancel classes=\"bg-emerald-400
    rounded-full text-slate-50 text-sm p-2\">\n  Back to index\n</:cancel>\n```\n\nNow,
    to get at the attributes of our named slots, we have to take into account that
    it's possible to define [multiple entries for the same named slot](https://hexdocs.pm/phoenix_live_view/Phoenix.Component.html#module-named-slots-with-attributes).
    When we call `render_slot/2`, it simply renders all the entries for the slot name.
    But inside the slot's assign, there's a list of attribute maps: one for each entry.
    Even if there's only one entry, and so one attributes map, we use a [`for` loop](https://elixir-lang.org/getting-started/comprehensions.html)
    to get into the list and access it.\n\nTo demonstrate: For every slot entry named
    `@confirm`, we'll make a button whose class is the string-interpolated value of
    the `classes` attribute of that entry. We'll do the same again for `@cancel`.\n\n```elixir\n<!--
    FOOTER -->\n<div class=\"modal-footer\">\n  <%%= for confirm <- @confirm do %>\n
    \   <button class={\"#{confirm.classes}\"}><%%= render_slot(@confirm) %></button>\n
    \ <%% end %>\n\n  <%%= for cancel <- @cancel do %>\n    <button class={\"#{cancel.classes}\"}><%%=
    render_slot(@cancel) %></button>\n  <%% end %>\n</div>\n```\n\nSince there's only
    one slot entry for each name, we'll only get one button from each!\n\n![A modal
    with buttons labelled \"Back to index\" and \"Return to profile\", but in different
    colours!](slot-attributes-modal.png)\n\nOur slot-specific classes have been applied,
    giving the buttons different colors!\n\n### Passing assign values to the caller's
    slot\n\nSometimes we'll want to use data from an assign passed to the function
    component, within content that's provided by a slot. The component has to pass
    that value into the slot to be rendered. We can do this by giving it as an argument
    to `render_slot/2`.\n\nSuppose our function component takes a `@user` assign with
    data about the current user in a struct. We want to display `@user.name` from
    that struct in the body of the modal.\n\nIn the function definition, we'll pass
    `render_slot/2` the `@user.name` as an argument. \n\n```elixir\n<!-- MODAL BODY
    -->\n<div class=\"mt-10\">\n  <p id={\"#{@id}-content\"} class=\"text-sm text-gray-500\">\n
    \ <%%= render_slot(@body, @user.name) %>\n  </p>\n</div>\n```\n\nIn the following
    slot definition, the `let` argument binds the incoming `@user.name` data to the
    name `username`, so we can ask for it to be interpolated using `<%%= username
    %>`. We have to use a named slot for the body this time, so we can use `let`.\n\n\n```elixir\n<:body
    let={username}>\n  <div class=\"text-center justify-center items-center\">\n    \n
    \   <h1 class=\"text-green-600\">Hey, <%%= username %>!</h1>\n    \n    <p>Your
    settings have been <strong>successfully</strong> saved</p>\n    <div class=\"flex
    items-center justify-center\">\n      <img class=\"h-20 w-20 rounded-full flex
    items-center\"\n      src={ Routes.static_path(@socket, \"/images/check.png\")
    }\n      alt=\"\">\n    </div>\n  </div>\n</:body>\n```\n\n![A modal like a lot
    of the others, but with \"Hey, Berenice Medel!\" at the start of the body message
    to show that username data made its way from the `@user` assign into the slot
    contents for rendering.](slots-let-modal.png)\n\nOur modal is now super-duper
    customized, with the current user's name provided to a slot from an assign!\n\nIt's
    your turn to build something with the power of LiveView function components!\n"
- :id: blog-free-postgres
  :date: '2022-01-20'
  :category: blog
  :title: Free Postgres Databases
  :author:
  :thumbnail: free-postgres-thumbnail.jpg
  :alt:
  :link: blog/free-postgres
  :path: blog/2022-01-20
  :body: "\n\n<div class=\"lead\">Postgres on Fly.io is now free for small projects.
    The hard part about free Postgres is storage, so this post is also about free
    storage. Read about it here, or [try us out first](https://fly.io/docs/speedrun/).
    You can be up and running in just a few minutes.</div>\n\nWe like building side
    projects and also hate paying for hosting for side projects. We also know that
    y&#39;all like free stuff. And we think that when you use free stuff for side
    projects, there&#39;s a pretty good chance you&#39;ll pay for similar stuff for
    real work.\n\nWe&#39;ve had a free tier since we launched ten years ago (in 2020).
    Until today, our free tier covered a little bit of CPU time, a little bit of RAM,
    and a little bit of bandwidth. We didn&#39;t include storage.\n\nThere&#39;s a
    good reason for this. CPU, RAM, and bandwidth are easy. It costs almost nothing
    to keep a process idling. It&#39;s easy to migrate application processes. And,
    importantly, it&#39;s easy to bounce back when hardware goes poof. These things
    are easy because there&#39;s almost no state involved. An application process
    that stops and starts fresh on different hardware continues to be valuable.\n\nYour
    enormous database of sandwiches is different, though.\n\nStored data occupies
    space all the time, though, whether your app is running or not. Recovering data
    from hardware failure means you have to be storing it with some redundancy. You
    need replication and backups. This means more disk space, but it also means more
    management. Disks are not easy!\n\nBut how boring are apps with no state? You
    could store some sandwiches right in your app, but you&#39;d have to deploy it
    again every time an ingredient changed. And you wouldn&#39;t remember who&#39;d
    ordered from you before, let alone what their favorite condiments are. The best
    apps save data, and the best-best apps keep track of it in a database so it&#39;s
    easy to remix and share: what better way to let your customers create a poll for
    the office lunch order?\n\n## What IS free Postgres?\n\n&quot;Free Postgres&quot;
    really means we&#39;ve added 3GB of persistent volume space to our free offering.
    Specifically, you&#39;re getting [LVM](https://en.wikipedia.org/wiki/Logical_Volume_Manager_(Linux))
    block devices as small as 1GB. Match that up with our free VM and RAM allowance,
    and you&#39;ve got what you need for a side-project Postgres setup.\n\nThe free
    VMs themselves are just a bit of memory and some idling CPU, but the state is
    obnoxious for us to manage.\n\nWe&#39;ve done the work to sort out some of the
    management headaches with [our baked-in Postgres](https://fly.io/docs/reference/postgres/).
    &quot;Production&quot; deployments set up 2-node Postgres clusters intentionally.
    They should have higher availability than a single node, and they have data redundancy.
    When you run HA Postgres, you’re getting data storage on two isolated NVMe drives.
    We also take snapshots of these volumes, to give your delicious data some buffer
    against mishap.\n\nFree Postgres would start with a single &quot;Development&quot;
    node, but if you scale it, it seamlessly becomes a leader-replica cluster for
    high availability.\n\n## So…we&#39;re really giving you free volumes\n\nThe lede
    is &quot;free Postgres&quot; because that&#39;s what matters to full stack apps.
    You don&#39;t _have_ to use these for Postgres. If SQLite is more your jam, mount
    up to 3GB of volumes and use &quot;free SQLite.&quot; Yeah, we're probably underselling
    that.\n\n\n<%= partial \"shared/posts/cta\", locals: {\n  title: \"Store it here.\",\n
    \ text: \"It'll take just a few minutes to add free Postgres to your Fly.io app.\",\n
    \ link_url: \"https://fly.io/docs/reference/postgres/\",\n  link_text: \"Add Postgres
    on Fly.io for free&nbsp;&nbsp;<span class='opacity-50'>&rarr;</span>\"\n} %>\n\n##
    A note about credit cards\n\nEven for our free services, we require a credit card
    number. We know that&#39;s the worst and it gives you heartburn. It&#39;s not
    because we plan to charge you. \n\nBut here&#39;s what happens if you give people
    freemium full access to a hosting platform: lots and lots of free VMs mining for
    cryptocurrencies.\n\nWe could tell you we want to prevent crypto mining because
    we care about the planet, and that would be true. We also have a capitalism nerve
    that hurts when people spend our money gambling. Your credit card number is the
    thin plastic line between us and chaos.\n"
- :id: phoenix-files-live-render-sticky-option
  :date: '2022-01-12'
  :category: phoenix-files
  :title: Keep LiveViews alive across live redirects
  :author: berenice
  :thumbnail: keep-liveview-alive-thumbnail.jpg
  :alt:
  :link: phoenix-files/live-render-sticky-option
  :path: phoenix-files/2022-01-12
  :body: "\n\n## _Problem_\n\nYou are building a website for listening to your favorite
    songs. You want to play or pause the current song while you are adding songs to
    your playlist or even editing your profile. You create a LiveView to handle the
    player controls and show the status of the current song… but something unexpected
    is happening: every time you navigate around different LiveViews, the LiveView
    for the player controls loses the current song state. Notice how the play progress
    bar flickers? It's being remounted!\n\n<%= video_tag \"without_sticky.mp4?card\"
    %>\n\nWhy is this happening?\n\nThe nested LiveView for the player controls loses
    its state and is recreated every time a `live_redirect` happens to the `parent`
    LiveView. The `live_redirects` happen when the user navigates.\n\nCan we avoid
    recreating our nested LiveView between live redirects?\n## _Solution_\n\n`LiveView
    0.17.6` added the [sticky](https://github.com/phoenixframework/phoenix_live_view/blob/c50c39c6ebc501393e014d446f4c747d5f876e06/lib/phoenix_live_view/helpers.ex#L372)
    option for the [`live_render/3`](https://hexdocs.pm/phoenix_live_view/Phoenix.LiveView.Helpers.html#live_render/3)
    function. The `sticky` option means a LiveView can remain active across live redirects
    within our application. Even if it is nested within another LiveView! \n\nThis
    means a few things:\n- the song player LiveView doesn't get re-rendered with each
    `live_redirect`\n- the player keeps its state (no expensive re-building the internal
    state)\n- the life-cycle of the LiveView is not affected\n\nHow do we use this
    new option? It is really easy:\n```elixir\n<%%= live_render(@socket, \n                LiveBeatsWeb.PlayerLive,
    \n                id: \"player\", \n                session: %{}, \n                sticky:
    true) %>\n```\n\nPassing the `sticky: true` option, LiveView does the necessary
    magic to keep our `PlayerLive` LiveView even in case of live redirects.\n\nLet's
    see it in action!\n\n<%= video_tag \"with-sticky.mp4?card\" %>\n\n\nOur player
    no longer renders over and over again!\n\n## _Discussion_\nLiveView is maturing
    nicely. Refinements like the [sticky](https://github.com/phoenixframework/phoenix_live_view/blob/c50c39c6ebc501393e014d446f4c747d5f876e06/lib/phoenix_live_view/helpers.ex#L372)
    option are evidence of that. This is a nice feature to know about when working
    with nested LiveViews. It means a child LiveView doesn't lose its state and get
    remounted when a `live_redirect` happens in the parent. This means you can create
    more advanced applications with a better user experience and still get all the
    productivity benefits of LiveView!\n"
- :id: blog-self-service-account-deactivation
  :date: '2022-01-10'
  :category: blog
  :title: 'Self-service account deactivation: why it’s harder than you would think'
  :author: lubien
  :thumbnail: bye-bye-bye-thumbnail.jpg
  :alt:
  :link: blog/self-service-account-deactivation
  :path: blog/2022-01-10
  :body: "\n\n<div class=\"lead\">Fly.io runs apps close to users, by transmuting
    Docker containers into micro-VMs that run on our own hardware around the world.
    This is a post about deactivating accounts on Fly.io, but if you're new, you might
    like to try us out first; [you can be up and running in just a couple of minutes](https://fly.io/docs/speedrun/).</div>\n\nWe've
    just introduced a self-service UI for deleting your Fly.io account.\n\nFrom time
    to time people ask us to deactivate their account. It happens! Maybe you just
    tried a walkthrough out of interest, or you created a new account with a different
    name, or (snif) you're moving your app to (snif) another platform. Whatever the
    case, until recently, to get your account deactivated, you had to email support.\n\nWhy
    was this? At first glance it seems simple. Just build a self-service \"delete
    everything\" button. However, to bring this feature to life, we need to consider
    Factors. Users share organizations, and have apps running. You need to understand
    what you're doing, and then make a decision, and our account deletion features
    need to help you with that.\n\nIt comes down to this: if we're not careful, the
    easier we make it to deactivate your account, the easier we make it to accidentally
    destroy a bunch of infrastructure in one dumb click. We're determined not to fall
    into that trap.\n\nLet's look at what we came up with:\n\nFirst, we need to delete
    your apps. Once your account is deleted, these apps are gone; you can't go back
    and fix things. We want to make sure you're not accidentally abandoning anything
    important. Also, we allocate resources to each app you own, so if you're breaking
    up with us, we'll take our volume storage space, mix tapes, and favorite sweater
    back. \n\nIt gets trickier. If you're migrating to (snif) another provider, there's
    more homework to take care of.  When you point a domain to a Fly.io app you'll
    have configured certificates, and when leaving you probably need to change some
    DNS records. \n\nFinally, in your time here you might've created an organization,
    and then invited friends (please do). So you're an admin of shared apps now. Given
    you're the admin, you probably want to either delete your organization or transfer
    it to someone else, so we will carefully remind you of that before leaving.\n\nWith
    all this in mind, we built a UI to guide you through these steps without having
    to look it all up in documentation. Following the links should bring you to the
    right page for each task.\n\n![Deactivation UI showing no organization-related
    problems and links for deleting existing apps and certificates](deactivate-simple.png)\n\nWhether
    you're just tearing down a test account or migrating an app off out platform,
    this should be all you need. Most of the time, you probably only have to delete
    an app or two, and then you're good to go.\n\nThat being said, there are still
    some cases where we can't automatically deactivate your account. If you're sharing
    organizations or are the admin of someone else's organizations, we can't automatically
    zap your account. At least for now, you will need a little help to get going in
    this case. The UI will let you know if it detects that you would just get stuck
    on the organization step.\n\n![Deactivation UI indicating to contact us due to
    organization needing manual intervention before account can be deactivated](deactivate-contact-us.png)\n\nIf
    you're excited about this new feature, we want to hear from you about how we can
    make Fly.io the platform you're excited to try again in the near future. Reach
    out at our [community forum](https://community.fly.io/).\n\n"
- :id: phoenix-files-passing-unknown-attributes-into-component
  :date: '2022-01-05'
  :category: phoenix-files
  :title: Passing Unknown Attributes into Your Component
  :author: mark
  :thumbnail: unknown-attributes-thumbnail.jpg
  :alt:
  :link: phoenix-files/passing-unknown-attributes-into-component
  :path: phoenix-files/2022-01-05
  :body: |2-


    <div class="lead">Fly.io runs apps close to users, by transmuting Docker containers into micro-VMs that run on our own hardware around the world. This is a recipe about making your stateless components in LiveView more reusable and generic. If you are interested in deploying your own Elixir project, the easiest way to learn more is to [try it out](https://fly.io/docs/speedrun/); you can be up and running in just minutes.</div>

    On your LiveView page, you are using a custom component. You want to be able to pass HTML attributes into the component, but the component doesn't know anything about the attributes being passed! You need a way to pass arbitrary attributes through and get them where you want them.

    ## _Problem_

    You're getting into LiveView and starting to create more components to reuse in your application. Now you're having a problem creating components and passing in common `phx-click` or `phx-value-myvalue` attributes and getting them where you want in your component's markup.

    Example:

    You want to use the component like this:

    ```html
    <.special_button title="Yes, sign me up!"
      phx-click="select-sign"
      phx-value-elect="yes" />
    ```

    The component's markup generates something like this:

    ```html
    <div class="container-classes">
      <div class="my-button-classes" I WANT THE CLICK SETTINGS HERE>
        Yes, sign me up!
      </div>
    </div>
    ```

    Notice the big "**I WANT THE CLICK SETTINGS HERE**"? That's where I want the `phx-click` and `phx-value-elect` attributes to go. But the component is supposed to be reusable! Somewhere else it's going to be a different `phx-value-other` attribute.

    How do I get the `phx-click`, `phx-value-*` and maybe even `phx-target` attributes through to the component and control where they go?

    ## _Solution_

    In LiveView 0.16, the function [`assigns_to_attributes/2`](https://hexdocs.pm/phoenix_live_view/Phoenix.LiveView.Helpers.html#assigns_to_attributes/2) was added. We can use this to help solve this problem!

    Here's why this function is helpful:

    > Useful for transforming caller assigns into dynamic attributes while stripping reserved keys from the result.

    If we call `assigns_to_attributes` with the assigns example from the problem above, this is returned:

    ```elixir
    assigns_to_attributes(assigns)
    #=> ["title": "Yes, sign me up!",
    #=>  "phx-click": "select-sign",
    #=>  "phx-value-elect": "yes"]
    ```

    The `assigns_to_attributes` function takes a 2nd argument that comes in handy now. It let's us _exclude_ assigns from becoming attributes. Since we're using `:title` internally, we want to exclude it.

    This is what it looks like when we exclude `:title`.

    ```elixir
    assigns_to_attributes(assigns, [:title])
    #=> ["phx-click": "select-sign",
    #=>  "phx-value-elect": "yes"]
    ```

    Now that we've removed all the assigns that the component is using, we want to take the rest and just splat them where _we_ want them but as HTML attributes.

    How do we do that?

    Let's say our simple component looks like this:

    ```elixir
    def special_button(assigns) do
      extra = assigns_to_attributes(assigns, [:title])

      assigns =
        assigns
        |> assign(:extra, extra)

      ~H"""
      <div class="container-classes">
        <div class="my-button-classes" {@extra}>
          <%= @title %>
        </div>
      </div>
      """
    end
    ```

    First, we get `extra` to hold the attributes we want to render.

    Next, we add `:extra` to the assigns so our HEEx (`~H`) template can use it.

    Finally, the `{@extra}` takes all the attributes not excluded and outputs them as HTML attributes on the `div` we want! Our rendered markup ends up looking like this:

    ```html
    <div class="container-classes">
      <div class="my-button-classes"
            phx-click="select-sign"
            phx-value-elect="yes">
        Yes, sign me up!
      </div>
    </div>
    ```

    Nice! That's exactly what we were aiming for!

    ## _Discussion_

    The [`assigns_to_attributes/2`](https://hexdocs.pm/phoenix_live_view/Phoenix.LiveView.Helpers.html#assigns_to_attributes/2) function is a good tool for making reusable LiveView components. It also gives us control over where common attributes like `phx-click` and `phx-value-*` get rendered.

    This means our components can be more generic and reusable!

    <%= partial "shared/posts/cta", locals: {
      title: "Fly ❤️ Elixir",
      text: "Fly is an awesome place to run your Elixir apps. It's really easy to get started. You can be running in minutes.",
      link_url: "https://fly.io/docs/elixir/",
      link_text: "Deploy your Elixir app today!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>"
    } %>
- :id: phoenix-files-restore-liveview-state-on-startup
  :date: '2022-01-03'
  :category: phoenix-files
  :title: Restore LiveView State on Startup
  :author: mark
  :thumbnail: saving-restoring-liveview-thumbnail.jpg
  :alt:
  :link: phoenix-files/restore-liveview-state-on-startup
  :path: phoenix-files/2022-01-03
  :body: |2-


    <div class="lead">Fly.io runs apps close to users, by transmuting Docker containers into micro-VMs that run on our own hardware around the world. This is a recipe about how you can optimize restoring Phoenix LiveView state that's stored in the browser. If you are interested in deploying your own Elixir project, the easiest way to learn more is to [try it out](https://fly.io/docs/speedrun/); you can be up and running in just minutes.</div>

    You are storing some [LiveView state in the browser](/phoenix-files/saving-and-restoring-liveview-state). You want to retrieve that saved state as early as possible to improve the user experience. How can you do that?

    ## _Problem_

    The approach in [Saving and Restoring LiveView State](/phoenix-files/saving-and-restoring-liveview-state) waits for the LiveView to request the client to send the data. In your situation, you know you want the data, so why wait around to be asked for it? Can you have it automatically pushed from the client without being requested? Can you do it in a way that is reusable for other pages and other LiveViews and not just this one page?

    ## _Solution_

    When you already know you want data stored in the browser during the startup of your LiveView, there is a way to send it up when the socket connection is being setup.

    The data could be written into the HTML of the page or stored in `sessionStorage` or `localStorage`. Regardless of _where_ it's stored, you have a way to send it to the server from the client without being asked.

    Assuming a freshly created Phoenix application, the `app.js` file will look something like this.

    ```javascript
    import "phoenix_html"
    // Establish Phoenix Socket and LiveView configuration.
    import {Socket} from "phoenix"
    import {LiveSocket} from "phoenix_live_view"
    import topbar from "../vendor/topbar"

    let csrfToken =
      document.querySelector("meta[name='csrf-token']")
      .getAttribute("content")
    let liveSocket =
      new LiveSocket("/live", Socket, {params: {_csrf_token: csrfToken}})

    // Show progress bar on live navigation and form submits
    topbar.config({barColors: {0: "#29d"}, shadowColor: "rgba(0, 0, 0, .3)"})
    window.addEventListener("phx:page-loading-start", info => topbar.show())
    window.addEventListener("phx:page-loading-stop", info => topbar.hide())

    // connect if there are any LiveViews on the page
    liveSocket.connect()

    // expose liveSocket on window for web console debug logs and latency simulation:
    // >> liveSocket.enableDebug()
    // >> liveSocket.enableLatencySim(1000)  // enabled for duration of browser session
    // >> liveSocket.disableLatencySim()
    window.liveSocket = liveSocket
    ```

    Let's look at this line in particular.

    ```javascript
    let liveSocket = new LiveSocket("/live", Socket, ...)
    ```

    [The `params` option to the `LiveSocket` constructor](https://hexdocs.pm/phoenix_live_view/js-interop.html#content) accepts a javascript closure (aka "function") which will be invoked on the client when generating the param value. We can change `params` to look something like this:

    ```javascript
    let params = () => {
      return {_csrf_token: csrfToken, restore: sessionStorage.getItem(...)}
    }

    let liveSocket = new LiveSocket("/live", Socket, {params: params})
    ```

    This reads the data we want to send up from `sessionStorage` on the client. Then on the server you read the connect params in the mount callback:

    ```elixir
      def mount(_params, _session, socket) do
        case get_connect_params(socket) do
          %{"restore" => token} -> ...
    ```

    This works well enough and depending on your needs, you may be done!

    However, the code in `app.js` get's loaded on **every** page load. If you have _multiple_ LiveViews and they don't care about or even _want_ the data stored on the client, then sending it up every time we connect the socket is a waste of resources! Especially if the payload is large.

    Can we make it better by only sending it up when we want it? If multiple pages store data, then we might want to be selective about what data we send up. In short, can we make this more reusable within a single application?

    If we add some custom DOM attributes to our template, the `app.js` file can look for it. If it's there, it can also look for what key the data is stored under and fetch that during the connect process.

    Our `html` might look like this:

    ```html
    <div data-state-restore="true" data-session-key="my_special_key">
      ...
    </div>
    ```

    Here we added `data-state-restore="true"` and `data-session-key="my-key"`. There's nothing special about these keys except that we'll look for their existence and act on it.

    In `app.js`, we can update the `param` function to look for those keys.

    ```js
    let params = (node) => {
      var restoreNode =
        node && node.querySelector("div[data-state-restore='true']")
      if (restoreNode) {
        var key = restoreNode.getAttribute("data-session-key")
        return {_csrf_token: csrfToken, restore: sessionStorage.getItem(key)}
      }
      else {
        return {_csrf_token: csrfToken}
      }
    }
    ```

    If your LiveView does _not_ include `data-state-restore`, then nothing extra happens. Your LiveView continues on as you'd expect. If it _is_ present, it uses the key we provide to look up in `sessionStorage` for any data we have stored there. That data is sent along when the connection opens in the `mount` callback.

    Sweet!

    ## _Discussion_

    Phoenix and LiveView already provide the hooks we need to make this feature work. The key is knowing that the `new LiveSocket(...)` params can be a closure that determines at runtime if and what to send when connecting to the server.

    When you know you want to restore LiveView state during the startup process, then this approach makes that easy. We took the extra steps here to make it reusable so our multiple LiveViews can save and restore their own state, or restore no state at all!

    Restoring state during page setup helps improve the user experience. State is restored faster, and we avoid the extra round-trip of requesting to restore the state.

    <%= partial "shared/posts/cta", locals: {
      title: "Fly ❤️ Elixir",
      text: "Fly is an awesome place to run your Elixir apps. It's really easy to get started. You can be running in minutes.",
      link_url: "https://fly.io/docs/elixir/",
      link_text: "Deploy your Elixir app today!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>"
    } %>
- :id: phoenix-files-tailwind-standalone
  :date: '2021-12-22'
  :category: phoenix-files
  :title: Tailwind Standalone for Phoenix
  :author: chris
  :thumbnail: tailwind-thumbnail.jpg
  :alt:
  :link: phoenix-files/tailwind-standalone
  :path: phoenix-files/2021-12-22
  :body: |2



    Tailwind v3 was [just released](https://tailwindcss.com/blog/tailwindcss-v3) with some great new additions. One such feature is a new &quot;standalone&quot; `tailwindcss` CLI that includes pre-built binaries for all major platforms. This enables all of Tailwind&#39;s great features without the dependency on `node` or `npm` being present on the user&#39;s system!

    The `phx.new` project generator will add Tailwind support in a future release, but adding Tailwind today takes just a few quick steps thanks to a [new Tailwind elixir library](https://github.com/phoenixframework/tailwind) that the Phoenix team just released. The library installs and runs the standalone client for your target system, just like the default `esbuild` support for new Phoenix projects.

    Let&#39;s see how.

    First, add the dependency to your `mix.exs` file:

    ```elixir
    defp deps do
      [
        ...,
        {:tailwind, "~> 0.1", runtime: Mix.env() == :dev}
      ]
    end
    ```

    Next, define a default profile for Tailwind to use in your `config/config.exs` file:

    ```elixir
    config :tailwind,
      version: "3.0.7",
      default: [
        args: ~w(
          --config=tailwind.config.js
          --input=css/app.css
          --output=../priv/static/assets/app.css
        ),
        cd: Path.expand("../assets", __DIR__)
      ]
    ```

    Finally, add Tailwind to your endpoint `:watchers` configuration in `config/dev.exs` so that it runs in development when you start your Phoenix server:

    ```elixir
    config :my_app, MyAppWeb.Endpoint,
      ...,
      watchers: [
        ...,
        tailwind: {Tailwind, :install_and_run, [:default, ~w(--watch)]}
      ]
    ```

    That&#39;s it! Now boot your server and watch as Tailwind is downloaded and installed automatically!

    ```cmd
    mix phx.server
    ```
    ```output
    [info] Running MyAppWeb.Endpoint with cowboy 2.9.0 at 127.0.0.1:4000 (http)
    [debug] Downloading tailwind from https://github.com/tailwindlabs/tailwindcss/releases/download/v3.0.7/tailwindcss-macos-x64
    [info] Access MyAppWeb.Endpoint at http://localhost:4000
    Rebuilding...
    Done in 482ms.
    [debug] Live reload: priv/static/assets/app.css
    ```

    Visit the Tailwind getting started guides to learn more about all that Tailwind has to offer [https://tailwindcss.com/docs/utility-first](https://tailwindcss.com/docs/utility-first)
- :id: blog-fly-io-is-hiring-support-engineers
  :date: '2021-12-20'
  :category: blog
  :title: Fly.io Is Hiring Support Engineers
  :author: kurt
  :thumbnail: jobs-cover-02-thumbnail.png
  :alt:
  :link: blog/fly-io-is-hiring-support-engineers
  :path: blog/2021-12-20
  :body: "\n\n<div class=\"lead\">Fly.io takes container images and converts them
    into fleets of Firecracker VMs running on our own hardware around the world. We
    make it easy to run applications near users, whether they’re in Singapore, Seattle,
    or São Paulo. [Try it out](https://fly.io/docs/speedrun/); if you’ve got a working
    container already, it can be running here in less than 10 minutes.</div>\n\nAt
    Fly.io, we have a small support team on the front lines helping customers get,
    and keep, their apps running happily. We’re looking for a Support Engineer (or
    two) to join us.\n\nThe principal channels for support at Fly.io are our public
    forum at [community.fly.io](https://community.fly.io), and email. The forum hosts
    an active, helpful community, and the Fly.io team (including, but not limited
    to, support engineers) is always around for backup (and sometimes head-on) assistance.
    We expect the number of customers actively using email support to increase soon.\n\nA
    lot of this job is triage and helping customers learn how to get help for themselves.
    But this isn’t a &quot;follow-the-script&quot; role. Our support engineers are
    _writing_ the script, and even triage can require creative problem solving and
    technical chops.\n\nYou’ll need to get familiar with many aspects of how the [Fly.io](https://Fly.io)
    platform works, and develop a good sense of which issues can be solved at the
    user end, and which you’ll need to recruit the help of platform engineers to solve.\n\nThis
    also involves negotiating our team dynamics, knowing when to ask for help and
    how to get help fast when it’s really needed.\n\n## About Us &amp; About the Job\n\n-
    We’re a small team, almost entirely technical.\n- Support engineers are experts
    in getting apps deployed on Fly, and they help customers figure out how to get
    past tricky spots when things don’t go as expected. \n- Product engineering also
    often directly supports customers; part of support’s role is to triage issues
    and collect enough detail to help other teams dig deeper into an extra thorny
    problem.\n- We are active in developer communities, including our own at community.fly.io.
    The support engineer plays a big role in this community by adding meaningful troubleshooting
    and diagnosis information in most posts.\n- Virtually all customer communication
    is in writing. We are a global company, but most of our communication is in English.
    Clear writing in English is essential.\n- We are remote, with team members in
    Colorado, Quebec, Chicago, London, Spain, Virginia, Brazil, Utah, and more! Most
    internal communication is also written, and often asynchronous. You’ll want to
    be comfortable with not getting an immediate response for everything, but also
    know when you need to get an immediate response for something.\n- We are an unusually
    public team; you’d want to be comfortable working in open channels rather than
    secretively over in a dark corner.\n- We’re a real company – hopefully that goes
    without saying – and this is a real, according-to-Hoyle full-time job with health
    care for US employees, flexible vacation time, hardware/phone allowances, the
    standard stuff. The compensation for this role is $120k-$165k USD plus equity.\n\n##
    What You’ll Do\n\n- Keep an eye on all the places that customers might ask for
    support - email, Slack, Community, etc. - and figure out the best way to respond.
    The response might change depending on the issue, and that’s ok!\n- Answer a pretty
    broad range of questions — sometimes the question is about deploying an app, sometimes
    a problem is because of a networking issue, and sometimes it requires some digging
    into log files to figure out where to start looking. You won’t be responsible
    for answering every problem! But it will help if you can quickly triage them,
    and can start people on the right path for getting a problem solved.\n- Triage
    issues, get help from other teams if needed, and provide useful information in
    most responses. Status updates help customers feel heard — this is good! Clear,
    actionable troubleshooting steps along with the updates are even better.\n- Monitor
    active support issues to make sure things are moving toward some kind of resolution.\n-
    Create and update documentation for questions that are answered more than a few
    times. Since we want to help customers be self-sufficient whenever we can, the
    most useful docs will not just include answers, but also troubleshooting steps
    and key data customers can collect to help them diagnose problems.\n\n## You’ll
    Be Good at This Job if You\n\n- Know your way around software development. Our
    customers are developers, so it is helpful if you have some experience building
    apps (even a simple one!), and bonus points if you are familiar with how it runs
    on Fly.io.\n- Are comfortable digging through code when the occasion calls for
    it. If you’ve built some apps of your own AND you love helping customers, this
    could be a fun role.\n- Have good instincts for balancing customer demands with
    healthy boundaries. Your favorite interactions are the ones where you taught a
    customer how to solve their own problem.\n- Have fun getting absorbed in a tricky
    new problem, but also know when to cut bait.\n- Like some structure, but are comfortable
    with not having a standard playbook for most problems. You also like putting some
    structure in place where there isn’t any, and are open to trying new things if
    something isn’t working.\n\n## You’ll Know You’re Succeeding in This Job If\n\n-
    Customers know what to expect when you respond to their support questions. They
    know what they should tell you about their problem, and they have a realistic
    idea of what to expect back from us.\n- You can triage whether an issue is one
    the customer can resolve on their own, or if the problem is on the Fly infrastructure.\n-
    When you need help from others on the team on a support issue, you’ve included
    enough information to get the problem closer to diagnosis or resolution.\n- You
    are helping to turn most support questions turn into support documentation.\n\n##
    How We Hire People\n\nOur hiring process may be a little different from what you’re
    used to. We respect career experience but we aren’t hypnotized by it, and we’re
    thrilled at the prospect of discovering new talent. So instead of resumes and
    interviews, we’re going to show you the kind of work we’re doing and then see
    if you enjoy actually doing it; “work-sample challenges”. Unlike a lot of places
    that assign “take-home problems”, our challenges are the backbone of our whole
    process; they’re not pre-screeners for an interview gauntlet. (We’re happy to
    talk, though!)\n\nFor this role, we’re going to ask you to deploy an app on Fly
    and then write about some issues that customers might run into. We’ll also ask
    you to identify some possible topics for support documentation.\n\nIf you’re interested,
    mail [jobs+supportengineer@fly.io](mailto:jobs+supportengineer@fly.io). You can
    tell us a bit about yourself, if you like. Please also include 1. your GitHub
    username (so we can create a private work sample repo for you) 2. your location
    (so we know what timezone you're in for scheduling) and 3. a sentence about your
    favorite food (so we know you're not a bot.)"
- :id: phoenix-files-liveview-active-nav
  :date: '2021-12-09'
  :category: phoenix-files
  :title: Active nav with LiveView
  :author: berenice
  :thumbnail: "../../static/images/phoenix-files-default.jpg"
  :alt:
  :link: phoenix-files/liveview-active-nav
  :path: phoenix-files/2021-12-09
  :body: "\n\nOne of the most important challenges when we are developing a new website
    is to give the user a great navigation experience, the user must know where they
    are and what navigation options they have at their disposal within the website.
    For this we use navigation components such as navbars or sidebars and we as developers
    are faced with the challenge of showing the user in each interaction with the
    website the place where they are in a clear and intuitive way.\n\n\nToday I'll
    share with you a recipe on how we can handle navigation within our Liveview applications
    and how we can offer the user a navigation bar that shows the active tab in which
    the user is located.\n\n## Defining the sidebar markup\n\nFirst we'll define the
    markup of the application sidebar within our live layout in `live.html.heex`,
    in this way it will be part of the life cycle of our LiveView components.\n\n```elixir\n<%=
    if @current_user do %>\n  <.sidebar_nav current_user={@current_user} active_tab={@active_tab}/>\n<%
    end %>\n```\n\nLet&#39;s take a closer look at the previous code; there are two
    assigns defined, `@active_tab` and `@current_user` (in a few moments I&#39;ll
    describe in detail where they were assigned) and, on the other hand, we have a
    function component called `sidebar_nav_links`, to which we are passing as parameters
    the `current_user` and `active_tab` assigns and is defined in the `LiveBeatsWeb.LayoutView`
    module as follows:\n\n\n\n```elixir\ndef sidebar_nav(assigns) do\n  ~H\"\"\"\n
    \ <nav class=\"px-3 mt-6 space-y-1\">\n    <%= if @current_user do %>\n      <.link\n
    \       navigate={profile_path(@current_user)}\n        class={\"#{if @active_tab
    == :profile, \n                    do: \"bg-gray-200\", else: \"hover:bg-gray-50\"}\"}\n
    \     >\n        <.icon name={:music_note} outlined />\n        My Songs\n      </.link>\n\n
    \     <.link\n        navigate={Routes.settings_path(Endpoint, :edit)}\n        class={\"#{if
    @active_tab == :settings, \n                    do: \"bg-gray-200\", else: \"hover:bg-gray-50\"}\"}\n
    \     >\n        <.icon name={:adjustments} outlined/>\n        Settings\n      </.link>\n
    \   <% end %>\n  </nav>\n  \"\"\"\nend\n```\n\nWe are using the `sigil_H` that
    returns a rendered structure that contains two links. The first with the text
    _My songs_ and another with the text _Settings_, which would look as follows:\n\n![](without_active_tab.png?1/3&center)\n\nIn
    addition, we have a css class in each of the links that we defined, depending
    on the content of the assign `@active_tab`, the css class `bg-gray-200` or the
    `hover:bg-gray-50` class will be applied:\n\n```elixir\n\"#{if @active_tab ==
    :profile, do: \"bg-gray-200\", else: \"hover:bg-gray-50\"}\"\n```\n\nIn this way,
    the user will be able to perceive in which part of the navigation options he is
    (in this example, within _My songs_ page):\n\n![](with_active_tab.png?1/3&center)\n\n##
    Setting the @active_tab assign\n\nSo far we have defined a list of `active_tabs`
    to which a conditional css class will be assigned in such a way that the location
    of the user within our application is perceived, however, we don&#39;t know how
    the `active_tab` and `current_user` assigns were assigned. We also need this assign
    to be available in all LiveViews since it is rendered in the live layout. Fortunately,
    LiveView lifecycle hooks make this easy.\n\nLet&#39;s take a look at the following
    code inside `router.ex`:\n\n```elixir\nlive_session :authenticated, on_mount:
    [\n  {LiveBeatsWeb.UserAuth, :ensure_authenticated}, \n  LiveBeatsWeb.Nav\n] do\n
    live \"/:profile_username/songs/new\", ProfileLive, :new\n live \"/:profile_username\",
    ProfileLive, :index\n live \"/profile/settings\", SettingsLive, :edit\nend\n```\n\n\n\nWe
    can see that a `live_session` is defined as  `:authenticated`, that session will
    include a list of routes that will go through a user validation process and will
    show content if the user is valid and authenticated in our application, otherwise
    user will be redirected to the log-in page.\n\nLater (lines 2 and 3), we attached
    a couple of hooks to the &quot;mount&quot; life cycle of each of the LiveViews
    defined in the session. The `live_session` macro allows us to define a group of
    routes with shared life-cycle hooks, and live navigate between them.\n\nThe first
    will be in charge of performing the user validation process by invoking the `on_mount`
    callback of the `LiveBeatsWeb.UserAuth` module with the parameter `:ensure_authenticated`.\n\nLets
    see the callback definition in `LiveBeatsWeb.UserAuth`:\n\n```elixir\n def on_mount(:ensure_authenticated,
    _params, session, socket) do\n    case session do\n      %{\"user_id\" => user_id}
    ->\n        new_socket = LiveView.assign_new(socket, :current_user, fn -> \n            Accounts.get_user!(user_id)
    \n          end)\n\n          {:cont, new_socket}\n\n      %{} ->\n        {:halt,
    redirect_require_login(socket)}\n    end\n  rescue\n    Ecto.NoResultsError ->
    {:halt, redirect_require_login(socket)}\n  end\n```\n\nAs we can see, in line
    4 we are assigning the value of `current_user` to the socket assigns in case of
    getting the current user of the session and get a result from the database,  otherwise,
    we redirect the user to the log-in view.\n\nThe second hook is  in charge of setting
    the assigns we need in order to show the active tab in the sidebar.\n\nLet&#39;s
    see the content of the `LiveBeatsWeb.Nav` module:\n\n```elixir\ndefmodule LiveBeatsWeb.Nav
    do\n  import Phoenix.LiveView\n\n  def on_mount(:default, _params, _session, socket)
    do\n    {:cont,\n     socket\n     |> attach_hook(:active_tab, :handle_params,
    &set_active_tab/3)}\n  end\n\n  defp set_active_tab(params, _url, socket) do\n
    \   active_tab =\n      case {socket.view, socket.assigns.live_action} do\n        {ProfileLive,
    _} ->\n          if params[\"profile_username\"] == current_user(socket) do\n
    \           :profile\n          end\n\n        {SettingsLive, _} ->\n          :settings\n\n
    \       {_, _} ->\n          nil\n      end\n\n    {:cont, assign(socket, active_tab:
    active_tab)}\n  end\n\n  defp current_user(socket) do\n    socket.assigns.current_user[:username]\n
    \ end\nend\n```\n\n\n\nFirst, in line 4 we are defining the `on_mount` callback
    that will be used as `:default` if no other option is sent in the invocation of
    the hook within `router.ex`\n\n\n\nThe most important part of this callback can
    be seen in line 7, where we call `attach_hook/4`. We named our hook `:active_tab`
    and attached it to the `handle_params` stage of the `socket` life cycle and we
    passed our `set_active_tab/3` function to be invoked for this stage.\n\nWithin
    \ `set_active_tab/3` , we implemented logic to set the `@active_tab` based on
    the params, LiveView module, and live action from the router. When viewing a user&#39;s
    profile, we only set the active tab to `:profile` if the current user is viewing
    their own profile. Likewise, we set the active tab to  `:settings` if routed to
    the `SettingsLive` LiveView. Otherwise, `@active_tab` is set to nil.\n\nNow when
    the user navigations across LiveViews, the live layout will reactively update
    as the URL changes, and our tabs will be highlighted appropriately. \n\nLet&#39;s
    see it in action!\n\n![](result.gif?1/3&center)\n"
- :id: phoenix-files-saving-and-restoring-liveview-state
  :date: '2021-12-08'
  :category: phoenix-files
  :title: Saving and Restoring LiveView State
  :author: mark
  :thumbnail: saving-restoring-liveview-thumbnail.jpg
  :alt:
  :link: phoenix-files/saving-and-restoring-liveview-state
  :path: phoenix-files/2021-12-08
  :body: |2+


    <div class="lead">Fly.io runs apps close to users, by transmuting Docker containers into micro-VMs that run on our own hardware around the world. This is a post about how you can save and restore Phoenix LiveView state to the browser. If you are interested in deploying your own Elixir project, the easiest way to learn more is to [try it out](https://fly.io/docs/speedrun/); you can be up and running in just minutes.</div>

    There are multiple ways to save and restore state for your LiveView processes. You can use an external cache like Redis, your database, or even the browser itself. Sometimes there are situations, [like I described previously](/blog/creating-the-livebook-launcher-in-liveview/), where you either _can't_ or don't _want_ to store the state on the server. In situations like that, you have the option of storing the state in the user's browser. This post explains _how_ you can use the browser to store state and how your LiveView process can get it back later. We'll go through the code so you can add something similar to your own project.

    > Saving Phoenix LiveView state to the browser is a bit like taking your brain, packaging it up, encrypting and signing it, and sending it to the browser for storage. Then asking for it back again later when you need it!

    ## What to Store

    First, we need to talk about _what_ we want to store. There may be a lot of different kinds of state being tracked in your LiveView. It may even include UI centric things like which tab is active or if a region is expanded. It doesn't make sense to store state like that. In fact, you might have some state that could cause problems if it was restored later into a new LiveView! So think about what values are actually important to save and restore.

    **Dont's:**

    - Don't store things that can easily be reconstructed from the database.
    - Don't store UI related state.
    - Don't store things you don't actually need.

    **Do's:**

    - Gather up all the data you want saved into a single data structure.
    - Use a map or struct.

    For example, in the Livebook Launcher, the data I'm saving is:

    - **Deployment Step** - Last known step in the deployment process.
    - **Version** - The version of the data structure.
    - **Deployment ID** - The ID of the deployment being tracked.
    - **App URL** - The URL the for the finished deployed app when it's done.
    - **Private Values** - Like the Livebook's password.

    <div class="callout">**TIP: Include a Version Number**

    If we ever need to change the data structure stored in the browser, then a newly deployed LiveView may restore an _old_ version that is out of date for what the app expects! This can cause nasty bugs or break the ability to restore the data at all.

    A handy tip is to include a version number in our token. Then, if we ever _need_ to create a breaking change, it gives us the option of detecting the older version and converting or migrating it to something we can still work with.
    </div>


    ## Storing it Securely

    We are talking about storing the _internal_ state of a LiveView in the user's browser. We should never blindly trust user input! So we need a way to keep our data both tamper proof and hidden as well. Fortunately, Phoenix has some built-in tools we can use for this.

    - [`Phoenix.Token.encrypt/4`](https://hexdocs.pm/phoenix/Phoenix.Token.html#encrypt/4) - Encrypt the data before sending it to the client.
    - [`Phoenix.Token.decrypt/4`](https://hexdocs.pm/phoenix/Phoenix.Token.html#decrypt/4) - Decrypt and validate the data from the client.

    The `decrypt/4` function takes the option `:max_age` which we'll use. This verifies the token only if it has been generated "max age" ago in seconds. This prevents us from having to deal with decrypting state from a month ago. Handy!

    ## Simple Hook

    In order to save and restore our LiveView state from the server to the browser and back, we need a simple [JS hook](https://hexdocs.pm/phoenix_live_view/js-interop.html#client-hooks-via-phx-hook). This gives us two-way communication between the server and browser. We will make the hook be general purpose so it could work for  multiple LiveView pages in our app if needed.

    This file knows how to read from and write to the browser's `sessionStorage`. Nothing in this file is explicit to our application or our usage, so it can be reused in multiple places if needed. Even the event name it pushes to the server is customized at the server. The `obj.event` value is provided by the server and is also handled by the server.

    Let's go over the hook now.

    File: `assets/js/hooks/local_state_store.js`.

    ```javascript
    // JS Hook for storing some state in sessionStorage in the browser.
    // The server requests stored data and clears it when requested.
    export const hooks = {
      mounted() {
        this.handleEvent("store", (obj) => this.store(obj))
        this.handleEvent("clear", (obj) => this.clear(obj))
        this.handleEvent("restore", (obj) => this.restore(obj))
      },

      store(obj) {
        sessionStorage.setItem(obj.key, obj.data)
      },

      restore(obj) {
        var data = sessionStorage.getItem(obj.key)
        this.pushEvent(obj.event, data)
      },

      clear(obj) {
        sessionStorage.removeItem(obj.key)
      }
    }
    ```

    The `mounted` function is executed in the browser when the hook is setup. Here, it registers that our hook code handles events called `store`, `clear`, and `restore`. When the server pushes one of those events, it executes our function on the Javascript object.

    The only "sending" that the hook does is in the `restore` function. When the server asks the client to restore state, the client gets any data under `key` stored in `sessionStorage`. Then it does a `pushEvent("restoreSettings", data)` back to the server with the loaded data.

    What if there is nothing to load? We'll worry about that at the server where pattern matching makes coding fun. I'm intentionally keeping this code very simple. It just tries to get whatever is asked for and push it to the server.

    If this is hard to visualize or follow, we'll cover it again from the server perspective below and it includes a chart to help piece the flow together.

    ## Our LiveView

    Most of the logic and code is in the LiveView and that's where we'll focus now.

    ### Restoring on Demand

    There are two places in your LiveView where you can request to restore the state during the startup process.

    - [`mount/3`](https://hexdocs.pm/phoenix_live_view/Phoenix.LiveView.html#c:mount/3)
    - [`handle_params/3`](https://hexdocs.pm/phoenix_live_view/Phoenix.LiveView.html#c:handle_params/3)

    The `mount` callback executes first followed by `handle_params`. They are both called in the following situations:

    - User visits the LiveView for the first time
    - User reloads the page while on the LiveView
    - Server deploys while user is on your LiveView. The process is killed at the server during the deploy.

    Actually, both callbacks fires twice when a LiveView is visited. Once to do the initial "dead" page load and then again when upgrading to the live socket.

    #### Which Should I Use?

    When the state you want to restore is not related to the URL or any query parameters, then `mount/3` is the better choice. This may be the case when the stateful data is linked to something in the session like a `user_id` or something similar.

    When the state you want to restore is connected to an ID or other data in the URL, then `handle_params/3` is the better choice. That was the case for the [Fly.io Livebook Launcher](https://fly.io/launch/livebook/). The `livebook` portion of the URL is a parameter and all the data we want to store is linked to it that. So using `handle_params/3` is the better choice in that situation.

    #### Requesting the Saved State

    For this example, I'm using `handle_params/3`. Just note that most of the code inside the function would be the same if I used the `mount/3` function instead.

    Check out this chart to help follow the flow between the server and client.

    ![Restoring state flow](live_view-state-restore-flow.png?centered)

    Our LiveView can only push events down to the browser once it's upgraded to a websocket connection. We will use the [`connected?/1`](https://hexdocs.pm/phoenix_live_view/Phoenix.LiveView.html#connected?/1) function call to know when it's ready.

    ```elixir
      @impl true
      def handle_params(params, _, socket) do
        # Only try to talk to the client when the websocket
        # is setup. Not on the initial "static" render.
        new_socket =
          if connected?(socket) do
            # This represents some meaningful key to your LiveView that you can
            # store and restore state using. Perhaps an ID from the page
            # the user is visiting?
            my_storage_key = "a-relevant-value-from-somewhere"
            # For handle_params, it could be
            # my_storage_key = params["id"]

            socket
            |> assign(:my_storage_key, my_storage_key)
            # request the browser to restore any state it has for this key.
            |> push_event("restore", %{key: my_storage_key, event: "restoreSettings"})
          else
            socket
          end

        {:noreply, new_socket}
      end
    ```

    The Elixir code fires the `restore` event in the JS hook. In the data passed to the client, we tell it which event to fire on the server. The browser client reads any state it has stored under the requested key and sends it to the server using a JS `pushEvent` function.



    ### Receiving Stored State

    When requested, the client sends whatever value it finds stored under the key to the server. If this is the first time the user has seen this page, the value found will be a javascript `null`. Rather than put logic into the javascript for how to handle that, we keep it simple and send it to the server anyway.

    We expect the `token_data` to be a string. Using pattern matching, we try to parse the token. If the value wasn't a string then it is handled by the second `handle_event("restoreSettings", ...)` function body and ignored.

    ```elixir
      # Pushed from JS hook. Server requests it to send up any
      # stored settings for the key.
      def handle_event("restoreSettings", token_data, socket) when is_binary(token_data) do
        socket =
          case restore_from_token(token_data) do
            {:ok, nil} ->
              # do nothing with the previous state
              socket

            {:ok, restored} ->
              socket
              |> assign(:state, restored)

            {:error, reason} ->
              # We don't continue checking. Display error.
              # Clear the token so it doesn't keep showing an error.
              socket
              |> put_flash(:error, reason)
              |> clear_browser_storage()
          end

        {:noreply, socket}
      end

      def handle_event("restoreSettings", _token_data, socket) do
        # No expected token data received from the client
        Logger.debug("No LiveView SessionStorage state to restore")
        {:noreply, socket}
      end
    ```

    When the server receives the encrypted token, it parses and validates it. Then we set the state using `assign(socket, :state, restored)`. Now our LiveView has restored the previously saved state!

    The function `restore_from_token` is pretty simple and is included at the end.

    ### Storing the State

    As the user interacts with the LiveView, we may reach a point when we want to save some important data to the browser. Usually some event triggers when this happens. It could be a "Start Game", "Save" or "Deploy" button click. In this example, I'll use a `handle_event` that is stripped down to only include parts needed for saving.

    ```elixir
      def handle_event("something_happened_and_i_want_to_store", params, socket) do
        # This represents the special state you want to store. It may come from the
        # socket.assigns. It's specific to your LiveView.
        state_to_store = socket.assigns.state

        socket =
          socket
          |> push_event("store", %{
            key: socket.assigns.my_storage_key,
            data: serialize_to_token(state_to_store)
          })

        {:noreply, socket}
      end
    ```
    It executes `push_event("store", ...)` which sends the data to the JS hook running in the browser. The browser writes the encrypted string to the SessionStorage.

    The function `serialize_to_token` is pretty simple and is included at the end.

    ## Other Files

    For completeness, there are a few other files needed to make this all work.

    We need to include and load the hook file in the project's `app.js` file. There are multiple ways to do this, here's an example:

    File: `assets/js/app.js`.

    ```javascript
    // ...
    import * as LocalStateStore from "./hooks/local_state_store"

    let Hooks = {}

    Hooks.LocalStateStore = LocalStateStore.hooks
    // ...
    ```

    In the LiveView template we need to link our JS hook to some part of the DOM. Since this hook doesn't do anything visible, we can hook it to the page's container. In my case, it's a `main` tag. Adapt this to fit your page. The important part is `phx-hook="LocalStateStore"`.

    ```html
    <main class="..." id="..." phx-hook="LocalStateStore">
      <!-- other page content -->
    </main>
    ```

    With a JS hook defined, included in `app.js` and linked to our LiveView's template, we are ready to go!

    ## Closing

    You now have all the tools needed to securely store and restore important LiveView state using the user's browser to keep it!

    The questions for you to consider:

    - Should you use `mount` or `handle_params`?
    - What **key** should the values be stored under?
    - **What** values are actually important to store and restore?
    - **How** should you structure the data? A map? A struct?
    - Does SessionStorage or LocalStorage make more sense?

    <div class="callout">**localStorage vs sessionStorage**

    In this example I chose to use [sessionStorage](https://developer.mozilla.org/en-US/docs/Web/API/Window/sessionStorage) for storing data in the browser. Another option is [localStorage](https://developer.mozilla.org/en-US/docs/Web/API/Window/localStorage). They both store the same data using the same API. So it's trivial to switch from one to the other.

      > `sessionStorage` is similar to `localStorage`; the difference is that while data in `localStorage` doesn't expire, data in `sessionStorage` is cleared when the _page session_ ends. ~[MDN Docs](https://developer.mozilla.org/en-US/docs/Web/API/Window/sessionStorage)

    For my purposes, using `sessionStorage` automatically removes any sensitive data when the user closes the tab. Choose the option that works best for your situation.
    </div>

    The bodies of `serialize_to_token`, `restore_from_token` and `clear_browser_storage` are include in the full code sample below.

    ### Combined LiveView Code

    ```elixir
    defmodule MyAppWeb.MyView do
      use MyAppWeb, :live_view
      alias Fly.DeployStateSerialize

      require Logger

      @impl true
      def handle_params(params, _, socket) do
        # Only try to talk to the client when the websocket
        # is setup. Not on the initial "static" render.
        new_socket =
          if connected?(socket) do
            # This represents some meaningful key to your LiveView that you can
            # store and restore state using. Perhaps an ID from the page
            # the user is visiting?
            my_storage_key = "a-relevant-value-from-somewhere"
            # For handle_params, it could be
            # my_storage_key = params["id"]

            socket
            |> assign(:my_storage_key, my_storage_key)
            # request the browser to restore any state it has for this key.
            |> push_event("restore", %{key: my_storage_key, event: "restoreSettings"})
          else
            socket
          end

        {:noreply, new_socket}
      end

      defp restore_from_token(nil), do: {:ok, nil}

      defp restore_from_token(token) do
        salt = Application.get_env(:my_app, MyAppWeb.Endpoint)[:live_view][:signing_salt]
        # Max age is 1 day. 86,400 seconds
        case Phoenix.Token.decrypt(MyAppWeb.Endpoint, salt, token, max_age: 86_400) do
          {:ok, data} ->
            {:ok, data}

          {:error, reason} ->
            # handles `:invalid`, `:expired` and possibly other things?
            {:error, "Failed to restore previous state. Reason: #{inspect(reason)}."}
        end
      end

      defp serialize_to_token(state_data) do
        salt = Application.get_env(:my_app, MyAppWeb.Endpoint)[:live_view][:signing_salt]
        Phoenix.Token.encrypt(MyAppWeb.Endpoint, salt, state_data)
      end

      # Push a websocket event down to the browser's JS hook.
      # Clear any settings for the current my_storage_key.
      defp clear_browser_storage(socket) do
        push_event(socket, "clear", %{key: socket.assigns.my_storage_key})
      end

      @impl true
      # Pushed from JS hook. Server requests it to send up any
      # stored settings for the key.
      def handle_event("restoreSettings", token_data, socket) when is_binary(token_data) do
        socket =
          case restore_from_token(token_data) do
            {:ok, nil} ->
              # do nothing with the previous state
              socket

            {:ok, restored} ->
              socket
              |> assign(:state, restored)

            {:error, reason} ->
              # We don't continue checking. Display error.
              # Clear the token so it doesn't keep showing an error.
              socket
              |> put_flash(:error, reason)
              |> clear_browser_storage()
          end

        {:noreply, socket}
      end

      def handle_event("restoreSettings", _token_data, socket) do
        # No expected token data received from the client
        Logger.debug("No LiveView SessionStorage state to restore")
        {:noreply, socket}
      end

      def handle_event("something_happened_and_i_want_to_store", params, socket) do
        # This represents the special state you want to store. It may come from the
        # socket.assigns. It's specific to your LiveView.
        state_to_store = socket.assigns.state

        socket =
          socket
          |> push_event("store", %{
            key: socket.assigns.my_storage_key,
            data: serialize_to_token(state_to_store)
          })

        {:noreply, socket}
      end
    end
    ```

- :id: blog-creating-the-livebook-launcher-in-liveview
  :date: '2021-11-18'
  :category: blog
  :title: Launching Livebook using LiveView
  :author: mark
  :thumbnail: livebook-launcher-deploying.png
  :alt:
  :link: blog/creating-the-livebook-launcher-in-liveview
  :path: blog/2021-11-18
  :body: |2


    <div class="lead">Fly.io runs apps close to users, by transmuting Docker containers into micro-VMs that run on our own hardware around the world. This is a post about writing our LiveView launcher that launches a Livebook instance for you on Fly.io. If you're more interested in deploying your own project, the easiest way to learn more is to [try it out](https://fly.io/docs/speedrun/); you can be up and running in just a couple minutes.</div>

    The [Livebook launcher](https://fly.io/launch/livebook/) was super fun to create. Everything happens in a single LiveView process. There was a problem though… it wasn't great if we were in the middle of deploying the app for the user and one these things happened:

    - User hits "refresh" on their browser
    - Server is deployed or restarted

    ## The Problem

    The problem was there are generated secrets like  `liveview_password`. Once we send that off to have our Livebook deployed, we could never get those values back! They are stored in [Vault](https://www.vaultproject.io/docs/secrets) and even we are protected from being able to access secrets stored there. After all, in cases like this, they aren't our secrets!

    This presented a UX problem. We had private data that was only temporarily stored in a LiveView process. If anything went wrong with the user's connection or the server was restarted, the UI would lose the secrets forever! Part of the UI design is to show all the information to the user at the end of the process.

    ![Livebook launcher deployment summary](livebook-launcher-deployed.png?centered&card)

    How could we do that in a situation like this? I considered using a database or a Redis server to store that state. I didn't like it though because we're trying _not_ to keep that data around on our servers.

    ## The Solution

    The solution was to encrypt the private data we want to keep using [`Phoenix.Token.encrypt/4`](https://hexdocs.pm/phoenix/Phoenix.Token.html#encrypt/4). This not only signs the data to prevent tampering but it also encrypts the contents keeping them private. Then we could store that encrypted text blob in the browser's [SessionStorage](https://developer.mozilla.org/en-US/docs/Web/API/Window/sessionStorage). That way is stays under the users's control. This also means they are available to be pushed back up to the server!

    LiveView has some powerful features. One is called [hooks](https://hexdocs.pm/phoenix_live_view/js-interop.html#client-hooks) and they let the server and client communicate in both directions. I defined some simple hooks in a Javascript file like this.

    ```javascript
    export const hooks = {
      mounted() {
        this.handleEvent("store", (obj) => this.store(obj))
        this.handleEvent("clear", (obj) => this.clear(obj))
        this.handleEvent("restore", (obj) => this.restore(obj))
      },

      store(obj) {
        sessionStorage.setItem(obj.key, obj.data)
      },

      restore(obj) {
        var data = sessionStorage.getItem(obj.key)
        this.pushEvent("restoreSettings", data)
      },

      clear(obj) {
        sessionStorage.removeItem(obj.key)
      }
    }
    ```

    If a user reloads the page, the server sends a [push_event](https://hexdocs.pm/phoenix_live_view/Phoenix.LiveView.html#push_event/3) command over the websocket to the browser asking it to `"restore"`  any settings it had under the "livebook" key in the SessionStorage.  If the client has something to send, the server gets back the data, decrypts it using [`Phoenix.Token.decrypt/4`](https://hexdocs.pm/phoenix/Phoenix.Token.html#decrypt/4), and resumes tracking the user's deployment progress.

    Likewise, if the server is restarted when we deploy a new version, the client provides the private data and after a little UI catch up, it continues tracking the progress of the deploy.

    That's pretty much all the javascript I had to write do deliver this feature!

    After the deployment succeeds or fails, we `"clear"` the cached data from the browser's SessionStorage just to keep things tidy. Also, when the user closes their browser tab everything in their SessionStorage is cleared out as well.

    Hope that sheds some light on what's happening and gives you some ideas for solving similar problems!

    <%= partial "shared/posts/cta", locals: {
      title: "Launch your Livebook now",
      text: "Deploy your Livebook instance now. Hit refresh during the deployment process and see it recover.",
      link_url: "https://fly.io/launch/livebook/",
      link_text: "Launch yours now&nbsp;&nbsp;<span class='opacity-50'>&rarr;</span>"
    } %>
- :id: phoenix-files-backfilling-data
  :date: '2021-11-15'
  :category: phoenix-files
  :title: Backfilling Data
  :author: bernheisel
  :thumbnail: backfilling-data-thumbnail.jpg
  :alt:
  :link: phoenix-files/backfilling-data
  :path: phoenix-files/2021-11-15
  :body: "\n\n<div class=\"lead\">Fly.io runs apps close to users, by transmuting
    Docker containers into micro-VMs that run on our own hardware around the world.
    This post is part of the [Safe Ecto Migrations](/phoenix-files/safe-ecto-migrations)
    series guide. If you just want to ship your Phoenix app, the easiest way to learn
    more is to [try it out](https://fly.io/docs/speedrun/); you can be up and running
    in just a couple minutes.</div>\n\nThis is part 4 in a 4-part series on designing
    and running [Safe Ecto Migrations](/phoenix-files/safe-ecto-migrations):\n\n-
    [Part 1 - Anatomy of an Ecto migration](/phoenix-files/anatomy-of-an-ecto-migration)\n-
    [Part 2 - How to migrate Mix Release projects](/phoenix-files/how-to-migrate-mix-release-projects) \n-
    [Part 3 - Migration Recipes](/phoenix-files/migration-recipes)\n- **Part 4 - Backfilling
    Data (you are here)**\n\nWhen I say &quot;backfilling data&quot;, I mean that
    as any attempt to change data in bulk. This can happen in code through migrations,
    application code, UIs that allow multiple selections and updates, or in a console
    connected to a running application. Since bulk changes affect _a lot_ of data,
    it’s always a good idea to have the code reviewed before it runs. You also want
    to check that it runs efficiently and does not overwhelm the database. Ideally,
    it's nice when the code is written to be safe to re-run. For these reasons, please
    don’t change data in bulk through a console!\n\nWe’re going to focus on bulk changes
    executed though Ecto migrations, but the same principles are applicable to any
    case where bulk changes are being made. Typical scenarios where you might need
    to run data migrations is when you need to fill in data for records that already
    exist (hence, backfilling data). This usually comes up when table structures are
    changed in the database.\n\nSome examples of backfilling:\n\n- Populating data
    into a new column\n- Changing a column to make it required. May require changing
    existing rows to set  a value.\n- Splitting one database table into several\n-
    Fixing bad data\n\nFor simplicity, we are using  `Ecto.Migrator`  to run our data
    migrations, but it's important to not let these migrations break developers' environments
    over time (more on this below). If using migrations to change data is a normal
    process that happens regularly, then you may consider exploring a migration system
    outside of `Ecto.Migrator` that is observable, hooks into error reporting, metrics,
    and allows for dry runs. This guide is intended as a  starting point, and since
    Ecto ships with a great migration runner, we'll leverage it to also run the data
    migrations.\n\nThere are both bad and good ways to write these data migrations.
    Let explore some:\n\n## Bad <span aria-hidden class=\"text-sm align-text-bottom
    ml-1\">❌</span>\n\nIn the following example, a migration references the schema
    `MyApp.MySchema`.\n\n```elixir\ndefmodule MyApp.Repo.DataMigrations.BackfillPosts
    do\n  use Ecto.Migration\n  import Ecto.Query\n\n  def change do\n    alter table(\"posts\")
    do\n      add :new_data, :text\n    end\n\n    flush()\n\n    MyApp.MySchema\n 
      |> where(new_data: nil)\n    |> MyApp.Repo.update_all(set: [new_data: \"some
    data\"])\n  end\nend\n```\n\nThe problem is  the code and schema may change over
    time. However, migrations are using a snapshot of your schemas at the time it's
    written. In the future, many assumptions may no longer be true. For example, the
    `new_data` column may not be present anymore in the schema causing the query to
    fail if this migration is run months later.\n\nAdditionally, in your development
    environment, you might have 10 records to migrate; in staging, you might have
    100; in production, you might have 1 billion to migrate. Scaling your approach
    matters.\n\nUltimately, there are several bad practices here:\n\n1. The Ecto schema
    in the query may change after this migration was written.\n1. If you try to backfill
    the data all at once, it may exhaust the database memory and/or CPU if it's changing
    a large data set.\n1. Backfilling data inside a transaction for the migration
    locks row updates for the duration of the migration, even if you are updating
    in batches.\n1. Disabling the transaction for the migration and only batching
    updates may still spike the database CPU to 100%, causing other concurrent reads
    or writes to time out.\n\n## Good <span aria-hidden class=\"text-base align-text-bottom
    ml-1\">✅</span>\n\nThere are four keys to backfilling safely:\n\n1. running outside
    a transaction\n1. batching\n1. throttling\n1. resiliency\n\nAs we've learned in
    this guide, it's straight-forward to disable the migration transactions. Add these
    options to the migration:\n\n```elixir\n@disable_ddl_transaction true\n@disable_migration_lock
    true\n```\n\nBatching our data migrations still has several challenges:\n\n- LIMIT/OFFSET by
    itself is an expensive query for large tables, so we must find another way to
    paginate.\n- Since we cannot use a database transaction, this also implies we
    cannot leverage cursors since they require a transaction.\n- This leaves us with
    [keyset pagination](https://www.citusdata.com/blog/2016/03/30/five-ways-to-paginate/)\n\nFor
    querying and updating the data, there are two ways to &quot;snapshot&quot; your
    schema at the time of the migration. We'll use both options below in the examples:\n\n1.
    Execute raw SQL that represents the table at that moment. Do not use Ecto schemas.
    Prefer this approach when you can.\n1. Write a small Ecto schema module inside
    the migration that only uses what you need. Then use that in your data migration.
    This is helpful if you prefer the Ecto API.\n\nFor throttling, we can simply add
    a `Process.sleep(@throttle)` for each page.\n\nFor resiliency, we need to ensure
    that we handle errors without losing our progress. You don't want to migrate the
    same data twice! Most data migrations I have run find some records in a state
    that I wasn't expecting. This causes the data migration to fail. When the data
    migration stops, that means I have to write a little bit more code to handle that
    scenario, and re-run the migration. Every time the data migration is re-run, it
    should pick up where it left off without revisiting already-migrated records.\n\nFinally,
    to manage  these data migrations separately, see the section titled &quot;Release
    Module&quot;. Put simply:\n\n1. Store data migrations separately from your schema
    migrations.\n1. Run the data migrations manually.\n\n## Batching Deterministic
    Data\n\nIf the data can be queried with a condition that is removed after update
    then you can repeatedly query the data and update the data until the query result
    is empty. For example, if a column is currently null and will be updated to not
    be null, then you can query for the null records and pick up where you left off.\n\nHere's
    how we can manage the backfill:\n\n1. Disable migration transactions.\n1. Use
    keyset pagination: Order the data, find rows greater than the last mutated row
    and limit by batch size.\n1. For each page, mutate the records.\n1. Check for
    failed updates and handle it appropriately.\n1. Use the last mutated record's
    ID as the starting point for the next page. This helps with resiliency and prevents
    looping on the same record over and over again.\n1. Arbitrarily sleep to throttle
    and prevent exhausting the database.\n1. Rinse and repeat until there are no more
    records\n\nFor example:\n\n```cmd\nmix ecto.gen.migration --migrations-path=priv/repo/data_migrations
    backfill_posts\n```\n\nAnd modify the migration:\n\n```elixir\ndefmodule MyApp.Repo.DataMigrations.BackfillPosts
    do\n  use Ecto.Migration\n  import Ecto.Query\n\n  @disable_ddl_transaction true\n 
    @disable_migration_lock true\n  @batch_size 1000\n  @throttle_ms 100\n\n  def
    up do\n    throttle_change_in_batches(&page_query/1, &do_change/1)\n  end\n\n 
    def down, do: :ok\n\n  def do_change(batch_of_ids) do\n    {_updated, results}
    = repo().update_all(\n      from(r in \"weather\", select: r.id, where: r.id in
    ^batch_of_ids),\n      [set: [approved: true]],\n      log: :info\n    )\n   
    not_updated = MapSet.difference(MapSet.new(batch_of_ids), MapSet.new(results))
    |> MapSet.to_list()\n    Enum.each(not_updated, &handle_non_update/1)\n    results\n 
    end\n\n  def page_query(last_id) do\n    # Notice how we do not use Ecto schemas
    here.\n    from(\n      r in \"weather\",\n      select: r.id,\n      where: is_nil(r.approved)
    and r.id > ^last_id,\n      order_by: [asc: r.id],\n      limit: @batch_size\n 
      )\n  end\n\n  # If you have BigInt or Int IDs, fallback last_pos = 0\n  # If
    you have UUID IDs, fallback last_pos = \"00000000-0000-0000-0000-000000000000\"\n 
    defp throttle_change_in_batches(query_fun, change_fun, last_pos \\\\ 0)\n  defp
    throttle_change_in_batches(_query_fun, _change_fun, nil), do: :ok\n  defp throttle_change_in_batches(query_fun,
    change_fun, last_pos) do\n    case repo().all(query_fun.(last_pos), [log: :info,
    timeout: :infinity]) do\n      [] ->\n        :ok\n\n      ids ->\n        results
    = change_fun.(List.flatten(ids))\n        next_page = results |> Enum.reverse()
    |> List.first()\n        Process.sleep(@throttle_ms)\n        throttle_change_in_batches(query_fun,
    change_fun, next_page)\n    end\n  end\n\n  defp handle_non_update(id) do\n   
    raise \"#{inspect(id)} was not updated\"\n  end\nend\n\n```\n\n## Batching Arbitrary
    Data\n\nIf the data being updated does not indicate it's already been updated,
    then we need to take a snapshot of the current data and store it temporarily.
    For example, if all rows should increment a column's value by 10, how would you
    know if a record was already updated? You could load a list of IDs into the application
    during the migration, but what if the process crashes? Instead we're going to
    keep the data we need in the database.\n\nTo do this, it works well if we can
    pick a specific point in time where all records _after_ that point in time do
    not need adjustment. This happens when you realize a bug was creating bad data
    and after the bug was fixed and deployed, all new entries are good and should
    not be touched as we clean up the bad data. For this example, we'll use `inserted_at`
    as our marker. Let's say that the bug was fixed on a midnight deploy on 2021-08-22.\n\nHere's
    how we'll manage the backfill:\n\n1. Create a &quot;temporary&quot; table. In
    this example, we're creating a real table that we'll drop at the end of the data
    migration. In Postgres, there are [actual temporary tables](https://www.postgresql.org/docs/12/sql-createtable.html) that
    are discarded after the session is over; we're not using those because we need
    resiliency in case the data migration encounters an error. The error would cause
    the session to be over, and therefore the temporary table tracking progress would
    be lost \U0001F641. Real tables don't have this problem. Likewise, we don't want
    to store IDs in application memory during the migration for the same reason.\n1.
    Populate that temporary table with IDs of records that need to update. This query
    only requires a read of the current records, so there are no consequential locks
    occurring when populating, but be aware this could be a lengthy query. Populating
    this table can occur at creation or afterwards; in this example we'll populate
    it at table creation.\n1. Ensure there's an index on the temporary table so it's
    fast to delete IDs from it. I use an index instead of a primary key because it's
    easier to re-run the migration in case there's an error. There isn't a straight-forward
    way to `CREATE IF NOT EXIST` on a primary key; but you can do that easily with
    an index.\n1. Use keyset pagination to pull batches of IDs from the temporary
    table. Do this inside a database transaction and lock records for updates. Each
    batch should read and update within milliseconds, so this should have little impact
    on concurrent reads and writes.\n1. For each batch of records, determine the data
    changes that need to happen. This can happen for each record.\n1. [Upsert](https://wiki.postgresql.org/wiki/UPSERT)
    those changes to the real table. This insert will include the ID of the record
    that already exists and a list of attributes to change for that record. Since
    these insertions will conflict with existing records, we'll instruct Postgres
    to replace certain fields on conflicts.\n1. Delete those IDs from the temporary
    table since they're updated on the real table. Close the database transaction
    for that batch.\n1. Throttle so we don't overwhelm the database, and also give
    opportunity to other concurrent processes to work.\n1. Rinse and repeat until
    the temporary table is empty.\n1. Finally, drop the temporary table  when empty.\n\nLet's
    see how this can work:\n\n```cmd\nmix ecto.gen.migration --migrations-path=priv/repo/data_migrations
    backfill_weather\n```\n\nModify the migration:\n\n```elixir\n# Both of these modules
    are in the same migration file\n# In this example, we'll define a new Ecto Schema
    that is a snapshot\n# of the current underlying table and no more.\ndefmodule
    MyApp.Repo.DataMigrations.BackfillWeather.MigratingSchema do\n  use Ecto.Schema\n\n 
    # Copy of the schema at the time of migration\n  schema \"weather\" do\n    field
    :temp_lo, :integer\n    field :temp_hi, :integer\n    field :prcp, :float\n   
    field :city, :string\n\n    timestamps(type: :naive_datetime_usec)\n  end\nend\n\ndefmodule
    MyApp.Repo.DataMigrations.BackfillWeather do\n  use Ecto.Migration\n  import Ecto.Query\n 
    alias MyApp.Repo.DataMigrations.BackfillWeather.MigratingSchema\n\n  @disable_ddl_transaction
    true\n  @disable_migration_lock true\n  @temp_table_name \"records_to_update\"\n 
    @batch_size 1000\n  @throttle_ms 100\n\n  def up do\n    repo().query!(\"\"\"\n 
      CREATE TABLE IF NOT EXISTS \"#{@temp_table_name}\" AS\n    SELECT id FROM weather
    WHERE inserted_at < '2021-08-21T00:00:00'\n    \"\"\", [], log: :info, timeout:
    :infinity)\n    flush()\n\n    create_if_not_exists index(@temp_table_name, [:id])\n 
      flush()\n\n    throttle_change_in_batches(&page_query/1, &do_change/1)\n\n    #
    You may want to check if it's empty before dropping it.\n    # Since we're raising
    an exception on non-updates\n    # we don't have to do that in this example.\n 
      drop table(@temp_table_name)\n  end\n\n  def down, do: :ok\n\n  def do_change(batch_of_ids)
    do\n    # Wrap in a transaction to momentarily lock records during read/update\n 
      repo().transaction(fn ->\n      mutations =\n        from(\n          r in MigratingSchema,\n 
            where: r.id in ^batch_of_ids,\n          lock: \"FOR UPDATE\"\n       
    )\n        |> repo().all()\n        |> Enum.reduce([], &mutation/2)\n\n      #
    Don't be fooled by the name `insert_all`, this is actually an upsert\n      #
    that will update existing records when conflicting; they should all\n      # conflict
    since the ID is included in the update.\n\n      {_updated, results} = repo().insert_all(\n 
          MigratingSchema,\n        mutations,\n        returning: [:id],\n       
    # Alternatively, {:replace_all_except, [:id, :inserted_at]}\n        on_conflict:
    {:replace, [:temp_lo, :updated_at]},\n        conflict_target: [:id],\n       
    placeholders: %{now: NaiveDateTime.utc_now()},\n        log: :info\n      )\n 
        results = Enum.map(results, & &1.id)\n\n      not_updated =\n        mutations\n 
          |> Enum.map(& &1[:id])\n        |> MapSet.new()\n        |> MapSet.difference(MapSet.new(results))\n 
          |> MapSet.to_list()\n\n      Enum.each(not_updated, &handle_non_update/1)\n 
        repo().delete_all(from(r in @temp_table_name, where: r.id in ^results))\n\n 
        results\n    end)\n  end\n\n  def mutation(record, mutations_acc) do\n   
    # This logic can be whatever you need; we'll just do something simple\n    # here
    to illustrate\n    if record.temp_hi > 1 do\n      # No updated needed\n     
    mutations_acc\n    else\n      # Upserts don't update autogenerated fields like
    timestamps, so be sure\n      # to update them yourself. The inserted_at value
    should never be used\n      # since all these records are already inserted, and
    we won't replace\n      # this field on conflicts; we just need it to satisfy
    table constraints.\n      [%{\n        id: record.id,\n        temp_lo: record.temp_hi
    - 10,\n        inserted_at: {:placeholder, :now},\n        updated_at: {:placeholder,
    :now}\n      } | mutations_acc]\n    end\n  end\n\n  def page_query(last_id) do\n 
      from(\n      r in @temp_table_name,\n      select: r.id,\n      where: r.id
    > ^last_id,\n      order_by: [asc: r.id],\n      limit: @batch_size\n    )\n 
    end\n\n  defp handle_non_update(id) do\n    raise \"#{inspect(id)} was not updated\"\n 
    end\n\n  # If you have BigInt IDs, fallback last_pod = 0\n  # If you have UUID
    IDs, fallback last_pos = \"00000000-0000-0000-0000-000000000000\"\n  # If you
    have Int IDs, you should consider updating it to BigInt or UUID :)\n  defp throttle_change_in_batches(query_fun,
    change_fun, last_pos \\\\ 0)\n  defp throttle_change_in_batches(_query_fun, _change_fun,
    nil), do: :ok\n  defp throttle_change_in_batches(query_fun, change_fun, last_pos)
    do\n    case repo().all(query_fun.(last_pos), [log: :info, timeout: :infinity])
    do\n      [] ->\n        :ok\n\n      ids ->\n        case change_fun.(List.flatten(ids))
    do\n          {:ok, results} ->\n            next_page = results |> Enum.reverse()
    |> List.first()\n            Process.sleep(@throttle_ms)\n            throttle_change_in_batches(query_fun,
    change_fun, next_page)\n          error ->\n            raise error\n        end\n 
      end\n  end\nend\n```\n\n## We're Done!\n\nThese data migrations will likely
    look different for every project and employer. Above are lessons I have learned
    over the years, and I sincerely hope you picked up some tips from this guide and
    learn from my hard-learned lessons. Almost certainly, you will need to adapt these
    example for your own projects.\n\nIf you have tips to offer, let me know at [twitter.com/bernheisel](https://twitter.com/bernheisel)
    and contribute to the [Safe Ecto Migrations GitHub repository](https://github.com/fly-apps/safe-ecto-migrations).
    Together, as the Elixir community, we'll make migrations safer, easier, and enjoyable.\n\nSpecial
    thanks for these reviewers:\n\n- Steve Bussey\n- Stephane Robino\n- Dennis Beatty\n-
    Wojtek Mach\n\nSpecial thanks for editing:\n\n- Mark Ericksen\n\n<%= partial \"shared/posts/cta\",
    locals: {\n  title: \"Fly ❤️ Elixir\",\n  text: \"Fly is an awesome place to run
    your Elixir apps. Deploying, clustering, connecting Observer, and more are all
    supported!\",\n  link_url: \"https://fly.io/docs/elixir/\",\n  link_text: \"Deploy
    your Elixir app today!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>\"\n}
    %>\n"
- :id: phoenix-files-how-to-migrate-mix-release-projects
  :date: '2021-11-15'
  :category: phoenix-files
  :title: How to migrate Mix Release projects
  :author: bernheisel
  :thumbnail: how-to-migrate-mix-release-projects-thumbnail.jpg
  :alt:
  :link: phoenix-files/how-to-migrate-mix-release-projects
  :path: phoenix-files/2021-11-15
  :body: "\n\n<div class=\"lead\">Fly.io runs apps close to users, by transmuting
    Docker containers into micro-VMs that run on our own hardware around the world.
    This post is part of the [Safe Ecto Migrations](/phoenix-files/safe-ecto-migrations)
    series guide. If you just want to ship your Phoenix app, the easiest way to learn
    more is to [try it out](https://fly.io/docs/speedrun/); you can be up and running
    in just a couple minutes.</div>\n\nThis is part 2 in a 4-part series on designing
    and running [Safe Ecto Migrations](/phoenix-files/safe-ecto-migrations):\n\n-
    [Part 1 - Anatomy of an Ecto migration](/phoenix-files/anatomy-of-an-ecto-migration)\n-
    **Part 2 - How to migrate Mix Release projects (you are here)**\n- [Part 3 - Migration
    Recipes](/phoenix-files/migration-recipes)\n- [Part 4 - Backfilling Data](/phoenix-files/backfilling-data)\n\nNot
    long ago, deploying and managing Elixir projects was not as straight-forward as
    \ today; some might say it was downright painful. Thankfully, since Elixir 1.9,
    Mix ships with tools to help developers assemble applications for deployment.
    How you ship that binary to its destination it still entirely up to you, but now
    it's a simpler and common task!\n\n<%= partial \"shared/posts/cta\", locals: {\n
    \ title: \"Fly Changes the Deployment Game\",\n  text: \"Fly is an awesome place
    to run your Elixir apps. Deploying, clustering, connecting Observer, and more
    are supported and even fun!\",\n  link_url: \"https://fly.io/docs/elixir/\",\n
    \ link_text: \"Deploy your Elixir app today!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>\"\n}
    %>\n\nBefore the adoption of pre-compiled releases (thanks to [Mix Release](https://hexdocs.pm/mix/1.9.0/Mix.Release.html) and
    before Mix was [Distillery](https://hexdocs.pm/distillery)), it was more common
    to install Elixir (and therefore mix) directly on the server. This meant you would
    copy your code and use mix to start your application directly on the servers.
    A significant downside of doing this was the long start-up time for an application.
    It required it to be compiled before starting and during that time the server
    was reporting &quot;unhealthy&quot; because the application wasn't started. Pre-compiled
    releases solve this problem and [include other benefits](https://hexdocs.pm/mix/1.12/Mix.Tasks.Release.html#module-why-releases).\n\nSince
    Mix is a development tool, it isn't included in a runtime release. This creates
    a challenge because another common Mix operation are creating and migrating databases.
    Locally for development and testing, you can run `mix ecto.migrate && mix phx.server` and
    you're done!  With the mix command missing on the server, developers need another
    way to manage the application's database in production.\n\nOn our servers, we
    need to easily perform the following tasks:\n\n1. Check the status of migrations.\n1.
    Migrate Repos up to X migration. Default to the latest migration.\n1. Rollback
    to X migration for a specific Repo.\n\nThe recommended way to encapsulate these
    commands is with a `MyApp.Release` module. This module serves as an entry point
    into your application for managing release-related tasks.\n\nLet's create that
    module.\n\n## Release Module\n\n- [Phoenix has examples](https://hexdocs.pm/phoenix/releases.html#ecto-migrations-and-custom-commands)\n-
    [Ecto  SQL has examples](https://hexdocs.pm/ecto_sql/Ecto.Migrator.html#module-example-running-migrations-in-a-release)\n\nHere
    is the Ecto  SQL example:\n\n```elixir\ndefmodule MyApp.Release do\n  @app :my_app\n\n 
    def migrate do\n    for repo <- repos() do\n      {:ok, _, _} = Ecto.Migrator.with_repo(repo,
    &Ecto.Migrator.run(&1, :up, all: true))\n    end\n  end\n\n  def rollback(repo,
    version) do\n    {:ok, _, _} = Ecto.Migrator.with_repo(repo, &Ecto.Migrator.run(&1,
    :down, to: version))\n  end\n\n  defp repos do\n    Application.load(@app)\n 
      Application.fetch_env!(@app, :ecto_repos)\n  end\nend\n```\n\nMost of the work
    \ happens in `Ecto.Migrator`, which is great because it keeps our code slim and
    neat. But, we need to add a little bit to it:\n\n- There isn't a function that
    prints out the migrations' status. This is helpful for a sanity check. You might
    want to know which migration will execute next when the migrations are run.\n-
    In most cases you want to be deploying frequently enough that only migration is
    run at a time. In some cases, a lot of work might be going out in a single deployment
    and it may contain multiple migrations. When deploying, we may only want to execute
    one migration at a time so they can be monitored individually. As it is written,
    the function does not allow us to only run one migration.\n- You may want to run
    manual data migrations.\n\n**Adding options to** `MyApp.Release.migrate/1`\n\nLet's
    adjust the migrate function to accept options that we can pass into `Ecto.Migrator`.\n\n```diff\n+
     @doc \"\"\"\n+  Migrate the database. Defaults to migrating to the latest, `[all:
    true]`\n+  Also accepts `[step: 1]`, or `[to: 20200118045751]`\n+  \"\"\"\n-  def
    migrate do\n+  def migrate(opts \\\\ [all: true]) do\n    for repo <- repos()
    do\n-     {:ok, _, _} = Ecto.Migrator.with_repo(repo, &Ecto.Migrator.run(&1, :up,
    all: true))\n+     {:ok, _, _} = Ecto.Migrator.with_repo(repo, &Ecto.Migrator.run(&1,
    :up, opts))\n    end\n  end\n```\n\nNow we can pass in options to allow us step
    through migrations one at a time or to go up to a specific version. For example,
    `migrate(step: 1)` or `migrate(to: 20210719021232)`.\n\nWhen rolling back, it's
    already a situation where something has gone wrong and you want to reverse some
    changes. If your application has multiple Ecto repos, running `mix ecto.rollback`
    will rollback the last migration on _each_ database. That's probably _not_ what
    you want! Because of this, you want to be more explicit with this command. The
    engineers deploying this change should be required to give the specific repo and
    version to which to rollback.\n\n[See available options](https://hexdocs.pm/ecto_sql/Ecto.Migrator.html#run/4)\n\n**Adding**
    `MyApp.Release.migration_status/0`\n\nBefore I run migrations, I like to make
    sure I know _which_ migration the application is going to run next. Locally, you
    can run `mix ecto.migrations` to see the status of your migrations. I want something
    like that on the server that works with releases.\n\nLet's update it for Mix Releases:\n\n```elixir\n@doc
    \"\"\"\nPrint the migration status for configured Repos' migrations.\n\"\"\"\ndef
    migration_status do\n  for repo <- repos(), do: print_migrations_for(repo)\nend\n\ndefp
    print_migrations_for(repo) do\n  paths = repo_migrations_path(repo)\n\n  {:ok,
    repo_status, _} =\n    Ecto.Migrator.with_repo(repo, &Ecto.Migrator.migrations(&1,
    paths), mode: :temporary)\n\n  IO.puts(\n    \"\"\"\n    Repo: #{inspect(repo)}\n 
        Status    Migration ID    Migration Name\n    --------------------------------------------------\n 
      \"\"\" <>\n      Enum.map_join(repo_status, \"\\n\", fn {status, number, description}
    ->\n        \"  #{pad(status, 10)}#{pad(number, 16)}#{description}\"\n      end)
    <> \"\\n\"\n  )\nend\n\ndefp repo_migrations_path(repo) do\n  config = repo.config()\n 
    priv = config[:priv] || \"priv/#{repo |> Module.split() |> List.last() |> Macro.underscore()}\"\n 
    config |> Keyword.fetch!(:otp_app) |> Application.app_dir() |> Path.join(priv)\nend\n\ndefp
    pad(content, pad) do\n  content\n  |> to_string\n  |> String.pad_trailing(pad)\nend\n```\n\nA
    lot of this code is borrowed from the Mix task `mix ecto.migrations`, but changed
    to not use the Mix module.\n\nWhen you run `bin/my_app eval \"MyApp.Release.migration_status()\"`,
    you should see something like the following.\n\n```\nRepo: MyApp.Repo\n  Status
       Migration ID    Migration Name\n--------------------------------------------------\n 
    up        20210718153339  add_test_table1\n  down      20210718153341  add_test_table2\n```\n\n**What
    If I Want to Migrate My Data?**\n\nPerhaps your database already has data in it
    and you need to change the **data** in-place. For example, in a blog system you
    might have assumed earlier that all the blog posts would be published as soon
    as it was written. Later, you hire an editor that wants to review the blog posts
    before they're made publicly available. You decide to add a new column `published_at`
    that accepts a timestamp of when the blog post is publicly available. Your new
    editor now reviews blog posts and when it's ready, they set the `published_at`
    timestamp. You already ran the schema migration to add the column, but now you
    also need to migrate the older data and fill the publishing date on older blog
    posts. This is where a **data migration** is helpful. We'll also refer to this
    as &quot;backfilling&quot; existing data.\n\nBecause data migrations are usually
    one-off processes that only need to run once, _data_ migrations need to happen
    separately from _schema_ migrations and we want to trigger them manually. Making
    them manual ensures that other automatic workflows don't try to run it multiple
    times. This is a case where a singleton in your workflow may be necessary \U0001F609.
    With Ecto, we can separate these data migrations into a different folder, which
    makes running them  more intentional. When generating a data migration with `mix
    ecto.gen.migration`, you can use the `--migrations-path=MY_PATH` flag to put them
    in a different folder, eg:\n\n```cmd\nmix ecto.gen.migration --migrations-path=priv/repo/data_migrations
    backfill_foo\n```\n```output\n* creating priv/repo/data_migrations\n* creating
    priv/repo/data_migrations/20210811035222_backfill_foo.exs\n```\n\nTo run these
    migrations in a Mix Release, we'll need a new function that looks in this custom
    folder for our data migrations.\n\n```elixir\n@doc \"\"\"\nMigrate data in the
    database. Defaults to migrating to the latest, `[all: true]`\nAlso accepts `[step:
    1]`, or `[to: 20200118045751]`\n\"\"\"\ndef migrate_data(opts \\\\ [all: true])
    do\n  for repo <- repos() do\n    path = Ecto.Migrator.migrations_path(repo, \"data_migrations\")\n 
      {:ok, _, _} = Ecto.Migrator.with_repo(repo, &Ecto.Migrator.run(&1, path, :up,
    opts))\n  end\nend\n```\n\nNow you can manually run your data migrations when
    using releases like this:\n\n```cmd\nbin/my_app eval 'MyApp.Release.migrate_data()'\n```\n\nIf
    you'd like more inspiration, read [Automatic and Manual Ecto Migration by Wojtek
    Mach](https://dashbit.co/blog/automatic-and-manual-ecto-migrations).\n\n## Start
    the Release\n\nIt's time to build your release and deploy it. Wonderful!  But
    wait! Before you start your application, let's ask some questions:\n\n1. Does
    the deployed code assume the migrations were already run? If so, you need to run
    your migrations _first_ and start the application _after;_ otherwise your application
    will crash!\n1. Does the release contain migrations that aren't used yet? For
    example, you have a migration that adds a column to a table but the Ecto schema
    doesn't even reference it yet. In this case, you can start your application before
    running the migration because the code does not need the column to exist. Feel
    free to run the migrations at your convenience.\n1. Are you using Kubernetes?
    Then consider [Init Containers](https://kubernetes.io/docs/concepts/workloads/pods/init-containers/).
    Init containers run to completion _before_ the application containers in the pod.
    This is a perfect place to start your Ecto Repo and migrate the database before
    starting the rest of your application. Combine this with [Kubernetes Jobs](https://kubernetes.io/docs/concepts/workloads/controllers/job/),
    and you have a way to run the migration only one time. Using Init Containers waits
    for the job to complete before starting the application. Tip: If you setup Kubernetes
    to run your migrations, be mindful to exclude one-off processes such as data migrations.\n\nNow
    that you've determined the order needed to safely roll out your changes, i.e.
    run database migrations or start the application, let's start running the release
    commands!\n\n## Check migration status\n\nWe can inspect the database migration
    status.\n\n```cmd\nbin/my_app eval 'MyApp.Release.migration_status()'\n```\n```output\nRepo:
    MyApp.Repo\n  Status    Migration ID    Migration Name\n--------------------------------------------------\n 
    up        20210718153339  add_test_table1\n  down      20210718153341  add_test_table2\n```\n\n##
    Run the migrations\n\nTo migrate the database structure, run `bin/my_app eval
    'MyApp.Release.migrate()'`.\n\nWhen running `bin/my_app eval ...`, a separate
    slim instance of the Erlang VM is started. Your app is loaded but not started.
    Only the Repo is started, and it's only started with 2 database connections. Since
    this is a new instance booting, this implies that it will also need the same environment
    variables as your running application. If you rely on environment variables for
    the database settings, ensure they're present when running this command.\n\nTo
    run _data_ migrations, run `bin/my_app eval 'MyApp.Release.migrate_data()'`.\n\n##
    OMG ROLL IT BACK\n\nBefore you roll back, you should consider if there's a safer
    way to continue forward and fix or work around any issues. I have never needed
    to roll back the database. I don't mean this as a weird flex (far from it!) it's
    just that me and many others have found a way around the issue without rolling
    back.\n\nBut that doesn't mean you shouldn't give yourself that escape hatch;
    so if necessary, the app can rollback using `bin/my_app eval 'MyApp.Release.rollback(MyApp.Repo,
    20210709121212)'`\n\n## Where to next?\n\nNext we look through our [recipe books
    for tips and techniques](/phoenix-files/migration-recipes) to make our migrations
    smooth and tasty!\n"
- :id: phoenix-files-migration-recipes
  :date: '2021-11-15'
  :category: phoenix-files
  :title: Migration Recipes
  :author: bernheisel
  :thumbnail: migration-recipes-thumbnail.jpg
  :alt:
  :link: phoenix-files/migration-recipes
  :path: phoenix-files/2021-11-15
  :body: |2


    <div class="lead">Fly.io runs apps close to users, by transmuting Docker containers into micro-VMs that run on our own hardware around the world. This post is part of the [Safe Ecto Migrations](/phoenix-files/safe-ecto-migrations) series guide. If you just want to ship your Phoenix app, the easiest way to learn more is to [try it out](https://fly.io/docs/speedrun/); you can be up and running in just a couple minutes.</div>

    This is part 3 in a 4-part series on designing and running [Safe Ecto Migrations](/phoenix-files/safe-ecto-migrations):

    - [Part 1 - Anatomy of an Ecto migration](/phoenix-files/anatomy-of-an-ecto-migration)
    - [Part 2 - How to migrate Mix Release projects](/phoenix-files/how-to-migrate-mix-release-projects) 
    - **Part 3 - Migration Recipes (you are here)**
    - [Part 4 - Backfilling Data](/phoenix-files/backfilling-data)

    This is a non-exhaustive guide on common migration scenarios and how to avoid trouble.

    These migration recipes may evolve over time, so be sure to check the git edition of these recipes at [https://github.com/fly-apps/safe-ecto-migrations](https://github.com/fly-apps/safe-ecto-migrations) with up-to-date information.

    - [Adding an index](#adding-an-index)
    - [Adding a reference or foreign key](#adding-a-reference-or-foreign-key-constraint)
    - [Adding a column with a default value](#adding-a-column-with-a-default-value)
    - [Changing the type of a column](#changing-the-type-of-a-column)
    - [Removing a column](#removing-a-column)
    - [Renaming a column](#renaming-a-column)
    - [Renaming a table](#renaming-a-table)
    - [Adding a check constraint](#adding-a-check-constraint)
    - [Setting NOT NULL on an existing column](#setting-not-null-on-an-existing-column)
    - [Adding a JSON column](#adding-a-json-column)

    ## Adding an index

    Creating an index blocks both reads and writes. This scenario is used as an example in the &quot;How to inspect locks in a query&quot; section.

    ### Bad <span aria-hidden class="text-sm align-text-bottom ml-1">❌</span>

    ```elixir
    def change do
      create index("posts", [:slug])
      # This obtains a ShareLock on "posts" which will block writes to the table
    end
    ```

    ### Good <span aria-hidden class="text-base align-text-bottom ml-1">✅</span>

    Instead, have Postgres create the index **concurrently** which does **not** block reads. To do this, you have to disable the migration transactions.

    ```elixir
    @disable_ddl_transaction true
    @disable_migration_lock true

    def change do
      create index("posts", [:slug], concurrently: true)
    end
    ```

    While the migration may still take some time to run, it does not block reads and updates on rows. For example, indexing  100,000,000 rows took 165 seconds (2.75 minutes) to run the migration, but it didn't lock up the table for selects and updates while it was running!

    When disabling transactions in a migration like this, **do not make other changes in the same migration**! Only create the index concurrently. Use separate migrations for other changes. Separating these changes into their own migrations will make failures clear and helps prevent leaving the database in a mangled state. For example, if  half of the steps in a migration succeed but the the remaining half failed— trying to re-run the migration will be confusing and cause problems.

    ## Adding a reference or foreign key constraint

    Adding a foreign key blocks writes on both tables.

    When a foreign key constraint is added, two things happen:

    1. It creates a new constraint for changing records going forward
    1. It validates the new constraint for existing records

    If these commands are happening at the same time, it obtains a lock on the table as it validates and scans the _entire_ table. To avoid a full table scan, we can separate the operations.

    ### Bad <span aria-hidden class="text-sm align-text-bottom ml-1">❌</span>

    ```elixir
    def change do
      alter table("posts") do
        add :group_id, references("groups")
      end
    end
    ```

    ### Good <span aria-hidden class="text-base align-text-bottom ml-1">✅</span>

    In the first migration:

    ```elixir
    def change do
      alter table("posts") do
        add :group_id, references("groups", validate: false)
      end
    end
    ```

    In the next migration:

    ```elixir
    def change do
      execute "ALTER TABLE posts VALIDATE CONSTRAINT group_id_fkey", ""
    end
    ```

    ## Adding a column with a default value

    Adding a column with a default value to an existing table may cause the table to be rewritten. While that happens, Postgres blocks both reads and writes, and MySQL and MariaDB block writes.

    ### Bad <span aria-hidden class="text-sm align-text-bottom ml-1">❌</span>

    Note: This becomes safe in:

    - [Postgres 11+](https://www.postgresql.org/docs/11/release-11.html),
    - [MySQL 8.0.12+](https://dev.mysql.com/doc/relnotes/mysql/8.0/en/news-8-0-12.html),
    - [MariaDB 10.3.2+](https://mariadb.com/kb/en/instant-add-column-for-innodb/)

    ```elixir
    def change do
      alter table("comments") do
        add :approved, :boolean, default: false
        # This took 34 seconds for 10 million rows with no fkeys,
        # This took 10 minutes for 100 million rows with no fkeys,

        # Obtained an AccessExclusiveLock on the table, which blocks reads and
        # writes.
      end
    end
    ```

    ### Good <span aria-hidden class="text-base align-text-bottom ml-1">✅</span>

    Add the column first and then alter it to include the default.

    First migration:

    ```elixir
    def change do
      alter table("comments") do
        add :approved, :boolean
        # This took 0.27 milliseconds for 100 million rows with no fkeys,
      end
    end
    ```

    Second migration:

    ```elixir
    def change do
      alter table("comments") do
        modify :approved, :boolean, default: false
        # This took 0.28 milliseconds for 100 million rows with no fkeys,
      end
    end
    ```

    Schema change to read the new column:

    ```diff
    schema "comments" do
    + field :approved, :boolean, default: false
    end
    ```

    In the Ecto schema, the `default: false` isn’t necessary, but it matches what the database will do so it avoids reading back the field on inserts and behaves consistently.

    ## Changing the type of a column

    Changing the type of a column may cause the table to be rewritten. While that happens, Postgres blocks both reads and writes, and MySQL and MariaDB block writes.

    ### Bad <span aria-hidden class="text-sm align-text-bottom ml-1">❌</span>

    Safe in Postgres:

    - increasing length on varchar or removing the limit
    - changing varchar to text
    - changing text to varchar with no length limit
    - [Postgres 9.2+](https://www.postgresql.org/docs/9.2/release-9-2.html) - increasing precision (NOTE: not scale) of decimal or numeric columns. eg, increasing 8,2 to 10,2 is safe. Increasing 8,2 to 8,4 is not safe.
    - [Postgres 9.2+](https://www.postgresql.org/docs/9.2/release-9-2.html) - changing decimal or numeric to be unconstrained
    - [Postgres 12+](https://www.postgresql.org/docs/release/12.0/) - changing timestamp to timestamptz when session TZ is UTC

    Safe in MySQL/MariaDB:

    - increasing length of varchar from &lt; 255 up to 255.
    - increasing length of varchar from &gt; 255 up to max.

    ```elixir
    def change do
      alter table("posts") do
        modify :my_column, :boolean, :text
      end
    end
    ```

    ### Good <span aria-hidden class="text-base align-text-bottom ml-1">✅</span>

    Multi deployment strategy:

    1. Create a new column
    1. In application code, write to both columns
    1. Backfill data from old column to new column
    1. In application code, move reads from old column to the new column
    1. In application code, remove old column from Ecto schemas.
    1. Drop the old column.

    ## Removing a column

    Removing a column happens when we are &quot;cleaning up&quot; or restructuring the database as it evolves to solve different problems.

    If your application deployment rolls out _new_ versions while the _old_ version is still running, then running database structure migrations that change tables out from under the older running app creates problems when they query for data. Queries may fail because the database is no longer structured how the code expects. This scenario is often forgotten or ignored. However, there are safe ways to remove columns that don't temporarily break things as they roll out.

    ### Bad <span aria-hidden class="text-sm align-text-bottom ml-1">❌</span>

    ```elixir
    # Without a code change to the Ecto Schema
    def change
      alter table("posts") do
        remove :no_longer_needed_column

        # Obtained an AccessExclusiveLock on the table, which blocks reads and
        # writes, but was instantaneous.
      end
    end
    ```

    ### Good <span aria-hidden class="text-base align-text-bottom ml-1">✅</span>

    A &quot;multi-stage&quot; deployment makes removing a column go smoothly. First, remove references to the column from the application so it's no longer loaded or queried. Then a second deploy safely removes the column from the table.

    First deploy, edit the Ecto schema:

    ```diff
    defmodule MyApp.Post do
      schema "posts" do
    -   column :no_longer_needed_column, :text
      end
    end
    ```

    Second deploy, run with the migration:

    ```elixir
    def change
      alter table("posts") do
        remove :no_longer_needed_column
      end
    end
    ```

    ## Renaming a column

    Ask yourself: &quot;Do I really need to rename a column?&quot;. Probably not, but if you must, read on and be aware it requires time and effort.

    Any running instances of your application that expect the field to still have the _old_ name will fail when the database structure changes. This happens when you have multiple application instances running and you roll out new versions.

    There is a shortcut: don't rename the database column. Instead, rename the schema's field name and configure it to point to the database column.

    ### Bad <span aria-hidden class="text-sm align-text-bottom ml-1">❌</span>

    In your schema:

    ```elixir
    schema "posts" do
      field :summary, :text
    end
    ```

    In your migration

    ```elixir
    def change do
      rename table("posts"), :title, to: :summary
    end
    ```

    ### Good <span aria-hidden class="text-base align-text-bottom ml-1">✅</span>

    **Strategy 1**

    Rename the field in the Ecto schema only. Configure the schema field to point to the unchanged column name. Ensure all calling code referencing the old field name is updated to reference the new field name.

    ```diff
    defmodule MyApp.MySchema do
      use Ecto.Schema

      schema "weather" do
        field :temp_lo, :integer
        field :temp_hi, :integer
    -   field :prcp, :float
    +   field :precipitation, :float, source: :prcp
        field :city, :string

        timestamps(type: :naive_datetime_usec)
      end
    end
    ```

    Update references in other parts of the codebase:

    ```diff
       my_schema = Repo.get(MySchema, "my_id")
    -  my_schema.prcp
    +  my_schema.precipitation
    ```

    **Strategy 2**

    Take a &quot;multi-stage&quot; approach:

    1. Create a new column
    1. In application code, write to both columns
    1. Backfill data from old column to new column
    1. In application code, move reads from old column to the new column
    1. In application code, remove old column from Ecto schemas.
    1. Drop the old column.

    ## Renaming a table

    Ask yourself: &quot;Do I really need to rename a table?&quot;. Probably not, but if you must, read on and be aware it requires time and effort.

    Any running instances of your application that expect the table to still have the _old_ name will fail when the database structure changes. This happens when you have multiple application instances running and you roll out new versions.

    There is a shortcut: rename the schema only, and do not change the underlying database table name.

    ### Bad <span aria-hidden class="text-sm align-text-bottom ml-1">❌</span>

    ```elixir
    def change do
      rename table("posts"), to: table("articles")
    end
    ```

    ### Good <span aria-hidden class="text-base align-text-bottom ml-1">✅</span>

    1. Create the new table. This should include creating new constraints (checks and foreign keys) that mimic the behavior of the old table.
    1. In application code, write to both tables, continuing to read from the old table.
    1. Backfill data from old table to new table
    1. In application code, move reads from old table to the new table
    1. In application code, remove the old table from Ecto schemas.
    1. Drop the old table.

    ## Adding a check constraint

    Adding a check constraint blocks reads and writes to the table in Postgres, and blocks writes in MySQL/MariaDB while every row is checked.

    When a check constraint is added, two things happen:

    1. It creates a new constraint for changing records going forward
    1. It validates the new constraint for existing records

    When these happening at the same time, it obtains a lock on the table as it validates and fully scans the entire table. To avoid a full table scan, we can separate the operations.

    ### Bad <span aria-hidden class="text-sm align-text-bottom ml-1">❌</span>

    ```elixir
    def change do
      create constraint("products", :price_must_be_positive, check: "price > 0")
      # Creating the constraint with validate: true (the default when unspecified)
      # will perform a full table scan and acquires a lock preventing updates
    end
    ```

    ### Good <span aria-hidden class="text-base align-text-bottom ml-1">✅</span>

    First migration, add the constraint but don't let it validate:

    ```elixir
    def change do
      create constraint("products", :price_must_be_positive, check: "price > 0"), validate: false
      # Setting validate: false will prevent a full table scan, and therefore
      # commits immediately.
    end
    ```

    Second migration, validate the constraint:

    ```elixir
    def change do
      execute "ALTER TABLE products VALIDATE CONSTRAINT price_must_be_positive", ""
      # Acquires SHARE UPDATE EXCLUSIVE lock, which allows updates to continue
    end
    ```

    These can be in the same deployment, but ensure they are 2 separate migrations.

    ## Setting NOT NULL on an existing column

    Setting NOT NULL on an existing column blocks reads and writes while every row is checked.  Just like the [Adding a check constraint](#adding-a-check-constraint) scenario, two things are happening:

    1. It creates a new constraint for changing records going forward
    1. It validates the new constraint for existing records

    To avoid the full table scan, we can separate these two operations.

    ### Bad <span aria-hidden class="text-sm align-text-bottom ml-1">❌</span>

    ```elixir
    def change do
      alter table("products") do
        modify :active, :boolean, null: false
      end
    end
    ```

    ### Good <span aria-hidden class="text-base align-text-bottom ml-1">✅</span>

    Add a check constraint without validating it, backfill data to satisfy the constraint and then validate it. This is functionally equivalent.

    In the first migration:

    ```elixir
    def change do
      create constraint("products", :active_not_null, check: "active IS NOT NULL"), validate: false
    end
    ```

    This enforces the constraint in all new rows, but does not care about existing rows until a row is updated. You'll likely also need a data migration to ensure that the constraint is satisfied.

    Then in the next deployment's migration, enforce the constraint on all rows:

    ```elixir
    def change do
      execute "ALTER TABLE products VALIDATE CONSTRAINT active_not_null", ""
    end
    ```

    If you're using Postgres 12+, you can add the `NOT NULL` constraint to the column after validating the constraint. From the Postgres 12 docs:

    > `SET NOT NULL`  may only be applied to a column provided none of the records in the table contain a null value for the column. Ordinarily this is checked during the `ALTER TABLE` by scanning the entire table; however, if a valid check constraint is found which proves no null can exist, then the table scan is skipped.

    ```elixir
    # **Postgres 12+ only**
    def change do
      execute "ALTER TABLE products VALIDATE CONSTRAINT active_not_null", ""

      alter table("products") do
        modify :active, :boolean, null: false
      end

      drop constraint("products", :active_not_null)
    end
    ```

    If the constraint fails, then first consider backfilling data to cover the gaps in your desired data integrity, then revisit validating the constraint.

    ## Adding a JSON column

    In Postgres, there is no equality operator for the json column type, which can cause errors for existing `SELECT DISTINCT` queries in your application.

    ### Bad <span aria-hidden class="text-sm align-text-bottom ml-1">❌</span>

    ```elixir
    def change do
      alter table("posts") do
        add :extra_data, :json
      end
    end
    ```

    ### Good <span aria-hidden class="text-base align-text-bottom ml-1">✅</span>

    Use jsonb instead. Some say it’s just like “json” but “better.”

    ```elixir
    def change do
      alter table("posts") do
        add :extra_data, :jsonb
      end
    end
    ```

    ## Where to next?

    Next finish up our journey by seeing how to safely fill holes in our data created by changing the structure of our tables in "[Backfilling Data](/phoenix-files/backfilling-data)"!

    ## References

    These recipes took a lot of inspiration from Andrew Kane and his library [strong_migrations](https://github.com/ankane/strong_migrations).

    [PostgreSQL at Scale by James Coleman](https://medium.com/braintree-product-technology/postgresql-at-scale-database-schema-changes-without-downtime-20d3749ed680)

    [Strong Migrations by Andrew Kane](https://github.com/ankane/strong_migrations)

    [Adding a NOT NULL CONSTRAINT on PG Faster with Minimal Locking](https://medium.com/doctolib/adding-a-not-null-constraint-on-pg-faster-with-minimal-locking-38b2c00c4d1c)

    [Postgres Runtime Configuration](https://www.postgresql.org/docs/current/runtime-config-client.html)

    [Automatic and Manual Ecto Migrations by Wojtek Mach](https://dashbit.co/blog/automatic-and-manual-ecto-migrations)

    ## Reference Material

    [Postgres Lock Conflicts](https://www.postgresql.org/docs/12/explicit-locking.html)

    |  |  | **Current Lock →** |  |  |  |  |  |  |
    | --- | --- | --- | --- | --- | --- | --- | --- | --- |
    | **Requested Lock ↓** | ACCESS SHARE | ROW SHARE | ROW EXCLUSIVE | SHARE UPDATE EXCLUSIVE | SHARE | SHARE ROW EXCLUSIVE | EXCLUSIVE | ACCESS EXCLUSIVE |
    | ACCESS SHARE |  |  |  |  |  |  |  | X |
    | ROW SHARE |  |  |  |  |  |  | X | X |
    | ROW EXCLUSIVE |  |  |  |  | X | X | X | X |
    | SHARE UPDATE EXCLUSIVE |  |  |  | X | X | X | X | X |
    | SHARE |  |  | X | X |  | X | X | X |
    | SHARE ROW EXCLUSIVE |  |  | X | X | X | X | X | X |
    | EXCLUSIVE |  | X | X | X | X | X | X | X |
    | ACCESS EXCLUSIVE | X | X | X | X | X | X | X | X |



    - SELECT acquires a ACCESS SHARE lock
    - SELECT FOR UPDATE acquires a ROW SHARE lock
    - UPDATE, DELETE, and INSERT will acquire a ROW EXCLUSIVE lock
    - CREATE INDEX CONCURRENTLY and VALIDATE CONSTRAINT acquires SHARE UPDATE EXCLUSIVE
    - CREATE INDEX acquires SHARE lock
- :id: phoenix-files-safe-ecto-migrations
  :date: '2021-11-15'
  :category: phoenix-files
  :title: Safe Ecto Migrations
  :author: bernheisel
  :thumbnail: safe-ecto-migrations-thumbnail.jpg
  :alt:
  :link: phoenix-files/safe-ecto-migrations
  :path: phoenix-files/2021-11-15
  :body: "\n\n<div class=\"lead\">Fly.io runs apps close to users, by transmuting
    Docker containers into micro-VMs that run on our own hardware around the world.
    This post launches off the [Safe Ecto Migrations](/phoenix-files/safe-ecto-migrations)
    series guide. If you just want to ship your Phoenix app, the easiest way to learn
    more is to [try it out](https://fly.io/docs/speedrun/); you can be up and running
    in just a couple minutes.</div>\n\nAs an Elixir developer who cares about system
    up-time and avoiding &quot;scheduled maintenance&quot; windows, and more importantly
    avoiding &quot;unscheduled maintenance&quot; windows \U0001F609, this guide dives
    deep into Ecto database migrations and how they can be used safely in production
    systems.\n\nThis is guide comes in a 4 part series of posts. The guide helps you:\n\n1.
    Understand Ecto migrations\n2. Migrate and rollback the database using Mix releases\n3.
    Avoid pitfalls during migrations\n\nThis guide includes a set of recipes for common
    migration scenarios. You can jump ahead to those, but it is helpful to understand
    what's going on at a deeper level and to get your project setup ahead of time
    for running migrations in a production environment and\n\n**Note**: This guide
    uses PostgreSQL and may differ if you're using a different database. I'll note
    where differences may be, but I do not go into depth on different database systems.
    This was also written using Ecto 3.6.x.\n\n## Guide Table of Contents\n\n- **Part
    1:** [Anatomy of an Ecto migration](/phoenix-files/anatomy-of-an-ecto-migration)\n
    \ - [Inspect SQL](/phoenix-files/anatomy-of-an-ecto-migration#inspect-sql)\n  -
    [Migration Options](/phoenix-files/anatomy-of-an-ecto-migration#migration-options)\n
    \ - [Inspecting Locks In a Query](/phoenix-files/anatomy-of-an-ecto-migration#inspecting-locks-in-a-query)\n
    \ - [Safeguards in the database](/phoenix-files/anatomy-of-an-ecto-migration#safeguards-in-the-database)\n
    \ - [Add a lock_timeout](/phoenix-files/anatomy-of-an-ecto-migration#add-a-lock_timeout)\n
    \ - [Statement Timeout](/phoenix-files/anatomy-of-an-ecto-migration#statement-timeout)\n-
    **Part 2:** [How to migrate Mix Release projects](/phoenix-files/how-to-migrate-mix-release-projects)\n
    \ - [Release Module](/phoenix-files/how-to-migrate-mix-release-projects#release-module)\n
    \ - [Start the Release](/phoenix-files/how-to-migrate-mix-release-projects#start-the-release)\n
    \ - [Check migration status](/phoenix-files/how-to-migrate-mix-release-projects#check-migration-status)\n
    \ - [Run the migrations](/phoenix-files/how-to-migrate-mix-release-projects#run-the-migrations)\n
    \ - [OMG ROLL IT BACK](/phoenix-files/how-to-migrate-mix-release-projects#omg-roll-it-back)\n-
    **Part 3:** [Migration Recipes](/phoenix-files/migration-recipes)\n  - [Adding
    an index](/phoenix-files/migration-recipes#adding-an-index)\n  - [Adding a reference
    or foreign key](/phoenix-files/migration-recipes#adding-a-reference-or-foreign-key-constraint)\n
    \ - [Adding a column with a default value](/phoenix-files/migration-recipes#adding-a-column-with-a-default-value)\n
    \ - [Changing the type of a column](/phoenix-files/migration-recipes#changing-the-type-of-a-column)\n
    \ - [Removing a column](/phoenix-files/migration-recipes#removing-a-column)\n
    \ - [Renaming a column](/phoenix-files/migration-recipes#renaming-a-column)\n
    \ - [Renaming a table](/phoenix-files/migration-recipes#renaming-a-table)\n  -
    [Adding a check constraint](/phoenix-files/migration-recipes#adding-a-check-constraint)\n
    \ - [Setting NOT NULL on an existing column](/phoenix-files/migration-recipes#setting-not-null-on-an-existing-column)\n
    \ - [Adding a JSON column](/phoenix-files/migration-recipes#adding-a-json-column)\n
    \ - [References](/phoenix-files/migration-recipes#references)\n  - [Reference
    Material](/phoenix-files/migration-recipes#reference-material)\n- **Part 4:**
    [Backfilling Data](/phoenix-files/backfilling-data)\n  - [Bad](/phoenix-files/backfilling-data#bad)\n
    \ - [Good](/phoenix-files/backfilling-data#good)\n  - [Batching Deterministic
    Data](/phoenix-files/backfilling-data#batching-deterministic-data)\n  - [Batching
    Arbitrary Data](/phoenix-files/backfilling-data#batching-arbitrary-data)\n  -
    [We're Done!](/phoenix-files/backfilling-data#were-done)\n\n  <%= partial \"shared/posts/cta\",
    locals: {\n  title: \"Fly ❤️ Elixir\",\n  text: \"Fly is an awesome place to run
    your Elixir apps. It's really easy to get started. You can be running in minutes.\",\n
    \ link_url: \"https://fly.io/docs/elixir/\",\n  link_text: \"Deploy your Elixir
    app today!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>\"\n} %>"
- :id: phoenix-files-anatomy-of-an-ecto-migration
  :date: '2021-11-15'
  :category: phoenix-files
  :title: Anatomy of an Ecto migration
  :author: bernheisel
  :thumbnail: anatomy-of-an-ecto-migration-thumbnail.jpg
  :alt:
  :link: phoenix-files/anatomy-of-an-ecto-migration
  :path: phoenix-files/2021-11-15
  :body: "\n\n<div class=\"lead\">Fly.io runs apps close to users, by transmuting
    Docker containers into micro-VMs that run on our own hardware around the world.
    This post is part of the [Safe Ecto Migrations](/phoenix-files/safe-ecto-migrations)
    series guide. If you just want to ship your Phoenix app, the easiest way to learn
    more is to [try it out](https://fly.io/docs/speedrun/); you can be up and running
    in just a couple minutes.</div>\n\nThis is part 1 in a 4-part series on designing
    and running [Safe Ecto Migrations](/phoenix-files/safe-ecto-migrations):\n\n-
    **Part 1 - Anatomy of an Ecto migration (you are here)**\n- [Part 2 - How to migrate
    Mix Release projects](/phoenix-files/how-to-migrate-mix-release-projects) \n-
    [Part 3 - Migration Recipes](/phoenix-files/migration-recipes)\n- [Part 4 - Backfilling
    Data](/phoenix-files/backfilling-data)\n\n[Ecto migrations](https://hexdocs.pm/ecto_sql/Ecto.Migration.html)
    are used to do the following:\n\n- Change the structure of the database, such
    as adding fields, tables, or indexes to improve lookups\n- Populating fields with
    modified data or new data.\n\nIn order for us to create and run safe Ecto migrations
    on our database, it is helpful to understand what is _actually_ happening with
    the database. To do that, we'll dig deeper into how Ecto migrations work by looking
    both at the code being executed and the Postgres logs.\n\nTo generate a migration,
    we'll use run a mix task:\n\n`mix ecto.gen.migration create_test_table`\n\n**Tip:** If
    you're using Phoenix, you might consider `mix phx.gen.schema` which will generate
    a migration and also allows you to pass in fields and types. See `mix help phx.gen.schema` for
    more information.\n\nThis command will generate a file in `priv/repo/migrations` given
    the repo name of Repo. If you named it `OtherRepo` the file would be in `priv/other_repo/migrations`.\n\nLet's
    look at that file:\n\n```elixir\ndefmodule MyApp.Repo.Migrations.CreateTestTable
    do\n  use Ecto.Migration\n\n  def change do\n\n  end\nend\n```\n\nLet's make some
    changes; how about we create a table that tracks a city's climate?\n\n```elixir\ndefmodule
    MyApp.Repo.Migrations.CreateTestTable do\n  use Ecto.Migration\n\n  def change
    do\n    create table(\"test\") do\n      add :city,    :string, size: 40\n   
      add :temp_lo, :integer\n      add :temp_hi, :integer\n      add :prcp,    :float\n\n 
        timestamps()\n    end\n  end\nend\n```\n\nNow that we have a migration, let's
    run it! Run `mix ecto.migrate`.\n\n```cmd\nmix ecto.migrate\n```\n```output\n21:26:18.992
    [info]  == Running 20210702012346 MyApp.Repo.Migrations.CreateTestTable.change/0
    forward\n21:26:18.994 [info]  create table test\n21:26:19.004 [info]  == Migrated
    20210702012346 in 0.0s\n```\n\n## Inspect SQL\n\nUnderstanding exactly what SQL
    commands are running is helpful to ensure safe migrations, so how do we see the
    SQL that is executed? By default, Ecto does not log the raw SQL. First, I'll rollback,
    and then re-migrate but with an additional flag `--log-sql` (or `--log-migrations-sql`
    in later versions) so we can see what actually runs.\n\n```cmd\nmix ecto.rollback\n```\n```output\n21:29:32.287
    [info]  == Running 20210702012346 MyApp.Repo.Migrations.CreateTestTable.change/0
    backward\n21:29:32.289 [info]  drop table test\n21:29:32.292 [info]  == Migrated
    20210702012346 in 0.0s\n```\n```cmd\nmix ecto.migrate --log-sql\n```\n```output\n21:29:36.461
    [info]  == Running 20210702012346 MyApp.Repo.Migrations.CreateTestTable.change/0
    forward\n21:29:36.462 [info]  create table test\n21:29:36.466 [debug] QUERY OK
    db=3.2ms\nCREATE TABLE \"test\" (\"id\" bigserial, \"city\" varchar(40), \"temp_lo\"
    integer, \"temp_hi\" integer, \"prcp\" float, \"inserted_at\" timestamp(0) NOT
    NULL, \"updated_at\" timestamp(0) NOT NULL, PRIMARY KEY (\"id\")) []\n21:29:36.467
    [info]  == Migrated 20210702012346 in 0.0s\n```\n\nEcto logged the SQL for _our_
    changes, but we're not seeing all the SQL that Ecto is running for the migration—
    we're missing the `Ecto.Migrator`  SQL that manages the migration. To get these
    missing logs, I'll set Postgres to log everything and then tail the Postgres logs
    and re-run the migration:\n\n```\nLOG:  statement: BEGIN\nLOG:  execute <unnamed>:
    LOCK TABLE \"schema_migrations\" IN SHARE UPDATE EXCLUSIVE MODE\nLOG:  execute
    ecto_3: SELECT s0.\"version\"::bigint FROM \"schema_migrations\" AS s0\nLOG:  statement:
    BEGIN\nLOG:  execute <unnamed>: CREATE TABLE \"weather\" (\"id\" bigserial, \"city\"
    varchar(40), \"temp_lo\" integer, \"temp_hi\" integer, \"prcp\" float, \"inserted_at\"
    timestamp(0) NOT NULL, \"updated_at\" timestamp(0) NOT NULL, PRIMARY KEY (\"id\"))\nLOG:
     execute ecto_insert_schema_migrations: INSERT INTO \"schema_migrations\" (\"version\",\"inserted_at\")
    VALUES ($1,$2)\nDETAIL:  parameters: $1 = '20210718204657', $2 = '2021-07-18 20:53:49'\nLOG:
     statement: COMMIT\nLOG:  statement: COMMIT\n```\n\n**Tip**: In the latest version
    of Ecto, you can use the flag `--log-migrator-sql` instead of tailing Postgres
    logs.\n\nLet's trace the code: when running migrations, Ecto executes these functions:\n\n1.
    [`Ecto.Migrator.run/4`](https://github.com/elixir-ecto/ecto_sql/blob/557335f9a2a1e6950c1d761063e84aa5d03cb312/lib/ecto/migrator.ex#L384)\n2.
    [`Ecto.Migrator.lock_for_migrations/4`](https://github.com/elixir-ecto/ecto_sql/blob/557335f9a2a1e6950c1d761063e84aa5d03cb312/lib/ecto/migrator.ex#L464)\n3.
    [The adapter's `lock_for_migrations` implementation](https://github.com/elixir-ecto/ecto_sql/blob/557335f9a2a1e6950c1d761063e84aa5d03cb312/lib/ecto/adapters/postgres.ex#L207)\n4.
    [Wrapped in another transaction](https://github.com/elixir-ecto/ecto_sql/blob/557335f9a2a1e6950c1d761063e84aa5d03cb312/lib/ecto/adapters/postgres.ex#L217)\n\nInside
    the transaction, the Ecto Postgres adapter obtains a `SHARE UPDATE EXCLUSIVE` lock
    of the &quot;schema_migrations&quot; table.\n\n**Why this lock is important**:
    Systems at scale may have multiple instances of the application connected to the
    same database, and during a deployment all of the instances rolling out may try
    to migrate that database at the same time, Ecto leverages this SHARE UPDATE EXCLUSIVE lock
    as a way to ensure that only one instance is running a migration at a time and
    only once.\n\nThis is what the migration actually looks like:\n\n```sql\nBEGIN;\nLOCK
    TABLE \"schema_migrations\" IN SHARE UPDATE EXCLUSIVE MODE;\nBEGIN;\nCREATE TABLE
    \"test\" (\"id\" bigserial, \"city\" varchar(40), \"temp_lo\" integer, \"temp_hi\"
    integer, \"prcp\" float, \"inserted_at\" timestamp(0) NOT NULL, \"updated_at\"
    timestamp(0) NOT NULL, PRIMARY KEY (\"id\"));\nINSERT INTO \"schema_migrations\"
    (\"version\",\"inserted_at\") VALUES ('20210718204657','2021-07-18 20:53:49');\nCOMMIT;\nCOMMIT;\n```\n\nWhen
    a migration fails, the transaction is rolled back and no changes are kept in the
    database. In most situations, these are great defaults.\n\nVeteran database administrators
    may notice the database transactions (BEGIN/COMMIT) and wonder how to turn those
    off in situations where transactions could cause problems; such as when adding
    indexes concurrently; Ecto provides some options that can help with transactions
    and locks.  Let's explore some of those options next.\n\n## Migration Options\n\nA
    typical migration has this structure (reminder: this guide is using Postgres;
    other adapters will vary):\n\n```sql\nBEGIN;\n  LOCK TABLE \"schema_migrations\"
    IN SHARE UPDATE EXCLUSIVE MODE;\n  BEGIN;\n    -- after_begin callback\n    --
    my changes\n    -- before_commit callback\n    INSERT INTO \"schema_migrations\"
    (\"version\",\"inserted_at\") VALUES ($1,$2);\n  COMMIT;\nCOMMIT;\n```\n\n`my_changes`
    refers to the changes you specify in each of your migrations.\n\n**@disable\\_migration\\_lock**\n\nBy
    default, Ecto acquires a lock on the &quot;schema_migrations&quot; table during
    the migration transaction:\n\n```sql\nBEGIN;\n  -- ↓ THIS LOCK ↓\n  LOCK TABLE
    \"schema_migrations\" IN SHARE UPDATE EXCLUSIVE MODE\n  BEGIN;\n    -- after_begin
    callback\n    -- my changes\n    -- before_commit callback\n    INSERT INTO \"schema_migrations\"
    (\"version\",\"inserted_at\") VALUES ($1,$2);\n  COMMIT;\nCOMMIT;\n```\n\nYou
    want this lock for most migrations because running multiple migrations simultaneously
    without this lock could have unpredictable results. In database transactions,
    any locks obtained inside the transaction are released when the transaction is
    committed, which then unblocks other transactions that touch the same records
    to proceed.\n\nHowever, there are some scenarios where you don't want a lock.
    We'll explore these scenarios later on (for example, backfilling data and creating
    indexes).\n\nYou can skip this lock in Ecto by setting the module attribute `@disable_migration_lock
    true` in your migration. When the migration lock is disabled, the migration looks
    like this:\n\n```sql\nBEGIN;\n  -- after_begin callback\n  -- my changes\n  --
    before_commit callback\n  INSERT INTO \"schema_migrations\" (\"version\",\"inserted_at\")
    VALUES ($1,$2);\nCOMMIT;\n```\n\n**@disable\\_ddl\\_transaction**\n\nBy default,
    Ecto wraps your changes in a transaction:\n\n```sql\nBEGIN;\n  LOCK TABLE \"schema_migrations\"
    IN SHARE UPDATE EXCLUSIVE MODE\n  -- ↓ THIS TRANSACTION ↓\n  BEGIN;\n    -- after_begin
    callback\n    -- my changes\n    -- before_commit callback\n    INSERT INTO \"schema_migrations\"
    (\"version\",\"inserted_at\") VALUES ($1,$2);\n  COMMIT;\n  -- ↑ THIS TRANSACTION
    ↑\nCOMMIT;\n```\n\nThis ensures that _when_ failures occur during a migration,
    your database is not left in an incomplete or mangled state.\n\nThere are scenarios
    where you **don't** want a migration to run inside a transaction. Like when performing
    data migrations or when running commands such as `CREATE INDEX CONCURRENTLY` that
    can run in the background in the database after you issue the command and cannot
    be inside a transaction.\n\nYou can disable this transaction by setting the module
    attribute `@disable_ddl_transaction true` in your migration. The migration then
    looks like this:\n\n```sql\nBEGIN;\n  LOCK TABLE \"schema_migrations\" IN SHARE
    UPDATE EXCLUSIVE MODE\n  -- my changes\n  INSERT INTO \"schema_migrations\" (\"version\",\"inserted_at\")
    VALUES ($1,$2);\nCOMMIT;\n```\n\n**Tip:** For Postgres, when disabling transactions,
    you'll also want to disable the migration lock since that uses yet another transaction.
    When running these migrations in a multi-node environment, you'll need a process
    to ensure these migrations are only run once since there is no protection against
    multiple nodes running the same migration at the same exact time.\n\nDisabling
    both the migration lock and the DDL transaction, your migration will be pretty
    simple:\n\n```sql\n-- my changes\nINSERT INTO \"schema_migrations\" (\"version\",\"inserted_at\")
    VALUES ($1,$2);\n```\n\n**Transaction Callbacks**\n\nIn the examples above, you'll
    notice there are `after_begin` and `before_commit` hooks if the migration is occurring
    within a transaction:\n\n```sql\nBEGIN;\n  -- after_begin hook  ← THIS HOOK\n 
    -- my changes\n  -- before_commit hook  ← AND THIS HOOK\n  INSERT INTO \"schema_migrations\"
    (\"version\",\"inserted_at\") VALUES ($1,$2);\nCOMMIT;\n```\n\nYou can use these
    hooks by defining `after_begin/0` and `before_commit/0` in your migration. A good
    use case for this is setting migration lock timeouts as safeguards (see later
    Safeguards section).\n\n```elixir\ndefmodule MyApp.Repo.Migrations.CreateTestTable
    do\n  use Ecto.Migration\n\n  def change do\n    # ... my potentially long-locking
    migration\n  end\n\n  def after_begin do\n    execute \"SET lock_timeout TO '5s'\",
    \"SET lock_timeout TO '10s'\"\n  end\nend\n```\n\nBe aware that these callbacks
    are **not** called when  `@disable_ddl_transaction` is `true` because they rely
    on the transaction being present.\n\n## Inspecting Locks In a Query\n\nBefore
    we dive into safer migration practices, we should cover how to check if a migration
    could potentially block your application. In Postgres, there is a `pg_locks` table
    that we can query that reveals the locks  in the system. Let's query that table
    alongside our changes from the migration, but return the locks so we can see what
    locks were obtained from the changes.\n\n```sql\nBEGIN;\n  -- Put your actions
    in here. For example, validating a constraint\n  ALTER TABLE addresses VALIDATE
    CONSTRAINT \"my_table_locking_constraint\";\n\n  -- end your transaction with
    a SELECT on pg_locks so you can see the locks\n  -- that occurred during the transaction\n 
    SELECT locktype, relation::regclass, mode, transactionid AS tid, virtualtransaction
    AS vtid, pid, granted FROM pg_locks;\nCOMMIT;\n```\n\nThe result from this SQL
    command should return the locks obtained during the database transaction. Let's
    see an example: We'll add a unique index without concurrency so we can see the
    locks it obtains:\n\n```sql\nBEGIN;\n  LOCK TABLE \"schema_migrations\" IN SHARE
    UPDATE EXCLUSIVE MODE;\n  -- we are going to squash the embedded transaction here
    for simplicity\n  CREATE UNIQUE INDEX IF NOT EXISTS \"weather_city_index\" ON
    \"weather\" (\"city\");\n  INSERT INTO \"schema_migrations\" (\"version\",\"inserted_at\")
    VALUES ('20210718210952',NOW());\n  SELECT locktype, relation::regclass, mode,
    transactionid AS tid, virtualtransaction AS vtid, pid, granted FROM pg_locks;\nCOMMIT;\n\n--
       locktype    |      relation      |           mode           |  tid   | vtid
     | pid | granted\n-- ---------------+--------------------+--------------------------+--------+-------+-----+---------\n--
     relation      | pg_locks           | AccessShareLock          |        | 2/321
    | 253 | t\n--  relation      | schema_migrations  | RowExclusiveLock         |
           | 2/321 | 253 | t\n--  virtualxid    |                    | ExclusiveLock
               |        | 2/321 | 253 | t\n--  relation      | weather_city_index
    | AccessExclusiveLock      |        | 2/321 | 253 | t\n--  relation      | schema_migrations
     | ShareUpdateExclusiveLock |        | 2/321 | 253 | t\n--  transactionid |  
                     | ExclusiveLock            | 283863 | 2/321 | 253 | t\n--  relation
         | weather            | ShareLock                |        | 2/321 | 253 |
    t\n-- (7 rows)\n```\n\nLet's go through each of these:\n\n1. `relation | pg_locks
    | AccessShareLock` - This is us querying the `pg_locks` table in the transaction
    so we can see which locks are taken. It has the weakest lock which only conflicts
    with Access Exclusive which should never happen on the internal `pg_locks` table
    itself.\n2. `relation | schema_migrations | RowExclusiveLock` - This is because
    we're inserting a row into the &quot;`schema_migrations`&quot; table. Reads are
    still allowed, but mutation on this table is blocked until the transaction is
    done.\n3. `virtualxid | _ | ExlusiveLock` - Querying `pg_locks` created a virtual
    transaction on the SELECT query. We can ignore this.\n4. `relation | weather_city_index
    | AccessExclusiveLock` - We're creating the index, so this new index will be completely
    locked to any reads and writes until this transaction is complete.\n5. `relation
    | schema_migrations | ShareUpdateExclusiveLock` - This lock is acquired by Ecto
    to ensure that only one mutable operation is happening on the table. This is what
    allows multiple nodes able to run migrations at the same time safely. Other processes
    can still read the `schema_migrations` table, but you cannot write to it.\n6.
    `transactionid | _ | ExclusiveLock` - This lock is on a transaction that is happening;
    in this case, it has an Exclusive Lock on itself; meaning that if another transaction
    occurring at the same time conflicts with this transaction, the other transaction
    will acquire a lock on this transaction so it knows when it's done. I call this
    &quot;lockception&quot;. \U0001F642\U0001F92F\n7. `relation | weather | ShareLock` -
    Finally, the reason why we're here. Remember, we're creating a unique index on
    the &quot;weather&quot; table without concurrency. This lock is our red flag \U0001F6A9.
    Notice it acquires a ShareLock on the table. This means it blocks writes! That's
    not good if we deploy this and have processes or web requests that regularly write
    to this table. `UPDATE`, `DELETE`, and `INSERT` acquire a RowExclusiveLock which
    conflicts with the ShareLock.\n\nTo avoid this lock, we change the command to
    `CREATE INDEX CONCURRENTLY ...`; when using `CONCURRENTLY,` it prevents us from
    using database transactions which is unfortunate because now we cannot easily
    see the locks the command obtains. We know this will be safer however because
    `CREATE INDEX CONCURRENTLY` acquires a ShareUpdateExclusiveLock which does not
    conflict with RowExclusiveLock ([See Reference Material](/phoenix-files/backfilling-data#reference-material)).\n\nThis
    scenario is revisited later in [Part 3 - Migration Recipes](/phoenix-files/migration-recipes).\n\n##
    Safeguards in the database\n\nIt's a good idea to add safeguards so no developer
    on the team accidentally locks up the database for too long. Even if you know
    all about databases and locks, you might have a forgetful day and try to add an
    index non-concurrently and bring down production. Safeguards are good.\n\nWe can
    add one or more safeguards:\n\n1. Automatically cancel a statement if the lock
    is held for too long. There are two ways to do this:\n    1. Apply to migrations.
    This can be done with a `lock_timeout` inside a transaction.\n    1. Apply to
    any statements. This can be done by setting a `lock_timeout` on a Postgres role.\n1.
    Automatically cancel statements that take too long. This is broader than #1 because
    it includes _any_ statement, not just locks.\n\nLet's dive into these safeguards.\n\n##
    Add a lock_timeout\n\nOne safeguard we can add to migrations is a lock timeout.
    A lock timeout ensures a lock will not last more than n seconds. This way, when
    an unsafe migration sneaks in, it only locks tables to updates and writes (and
    possibly reads) for n seconds instead of indefinitely when the migration finishes.\n\nFrom
    the Postgres docs:\n\n> Abort any statement that waits longer than the specified
    amount of time while attempting to acquire a lock on a table, index, row, or other
    database object. The time limit applies separately to each lock acquisition attempt.
    The limit applies both to explicit locking requests (such as LOCK TABLE, or SELECT
    FOR UPDATE without NOWAIT) and to implicitly-acquired locks. If this value is
    specified without units, it is taken as milliseconds. A value of zero (the default)
    disables the timeout.\n>\n> Unlike `statement_timeout`, this timeout can only
    occur while waiting for locks. Note that if `statement_timeout` is nonzero, it
    is rather pointless to set `lock_timeout` to the same or larger value, since the
    statement timeout would always trigger first. If `log_min_error_statement` is
    set to ERROR or lower, the statement that timed out will be logged.\n\nThere are
    two ways to apply this lock:\n\n1. localized to the transaction\n2. default for
    the user/role\n\nLet’s go through those options:\n\n### Transaction lock_timeout\n\nIn
    SQL:\n\n```sql\nSET LOCAL lock_timeout TO '5s';\n```\n\nLet's move that to an
    [Ecto migration transaction callback](https://hexdocs.pm/ecto_sql/Ecto.Migration.html#module-transaction-callbacks).
    Since this `lock_timeout` will be in a database transaction for Postgres, we will
    use `SET LOCAL lock_timeout` so that the `lock_timeout` only alters this database
    transaction and not the session.\n\nYou can set a lock timeout in every migration:\n\n```elixir\ndef
    after_begin do\n  # execute/2 can be ran in both migration directions, up/down.\n
    \ # The first argument will be ran when migrating up.\n  # The second argument
    will be ran when migrating down. You might give yourself\n  # a couple extra seconds
    when rolling back.\n  execute(\"SET LOCAL lock_timeout TO '5s'\", \"SET LOCAL
    lock_timeout TO '10s'\")\nend\n```\n\nBut this can get tedious since you'll likely
    want this for every migration. Let's write a little macro to help with this boilerplate
    code.\n\nIn every migration, you'll notice that we `use Ecto.Migration` which
    inserts some code into your migration. Let's use this same idea to inject a boilerplate
    of our own and leverage an option to set a lock timeout. We define the [after_begin](https://hexdocs.pm/ecto_sql/Ecto.Migration.html#c:after_begin/0) callback
    to set the lock timeout.\n\n```elixir\ndefmodule MyApp.Migration do\n  defmacro
    __using__(opts) do\n    lock_timeout = Keyword.get(opts, :lock_timeout, [up: \"5s\",
    down: \"10s\"])\n\n    quote do\n      use Ecto.Migration\n\n      if unquote(lock_timeout)
    do\n        def after_begin do\n          execute(\n            \"SET LOCAL lock_timeout
    TO '#{Keyword.fetch!(unquote(lock_timeout), :up)}'\",\n            \"SET LOCAL
    lock_timeout TO '#{Keyword.fetch!(unquote(lock_timeout), :down)}'\"\n         
    )\n        end\n      end\n    end\n  end\nend\n```\n\nAnd adjust our migration:\n\n```diff\ndefmodule
    MyApp.Repo.Migrations.CreateTestTable do\n-  use Ecto.Migration\n+  use MyApp.Migration\n\n 
    def change do\n    # my changes\n  end\nend\n```\n\nNow the migrations will only
    be allowed to acquire locks up to 5 seconds when migrating up and 10 seconds when
    rolling back. Remember, these callbacks are  **not called** when `@disable_ddl_transaction` is
    set to `true`.\n\nYou can override the lock timeout if needed by passing in options:\n\n```elixir\n#
    disable the lock_timeout\nuse MyApp.Migration, lock_timeout: false\n\n# or change
    the timeouts\nuse MyApp.Migration, lock_timeout: [up: \"10s\", down: \"20s\"]\n```\n\n###
    Role-level lock_timeout\n\nAlternatively, you can set a lock timeout for the user
    in all commands:\n\n```sql\nALTER ROLE myuser SET lock_timeout = '10s';\n```\n\nIf
    you have a different user that runs migrations, this could be a good option for
    that migration-specific Postgres user. The trade-off is that Elixir developers
    won't see this timeout as they write migrations and explore the call stack since
    database role settings are in the database which developers don't usually monitor.\n\n##
    Statement Timeout\n\nAnother way to ensure safety is to configure your Postgres
    database with statement timeouts. These timeouts apply to _all_ statements, including
    migrations and the locks they obtain.\n\nFrom Postgres docs:\n\n> Abort any statement
    that takes more than the specified amount of time. If `log_min_error_statement`
    is set to ERROR or lower, the statement that timed out will also be logged. If
    this value is specified without units, it is taken as milliseconds. A value of
    zero (the default) disables the timeout.\n>\n> The timeout is measured from the
    time a command arrives at the server until it is completed by the server. If multiple
    SQL statements appear in a single simple-Query message, the timeout is applied
    to each statement separately. (PostgreSQL versions before 13 usually treated the
    timeout as applying to the whole query string.) In extended query protocol, the
    timeout starts running when any query-related message (Parse, Bind, Execute, Describe)
    arrives, and it is canceled by completion of an Execute or Sync message.\n\nYou
    can specify this configuration for the Postgres user. For example:\n\n```sql\nALTER
    ROLE myuser SET statement_timeout = '10m';\n```\n\nNow any statement automatically
    times out if it runs for more than 10 minutes; opposed to running indefinitely.
    This can help if you accidentally run a query that runs the database CPU hot,
    slowing everything else down; now the unoptimized query will be limited to 10
    minutes or else it will fail and be canceled.\n\nSetting this `statement_timeout` requires
    discipline from the team; if there are runaway queries that fail (for example)
    at 10 minutes, an exception will likely occur somewhere. You will want to equip
    your application with sufficient logging, tracing, and reporting so you can replicate
    the query and the parameters it took to hit the timeout, and ultimately optimize
    the query. Without this discipline, you risk creating a culture that ignores exceptions.\n\n<%=
    partial \"shared/posts/cta\", locals: {\n  title: \"Fly ❤️ Elixir\",\n  text:
    \"Fly is an awesome place to run your Elixir apps. Deploying, clustering, connecting
    Observer, and more are all supported!\",\n  link_url: \"https://fly.io/docs/elixir/\",\n
    \ link_text: \"Deploy your Elixir app today!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>\"\n}
    %>\n\n## Where to next?\n\nNext we dig into running your [migrations in a Mix
    Release project](/phoenix-files/how-to-migrate-mix-release-projects)!\n"
- :id: blog-fly-io-is-hiring-rust-developers
  :date: '2021-11-04'
  :category: blog
  :title: Fly.io Is Hiring Rust Developers
  :author: thomas
  :thumbnail: jobs-cover-01-thumbnail.png
  :alt:
  :link: blog/fly-io-is-hiring-rust-developers
  :path: blog/2021-11-04
  :body: "\n\n<div class=\"lead\">\n  Fly.io runs apps close to users. We transmogrify
    Docker containers into Firecracker micro-VMs that run on our hardware around the
    world, and connect all of them to a global Anycast network that picks up requests
    from around the world and routes them to the nearest VM. The easiest way to learn
    more is to sign up. It’ll take you just a minute or two to [get up and running](https://fly.io/docs/speedrun/).\n</div>\n\nThe
    platform that makes all this stuff work, the engine of our system, is written
    in two different systems languages: Rust and Go. Go code powers our orchestration;
    it’s what [converts Docker images and provisions VMs](https://fly.io/blog/docker-without-docker/).
    Rust code drives `fly-proxy`, our Anycast network.\n\nWe’re looking for people
    who want to work on `fly-proxy`.\n\n`fly-proxy` is an interesting piece of code.
    A request for some Fly.io app running in Dallas and Sydney arrives at our edge
    in, say, Toronto. We need to get it to the closest VM (yes, in this case, Dallas).
    So `fly-proxy` needs a picture of where all the VMs are running, so it can make
    quick decisions about where to bounce the request.\n\nWe don’t simply forward
    those requests, though. `fly-proxy` runs on both our (external-facing) edge hosts
    and our (internal) workers, where the VMs are. It builds on-the-fly, multiplexed
    HTTP2 transports (running over our [internal WireGuard mesh](https://fly.io/blog/incoming-6pn-private-networks/))
    to move requests over. The “backhaul” configuration running on the worker demultiplexes
    and routes the request over a local virtual interface to the VM.\n\nIt gets more
    interesting in every direction you look. For instance: we don’t just route simple
    HTTP requests; we also do raw TCP (no HTTP2 for that forwarding path). And WebSockets.
    All these requests get balanced (most of our users run a bunch of instances, not
    just one). And in the HTTP case, we automatically configure HTTPS, and [get certificates
    issued with the LetsEncrypt ALPN challenge](https://fly.io/blog/how-cdns-generate-certificates/).\n\nZoom
    in on the raw request routing and there’s still more stuff going on. `fly-proxy`
    is build on [Tokio](https://fly.io/blog/the-tokio-1-x-upgrade/), [Hyper](https://github.com/hyperium/hyper),
    and [Tower](https://docs.rs/tower/0.4.10/tower/). A single `fly-proxy` is managing
    connectivity for lots and lots of Fly.io apps, and isolates the concurrency budget
    for each of those apps, so a busy app can’t starve the other apps. We’re [tracking
    metrics for each of those apps](https://fly.io/blog/measuring-fly/) and making
    them accessible to users.\n\n`fly-proxy` also has some fun distsys problems. That
    global picture of where the VMs are is updating all the time. So too are the load
    stats for all those VMs, which impact how we balance requests. Requests can fail
    and get retried automatically elsewhere; in fact, that’s the core of [how we do
    distributed Postgres](https://fly.io/blog/globally-distributed-postgres/).\n\nAll
    this is before we get to the [eBPF code that runs alongside the proxy](https://fly.io/blog/bpf-xdp-packet-filters-and-udp/)
    to make low-level networking things work.\n\nIt’s gnarly and technical and always-on
    and regularly updated but also needs to have high uptime and if this sounds fun,
    congradu-dolences! This might be the gig for you.\n\n## Things To Know About Us\n\n-
    We’re a small team, almost entirely technical, and everyone wears a lot of hats.
    You’ll have the opportunity to get your hands dirty in a lot of different things
    here. But first and foremost, this is The Rust Job at Fly.io.\n- We&#39;re at
    a stage where our engineering team will feel more like working on a big open source
    project than on a buttoned-down engineering team. Good things and bad things about
    that. You want to be comfortable working without a roadmap or an MRD, and with
    finding useful stuff to build.\n- We’re remote, with team members in Colorado,
    Quebec, Chicago, London, Virginia, Rwanda, Spain, Brazil, and Utah.\n- We’re an
    unusually public team, with an online community (at [community.fly.io](https://community.fly.io/))
    that we try to be chatty with. If we’re doing things right, this role will likely
    increase your public profile.\n- We’re a team, not a family, but we have families
    and want to be the kind of place where work doesn’t get in the way of that.\n-
    We’re a real company – we hope that goes without saying – and this is a real,
    according-to-Hoyle full-time job with health care for US employees, flexible vacation
    time, hardware/phone allowances, the standard stuff. The comp range for this role
    is $160k-$200k, plus equity. \n\n## How We Hire People\n\nWe are weird about hiring.
    We’re skeptical of resumes and we don’t trust interviews (we’re happy to talk,
    though). We respect career experience but we aren’t hypnotized by it, and we’re
    thrilled at the prospect of discovering new talent.\n\nThe premise of our hiring
    process is that we’re going to show you the kind of work we’re doing and then
    see if you enjoy actually doing it; “work-sample challenges”. Unlike a lot of
    places that assign “take-home problems”, our challenges are the backbone of our
    whole process; they’re not pre-screeners for an interview gauntlet.\n\nFor this
    role, we’re asking people to write us a small proxy that does just a couple of
    interesting things (we’ll tell you more). We’re looking for people who are super-comfortable
    with Rust and network programming in general, but we’re happy to bring people
    up to speed with the domain-specific stuff in Fly.io.\n\nIf you’re interested,
    mail ~~jobs+rust&#64;fly.io~~. You can tell us a bit about yourself, if you like.
    Either way, we’ll ask you tell us what your least favorite Rust crate is (it can
    be a good crate, just one you didn’t have fun working with). We’re happy to answer
    questions: send them along!\n\nThere are lot of cool directions to take `fly-proxy`
    in. It&#39;s a big deal to us. We&#39;re psyched to talk to you about it.\n"
- :id: blog-how-safari-ruined-my-tuesday
  :date: '2021-10-21'
  :category: blog
  :title: How Safari Ruined My Tuesday
  :author: chris
  :thumbnail:
  :alt:
  :link: blog/how-safari-ruined-my-tuesday
  :path: blog/2021-10-21
  :body: "\n\n<div class=\"lead\">Fly.io runs apps close to users, by transmuting
    Docker containers into micro-VMs that run on our own hardware around the world.
    This is a post about a bug in Safari, but if you just want to ship a Phoenix app,
    the easiest way to learn more is to [try it out](https://fly.io/docs/speedrun/);
    you can be up and running in just a couple minutes.</div>\n\nSafari 15 shipped
    in September 2021 and included an obscure CSS bug that broke most LiveView applications.
    The LiveView client operates in two modes – connected, when it can talk to the
    server over websockets, and disconnected when offline. When LiveView disconnects,
    it's useful to communicate the change to users. CSS includes a handy `pointer-events`
    attribute that makes buttons and links flip to non-interactive. Set `pointer-events:
    none`, and elements won't get a pointer cursor or visibly respond to clicks.\n\nCSS
    cascades (it's literally in the name!) so we can make a bunch of elements non-interactive
    at once by adding this rule, then toggling a `.phx-disconnected` class on container
    elements:\n\n```css\n.phx-disconnected *{ pointer-events: none; }\n```\n\nThis
    has been part of the default `app.css` in LiveView apps for three years. So we're
    some of the first to feel the pain of a Safari 15 bug that makes `pointer-events:
    none` stick – forever. It stays `none` even if you update classes or explicitly
    overwrite `pointer-events`. Which means, LiveView stays non-interactive for Safari
    users even when we reconnect.\n\nIn the whole of the internet, there's only one,
    lonely [Stack Overflow thread](https://stackoverflow.com/questions/69450411/setting-pointer-events-dynamically-on-ios-15-safari-is-unreliable-and-unpredicta)
    talking about this issue affecting mobile Safari. And it includes tiny HTML and
    CSS example reproduction. You can see the bug in action below. The link remains
    in a `none` state an clicks aren't registered when we remove the class.\n\n<%=
    video_tag \"safari-fail.mp4?card\" %>\n\nThis was an annoyingly difficult issue
    to debug. Safari's web inspector will tell you the computed property of the elements
    is pointer-events: auto, but it's lying. Elements won't respond to click events.
    Bad times. It's like living with IE6 again\n\nThere _is_ a workaround for LiveView
    apps, though. Removing `point-events: none;` from `app.css` and carry on. LiveView
    already ignores on clicks on Phoenix links and buttons when disconnected, so `pointer-events:
    none` is mostly redundant. Phoenix LiveView 0.17 works around this issue – by
    using an entirely different class name for disconnected state. \n\nSadly, this
    isn't an elegant breaking change. But it's a fix that allows us to avoid applying
    the erroneous CSS rules from existing user-land CSS while still supporting custom
    styling of loading states. It looks like no bug is currently being tracked on
    the Webkit issue tracker, but the latest Safari Development Preview build might
    fix the underlying bug. Maybe even accidentally. Safari release cycles are quite
    slow, though, so a breaking CSS change in LiveView is the best way to reduce the
    end user impact.\n\nI spent my entire Tuesday tracking this one down. I'll be
    able to write Elixir again someday, right? \U0001F604 In the meantime, feel free
    to read a _mildly_ frustrated [commit message](https://github.com/phoenixframework/phoenix_live_view/commit/7205df526863b8f391a432ff93700d06e951dfc0).\n\n"
- :id: blog-32-bit-real-estate
  :date: '2021-10-18'
  :category: blog
  :title: 32 Bit Real Estate
  :author: kurt
  :thumbnail: ipbank-thumbnail.png
  :alt:
  :link: blog/32-bit-real-estate
  :path: blog/2021-10-18
  :body: "\n\n<div class=\"lead\">Fly.io runs apps close to users, by transmuting
    Docker containers into micro-VMs that run on our own hardware around the world.
    This is a post about one of the major costs of running a service like ours, but
    if you're more interested in how Fly.io works, the easiest way to learn more is
    to [try it out](https://fly.io/docs/speedrun/); you can be up and running in just
    a couple minutes.</div>\n\nTwo obvious costs of running Internet apps for users
    on your own hardware: hardware and bandwidth. We buy big servers and run them
    in racks at network providers that charge us to route large volumes of traffic
    using BGP4 Anycast. You probably have at least a hazy intuition for what those
    costs look like.\n\nBut there's a non-obvious cost to what we do: we need routable
    IPv4 addresses. To make Anycast work, so users in Singapore are routed to Singapore
    instances, particularly for non-HTTP applications, like UDP DNS servers or TCP
    game servers, we assign distinct public IPv4 addresses to each app running on
    Fly.io.\n\nYou don't have to think about any of this stuff when you deploy on
    Fly.io. We just assign apps addresses and get out of the way. But we have to think
    a lot about this stuff, because IPv4 addresses cost money. \n\n## You can't, like,
    own a number, man!\n\nLet's take a second to reflect on what it means to acquire
    an IPv4 address. To own an IP address is to control what it routes to. The rock-bottom
    source of truth for IPv4 routing is BGP4. You announce IP address prefixes over
    IPv4 to your peers, your peers relay those announcements, and the world learns
    that traffic for your addresses needs to be sent your way.\n\nLots of providers
    will accept a BGP4 peering session with a customer. But no reputable provider
    will take a random announcement over that session. You'll need to demonstrate
    authority over the prefixes you plan to announce.\n\nTechnically, nobody owns
    an IPv4 address. They're administered, ostensibly as a public benefit, by the
    [5 Regional Internet Registries](https://en.wikipedia.org/wiki/Regional_Internet_registry)
    --- ARIN, RIPE, APNIC, AFRINIC, and LACNIC. In the long long ago, it was the job
    the RIRs to allocate blocks of addresses to providers. Today, their IPv4 hives
    are void of honey, and their new job is just to keep track of who's claiming which
    long-allocated blocks as they get shuffled around between their new, er, stewards.\n\nAll
    this is to say, if you want to \"own\" an IPv4 address, you find its previous
    owner and get them to arrange a transfer with an RIR. [There is paperwork](https://www.arin.net/blog/2016/07/07/origin-as-an-easier-way-to-validate-letters-of-authority/)
    [involved](https://www.arin.net/resources/guide/quickguide.pdf), and it's sometimes
    whispered, [a usage justification.](https://www.arin.net/participate/policy/nrpm/#8-transfers)
    \n\nWith the transfer completed, your newly \"acquired\" IPv4 addresses will be
    associated with your BGP4 ASN (oh, you'll need to shell out a couple hundred bucks
    for the ASN, too), and you can hope that an upstream provider will accept announcements
    for them. The addresses are now \"yours\".\n\nFor illustration: here's a block
    of IPv4 addresses we control [in IRR Explorer](https://irrexplorer.nlnog.net/prefix/77.83.142.0).
    \n\n## Certain Plans Require Additional Gribbles\n\nSo, Anycast. Easy. You have
    a routable IPv4 address. You have machines in a bunch of places. In each of those
    places, you peer BGP4 with your upstream and advertise that exact same IPv4 address.
    Global BGP4 routing ensures that when people try to connect to to that address,
    they'll be sent to the \"closest\" machine. That's how Fly.io's Anycast network
    functions: we have blocks of addresses, machines [in a bunch of regions](https://fly.io/docs/reference/regions/),
    BGP4 peering in each of them, and a Rust-based proxy that picks up connections
    for those addresses.\n\nYou can think of ways to work around needing routable
    IPv4 addresses for Anycast applications. None of them work for us.\n\nHere's the
    first thing that doesn't work: IPv6. Let's get it out of the way first. [Yes,
    there is such a thing as IPv6.](https://arstechnica.com/information-technology/2007/07/the-declaration-of-ipv6-independence/)
    Yes, it has enough address space to give distinct addresses to each electron in
    every atom of every living human. And, yes, we [do a lot with IPv6](https://fly.io/blog/incoming-6pn-private-networks/),
    including allocating public IPv6 prefixes to all of our applications. We're IPv6
    fans.\n\nBut if I had to place a bet, it'd be that you aren't using IPv6. And
    you're nerdy enough to be reading a blog post on how much IPv4 addresses cost!
    The silent majority of non-nerd users are nowhere close to being reliably IPv6-connected.\n\nThe
    next thing that doesn't work is name-based hosting. Applications can share IPv4
    addresses using TLS SNI. We could park most apps on a single address and just
    let browsers tell our proxies which apps people are looking for. So far so good:
    the people who want their own IPv4 addresses would be flying business class, and
    we could charge them 2x as much.\n\n<div class=\"callout\">**A word about market
    segmentation.** As a software business operator, you have two basic goals: you
    want everyone to use your software, and you want to earn profits. Those goals
    are in tension. The classic b-school 101 answer is to identify the customers who
    derive the most value from your software. Now figure out their distinctive use
    cases, charge more for them, and less for everything else. I can keep explaining
    this poorly, but [Joel Spolsky has already explained it well, and you should go
    read his post](https://www.joelonsoftware.com/2004/12/15/camels-and-rubber-duckies/).\n\nThere's
    a standard market-seg playbook for SAAS companies. You've seen it every time you've
    looked at a product pricing grid. Two of the most infamous segmentation premiums
    are HTTPS (thankfully, this is going extinct) and the [SSO tax](https://sso.tax/)
    (still very much alive).\n\nWe're not above market segmentation. If you're a billion
    dollar company, rest assured, we will find ways to extract money from you! But
    IPv4 addresses aren't one of those ways, because charging extra for them gets
    in everyone's way. For what it's worth: when enough people ask, we'll ship SSO.
    But we won't tax it.</div>\n\nWe like money as much as anyone else. But SNI breaks
    down for apps that don't use TLS, and we want to support all kinds of apps, not
    just web apps. Now, most Fly.io apps are web apps. And we know which apps are
    which. So we could use SNI for the majority of our apps and IPv4 addresses for
    the exceptions. But that feels weird, and worse, means we'd be maintaining multiple
    provisioning paths for customers. It's not worth the hassle, for us or our customers,
    so IPv4 addresses are just bundled in for everyone.\n\n## IPv4: The Internet's
    Real Estate Market\n\nThere are lot of organizations holding addresses they don't
    need. Internet routing used to be real, real dumb, and to give a big company enough
    addresses to number their machines, you often had to give them way more addresses
    than they really needed. Apple has a /8 – that's 256 /16s, each of which has 256
    /24s, each of which is 256 addresses. So does Prudential Insurance(?!), and so
    does Ford. Lots of other organizations had \"smaller\" blocks that are still unfathomably
    large to modern sensibilities. So, like a crappy cryptocurrency, IPv4 is pre-mined.\n\nIf
    you [invested in IP addresses 15 years ago](https://labs.ripe.net/author/gih/valuing-ip-addresses/),
    you're doing pretty well. If you invested in IP addresses 15 months ago, you're
    also doing well.\n\nWhen we wrote the first draft of this post, the going rate
    for smaller IPv4 blocks was $45 per IP address. was the rate in September of 2021.
    We looked just now, and the spot price was $50. We give up quoting them; by the
    time you get to this sentence, they'll have gotten more expensive. [Here's some
    recent transactions](https://auctions.ipv4.global/prior-sales/).\n\nDid you know
    big cloud providers publish their IP ranges? They do this because it's useful
    for automating firewalls. It's also useful if you want to guess how much IPv4
    wealth they're sitting on. You could even [write a Ruby script](https://gist.github.com/mrkurt/6f2123434cea8a1e47ad21aa9664de12) to
    tell you. Which I did. (Dan Goldin [did the same thing](https://dangoldin.com/2020/12/11/amazon-owns-more-than-2b-worth-of-ipv4-addresses/)
    6 months before me).\n\nHere's what I saw in March of 2021 when IPv4s cost $25
    each:\n\n| **Provider** | Blocks | Total IPs | Estimated Value |\n| --- | ---
    | --- | --- |\n| **AWS** | 3,008 | 55,708,181 | ~$1,392,704,525 |\n| **Google
    Cloud** | 388 | 7,582,976 | ~$189,574,400 |\n| **DigitalOcean** | 1,581 | 2,368,896
    | ~$59,222,400 |\n\nAnd here's what it looked like in September of 2021:\n\n|
    **Provider** | Blocks | Total IPs | Estimated Value |\n| --- | --- | --- | ---
    |\n| **AWS** | 4,438 | 65,322,182 | ~$2,939,498,190 |\n| **Google Cloud** | 442
    | 9,100,288 | ~$409,512,960 |\n| **DigitalOcean** | 1,601 | 2,528,640 | ~$113,788,800
    |\n\nThese estimates are low: Amazon [announces more than 100 million IP addresses](https://toonk.io/aws-and-their-billions-in-ipv4-addresses/index.html).
    That's 2.4% of all possible IPv4 addresses!\n\nIP prices vary by block size. It's
    helpful to think of a 256-address /24 as the irreducible block size. That /24
    is 10-15% less expensive than a /18, which is 64 contiguous /24s. There are reasons
    for the price difference, and we don't know all of them. One big reason is that
    if you're doing classical network engineering and you need to number 10,000 hosts,
    the routing is easier to do with a /18 than with 40 /24s. Another reason is probably
    vanity.\n\n<div class=\"callout\">\nA fun, useless fact: time was when there were
    particularly valuable /24s. Backbone routing was done\nprincipally with Cisco
    7500 routers, which used [TCAM memory](http://www.bgpexpert.com/article.php?article=145)
    --- think \"hardware hash table\" --- to make\nquick forwarding decisions. TCAM
    space was limited, and the Tier 1 network providers used BGP filters to\nenforce
    a maximum prefix length; for instance, in the late 1990s, Sprint wouldn't forward
    anything longer than a /19, which meant in effect that if you wanted to be reachable
    to anyone through Sprint, you needed at least a /19 to play.\n\nBut because IPv4
    allocation had been so janky in the early 1990s (it literally only worked on octet
    boundaries, so you either had a /24 or you had a /16, with nothing in between),
    there were legacy prefixes\nthat had to be grandfathered in through the filters.
    These were known as \"swamp space\"; a swamp /24 was announceable across the Internet,
    even though a commodity /24 wasn't.\n\n[None of this matters anymore](https://www.apnic.net/manage-ip/manage-resources/address-status/min-prefix/),
    and you can announce all the /24s you want today.\n</div>\n\nAnyways, AWS's _average_
    block comprises 18,000 IPv4s. Bonkers. Giant booksellers who sell cloud services
    as a side hustle don't even bother with IPv6. Here is a complete list of the IPv6
    addresses `dig aaaa amazon.com` returns:\n\n```cmd\ndig aaaa amazon.com +short
    \                                                          \n```\n```output\n```\n\n###
    Financial Network Engineering\n\nImagine for a moment that you're us. Take a drink,
    [eat an Italian beef sandwich](https://www.chicagomag.com/chicago-magazine/march-2019/suburbs-guide/johnnies-beef-elmwood-park/),
    pet your dog. Now get apps working for developers in, say, twenty different cities.
    You can buy a beast of a server for $20k USD, chop it up into 500+ virtual machines,
    and build an API for turning those on and off very quickly. People are pretty
    happy to pay for virtual machine time; you're almost there.\n\nVMs are mostly
    useful when they can talk to the Internet. Your developers need IP addresses.
    Say it costs $45 for a single IP address. That means it costs $23,040 to handle
    512 virtual machines with IPv4 addresses. We've now doubled our costs.\n\nBut
    hold up for a second.\n\nServers depreciate; that's their job. That $20k server
    depreciates to $500 relatively quickly. But IP blocks are, at least for now, appreciating.
    Alarmingly.\n\nThis is a hard market to time. Nobody believes the Internet will
    be IPv6-only within the next few years. There are credible people who believe
    IPv4 addresses will be scarce and useful indefinitely. We might put money on you
    getting away with an IPv6-only app 20 years from now. But we'd have said that
    20 years ago, too.\n\nSo, unfortunately, the smart thing to do if you own $1B
    worth of IPv4 addresses is to buy $1B more IPv4 addresses. Fortunately, everyone
    is starting to recognize this, so you have some flexibility in financing.\n\nIf
    you're a farmer and you need a new tractor, you probably don't just go buy it
    in cash. You go to a bank and get a loan, collateralized by the value of the tractor.
    If you're a company with $500,000 in receivables, you don't necessarily have to
    wait for your customers to remit payments; financial institutions will loan you
    money collateralized by your invoices. And, of course, when you buy a house, you
    live in it while you pay back a mortgage to the bank.\n\nYour local credit union
    won't collateralize a loan on IPv4 blocks like it would for a car or tractor.
    IPv4 addresses are just numbers, man. Logan Paul can convince unsophisticated
    people to put a dollar value on  [completely useless numbers](https://twitter.com/smdiehl/status/1445795667826208770).
    But your bank isn't so forward thinking, and the conversation putting a $50 value
    to a single 32-bit number isn't going to be productive.\n\nBut startups have another
    option: venture debt. Banks will happily lend startups money based almost entirely
    on the credibility of a startup's investors, and, transitively, their blog posts.
    When a company raises a $12MM round, they'll spend the next few weeks raising
    $3MM of debt from banks friendly with that firm.\n\nVenture debt is surprisingly
    simple. It has an interest rate, which can be pretty close to prime. It usually
    includes an option grant – the bank gets a small amount of equity (much less than
    employees get). And venture debt typically requires that you do all your banking
    with the bank that gave you the loan. Unlike raising a funding round, once you've
    got investors, there isn't much pitching involved in raising debt. Banks win or
    lose venture debt deals based on how much certain founders like their web interface.
    It's not _not_ the most important factor when choosing a bank.\n\nAll this is
    to say, you can in a sense take out a mortgage on a block of IP addresses.\n\n<%=
    partial \"shared/posts/cta\", locals: {\n  title: \"Fly.io apps get routable IPv4
    addresses.\",\n  text: \"You can run more than just an HTTPS app here; DNS servers,
    game servers, and media relays work too. If your app has a Docker container, you
    \nshould be able to get it running here easily.\",\n  link_url: \"https://fly.io/docs/getting-started/\",\n
    \ link_text: \"Deploy your app in minutes.&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>\"\n}
    %>\n\n## How much sense does this make long-term?\n\nIPv6 will be viable – most
    people will be able to run apps with no IPv4 addresses – someday. Could happen
    in 2 years. Far more likely to happen in 20.\n\nThe kinds of companies that benefit
    the most from a decisive transition to IPv6 tend to look a lot like us: new, small,
    minimal legacy infrastructure. Not coincidentally, those companies also tend not
    to have much market power. Strong incentive, weak market power: not exactly a
    recipe for change.\n\nThere are other reasons to think IPv4 addresses will retain
    value. Path dependence is one of them; `.COM` names are still pricey despite the
    universal acceptance of new TLDs. Switching costs are another; lots of very large
    networks are IPv4-only, some with significant numbers of nodes that can't easily
    be dual-stack. It's easy to imagine a world in which IPv6-only is viable, but
    IPv4 addresses remain scarce.\n\nStill, IPv4 will be obsolete some day. Addresses
    will increase in value and then one night, while we're all sleeping, they'll become
    worthless.\n\nThis could be a knock against using a bunch of debt to pay for IP
    assignments, or not.\n\nAn alternative is leasing: companies of varying levels
    of sketchiness will happily lease IP blocks. We've had a bunch of offers for two
    bucks per month per address. Maybe we could negotiate that down to one bucks.
    A three year lease for 65,000 IP addresses at $1/mo each works out to $2.34 million.
    Four years is $3.12 million.\n\nWe'd almost certainly get more addresses by leasing.
    And we wouldn't have the inventory risk of holding thousands of IPv4 addresses
    that could conceivably be worth $0 if IPv6 takes over. But leases have fixed terms,
    and running apps don't. Meanwhile, we do know empirically that people hate renumbering.
    So do the IPv4 lessors. We'd be in no position to negotiate a good rate on lease
    renewal – in fact, we'd be in a bad position even if IPv4 addresses got much cheaper.
    We want to know that when we assign an address to an app, the people running that
    app will get to keep using that address indefinitely. Leasing isn't worth the
    hassle for us.\n\n## IPv4 is exhausting\n\nAt scale, routable IPv4 addresses are
    surprisingly expensive. But they're appreciating. Like an office building or a
    house, we have tools for financing them that we don't for machines or salaries.
    So, part of our business is acquiring addresses outright, and attaching them to
    apps people run here.\n\nFor most ordinary apps, this doesn't matter much; HTTPS
    web applications would run just as well with SNI and shared (or temporary) addresses
    as they do with permanent IPv4 allocations. But not all apps are ordinary, and
    we want the oddball apps to work well on Fly.io without anyone having to think
    much about them.\n\nMostly, we just believe that people who run apps on Fly.io
    now will still be running them in 10 years, and Fly.io is better if we can keep
    that IP for them the entire time.\n\nReally, though, we're nerds, and we think
    it's funny that for all the talk of NFTs and ICOs, the Internet itself has been
    running a high-dollar token auction for the last 20 years. \n\n<div class=\"callout\">**One
    last thing**: a new 5-digit ASN will cost you about $500 from ARIN, but there
    are auctions for 4-digit ASNs, and they run into mid-five-figures. If any of you
    can explain this to us, we'd be grateful.</div>"
- :id: jobs-technical-writer
  :date: '2021-10-01'
  :category: jobs
  :title: Technical Writer
  :author: chris-n
  :thumbnail:
  :alt:
  :link: jobs/technical-writer
  :path: jobs/2021-10-01
  :body: "\nWe're doing something ambitious at Fly.io: a new public cloud, running
    on our own hardware around the world, designed to make it easy to run distributed
    and real-time apps close to users wherever they happen to be.\n\nWe do love writing
    here at Fly.io. Outside of docs, we write:\n\n1. [On our engineering blog](https://fly.io/blog/),
    in [long-](https://fly.io/blog/docker-without-docker/) and [short-form](https://fly.io/blog/building-a-distributed-turn-based-game-system-in-elixir/)
    pieces about [how our product works](https://fly.io/blog/ssh-and-user-mode-ip-wireguard/),
    with an audience of basically all the nerds on the Internet.\n1. On [our](https://fly.io/phoenix-files/)
    \ [framework](https://fly.io/laravel-bytes/)  [blogs](https://fly.io/ruby-dispatch/),
    where our Framework Specialists share guides and blog posts for the benefit of
    devs in their respective communities.\n1. In conversations with our users on our
    [community forum](https://community.fly.io/) and social media.\n\nOur readership
    is software developers; our product and the kinds of problems we work on are very
    technical and can be somewhat subtle.\n\nWe said \"aside from docs,\" up there.
    Docs are an important part of our product. We ship things for thousands of developers,
    and they use our platform in exceedingly clever ways. In many cases, docs are
    the only features that matter to these users. No docs, no product.\n\n[Our documentation](https://fly.io/docs/)
    is all public and online. If there are gaps, it's because there aren't enough
    hours in the day, not because we don't care. That's why we need you and your hours!\n\n##
    The job\n\nHere's what you'd be doing in this role:\n\n- Surveying our existing
    documentation, identifying and closing gaps (learning whatever random technical
    stuff it takes to write that stuff).\n- Working with our whole team to spot opportunities
    for public writing, about features we’re working on, or the inner workings of
    our stack, or the technology ecosystem we fit into, or whatever interesting thing
    is bugging someone on a particular day.\n- Working with our design team to create
    materials that communicate deeply technical ideas in cogent and appealing ways.
    \n- Interacting with users on the community forum. This is a great way to learn
    about how customers are experiencing our platform and where we could communicate
    better through docs.\n- Not gonna lie, you're going to be asked to do some proofreading
    and copy editing.\n\nTechnical writing at Fly.io isn't an anonymous cost-center
    role. We are emphatically not the kind of place where writing is seen as a necessary
    evil, something to get out of the way with the minimum possible investment.\n\n##
    Our writing tools\n\nThe vast majority of our public writing right now — that's
    docs and articles — goes through Middleman, a Ruby static site generator.\n\nFor
    new articles, we'll typically share a draft on Slab, discuss it on Discourse (we're
    big on async communication), and format it in Markdown and ERB. We publish by
    merging a pull request on a GitHub repository.\n\n## You may be a good fit for
    this role if you:\n\n- Can write excellent English, but know when to break the
    rules. You won't, like, flinch when you see \"like\" in the middle of a sentence.\n-
    Relish the challenge of arranging a pile of technical information into a logical,
    easy-to-digest package.\n- Get very suspicious when someone uses marketing buzzwords
    at you.\n- Enjoy having autonomy within a collaborative environment.\n- Are comfortable
    with programming in general, and comfortable with a high-level language like Ruby,
    Python, or Javascript. \n- Are happy diving into GitHub repos and talking to busy
    devs to get the goods.\n- Love learning new things. You won't be confined to a
    single focus, and you'll want a solid understanding to make sure the finished
    product is coherent.\n- Like the idea of a high-profile role working in public
    communicating directly with our users.\n- Want to work in a diverse and respectful
    team that values communication, glue work, and small, autonomous teams making
    decisions for themselves.\n\nMostly, we want to work with people who love to write
    and who find the problems we work on exciting. Global Anycast, modern language
    frameworks like Elixir/Phoenix, containers and virtualization, and security are
    all part of our beat.\n\n## Things to know about us\n\n- We're remote-first and
    hire around the world. That means a lot of asynchronous communication—and flexible
    hours.\n- We're still small, but growing fast. We're still building structures
    and processes. This can be good, and bad.\n- The vast majority of our communication
    is written, in English. This will probably suit you fine, if you're reading this.\n-
    We love what we do. But it's not our entire lives. We offer flexible time off,
    with a minimum.\n\n## How we hire people\n\nIf this sounds like an awesome gig
    (it is) but you're not sure if you should go for it because maybe you don't have
    the background we're looking for, you should go for it.\n\nWe are weird about
    hiring. We’re skeptical of resumes and we don’t trust interviews (we’re happy
    to talk, though). We respect career experience but we aren’t hypnotized by it,
    and we’re thrilled at the prospect of discovering new talent.\n\nThe premise of
    our hiring process is that we’re going to show you the kind of work we’re doing
    and then see if you enjoy actually doing it; “work-sample challenges”. Unlike
    a lot of places that assign “take-home problems”, our challenges are the backbone
    of our whole process; they’re not pre-screeners for an interview gauntlet.\n\nFor
    this role, we’re going to give a couple of writing assignments. We'll ask you
    to write up a how-to and a feature reference doc, adapting existing reference
    materials, and then have you hammer out a draft blog post about a hypothetical
    new feature.\n\nYou can read some of this again and more, over [at our hiring
    documentation](https://fly.io/docs/hiring/). It was written for software engineering
    positions, but all the good stuff applies to us in Content too.\n\nIf you're interested,
    mail jobs+techwriter@fly.io. You can tell us a bit about yourself, if you like.
    Either way, include a paragraph or two about the most frustrating technical blog
    post or article that you've read recently. Bonus points if we wrote it.\n"
- :id: blog-fly-io-is-hiring-technical-writers
  :date: '2021-10-01'
  :category: blog
  :title: Fly.io Is Hiring Technical Writers
  :author: thomas
  :thumbnail: jobs-cover-02-thumbnail.png
  :alt:
  :link: blog/fly-io-is-hiring-technical-writers
  :path: blog/2021-10-01
  :body: |2


    <div class="lead">
    Fly.io runs apps close to users. We take Docker containers and transmute them into Firecracker micro-vms that run on our hardware in data centers from Chennai to Syndey to Dallas. The easiest way to learn more is to sign up, and [you should try that now](https://fly.io/docs/speedrun/); it’ll take you just a minute or two to get up and running.
    </div>

    [We write a lot at Fly.io](https://fly.io/blog/). It’s our most important tool for reaching potential customers.

    We write mostly in 3 different places:

    1. [In our documentation](https://fly.io/docs/), which is all public and online, and which (of course) targets people already working with our product, and

    2. [On our engineering blog](https://fly.io/blog/), in [long-](https://fly.io/blog/docker-without-docker/) and [short-form](https://fly.io/blog/building-a-distributed-turn-based-game-system-in-elixir/) pieces about [how our product works](https://fly.io/blog/ssh-and-user-mode-ip-wireguard/), with an audience of basically all the nerds on the Internet.

    3. In conversations with our users on our [community forum](https://community.fly.io/) and places like [Twitter](https://twitter.com/mrkurt).

    We’re looking for people who want to do this kind of writing with us, full time.

    The predictable next thing for us to say is that we put a high value on writing. But, you know what turns out to be hard to write? A paragraph about how seriously you take writing. A paragraph like that needs to be pretty well-written itself. Its quality signals how seriously you take the process. Is this paragraph good enough? We could be here editing for all night.

    So, rather than talk about how we’re good writers (we aren’t), we’ll say instead that we’re opinionated, enthusiastic writers. Also, that writing isn’t just something the rest of the team throws over the wall to “writers” to do.

    More importantly, though: technical writing at Fly.io isn&#39;t an anonymous cost-center role. We&#39;ve all worked at places where writing was treated as a necessary evil, something to get out of the way with the minimum possible investment. We are emphatically not that kind of place. Anything we can do to raise your profile, we&#39;re going to want to do (and if you&#39;re a low-profile sort, you should keep that in mind).

    ## The Job, In Brief

    We have a couple problems we’d like to solve. First, our documentation is inconsistent and weak in places. And then, we have a lot of stuff to say about what we’re building and what it all means, but our team members with interesting things to talk about have wildly varying levels of comfort writing stuff.

    In this role, what you’d be doing is:

    - Surveying our existing documentation, identifying and closing gaps (learning whatever random technical stuff it takes to write that stuff), and building a process that keeps our documentation sane going forward.
    - Working with our whole team to spot opportunities for public writing, about features we’re working on, or the inner workings of our stack, or the technology ecosystem we fit into, or whatever interesting thing is bugging someone on a particular day.

    These are two very different kinds of writing, and we know that. Our product and the kinds of problems we work on are very technical and can be somewhat subtle. On a dial with “technical writer” at “1” and “technical journalist” on the other side at “10”, you’ll probably be happiest if you’re at, like, a 4. But the tone of all of our writing is conversational, so you also want to be the kind of person who doesn’t, like, flinch when they see “like” in the middle of a sentence.

    Mostly, we want to work with people who love to write and who find the problems we work on exciting. Global Anycast, modern language frameworks like Elixir/Phoenix, containers and virtualization, and security are all part of our beat. If you want an opportunity to nurture or develop opinions about these topics and then broadcast them emphatically on the Internet, this is a fun way to do it.

    Fly.io is, at its heart, a tool for software developers. You don&#39;t need to be a professional software developer to do this job, but you&#39;ll want to be comfortable with programming in general, and be comfortable with a high-level language like Ruby, Python, or Javascript.

    ## Things To Know About Us

    - We’re a small team, almost entirely technical, and everyone wears a lot of hats. Once again: you won’t be the only person writing here!
    - We’re remote, with team members in Colorado, Quebec, Chicago, London, Virginia, Rwanda, and Utah.
    - We’re an unusually public team, with an online community (at community.fly.io) that we try to be chatty with. If we’re doing things right, this role will likely increase your public profile.
    - We’re a team, not a family, but we have families and want to be the kind of place where work doesn’t get in the way of that.
    - We’re a real company – we hope that goes without saying – and this is a real, according-to-Hoyle full-time job with health care for US employees, flexible vacation time, hardware/phone allowances, the standard stuff. The compensation range for this role is $120k-$160k plus equity.

    ## How We Hire People

    We are weird about hiring. We’re skeptical of resumes and we don’t trust interviews (we’re happy to talk, though). We respect career experience but we aren’t hypnotized by it, and we’re thrilled at the prospect of discovering new talent.

    The premise of our hiring process is that we’re going to show you the kind of work we’re doing and then see if you enjoy actually doing it; “work-sample challenges”. Unlike a lot of places that assign “take-home problems”, our challenges are the backbone of our whole process; they’re not pre-screeners for an interview gauntlet.

    For this role, we’re going to give a couple of deadline writing assignments. We&#39;ll ask you to document a feature from a rough description (not a technical challenge! we&#39;ll talk you through it!), and also have you punch up and flesh out a draft blog post.

    If you&#39;re interested, mail ~~jobs+techwriter&commat;fly.io~~. You can tell us a bit about yourself, if you like. Either way, include a paragraph or two about the most frustrating technical blog post or article that you&#39;ve read recently. Bonus points if we wrote it.
- :id: blog-how-we-got-to-liveview
  :date: '2021-09-22'
  :category: blog
  :title: How We Got to LiveView
  :author: chris
  :thumbnail: switches-thumbnail.jpg
  :alt:
  :link: blog/how-we-got-to-liveview
  :path: blog/2021-09-22
  :body: |2


    <div class="lead">I'm Chris McCord. I work at Fly.io and created Phoenix, an Elixir web framework. Phoenix provides features out-of-the-box that are difficult in other languages and frameworks. This is a post about how we created LiveView, our flagship feature.</div>

    LiveView strips away layers of abstraction, because it solves both the client and server in a single abstraction. HTTP almost entirely falls away. No more REST. No more JSON. No GraphQL APIs, controllers, serializers, or resolvers. You just write HTML templates, and a stateful process synchronizes it with the browser, updating it only when needed. And there's no JavaScript to write.

    Do you remember when Ruby on Rails was first released? I do. Rails was also a revolution. It hit on the idea of using an expressive language and a few well-chosen, unifying abstractions to drastically simplify development. It's hard to remember what CRUD app development was like before Rails, because the framework has been so influential.

    Today, I work in a language called [Elixir](https://elixir-lang.org/). I spend my days building [Phoenix](https://www.phoenixframework.org/), which is Elixir's goto web framework. Unlike Rails, Phoenix is more than just an Elixir web framework. In the process of building Phoenix, I believe we've hit on some new ideas that will change the way we think about building applications in much the same way Rails did for CRUD apps.

    That's a big claim. To back it up, I want to talk you through the history of Phoenix, what we were trying to do, and some of the problems we solved along the way. There's a lot to say. I'll break it down like this:

    * How real-time app features got me thinking about syncing updates, rather than rendering static content, as the right primitive for connecting app servers to browsers.
    * How trying to build sync.rb with Ruby led me to Elixir, and why I think Elixir is uniquely suited to solve these problems.
    * How working in Elixir lit up "distributed computing", not as a radioactive core of a complicated database but as a basic building block for any web application.
    * How distributed computing and the ideas from sync.rb culminated in a Phoenix feature called LiveView.
    * And, finally, what we've been doing with these building blocks we've come up with.

    Let's get started.

    ## 2013: Ruby on Rails

    I think it's safe to say that there wouldn't be a Phoenix framework if I'd gotten the job I applied for at 37signals in 2013. I liked building with Rails, and I'd doubtless be working on new Rails features instead of inventing Phoenix.

    That's because Rails is an amazingly productive framework for shipping code. It's about the flow. When you build something the Rails team had in mind, the flow is powerful and enjoyable. Rails people have a name for this; they call it ["The Golden Path"](https://steveklabnik.com/writing/rails-has-two-default-stacks).

    But when you need "real-time" features, like chat or activity feeds, you're off the Golden Path and out of the flow. You go from writing Ruby code with Ruby abstractions to Javascript and a [whole different set of abstractions](https://reactjs.org/docs/hooks-intro.html). Then you write a pile of HTTP glue to make both worlds talk to each other.

    So, I had a simple idea: "what if we could replace Rails' `render` with `sync` and the UI updates automatically?". A few days later, I had [sync.rb.](https://github.com/chrismccord/render_sync)

    Sync.rb works like this: the browser WebSockets to the server, and as Rails models change, templates get re-rendered on the server and pushed to the client. HTML rendered from the server would sign a tamper-proof subscription into the DOM for clients to listen to over WebSockets. The library provides JavaScript for the browser to run, but sync.rb programmers don't have to write any themselves. All the dynamic behavior is done server-side, in Ruby.

    This actually worked! Kind of.

    Sync.rb provides a concurrent abstraction, and Rails wasn't especially concurrent. So I built on top of [EventMachine](https://github.com/eventmachine/eventmachine), which is a Ruby/C++ library that runs an IO event loop. EventMachine is a useful library, but it has an uneasy relationship with core Ruby and its ecosystem, most of which doesn't expect concurrent code.

    My EventMachine threads would silently die without errors. I had to check if the EventMachine thread had secretly died, and restart it, for every call, for every user, every time we wanted to async publish updates. I wanted to build dynamic features in a stack I knew and loved, but I didn't have the confidence the platform could deliver what I was trying to build.

    Still, it worked. I had a minimal viable demo, and knew the approach could work if I could find a way to make it reliable. I looked to see how other languages addressed the "real-time" problem at scale. I found Elixir.

    Elixir is José Valim's developer-focussed language built on the [Erlang VM](https://blog.erlang.org/a-brief-BEAM-primer/). Erlang is a battle-tested language designed for telecom applications, notable for the quality and reliability of the virtual machine it runs on. Erlang powered WhatsApp, which served 10 million users _per server._ The $19B Facebook acquisition was a nice calling card as well.

    One look at Elixir and I was instantly hooked. I saw all the power and heritage of the Erlang VM, but also modern features like [protocols](https://elixir-lang.org/getting-started/protocols.html), the best [build tool/code runner/task runner](https://elixir-lang.org/getting-started/mix-otp/introduction-to-mix.html) I'd ever seen, [real macros](https://elixir-lang.org/getting-started/meta/macros.html), and first-class documentation. All it needed was a web framework.

    ## 2014-2015: Phoenix

    I created Phoenix to build real-time applications. Before it could even render HTML, Phoenix supported real-time messaging. To do that, Phoenix provides an abstraction called [Channels](https://hexdocs.pm/phoenix/channels.html).

    To understand Channels, you need to know a few things about Erlang. Concurrency in the Erlang VM is first-class. Erlang apps are comprised of "[processes](https://elixir-lang.org/getting-started/processes.html)", which are light-weight threads that communicate with messages rather than shared memory. You can run millions of processes in a single VM. Erlang is famously resilient: processes can exit (or even crash) and restart seamlessly. Processes message each other with abstractions like [mailboxes](https://elixir-lang.org/getting-started/processes.html#send-and-receive) and [PubSub](https://hexdocs.pm/phoenix_pubsub/Phoenix.PubSub.html); messages are routed transparently between servers.

    Channels exploit Elixir/Erlang messages to talk with external clients. They're bidirectional and transport-agnostic.

    <div class="callout">Channels are usually carried over WebSockets, but you can implement them on anything – the community even showed off a working Telnet client.</div>

    In a web app, a browser opens a single WebSocket, which multiplexes between channels and processes. Because Elixir is preemptively scheduled, processes load-balance on both IO _and CPU._ You can block on one channel, transcode video on another, and the other channels stay snappy.

    This is starkly different from how Rails applications work. Rails uses "processes" too, but they're the "heavy" kind. Generally, a Rails request handler monopolizes a whole OS process, from setup to teardown. Rails is "concurrent" in the sense that the underlying database is concurrent, and multiple OS processes can update it. This complicates things that should be simple. For instance, running a timer that fires events on an interval requires a [background job framework](https://github.com/mperham/sidekiq), and a scheme for relaying messages back through persistent storage. It's a level of friction that changes and limits the way you think about building things.

    In Phoenix, Channels hold state for the lifetime of a WebSocket connection, and can relay events across a server fleet. They scale vertically and horizontally.

    In November 2015, we put the Elixir/Erlang promise to the test. We load-tested Channels, sending [2 million concurrent WebSocket connections](https://www.phoenixframework.org/blog/the-road-to-2-million-websocket-connections) to a single Phoenix server, broadcasting chat messages between all the clients. It took 45 client servers just to open that many connections. Our 128GB Phoenix server still had 45GB of free memory. We'd have run more clients, but we stopped when we ran out of file descriptors supported by our Linux kernel.

    Phoenix did what sync.rb couldn't. _Millions_ of concurrent websocket users with trivial user land code. We knew we had the foundation for a developer friendly real-time application framework. We just needed to figure out how to make it sing.

    ## 2016-2017: Presence

    In 2016, developers didn't usually think they were building real-time applications. "Real-time" generally made people think of WhatsApp and Twitter and Slack. Many didn't have a firm idea of why they'd even want real-time, or the costs were too high to implement such things. Still, everyone had a need for standard web apps. [The core team](https://github.com/phoenixframework/phoenix/graphs/contributors) spent a lot of time making Phoenix a great MVC framework for building conventional database based web apps. But my head was still in real-time features.

    So we released [Phoenix Presence](https://hexdocs.pm/phoenix/Phoenix.Presence.html), a distributed group manager with metadata backed by [CRDTs](https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type). Presence broadly solves the "Who's currently online?" feature.

    Imagine your boss asks you to display how many other visitors are viewing a nearly sold out item to convert sales, or your client asks how hard it would be to show which team members are currently online. You no longer have to figure these things out. Apps become more interesting with live presence features. In fact, most collaborative apps get more interesting with this feature, even if they aren't "real-time" in any other way.

    There are two important things to understand about Presence. The first is that it just works, with self-healing and without dependencies on [external message queues or databases](https://redis.io/). Elixir has all this built in and Presence takes full advantage. You don't have to think about how to make it work.

    The second is that Presence exploits a powerful, general abstraction. CRDT-based statekeeping has applications outside of online buddy lists. Presence gets used in IOT apps to track devices, cars, and other things. If you don't have something like Presence, you probably don't think to build the kinds of features it enables. When you do have it, it changes the frontiers of what you can reasonably accomplish in an application.

    ## 2018: LiveView

    It's 2018. We've had multiple minor point releases under our belt. Two Phoenix books have been printed, and the community is growing and happy.

    I was thrilled with Channels. They're a fantastic abstraction for scalable _client-heavy_ applications, where the server is mostly responsible for sending data and handling client events. They're still the Phoenix go-to for JavaScript and native applications.

    But I knew I wasn't all the way there yet. Phoenix still relied on SPA libraries for real-time UI. But I'd moved on from that architecture, and internalized a new way of building applications in Elixir.

    <div class="callout">
    Think about how a normal web application works. Everything is stateless. When a user does something, you fetch the world, munge it into some format, and shoot it over the wire to the client. Then you throw all that state away. You do it all over and over again with the next request, hundreds or thousands of times per second. It worked this way in PHP and Perl::Mason, and it still works this way in modern frameworks like Rails.

    But Elixir can do _stateful_ applications. It allows you to model the entire user visit as a cheap bit of concurrency on the server, and to talk directly to the  browser. We can update the UI as things happen, and those updates can come from anywhere - the user, the server, or even some other server in our cluster.

    You don't want to build applications in Elixir the way you would in other frameworks.
    </div>

    I had a glimpse of how stateful UI applications worked on the client-side with React.js. We borrowed that React programming model and moved it to our Elixir servers.

    With React, any state change triggers a re-render, followed by an efficient patch of the browser DOM.  LiveView is the same, except the _server_ re-renders the template and holds the state. Where React templates annotate the DOM with client events, such as `<button onClick={this.clicked()}`, in LiveView it's RPC events, like `<button phx-click="clicked">` . With React, you're still context switching and gluing things with HTTP and serializers. Not with LiveView. Interactive features are friction-free.

    <%= partial "shared/posts/cta", locals: {
      title: "Phoenix screams on Fly.io.",
      text: "Phoenix is a win anywhere. But Fly.io was practically born to run it. With super-clean built-in private networking for clustering and global edge deployment, LiveView apps feel like native apps anywhere in the world.",
      link_url: "https://fly.io/docs/elixir/",
      link_text: "Deploy your Phoenix app in minutes.&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>"
    } %>

    This, again, alters the frontiers of what you can reasonably do in an application. Elixir applications don't need to arrange ensembles of libraries and transfer formats and lifecycles to build dynamically updating UI; dynamic server rendering is built in. Once you acclimate to having serverside-stateful UI, you look at your applications differently. There's no SPA complexity tax to pay for making features dynamic, and so whatever makes sense to be dynamic gets to be dynamic.

    Still, at this point, we had only a worse-is-better approach compared to SPA alternatives. Any tiny change re-rendered the entire template. The server not only re-computed templates, but re-sent redundant data, usually just to  patch a tiny part of the page. To make LiveView the first tool developers picked up off the shelf, we needed it to scale better.

    ## 2019-2020: LiveEEx

    Part of the magic of React is that it's a [powerful abstraction](https://reactjs.org/docs/thinking-in-react.html) with an [efficient implementation](https://reactjs.org/docs/faq-internals.html). When a small piece of the state underlying a React app changes, React constrains the update, and diffs it into place, minimizing changes to the DOM.

    Through José Valim's work on the LiveEEx engine, Phoenix LiveView templates became diff-aware. They extract static and dynamic data from the template at compile-time. We compute a minimal diff to send to the client of _only the dynamic data that has changed_.

    The abstraction doesn't change. You write the same code you did before. But now we send a payload better optimized than the best hand-written JSON API. It sounds too good to be true. Let's run through a tiny template to understand how this checks out.

    Imagine we're building the interface for a thermostat with LiveView. We'll call it "Roost". Roost can display the current temperature, and you can bump it up and down with UI controls.

    The first time you load the Roost interface it send a regular HTML response with your page. Next, a WebSocket connection is established, and LiveView sends an initial payload to the browser with the static and dynamic parts of the template split out:

    ![Check out all its majesty](roost-ui.png?2/3&centered)

    The browser now has a cache, not just of the template but of the state data that backs it. And the browser knows how to merge diffs from the server. UI updates are easy. LiveView's browser code zips the static values with the dynamics to produce a new string of HTML for the template. It hands that string off to [morphdom](https://github.com/patrick-steele-idem/morphdom) to efficiently patch only those areas of the DOM that have changed.

    Say a houseguest wants it warmer. They click the `<button phx-click="inc">+</button>` element. This wires up to an RPC function on the server which only increments `@val`:

    ```elixir
    def handle_event("inc", _, socket) do
      {:noreply, update(socket, :val, &(&1 + 1))
    end
    ```

    The server propagates the update. But not much changed! Up above, you might have noticed that we delivered the current temperature (a dynamic state variable) under the `2` key in our initial state. That's all we need to update; we just send `{2: 70}`to browsers. Here's what happens:

    ![](roost-state.png?3/4&centered)

    Wars have been fought over encoding formats for application state updates. Here we've sidestepped the controversy. A variable changes, we recompute linked templates, and the result goes on the wire.

    Now, someone has left a window open, and the temperature is changing. The server notices, and lets clients know about it:

    ```elixir
    Pubsub.broadcast(MyApp.PubSub, "device:#{id}", {:set_temp, val})
    ```

    Each LiveView process picks up the broadcast:

    ```elixir
    def handle_info({:set_temp, new_val}, socket) do
      {:noreply, assign(socket, :val, new_val)}
    end
    ```

    Everyone's browser receives another tiny `{2: 68}` payload, regardless of which server they are connected to. We also don't execute other template code like `<%%= strftime(@time, "%r") %>`, because we know it didn't change.

    Of course, it's not just numbers. Let's build an online dictionary, and let's give it autocomplete. Bear with me, this is neat.

    <%= video_tag "dict.mp4?centered&card" %>

    Here's a simple HTML form:

    ```elixir
    defmodule DemoWeb.SearchLive do
      use DemoWeb, :live_view

      def render(assigns) do
        ~H"""
        <.form phx-change="suggest" phx-submit="search">
          <input type="text" name="q" value="{@query}" list="matches">
          <datalist id="matches">
            <%%= for match <- @matches do %>
              <option value={match}><%%= match %></option>
            <%% end %>
          </datalist>
          <pre><%%= @result %></pre>
        </.form>
        """
      end
    ```

    When a user types something, we fire the `suggest` event, along with the contents of the text field. Here's a handler:

    ```elixir
      def handle_event("suggest", %{"q" => q}, socket) when byte_size(q) <= 100 do
        {words, _} = System.cmd("grep", ~w"^#{q}.* -m 5 /usr/share/dict/words")
        {:noreply, assign(socket, matches: String.split(words, "\n"))}
      end
    ```

    We update `matches` for this connection, which updates the `datalist` in the template. What falls out of the diffing engine is _only the new strings_! And just as importantly, we don't think about how that happens or make arrangements in advance; it just works.

    Here's a problem that's familiar to lots of people who've built dynamic server rendered applications. You want to provide CSS feedback when users click a button. But the server can, at any point, be sending the client unrelated updates. Those in-flight update wipes out the CSS loading states you wanted for feedback. It looks like the server handled the click, but because of the race condition, the UI is lying. This leads to confusing and broken UX.

    The Phoenix team solved this by annotating the DOM with `phx-ref` attributes that we attach to client diff payloads. These refs indicate that certain areas of the DOM are locked _until that specific message ref_ is acknowledged by the server. All of this happens automatically for the user.

    Features like this aren't glamorous, but they're the difference between a proof of concept for dynamic real-time applications and an actual, working application.

    ## 2021: LiveView Uploads, Redirects, and HEEx

    First, we shipped [binary uploads](https://hexdocs.pm/phoenix_live_view/uploads.html). Uploads use the existing LiveView WebSocket connection. You get trivial interactive file uploads, with file progress and previews. Once again, you don't have to think about how to make it work; it just does.

    We also landed on a solution that allows you to perform direct-to-cloud uploads with the same code. You can start simple and upload direct-to-server, then shift to cloud uploads when that makes sense. Let's see it in action:

    <%= video_tag "upload.mp4?card" %>

    The code that makes this happen is shockingly small.  Above, we have file progress, file metadata validation, interactive file selection/pruning, error messages, drag and drop, and previews. This takes just [60 lines](https://gist.github.com/chrismccord/4dec0b6b410c0345b7dc25e60e15ed6d).

    You don't have to understand all that code, but I want to call attention to this:

    ```elixir
      def mount(_params, session, socket) do
        {:ok,
         socket
         |> assign(:uploaded_files, [])
         |> allow_upload(:avatar, accept: ~w(.jpeg .png), max_entries: 3)}
      end
    ```

    Notice how the declarative `allow_upload` API allows you to specify what uploads you want to allow, then the template defined in `render/1` is reactive to any updates as they happen.

    Uploading over WebSockets is neat. We're absolutely certain that the file landed on the same Elixir server our visitor initially randomly load-balanced to. This allows the LiveView process (your code) to do any post-processing with the local file right there on the same box. You can watch a live coding upload deep-dive [here](https://www.phoenixframework.org/blog/phoenix-live-view-upload-deep-dive).

    We also shipped a `live_redirect` and `live_patch` feature which allows you to navigate via `pushState` on the client without page reloads over the existing WebSocket connection. This might look like [pjax](https://github.com/MoOx/pjax) or [turbolinks](https://github.com/turbolinks/turbolinks). But it's not. Navigation happens over WebSockets. There's no extra HTTP handshake, no parsing authentication headers, no refetching the world. The UX is faster and cleaner than an SPA framework, latency is reduced, and it's easier to build.

    You may have also noticed the `~H` Elixir sigil in the previous examples. This is [HEEx](https://hexdocs.pm/phoenix_live_view/assigns-eex.html), and it's new. It's an HTML-aware template syntax, including validation at compile-time. Like React JSX, components can be expressed as custom tags. It looks like this:

    ```elixir
    <div class="profile">
      <Avatar.icon for={@user} width="100" height="100"/>
      <%%= for suggestion <- @suggested_users do %>
        <.follow_button target={@suggestion} />
      <%% end %>
    </div>
    ```

    The HEEx engine extends standard Elixir EEx templates with tag syntax, such as `<Avatar.icon for={@user} …` which internally compiles to a `<%%= component &Avatar.icon/1, for: @user %>` macro call. It's structured markup and it composes nicely with HTML.

    With the base HEEx engine in place, we foresee a budding Elixir ecosystem of off-the-shelf components for building out LiveView applications. Watch this space while we continue to extend our HEEx engine with features like slots and declarative assigns for compile-time checks.

    ## The Future

    We are excited to take LiveView, quite literally, around the globe. Fly.io makes it easy to deploy your LiveView processes close to users – and when LiveView is close to your users, interactions are immediate. We're already exploring optimistic UI _on the server_, where we can front-run database transactions with Channels, reflect the optimistic change on the client, and reconcile when database transactions propagate. Much of this is already possible today in LiveView with a little work, but with a turn-key global deployment under our feet, the Phoenix team can really dig in and explore making these kinds of previously unheard of ideas the status quo for day to day LiveView applications. Stay tuned!
- :id: blog-fly-io-is-hiring-full-stack-developers
  :date: '2021-09-01'
  :category: blog
  :title: Fly.io Is Hiring Full-Stack Developers
  :author: michael
  :thumbnail: jobs-cover-01-thumbnail.png
  :alt:
  :link: blog/fly-io-is-hiring-full-stack-developers
  :path: blog/2021-09-01
  :body: "\n\n<div class=\"lead\">Fly.io makes it easy to host applications worldwide
    the same way a CDN hosts HTML pages. Our users ship us containers, and we transmute
    them into Firecracker micro-vms that run on our hardware in data centers around
    the world. [The easiest way to learn more is to sign up](https://fly.io/docs/speedrun/);
    if you’ve got a working container now, it can be running in Sydney, Chennai, or
    Amsterdam in just a few minutes.</div>\n\nWe’re working on super fun problems
    and are looking for more people to join us. In particular: we’re looking for full-stack
    developers. We expect this will be a good role for early-to-mid-level career developers.\n\n##
    Some Background\n\nFly.io’s users interact with us almost entirely with [`flyctl`,
    our CLI interface](https://github.com/superfly/flyctl). We are a CLI-first team.
    But our UI and UX work is starting to get more ambitious. And that’s what “full-stack”
    means here: the parts of our product that users interact directly with, particularly
    when those parts have user interfaces.\n\nOur UX stack today is a combination
    of Rails, GraphQL, and some Go (our platform stack is Rust and Go). But a few
    months ago, [we fell in with a bad crowd](https://fly.io/blog/we-are-hiring-elixir-developer-advocates/),
    and got ourselves a little hooked on Elixir (Elixir is easy to love, but it also
    really sings on Fly.io). So there’s Elixir in our plans as well; probably more
    new Elixir than Rails.\n\n**We don’t care if you’re a Rails or Elixir expert,
    or if you have a mile-long resume.** All we really care about is whether, if we
    give you a problem that involves reading or writing some Rails or Elixir code,
    you can marshal the resources needed to solve it. If that involves having _Programming
    Elixir_ open in front of you while you type your first ever Elixir symbols into
    an editor, that’s fine, as long as your work holds up!\n\n## The Part Where We
    Sell The Role\n\nThere are lots of different dev jobs out there and different
    gigs are good fits for different people. Here’s a stab at some things that might
    indicate this role is a good fit for you:\n\n- You’re the kind of person who gets
    a dopamine hit from standing up Rails, Phoenix, or Django-type applications, maybe
    sometimes just to scratch an itch. Unlike platform and backend development, which
    involves a lot of deep and subtle low-level changes, this role is all about stuff
    that directly impacts our customers experience. It’s a lot of visible stuff, and,
    unlike platform changes that can be tricky to roll out, this is mostly “instant
    gratification” development in rapid development environments. \n- You enjoy learning
    new stuff, even when the learning is getting out of hand. We’re a small team building
    something that is both very ambitious — a whole new cloud hosting platform! —
    and very technically deep. We’re unafraid of new technology, from frameworks to
    languages to kernel features, maybe even to a fault. \n- You think about starting
    your own company some day. This work is connected directly into the mains electricity
    of our business. Your portfolio here would start from the moment a prospective
    user reads one of our blog posts, clicks through to our home page, continues through
    our sign-up flow, and gets their own app deployed successfully. \n\nSome other
    things you’d want to know about us:\n\n- We’re a small team, almost entirely technical,
    and everyone wears a lot of hats. \n- We’re remote, with team members in Colorado,
    Quebec, Chicago, London, Virginia, Rwanda, and Utah.\n- We’re an unusually public
    team, with an online community (at community.fly.io) that we try to be chatty
    with; you’d want to be comfortable not working secretively in a dark room (you
    can work noisily in a dark room if that’s your thing).\n- We’re a team, not a
    family, but we have families and want to be the kind of place where work doesn’t
    get in the way of that.\n- We’re a real company – we hope that goes without saying
    – and this is a real, according-to-Hoyle full-time job with health care for US
    employees, flexible vacation time, hardware/phone allowances, the standard stuff.
    The compensation range for this role is $120k-$140k plus equity.\n\n## How We
    Hire People\n\nWe are weird about hiring. We’re skeptical of resumes and we don’t
    trust interviews (we’re happy to talk, though). We respect career experience but
    we aren’t hypnotized by it, and we’re thrilled at the prospect of discovering
    new talent.\n\nThe premise of our hiring process is that we’re going to show you
    the kind of work we’re doing and then see if you enjoy actually doing it; “work-sample
    challenges”. Unlike a lot of places that assign “take-home problems”, our challenges
    are the backbone of our whole process; they’re not pre-screeners for an interview
    gauntlet.\n\nWe don’t time candidates or look over their shoulders. We’re not
    interested in gotchas; we want you to show yourself in your best light, and we
    want the environment you work in to be comfortable and realistic (every one of
    us has 90 Google tabs open looking up random programming stuff).\n\nAs a heads
    up, here’s some stuff you’d want to be comfortable getting yourself into gear
    to work on:\n\n- Comprehending and hacking on existing Rails and Elixir/Phoenix
    code. Once again: we don’t care if you’ve never before updated an ActiveRecord
    object in anger, or run an IEx session before; we just need you to be comfortable
    jumping into it now.\n- Deploying an application soup-to-nuts on your own (we’ll
    ask you to do it on Fly.io, which should make it easier). \n- Building code with
    browser user-interface components. This isn’t a visual design role; we’ve got
    visual design people on the team. But you’d want to be comfortable building stuff
    that visual designers can chrome up.\n\n## If You&#39;re Interested\n\nShoot us
    an email at ~~jobs+fullstack&#64;fly.io~~. You can tell us a little about yourself
    if you like. Either way, tell us a short story about the last language or framework
    you picked up that you didn&#39;t enjoy (maybe you like it now, though).\n\nWe&#39;ll
    get back to you with an opportunity to connect with us on a call at your convenience
    (we don&#39;t do phone screens, but we will certainly take the time to answer
    any questions you have) and details about our work sample challenge.\n\n"
- :id: blog-api-tokens-a-tedious-survey
  :date: '2021-08-24'
  :category: blog
  :title: 'API Tokens: A Tedious Survey'
  :author: thomas
  :thumbnail: tedious-survey-thumbnail.jpg
  :alt:
  :link: blog/api-tokens-a-tedious-survey
  :path: blog/2021-08-24
  :body: "\n\n<meta property=\"og:url\" content=\"https://fly.io/blog/api-tokens-a-tedious-survey/\"
    />\n<meta property=\"og:image\" content=\"https://fly.io/blog/2021-08-24/scorecard.png\"
    />\n<meta property=\"og:title\" content=\"API Tokens: A Tedious Survey\" />\n<meta
    property=\"og:type\" content=\"article\" />\n<meta property=\"og:description\"
    content=\"More, I promise, than you ever wanted to know about API tokens.\" />\n
    \n<div class=\"lead\">We’re Fly.io. This post isn’t about Fly.io, but you have
    to hear about us anyways, because my blog, my rules. Our users ship us Docker
    containers and we transmute them into Firecracker microvms, which we host on our
    own hardware around the world. With  a working Dockerfile, [getting up and running
    will take you less than 10 minutes](https://fly.io/docs/speedrun/).</div>\n\nThis
    is not really a post about Fly.io, though I&#39;ll talk about us a little up front
    to set the scene.\n\nThe last several weeks of my life have been about API security.
    I&#39;m working on a new permissions system for Fly.io, and did a bunch of research
    into my options. [We even recorded a podcast about it](https://securitycryptographywhatever.buzzsprout.com/1822302/9020991-what-do-we-do-about-jwt-feat-jonathan-rudenberg).
    I won&#39;t leave you hanging and tell you right up front: we&#39;re working on
    rolling out a Macaroon-based scheme, which you&#39;ll read more about later.\n\nThis
    post is long. You may be interested in just one kind of token. I'll make easy
    for you: here's a table of contents:\n\n  1. [Simple Random Tokens](#random-token)\n
    \ 1. [Platform Tokens](#platform-token)\n  1. [OAuth 2.0](#oauth)\n  1. [JWT](#jwt)\n
    \ 1. [PASETO](#paseto)\n  1. [Protobuf Tokens](#protobuf)\n  1. [Authenticated
    Requests](#authenticated-requests)\n  1. [Facebook CATs](#cats)\n  1. [Macaroons](#macaroons)\n
    \ 1. [Biscuits](#biscuits)\n  \nFly.io is an application hosting platform. Think
    of us as having a control plane that applications running on Fly.io interact with,
    and an API that our users interact with — mostly through our CLI, `flyctl`. It&#39;s
    that `flyctl` piece we&#39;re talking about here.\n\nToday, Fly.io API access
    is all-or-nothing. Everyone has root credentials. What we want is fine-grained
    permissions. Here&#39;s two big problems we want to solve:\n\n- Our API publishes
    [Prometheus metrics](https://fly.io/blog/measuring-fly/). You can point Grafana
    at it and start building dashboards. You&#39;d like to give [Grafana Cloud](https://grafana.com/)
    a credential that lets them host dashboards without mucking with your apps.\n-
    You&#39;d like to give a contractor access to an app without letting them get
    access to secrets from other applications.\n\nThis is the API job people generally
    refer to as IAM. There are a bunch of different ways to do the IAM job, and they&#39;re
    all fun to nerd out about.\n\n## Let’s First Clarify Some Stuff\n\nWhat I’m interested
    in here is API security, for end-users; “retail” security.\n\nThere&#39;s a closely
    related API security problem I&#39;m not talking about: inter-service authentication.
    Modern applications are comprised of ensembles of small services. Ideally, there&#39;s
    a security layer between them. But nobody does retail API IAM with Kerberos or
    mTLS. If you want to read more about these approaches, [I wrote a long post about
    them elsewhere](https://web.archive.org/web/20200507173734/https://latacora.micro.blog/a-childs-garden/).\n\nAnother
    related problem is federated authentication and single sign-on. Google, Apple,
    and Okta will give you tokens that map requests to identities on their platforms.
    Those token formats are relevant here, but I want to be clear that federated identity
    is not what I&#39;m after.\n\nMost API security schemes boil down to a token that
    accompanies API requests. The tokens are somehow associated with access rules.
    The API looks takes the request, extracts the token, finds the access rules, and
    decides how to proceed.\n\nSome questions to keep in your brain as you read through
    this:\n\n- How do we revoke tokens? [Credentials get compromised](https://github.com/magoo/ato-checklist).
    If you can&#39;t revoke a token, you can&#39;t recover from a compromise. \n-
    How often are we hitting the database to satisfy requests? In a microservice contraption,
    it&#39;s painful to give every service direct access to the database. \n- Are
    we introducing vulnerabilities? Software developers will make [every possible
    mistake](https://www.cryptofails.com/post/70059600123/saltstack-rsa-e-d-1). Muzzle
    discipline: don&#39;t point the guns at our feet. \n\n## Let&#39;s Take Passwords
    Off The Table\n\nIt’s 2021 and so I don’t need to tell you that having your API
    pass a username and password through HTTP basic authentication is a bad idea.
    Your tokens should look large and random, whatever they are.\n\n<a name=\"random-token\"></a>
    \ \n## Simple Random Tokens: Unsung Heroes\n\nHere is a token generator that,
    from a security perspective, is pretty hard to beat:\n\n```\n>>> binascii.hexlify(os.urandom(16))\nb'46d684a052c29cdce14c7e03e19da0f9'\n```\n\nKeep
    a table of random tokens, associate them with a table of users, and associate
    those users with allowed actions. You don’t need me to tell you how to do this;
    it’s simply how CRUD apps work.\n\nWhat you might need me to tell you is that
    this is a good way, even over the long term, to handle the IAM problem. Random
    tokens aren&#39;t cryptographically scary. They’re easily revoked and expired.
    The accompanying permissions logic is clean and expressive; it&#39;s just your
    API code.\n\nFrankly, the biggest knock against simple random tokens is that they’re
    boring. If you can get away with using them — and most applications can — you
    probably should. Give yourself permission by saying you’re doing it for security
    reasons. Security is a problem for all the fancy tokens I&#39;m going to talk
    about from now on.\n\n <a name=\"platform-token\"></a>  \n## Platform Tokens\n\nAssume
    we&#39;re trying to minimize the fraction of requests that have to hit the database.
    Mainstream web application frameworks tend to already have features that help
    with this.\n\nRails, for instance, has [`MessageVerifier`](https://api.rubyonrails.org/classes/ActiveSupport/MessageVerifier.html)
    and [`MessageEncryptor`](https://api.rubyonrails.org/v5.2.3/classes/ActiveSupport/MessageEncryptor.html).
    Give them a bag of attributes and get back a tamper-proof (optionally encrypted)
    string, using HMAC-SHA2 and [encrypt-then-MAC](https://eprint.iacr.org/2000/025)
    AES-CBC. Put the string in a cookie. The server only remembers a root secret,
    and can pull user data out of the cookie instead of  the database. This is how
    Rails sessions work.\n\nPython frameworks have similar features, but also the
    excellent Python [pyca/cryptography libraries](https://cryptography.io/en/latest/fernet/),
    which include `Fernet`, which provides the same functions optimized for tokens.\n\nYou
    can’t generally use general-purpose user sessions for API tokens (the defining
    feature of an API token is that it doesn&#39;t log out). But you can use the same
    features to build API tokens. Share the root secret among multiple services —
    maybe that&#39;s fine — and microservices don&#39;t have to rely on a central
    service.\n\nPlatform tokens are relatively simple, and can be stateless. What&#39;s
    the catch? Well, you’re effectively using tokens as a database cache, and cache
    consistency is frustrating.\n\nRight off the bat, you’ve lost the simplest form
    of token revocation. The whole premise is that you&#39;re not validating tokens
    against the database, so now you have to come up with another way to tell if they’ve
    been revoked. Without a standard protocol for renewing them, short-expiry tokens
    don&#39;t work either.\n\n<div class=\"callout\">A pattern I’ve seen a bunch here,
    and one that I kind of like, is to “version” the users. Stick a token version
    in the user table, have tokens bear the current version. To revoke, bump the version
    in the database; outstanding tokens are now invalid. Of course, you need to keep
    state to do that, but the state is very cheap; a Redis cache of user versions,
    falling back to the database, does the trick.</div>\n\n<a name=\"oauth\"></a>
    \ \n\n## OAuth 2\n\n<i>All exhibits and addenda attached previously to the section
    on “Platform Tokens” is hereby incorporated into this section and made a part
    thereof.</i>\n\nBy design, OAuth is a federation protocol. Canonically, OAuth
    lets a 3rd party  post a tweet with your account. That’s not the problem we’re
    trying to solve.\n\nBut OAuth 2.0 is popular and has bumped into every tedious
    problem you're likely to encounter with tokens, and they've come up with solutions
    of varying levels of grossness. You can, and lots of people do, draft off that
    work in your own API IAM situation.\n\nFor instance, OAuth 2.0 has a built-in
    solution for short-expiry tokens; OAuth 2.0 has a “Refresh Token”, which exchanges
    for “Access Tokens”. Access Tokens are the ones you actually do stuff in the API
    with, and they expire rapidly. OAuth 2.0 libraries know how to use Refresh Tokens.
    And they&#39;re easy to revoke, because they’re used less frequently and don’t
    punish the database.\n\nOAuth 2.0 Access Tokens are opaque strings, so you can
    do the same things with them that you would with a Platform Token (or just stuff
    a Platform Token in there).\n\nThe “cryptography” in OAuth 2.0, such as it is,
    is simple. It gets tricky in standalone single-page applications, but so does
    everything else. [I used to snark about people cargo-culting OAuth](https://news.ycombinator.com/item?id=16159301)
    into simple client-server apps. Not anymore.\n\n<a name=\"jwt\"></a>  \n\n## JSON
    Web Tokens \n\nA brief history lesson. We got OAuth, and apps could tweet on behalf
    of users, and God [saw what he had made and it was good](https://thebricktestament.com/genesis/creation/04_gn01_06.html).
    Then someone realized that if you could post a tweet on behalf of a user, you
    could use that capability as a proof of identity and “[log users in with Twitter](https://thebricktestament.com/genesis/cain_kills_abel/12_gn04_08c.html)”.
    The tweet itself became extraneous and people just used OAuth tokens that could,
    like, read your user profile as an identity proof.\n\nThis is a problem because
    the ability to read your user profile isn’t a good identity proof. You might grant
    that capability to applications for reasons having nothing to do with whether
    they can “log in with Twitter” to a dating app. People found a [bunch of vulnerabilities](https://medium.com/securing/what-is-going-on-with-oauth-2-0-and-why-you-should-not-use-it-for-authentication-5f47597b2611).\n\nEnter
    OpenID Connect (OIDC). OIDC is the demon marriage of OAuth 2.0 and a cryptographic
    token standard called JWT. OIDC’s is unambiguous: it gives you an “Identity Token”,
    JWT-encoded, that tells you who’s logging in.\n\nWe’re not so much interested
    in OIDC here, but the eldritch rituals that brought OIDC into being unleashed
    a horde of JWTs into the world, and that’s now a thing we have to think about.\n\nFrom
    a purely functional perspective, JWT isn’t doing much more than a Platform Token
    embedded in OAuth 2.0. But JWT is standardized, and “JSON encrypted with Fernet
    and embedded in OAuth Access Token” isn’t, and so a whole lot of dev UX has sprung
    up around JWT. So, unfortunately, JWT has really good ergonomics.\n\nWhat makes
    that unfortunate? [JWT is bad](https://twitter.com/SchmiegSophie/status/1413248130225631232).\n\nThis
    is not a post about why JWT is bad, though I do hope you come away from this agreeing
    with me. So I&#39;ll be brief.\n\nFirst, JWT is a [design-by-committee](https://datatracker.ietf.org/wg/jose/about/)
    cryptographic [kitchen sink](https://gist.github.com/jasonk000/26f987681b56fe34c235248c980b5c2e).
    JWTs can be protected with a MAC, like HMAC-SHA2. Or with RSA digital signatures.
    Or  encrypted, with [static-ephemeral P-curve elliptic curve Diffie Hellman](https://twitter.com/tqbf/status/841409698213580801).
    This isn’t so much a footgun as it is the entire Rock Island Arsenal deployed
    against your feet. If you’re an aficionado of crypto vulnerabilities, you almost
    have to love it. Where else outside of TLS are you going to find [invalid curve
    point attacks](https://eprint.iacr.org/2018/298.pdf)?\n\nNext, the JSON semantics
    of JWT are not thoughtfully designed. JWT doesn’t bind purpose or even [domain
    parameters](https://crypto.stackexchange.com/questions/66969/what-is-meant-by-domain-separation-in-the-context-of-kdf)
    to keys, and JWT libraries are written with the assumption that RSA and HMAC-SHA2
    are just interchangeable solutions to the same problem. So you get bugs where
    people take RSA-signed JWTs and [switch the JWT header from RS256 to HS256](https://github.com/cyberblackhole/TokenBreaker)
    (don’t even get me started on these names), and the libraries obliviously treat
    public signing keys as private MAC keys. Also, there’s `alg=none`.\n\nJWT is so
    popular that it has become synonymous with the concept of stateless authentication
    tokens, despite the fact that stateless tokens are straightforward without (and
    were in wide use prior to) JWT.\n\n<div class=\"callout\">There’s a sense in which
    complaining about JWT is just howling at the moon, because it’s non-optional in
    OIDC, and OIDC is how Google and Apple implement single sign-on. [Friend-of-our-dumb-podcast
    Jonathan Rudenberg](https://securitycryptographywhatever.buzzsprout.com/1822302/9020991-what-do-we-do-about-jwt-feat-jonathan-rudenberg)
    has a good observation about this: if your application retains direct connectivity
    to (say) Apple, you can somewhat safely use OIDC JWT simply by dint of trusting
    the TLS connection you have to Apple’s servers; you don’t so much even need to
    care about the cryptographic misfeatures of the token itself.</div>\n\n\n## Aside:
    Never, Ever SAML\n\nThere are rituals even demons won’t stomach. OIDC’s competitor
    is SAML, which is based on [XML DSIG](https://docs.microsoft.com/en-us/previous-versions/dotnet/articles/ms996502(v=msdn.10)?redirectedfrom=MSDN),
    which is a way of turning XML documents into signed tokens. You should not turn
    XML documents into signed tokens. You should not sign XML. XML DSIG is the worst
    cryptographic format in common use on the Internet. Take all the flaws JWT, including
    the extensive parsing of untrusted data just to figure out how to verify stuff.
    Mix in a DOM model where a single document could potentially have [dozens of different
    signed subtrees](https://www.ws-attacks.org/XML_Signature_Wrapping), then add
    a pluggable canonicalization layer that [transforms documents before they’re signed](https://i.blackhat.com/us-18/Thu-August-9/us-18-Ludwig-Identity-Theft-Attacks-On-SSO-Systems.pdf).
    Make it complicated enough that there is essentially a single C-language implementation
    of the spec that every SAML library wraps. You&#39;re obviously not going to use
    to authenticate your API, but, in case you can’t tell, I’m [getting some stuff
    out of my system](https://news.ycombinator.com/item?id=28080553) here.\n\n<a name=\"paseto\"></a>\n##
    PASETO\n\nPASETO (rhymes with &quot;potato&quot;) is hipster JWT. I mean that
    in the nicest way. It has essentially the same developer UX as JWT, but tries
    to lock the token into modern cryptography.\n\nJWT is a cryptographic kitchen
    sink. PASETO is the smaller bathroom vanity sink. I&#39;m critical here, because
    PASETO has, for some good reasons, done well among token nerds and doesn&#39;t
    need my help.\n\nThere are today four versions, each of which define two kinds
    of token, a symmetric &quot;local&quot; and an asymmetric &quot;public&quot;.
    [Version 1](https://github.com/paseto-standard/paseto-spec/blob/master/docs/01-Protocol-Versions/Version1.md)
    uses &quot;NIST-compliant&quot; AES-CTR, HMAC-SHA2, and RSA. [Version 2](https://github.com/paseto-standard/paseto-spec/blob/master/docs/01-Protocol-Versions/Version2.md)
    has XChaPoly and Ed25519. [Version 3](https://github.com/paseto-standard/paseto-spec/blob/master/docs/01-Protocol-Versions/Version3.md)
    replaces RSA with a P-384 ECDSA. [Version 4](https://github.com/paseto-standard/paseto-spec/blob/master/docs/01-Protocol-Versions/Version4.md)
    replaces XChaPoly with XChaCha and a Blake2 KMAC. You can swap `v4` with `v4c`
    to use CBOR instead of JSON. It’s a lot.\n\nMy issue with PASETO is that it&#39;s
    essentially the same thing as JWT. You could almost build it from JWT, by adding
    some algorithms and banning some others.\n\nPASETO advocates for the now-accepted
    practice of versioning whole protocols rather negotiating parameters on the fly.
    That should be a powerful advantage. But PASETO has 8 versions, 4 of which are
    &quot;current&quot;, and I think part of the idea of protocol versioning that
    PASETO misses is that you&#39;re not supposed to keep multiple versions flying
    around. Versions 3 and 4 are partly the result of a vulnerability (not a super
    serious one) [Thai Duong found](https://twitter.com/XorNinja/status/1157882553610563585).
    PASETO libraries support multiple versions, in some cases dynamically. Kill the
    old versions!\n\nThe IRTF CFG is the IETF&#39;s cryptography review board. For
    reasons I will never understand, the PASETO authors [submitted it to CFRG for
    consideration](https://mailarchive.ietf.org/arch/browse/cfrg/?gbt=1&index=N0I_gQrNdZho-m7RafFj1mSavlc).
    Never do this. In the thread, Neil Madden pointed out that it had [managed to
    inherit JWT’s RSA/HMAC problem](https://mailarchive.ietf.org/arch/msg/cfrg/Yd85GHaPfkUYvCsWAdikByQsiLQ/);
    all the PASETO versions now have an “[Algorithm Lucidity](https://github.com/paseto-standard/paseto-spec/blob/master/docs/02-Implementation-Guide/03-Algorithm-Lucidity.md)”
    warning  telling people to make sure they’re strongly typing their keys.\n\nI
    don’t think this is PASETO’s fault so much as I think that the fundamental idea
    is an impossible trinity: cryptographic flexibility, cryptographic misuse-resistance,
    and Javascript-y developer UX.\n\nAlso: the “NIST-compliant” PASETO versions were
    an unforced error.\n\n<div class=\"callout\">I&#39;m peeved by JSON tokens that
    authenticate bags of random user attributes alongside token metadata like issuance
    dates and audiences. Cryptography engineers hear me rant about this and scratch
    their heads, but I think they&#39;re mostly thinking about OIDC JWTs that carry
    minimal data, and not all the weird  JWTs developers cook up, where user data
    mingles with metadata. This, too, seems like an unforced error for me. So does
    the fact that a lot of this metadata is optional. Why? It&#39;s important!</div>\n\nStill,
    you’re far better off using PASETO than JWT. My take regarding PASETO is that
    if you use it, you should find real lucidity about whether you want symmetric
    or asymmetric tokens; they’re two different things with different use cases. Support
    just one version.\n\n<a name=\"protobuf\"></a>\n## Protocol Buffer Tokens: The
    Anti-PASETO\n\nYou can get essentially the thing PASETO tries to do, without any
    of the downsides, just by defining your own strongly typed protocol format. David
    Adrian calls these “Protobuf Tokens”.\n\nAll you do is, define a Protocol Buffer
    schema that looks like this:\n\n```\nmessage SignedToken {\n  bytes signature
    = 0;\n  bytes token = 1;  \n}\nmessage Token {\n  string userId = 0;\n  uint64
    not_before = 1;\n  uint64 not_after = 2;\n  // and other stuff\n}\n```\n\nPush
    all your token semantics into the `Token` message, and marshal it into a string
    with a first pass of Protobuf encoding. Sign it with Ed25519 (concatenate a version
    string like “Protobuf-Token-v1” into the signature block), stick the token byte
    string in the `token` field of a `SignedToken`, and populate the signature. Marshal
    again, and you’re done.\n\nThis two-pass encoding gives you two things. First,
    there’s only one way to decode and verify the tokens. Second, everything in the
    token is signed, so there’s no ambiguity about metadata being signed. The tokens
    are compact, easy to work with, and can be extended (Protocol Buffers are good
    at this) to carry arbitrary optional claims.\n\n<a name=\"authenticated-requests\"></a>\n##
    Authenticated Requests\n\nYou don’t need  tokens at all. You can instead just
    have keys. Use them to authenticate requests. That’s how the AWS API works.\n\nWe
    tend to send normal HTTP requests to our APIs, and pass an additional header carrying
    a “bearer token”. Bearer tokens are like bearer bonds, in that if you have your
    bear paws on them, it’s game over. Authenticated requests don’t have this problem.\n\nTo
    do this, you need a canonicalization scheme for your HTTP requests. The same HTTP
    request has multiple representations; we need to decide on just one to compute
    a MAC tag. This seems easy but was a [source of vulnerabilities in early implementations
    of the AWS scheme](https://www.daemonology.net/blog/2008-12-18-AWS-signature-version-1-is-insecure.html).
    Just use [AWS&#39;s Version 4](https://docs.aws.amazon.com/general/latest/gr/sigv4_signing.html).\n\nCompute
    an HMAC over the canonicalized request (with AWS, you’d just use your `AWS_SECRET_ACCESS_KEY`)
    and attach the resulting tag as a parameter.\n\n<div class=\"callout\">God help
    you, you could use X509 here and issue people certificates and keys they can use
    to sign requests, which is a thing Facebook apparently did internally.</div>\n\nThere
    are nice things about authenticated requests. No bearer tokens, no bears. The
    biggest problem is logistical: it’s a pain to build request authenticating code,
    so, unless your app gets huge, the only way to talk to it will be with your official
    SDK that does all the request signing work.\n\n<a name=\"cats\"></a>\n## Facebook’s
    CATs\n\nSo, [here’s a cool trick](https://eprint.iacr.org/2018/413.pdf). You’ve
    got a bunch of services, like `Messages` and `Photos` and `Presence` and `Ivermectin
    Advocacy`. And you’ve got a central `Authentication` service, to which both your
    services and your users can talk.\n\n`Authentication` holds a root key.  `Messages`
    comes on line, and makes (say) an identity-proving mTLS connection to `Authentication`.
    It’s issued a service key, which is `HMAC(k=root, v=“Messages”)`.\n\nNow a user
    “Alice” arrives. `Authentication` issues her a key. It&#39;s `HMAC(k=HMAC(k=root,
    v=“Messages”), v=“Alice”)`.\n\n![CAT diagram](./cats.png?3/4&centered)\n  \nSee
    what we did there? `Messages` doesn’t have Alice’s key. But her key is simply
    the HMAC of her username under the `Messages`  key, so the service can reconstruct
    it and verify the message.\n\nYou can use a CATS-like construction to sign requests,
    or to sign a Protobuf Token (with HMAC or an AEAD, rather than Ed25519). You’re
    getting some of the decoupling advantage of public key cryptography. `Messages`
    \ requires only sporadic contact with `Authentication`,  to enroll themselves
    and periodically rotate keys. That’s enough  to authenticate requests from all
    comers, trusting that the only way Alice got her key was if `Authentication` OK’d
    it.\n\n<%= partial \"shared/posts/cta\", locals: {\n  title: \"If this was a post
    about deploying on Fly.io, you'd have been done 22 minutes ago.\",\n  text: \"We
    don't need 5,000 words to tell you how to get an application running close to
    users around the world, from Sydney to Amsterdam. All it takes is a working Dockerfile\",\n
    \ link_url: \"https://fly.io/docs/speedrun/\",\n  link_text:\"Try it for free&nbsp;&nbsp;<span
    class='opacity-50'>&rarr;</span>\"\n} %>\n\n<a name=\"macaroon\"></a>\n## Macaroons\n\nWe
    can go for a walk where it&#39;s quiet and dry and [talk about Macaroons](https://blog.gtank.cc/macaroons-reading-list/).\n\nImagine
    a golden ticket for your service, an authenticated token permitting any action.
    It’s much too dangerous to pass around as a bearer token.\n\nNow imagine adding
    caveats to that golden token. You’re allowed only to read, not to write.  Only
    for a single document.  Only on a request from a specific IP, or on a session
    independently authenticated to a specific user ID. This attenuated token is much
    less dangerous. In fact, you might get it so locked down that it’s not even sensitive.\n\nWe
    exploit the same trick CAT uses to derives user keys. Start with your golden ticket
    and HMAC it under a root key. Now you want to make it read-only, so you add another
    message layer to the token, and you MAC that new layer, using the MAC tag of the
    previous layer as the key. The holder of the new token can’t work out the original
    MAC tag of the golden ticket; the token carries only the new chained MAC tag.
    But services have the root key and can re-derive all the intermediate values.\n\n![Macaroon
    diagram](./macaroons.png?3/4&centered)\n  \nMacaroons are a token format built
    around this idea. They do three big things.\n\n**Attenuation**: user can restrict
    tokens without talking to the issuing service. All the caveats must evaluate `true`.
    You can’t undo previous caveats with new ones. The service just knows about the
    basic caveat types and doesn’t need special-case code for all the goofy combinations
    users might want.\n\n**Confinement**: If you have the right caveat types, you
    can set it up so there are useful Macaroons that are safe to pass around, because
    they’re only (say) valid on a session under a particular mTLS client certificate,
    or at a particular time of day.\n\n**Delegation**: Macaroons have [“third-party
    caveats”](https://web.archive.org/web/20160331215837/https://blog.bren2010.io/2014/12/04/macaroons.html),
    which delegate logic to other systems. Third-party caveats are encrypted; users
    can see only the URL of a third-party service they can talk with to resolve them.
    The third-party system issues a “discharge Macaroon”, which is submitted alongside
    the original Macaroon to resolve the caveat.\n\nThese ideas synergize. You can
    delegate authentication to an IAM service, and then add additional service-specific
    access rules as first-party caveats. A revocation service verifies a user’s tokens;
    the rest of your system doesn’t need to know how revocation is implemented. The
    same goes for audit logging, and for anti-abuse.\n\nSometimes there’s so much
    beauty in the world I feel like I can’t take it, like my heart’s going to cave
    in.\n\nBut Macaroons are [unpopular for good reasons](https://www.youtube.com/watch?v=MZFv62qz8RU).\n\nFirst:
    there’s a library ecosystem for Macaroons and it&#39;s not great.  No library
    can support all or even most of the caveats developers will want. So “standard”
    Macaroons use an untyped string DSL as their caveat format and ask relying services
    to parse them.\n\nThey&#39;re also clunky. With most of the previous  formats,
    you can imagine slotting them into OAuth 2.0. But third-party caveats break that.
    Your Macaroon API will be fussy. Users might have to make and store the results
    of a bunch of queries to issue a real request.\n\nMacaroons rely on symmetric
    cryptography. This is good and bad. It radically simplifies the system, but means
    you have to express  relationships between your services with shared keys. You
    have to do that with HS256 JWT too, of course, but unless you depart pretty radically
    from  the [Macaroon paper](https://research.google/pubs/pub41892/), you can&#39;t
    get the public key wins, without coming up with something like CAT-caroons.\n\nIn
    practice, caveats can be tricky to reason about. It’s easy to write a loop over
    a set of caveats that bombs as soon as one evaluates `false`. But you can accidentally
    introduce semantics that produce caveats that expand instead of contract  authority.
    You&#39;ve got code that wants to answer “can I do this?” questions by asking
    the database about a user ID, and you can write caveat constructions that do similar
    things, which is never what you want in a coherent Macaroon design.\n\nI have
    more to say about these problems! For now, though, it suffices to say that I spent
    many years beating the drum for Macaroons, and then I went and implemented them,
    and I probably won’t be beating that drum anymore. But where they work well, I
    think they probably work really well. My take is: if all three of attenuation,
    confinement, and delegation resonate with your design, Macaroons will probably
    work fine. If you skip any of the three, consider something else.\n\n<a name=\"biscuits\"></a>\n##
    Biscuits\n\nFinally, there’s [Geoffroy Couprie’s Biscuits](https://github.com/CleverCloud/biscuit/blob/master/SUMMARY.md).
    Biscuits are what you’d get if you sat down to write an over-long blog post like
    this one, did all the research, and then decided instead to write a cryptographic
    token to address the shortcomings of every other token.\n\nBiscuits are heavily
    influenced by Macaroons (Couprie claims they’re JWT-influenced as well, but I
    don’t see it). Like Macaroons, users can attenuate Biscuits. But unlike Macaroons:\n\n1.
    Biscuits rely on public key signatures instead of HMAC, which somewhat dampens
    the need for third-party caveats.\n\n2. Rather than simple boolean caveats, Biscuits
    embed Datalog programs to evaluate whether a token allows an operation.\n\nBiscuits
    are incredibly ambitious.\n\nTo begin with, swapping out the simple cryptography
    in Macaroons for public key signatures isn’t an easy task. The cryptographic process
    of adding a caveat to a Macaroon is trivial: you just feed the MAC tag from the
    previous caveat forward as the HMAC key for the new caveat. But there’s no comparably
    straightforward operation for signatures.\n\nThe [cryptography proposed for Biscuits](https://github.com/CleverCloud/biscuit/blob/master/SPECIFICATIONS.md)
    started with pairing curve moon math. Keller Fuchs pulled them back to low-earth
    orbit with curve VRFs. Then they took a detour into blockchainia with aggregated
    Gamma-Signatures. Ultimately, though, Biscuit’s core cryptography [came back to
    Earth](https://github.com/CleverCloud/biscuit/issues/73) with a pretty straightforward
    chaining of Ed25519 signatures.\n\nThe caveat structure of Biscuit tokens is flexible,
    probably to a fault, but formally rigorous, which is an interesting combination.
    It works by evaluating a series of signed programs (compiled and marshaled with
    Protocol Buffers). Services derive fact patterns from requests, like “you’re asking
    for `cats2.jpg`” or “the operation you’re requesting is `WRITE`”. The tokens themselves
    include rules that derive new fact patterns, and checkers that test those patterns
    against predicates.\n\nHonestly, when I first read about Biscuits, I thought it
    was pretty nuts. If the proposal hadn’t lost me at “pairing curves”, it had by
    the time it started describing Datalog. But then I implemented Macaroons for myself,
    and now, I kind of get it. One thing Biscuits get you that no other token does
    is clarity about what operations a token authorizes. Rendered in text, Biscuit
    caveats read like policy documents.\n\nThat’s I think the only big concern I have
    about them. I wonder whether taking real advantage of Biscuits requires you to
    move essentially all your authorization logic into your tokens. Even with Macaroons,
    which previously held the title for “most expressive token”, the host services
    were still making powerful choices about what caveats could be expressed in the
    first place. Biscuits strips the service’s contribution to authorization policy
    down to what seems like their constituent atoms, and derives all security policy
    in Prolog. I see how that could be powerful, but also how you’d kind of have to
    buy into it wholesale to use it.\n\n## Now What?\n\nHere's [a scorecard](scorecard.png):\n\n![token
    scorecard](./scorecard.png?3/4&centered) \n  \nBelieve it or not, with the exception
    of passwords and SAML, I think there’s something to like in all of these schemes.\n\nI
    continue to believe that boring, trustworthy random tokens are underrated, and
    that people burn a lot of complexity chasing statelessness they can&#39;t achieve
    and won’t need, because token databases for most systems outside of Facebook aren’t
    hard to scale.\n\nA couple months ago, I’d have said that Macaroons are underrated
    in a different way, the way Big Star’s “#1 Record” is. Now I think there&#39;s
    merely underrated like the first Sex Pistols show; everyone who read about them
    created their own token format. We’re moving forward with Macaroons, and I’m psyched
    about that, but I’d hesitate to recommend them for a typical CRUD application.\n\nBut,
    don’t use JWT.\n  \n"
- :id: ruby-dispatch-run-ordinary-rails-apps-globally
  :date: '2021-08-10'
  :category: ruby-dispatch
  :title: Run Ordinary Rails Apps Globally
  :author: jsierles
  :thumbnail: run-rails-globally-thumbnail.jpg
  :alt:
  :link: ruby-dispatch/run-ordinary-rails-apps-globally
  :path: ruby-dispatch/2021-08-10
  :body: |2


    <div class="lead">Postgres on Fly.io is now free for small projects. This post is about making Rails fast, though. If you just want to get your app launched, [try us out first](https://fly.io/docs/speedrun/). You can be up and running in just a few minutes.</div>

    If you've used your own Rails application from another continent, you may get the feeling that physics has beaten your performance tuning efforts. Page loads feel a bit sluggish, even with all the right database indexes and fancy CDN-backed assets.

    [We've said it before](https://fly.io/blog/last-mile-redis/): when it comes to responsiveness, sub-100ms times are the magic number; below 100ms, and things _feel_ instantaneous.

    Now, simple regional asset caching — the CDN pitch — can bring apps closer to 100ms response times. But what if you could easily deploy your _application_ globally? Not just images, but application logic. And what if you could do it without changes to your code?

    This type of global deployment sounds like a major infrastructure project  — unrealistic to undertake in the short term, and long-term reserved for giant companies with serious technical faangs.  It shouldn't be that way. All apps should run close to end users. And with the right plumbing, we can distribute and scale globally from day one.

    Fly.io has been doing a lot of cool stuff with [Elixir and Phoenix](https://fly.io/blog/building-a-distributed-turn-based-game-system-in-elixir/). Elixir, built on Erlang's distributed-by-design BEAM runtime, begs to be clustered and geographically distributed, and it really sings on Fly.io. But people do real work in Rails, too. That's why I wrote the [fly-ruby gem](https://github.com/superfly/fly-ruby). It's a tiny library that makes it trivial to deploy Rails apps on a global platform like Fly.io. No new framework or functional language learning required.

    This post is going to talk you through how [fly-ruby](https://github.com/superfly/fly-ruby) works. Before we dig into the details of the gem itself, it's worth a minute to talk about how Fly.io works  and what it means to optimize an application for it.

    ## What Fly.io Does

    For our 100ms performance goal, Fly.io has two major features that Rails can take advantage of.

    **Region-local database replicas:** [Deploying a global Postgres cluster of read replicas on Fly.io](https://fly.io/docs/getting-started/multi-region-databases/) is easy. And read replicas are, by themselves, a good first step to improving Rails performance.

    Rails instances read from their corresponding regional replica, so your &quot;find local recipes&quot; app serves information about Pan-fried rice noodles in Hong Kong from a Postgres replica in Hong Kong, and Italian beef sandwiches from a replica in Chicago.

    **Replayable HTTP requests:** Read replicas work great for _retrieving_ data locally. But we sometimes need to write to the database, and replicas don't handle writes.

    Somewhere in the world (let's say Paris) we'll have a main Postgres — that means our write requests need to make their way to Paris, over Fly.io's private network.

    For example, an HTTP request may arrive in Hong Kong that needs to write to the main database in Paris. We can tell Fly.io's proxy - via the `Fly-Replay`  response header - to _move the entire HTTP request to Paris's Rails instance_, and it will just work.

    ## The magical Fly.io Ruby Gem

    So let's do some testing and figure out how to get below that 100ms  threshold from Paris, Chicago, Sydney and Santiago, Chile.

    We've tried most performance testing tools and our current favorite is  [k6](https://k6.io), a modern, open source web performance testing tool. It's unique in its approach: you write tests in Javascript, interpreted and executed in a Go runtime. It has exquisite documentation - especially for those unfamiliar with web performance testing. Their hosted option supports distributed tests, but we can also [run  tests from a global Fly.io app](https://github.com/jsierles/fly-k6)!

    First, we should see how a  vanilla, single-region deployment fares. We just need [a Postgres database](https://fly.io/docs/getting-started/multi-region-databases) in Paris, and a Rails app deployed in the same region. It would be weird to write a whole article without mentioning food, so here's a little [recipe search app](https://cookherenow.com) that's good for testing. For bonus points, it shows different recipes to people in different cities.

    Here's how it performs:

    ```
    Single region deployment: Time to First Byte
    cdg 72.7ms
    ord 168.7ms
    syd 286.2ms
    scl 442.9ms
    ```

    With the current, single region app config, every request is bounced to Paris. Great for people in Paris, not great for people in Santiago with a hankerin' for Pastel de Choclo.

    If we deploy our app to more regions, we get a nasty surprise:

    ```
    Multiregion deployment with single database: Time to First Byte
    cdg 76ms
    ord 258ms
    syd 528.7ms
    scl 1341ms
    ```

    Performance got worse!?  This isn't a very good sales pitch. There's a simple explanation, though, and we're halfway to faster Chilean recipe suggestions.

    The Rails instance in Sydney still needs to query the database — often multiple times — and _each_ of those database queries bounces around the world to Paris (over an encrypted private network). Adding latency between the app server and database  multiplies internet latency. One round trip from Sydney to Paris might take 400ms. Ten in a row feels like an hour.

    Now, here's the sales pitch.  The  `fly-ruby`  gem will switch to regional replicas for database reads and _magically route write request to the primary database_.

    If we add the `fly-ruby` gem, set the PRIMARY_REGION environment variable, here's what happens:

    ```
    Multiregion deployment with regional database replicas: Time to First Byte
    cdg 75.1ms
    ord 50.4ms
    syd 45.8ms
    scl 84.4ms
    ```

    One tiny configuration change, 90% latency reduction, and our Rails app suddenly responds in sub-100ms. No architecture work required.

    ## It's not actually magic

    This 300-line gem doesn't really do much. Postgres and the Fly.io global proxy  [do all the heavy lifting](https://fly.io/blog/globally-distributed-postgres/). It's a set of Rack middleware that does the last little bit of work for you. And it's usable in any Rack-compatible application, for people who like their Ruby without restrictive rails.

    The magic here lives in the `Fly-Replay` header. I pass a **state**_:_ an arbitrary value written to the `Fly-Replay-Src` header, appended to the final replayed request to the primary application instance.

    This state assists the middleware in handling the replay under different conditions, as we'll see below.

    ```ruby
    def self.replay_in_primary_region!(state:)
      res = Rack::Response.new(
        "",
        409,
        {"Fly-Replay" => "region=#{Fly.configuration.primary_region};state=#{state}"}
      )
      res.finish
    end
    ```

    I exploit the web perf rule of thumb that **most requests are reads, and most reads use HTTP GET requests.** I can safely reconnect Rails to the region-local database replica. The gem builds the replica URI using Fly.io's DNS service discovery and the `FLY_REGION` environment variable.

    ```ruby
    database_uri = URI.parse(ENV['DATABASE_URL'])
    database_uri.host = "#{ENV['FLY_REGION']}.#{database_uri.hostname}"
    database_uri.port = 5433
    database_uri.to_s
    ```

    As a result, HTTP GET requests are passed directly down to the Rails application. And, in the normal case, they return after a speedy round trip to the database replica.

    But GET requests occasionally perform writes. It's dirty, but true.

    Fortunately, Postgres won't allow writes to a Postgres read replica. When a database write slips through, the Ruby Postgres library throws an exception. The gem inserts another middleware — at the _bottom_ of the stack — to catch the`PG::ReadOnlySqlTransaction` exception. This halts the response and asks Fly.io to replay the original request in the primary region.

    ```ruby
    def call(env)
      @app.call(env)
    rescue PG::ReadOnlySqlTransaction, ActiveRecord::StatementInvalid => e
      if e.is_a?(PG::ReadOnlySqlTransaction) || e&.cause&.is_a?(PG::ReadOnlySqlTransaction)
        RegionalDatabase.replay_in_primary_region!(state: "captured_write")
      else
        raise e
      end
    end
    ```

    It could stop here. But there are a bunch of requests for which we don't have to do this dance. It's safe to assume that **non-idempotent HTTP requests intend to write to the database.** This includes, by default, POST, PUT, PATCH and DELETE requests.

    So, from high in the middleware stack, the gem halts and replay probably-write requests in the primary region, which prevents unnecessary application requests in the secondary region.

    One catch with this setup is: physics. Imagine we're handling a large replicated write — say, an HTTP POST of a large recipe entry in Santiago, Chile. Something that can happen is that a request to read that entry back from Santiago can race the replication of the write from Paris, and lose. You see this pattern, &quot;_create-and-redirect-to-show_&quot;, somewhat regularly in Rails apps, and if you break it, you can get a poor user experience.

    To prevent this, **replayed requests set a configurable time threshold in a cookie**. Requests arriving within the threshold sent by the browser will be sent to the primary region. This is a simple but valuable trade-off: a temporary performance penalty in exchange for consistency. Remember, we assume that most uses of the application won't write at all; the worst case isn't terrible, and the common case is very fast. It's usually the right trade.

    Curiously, this approach mirrors the Rails default implementation of [read/write splitting between primary and replica databases](https://edgeguides.rubyonrails.org/active_record_multiple_databases.html#activating-automatic-connection-switching).

    ## I ❤️  Rack

    Apart from Fly.io's magic, the Rack standard made this gem a cinch to implement. Rack is one of the major successes of the Ruby and Rails development environment. It's underappreciated and deserves more appreciation, so here's some love.

    Most web apps share a lot of common behavior in marshaling, unmarshaling, validating, and routing requests. These are the basic features that a web framework provides, and why frameworks are so popular. It used to be difficult (and on some platforms it still is) to change those behaviors: you had to change your application, or, worse, the framework itself to accomplish it.

    Python's WSGI was probably the first standard aimed at solving this problem. Rack came shortly after, inspired by WSGI. Both provide a simple, elegant interface for inserting common behavior between web servers and applications. This also happens to be a great way to simplify framework-specific behavior.

    Try typing `rails middleware` in a Rails app production environment:

    ```ruby
    use ActionDispatch::HostAuthorization
    use ActionDispatch::SSL
    use Rack::Sendfile
    use ActionDispatch::Static
    use ActionDispatch::Executor
    use ActiveSupport::Cache::Strategy::LocalCache::Middleware
    use Rack::Runtime
    use Rack::MethodOverride
    use ActionDispatch::RequestId
    use ActionDispatch::RemoteIp
    use Rails::Rack::Logger
    use ActionDispatch::ShowExceptions
    use ActionDispatch::DebugExceptions
    use ActionDispatch::ActionableExceptions
    use ActionDispatch::Callbacks
    use ActionDispatch::Cookies
    use ActionDispatch::Session::CookieStore
    use ActionDispatch::Flash
    use ActionDispatch::ContentSecurityPolicy::Middleware
    use ActionDispatch::PermissionsPolicy::Middleware
    use Rack::Head
    use Rack::ConditionalGet
    use Rack::ETag
    use Rack::TempfileReaper
    run Cookherenow::Application.routes
    ```

    Exception handling, caching, session management, cookie encryption, static file delivery - all implemented as Rack middleware. Building apps this way provides a clear path for a request to reach an application, and more importantly, a standard way to _insert middlewares at a specific location._ The framework is now programmable.

    The `fly-ruby` gem implements two Rack &quot;middlewares&quot;. It's idiomatic, and easy to shoplift (from, say, [Sentry's exception handling library](https://github.com/getsentry/sentry-ruby)).

    ## What about background jobs?

    Background jobs are a core piece of infrastructure for most Rails apps. Naturally, they'll need to write to the database.Restricting worker processes to the primary region is the simplest way to handle such jobs in a multi-region scenario.

    But if we're using a database - like Postgres or Redis - to store the jobs, _queuing up the job itself will be slow_ from secondary regions. If we enqueue lots of jobs in GET requests, this performance loss could offset our gains.

    Furthermore, some apps - like [Discourse](https://www.discourse.org) - run smaller background jobs [in the web process itself](https://github.com/discourse/discourse/blob/main/lib/scheduler/defer.rb#L85). Both scenarios need to write the primary database without relying on HTTP trickery.

    For example, we might add code to `fly-ruby` like this.

    ```ruby
    Fly.on_primary do
      Recipes.transform
    end
    ```

    The Rails support for read/write splits takes a similar path to [force a specific database connection](https://edgeguides.rubyonrails.org/active_record_multiple_databases.html#using-manual-connection-switching).

    ## Where this breaks down

    Some complex Rails applications make this kind of setup difficult, like [Discourse](https://www.discourse.org).

    **Some apps write on every request**. Think about things like lazy authentication session token refresh, or touching a user's `last_seen` attribute. These generate unexpected writes, and, worse, waste cycles on regional app servers.

    Moving work like this to a background job is a fine solution to this problem. It also happens to be a best practice for keeping applications performant and resilient.  So if you can do this, you should.

    Background jobs in Rails apps without infrastructure support for jobs might seem like a pain. But it doesn't have to be. You could implement the [ActiveJob in-memory queue](https://github.com/rails/rails/blob/main/activejob/lib/active_job/queue_adapters/async_adapter.rb) for jobs you would not mind losing on restart.

    **Complex interactions with other data stores may slow requests down.** By default, Discourse backs statistics and logs into Redis, and reads and writes to it on every request. This can be tricky to deal with in a global deployment. Solutions like read/write splitting may be useful here, but they're not &quot;just install this gem&quot;-simple to implement.

    **Relying on catching read-only exceptions could lead to inconsistent data.** For example, a visit counter being incremented in Redis _before the Postgres exception is raised_ would be bumped twice: once in the secondary region request, and again in the replayed primary region request. Most apps aren't going to care about this, but you want to be aware of it.

    **Large multipart file uploads might be doubly slow** if they're replayed _after_ the browser upload completes.

    ## What's next?

    [Region-local Redis caches](https://fly.io/blog/last-mile-redis/) would be dope. For Rails apps, this could mean that the common approach of fragments or Russian-doll caching could get a boost at the global level without much work.

    And more adapters! Adapters for [Nodejs/Express](https://github.com/superfly/fly-node), Phoenix, Django, and friends. They're totally doable and you should [get in touch](https://community.fly.io/) if you like these kind of projects.
- :id: blog-better-business-intelligence-in-elixir-with-livebook
  :date: '2021-07-30'
  :category: blog
  :title: Better Business Intelligence in Elixir with Livebook
  :author: mark
  :thumbnail:
  :alt:
  :link: blog/better-business-intelligence-in-elixir-with-livebook
  :path: blog/2021-07-30
  :body: |2


    <div class="lead">Fly runs apps (and databases) close to users, by taking Docker images and transmogrifying them into Firecracker micro-vms, running on our hardware around the world. You should try deploying an Elixir app, right now: [it only takes a few minutes](https://fly.io/docs/elixir/).</div>

    As a developer, has your manager ever come and asked a question like, "How much money are we making?" If you were a line-of-business developer at a global insurance company, you'd reach for your handy, nosebleed-expensive Business Intelligence (BI) suite to answer this question. But you're not, so how did you answer it for them?

    Obviously, you'd do what we all do. You'd SSH into your server, start an Elixir `iex` session or a Rails console, then run some scripts to query data, sum numbers, and come up with answers.

    Well, give yourself a raise! Because you just built a  BI suite.

    It may not seem super sophisticated, but it solves the business need. And for problems like this, Livebook can be a better BI tool for Elixir developers.

    ## BI What?

    What is a [BI tool](https://en.wikipedia.org/wiki/Business_intelligence)?

    > Business intelligence (BI) comprises the strategies and technologies used ... for the data analysis of business information.

    Translated from Gartner-speak, that means any tools you run to get a picture of how the business is doing are BI tools.

    In the last bootstrapped startup I worked at, management routinely asked backend developers for business numbers. It was simple stuff, like:

    - How many new clients did we add this week?
    - What was our customer's spend?
    - Who are our top 10 customers this week and what were their numbers?

    As simple as this stuff seems, it's really important for those business focused leaders to understand and make better decisions. That's why global insurance companies with applications that are too complicated to bring up a Rails console on spend six figures on BI suites.

    How did we get those answers? Using our Elixir `iex`, or interactive shell. We ran some scripts and gave them CSV friendly rows they could add to their spreadsheets. In that early stage startup, we were using `iex` as our BI tool. At a startup before that, we did the same thing but using the Rails console.

    If you're using the Rails console, Elixir's `iex`, or another REPL to examine your data, then that's your BI tool for now. But with Elixir, we can do better. Livebook gives you data, charts and graphs too, but because it's executing your Elixir code, it can also call out to your other integrations and pull in even more.

    To understand why Livebook can be a better tool, let's go further and talk about BI tools in general, not just your REPL.

    ## Old School Business Intelligence

    Our premise in this post is that we can give "serious" BI tools a run for their money with Elixir and Livebook. Let's see what we're up against.

    Companies spend lots of money every year on their BI tools. You hear some of the numbers and it seems bananas. But it's because they _add_ a lot of value. Spotting trends in your data and customer behavior can make the difference of success and failure for a company.

    Most BIS tools are commercial. But there's a handful of credible open source projects. [Metabase](https://www.metabase.com/), for example, is an open source BI tool that works quite well. It connects directly to an application's database and helps you do some spelunking, aggregation, and  shiny graphing. You can even create and share custom dashboards. Think of it as Grafana, but for MBAs – it's a great tool.


    <div class="callout">
    Deploying Metabase alongside your app might look like this:

    ![Metabase app setup](livebook-metabase-connection.png)

    Metabase is an application that probably shouldn't be exposed publicly and it needs direct access to your database. It's also a bit of a mammoth – one doesn't just walk into Metabase and expect to get anything done, there's a real learning curve even when you know how to write SQL. It can be a heavy tool when you just want to do some quick poking around.

    It's also an app you need to keep running. We sell hosting, so we're generally OK with that. In fact, [Metabase ships a Docker image](https://www.metabase.com/docs/latest/operations-guide/running-metabase-on-docker.html) and Fly lets you quickly [deploy apps using Docker](http://127.0.0.1:43579/docs/app-guides/run-a-global-image-service/#deploying-docker-images-to-fly). Money.

    </div>

    This is fine when you want a dedicated data dashboard or you want to let non-developers see reports and graphs and be business-intelligent. However, when a project is young and you're a developer, digging with code is powerful. This is where Livebook can help!

    ## Why is Livebook Better?

    Let's start with what Livebook is.

    [Livebook](https://github.com/elixir-nx/livebook) started out as Elixir's version of [Jupyter Notebooks](https://jupyter.org/). Jupyter is pretty great. But it turns out that code notebooks on Elixir are something special; they do something you usually can't pull off in Python. That's because Elixir has powerful built-in clustering, built on Erlang's BEAM/OTP runtime. Livebook notebooks can talk directly to running Elixir apps.  And so we can do analysis and visualization directly off the models in our applications.

    Livebook really sings on Fly.io. We make it easy to deploy clusters of Elixir applications that can talk privately between themselves. More importantly: it's easy to bring up a secure [WireGuard](https://www.wireguard.com/) tunnel to those applications. So I can run Livebook locally on my machine, and attach it to any of my apps running on Fly.io!

    ![Livebook remote connection](livebook-remote-lb-connection.png)

    For lean-and-mean startups, this is a win. You only need your app and your database running. Then, on an "as-needed" basis, you connect to your app with Livebook for analysis. Inside Livebook, analysis is done using your app's Elixir code. Livebook therefore doesn't need to connect directly to the database to run queries, and, even better, we get to re-use the business logic, associations, and schemas our apps already have.

    As a BI tool, Livebook notebooks  have these benefits:

    - They use your application's code, like a REPL lets you.
    - They have the ability to generate charts, graphics, and tables.
    - Notebooks are markdown files and can be checked-in with your project. They can be shared with the team! No more, "I can't run the numbers today because Bill is out and he has the scripts."
    - They are self-documenting because it's just markdown.
    - They are designed to be highly reproducible. They are easy to re-run when you want updated numbers.

    Some of this you can pull off using just your REPL and some raw SQL queries. But why would you?   It's easier to use your project's code, database models (Ecto schemas for Elixir) and associations.

    Further,  your apps probably rely on external services like Stripe. Because Livebook talks directly to your Elixir code, you can query those external services with  that code, and then combine the results with your data. This is a combination that even dedicated tools like Metabase have a hard time beating.

    ## Connecting to Your App on Fly

    Great! You have a notebook that loads and visualizes some data! To get the benefits, you need to connect it to your app running on Fly.io.

    Follow the Fly.io [Elixir Guide for Connecting Livebook to your App in Production](/docs/app-guides/elixir-livebook-connection-to-your-app/) to connect Livebook to your app.

    With Livebook connected to your app, you can run your notebook and start gaining insight to your data!

    ## Gaining Intelligence

    How you use Livebook depends on your application and your industry.

    Here are some ideas to get the brain juices flowing. Each of these example notebooks would be a "module" in a serious commercial BI suite, and you'd pay $45k for a license for it.

    ### User Account Setup

    How many of your users have fully set up their accounts?

    Build and share a notebook that tracks where users are in your onboarding process.

    ### Users Bouncing From The App

    Where are accounts stalling out in onboarding?

    Graph the stages accounts are at and let analysts drill into the onboarding funnel.

    ### Sales Analysis

    How were sales for your products or services last month?

    Graph a multi-series chart comparing sales across products.

    What about total sales per week?

    Build a notebook with a date input making it easy to switch the week being charted.

    ### Integrations

    What external financial systems are you integrated with?

    Notebooks can execute your code to query those services and visualize refund rates, processing fees, and more.

    ## Wrapping Up

    What's great about the Livebook approach is you are writing working Elixir code. When you are ready to build an Admin Dashboard page in your app, you've already done the hard work of figuring out what data is valuable and even the code for how to get it!

    <%= partial "shared/posts/cta", locals: {
      title: "Deploy that Elixir App Today",
      text: "You've got that cool Elixir app you've been working on, Fly.io is a great place to deploy it! It's easy to get started.",
      link_url: "https://fly.io/docs/elixir/",
      link_text: "Deploy your Elixir app!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>"
    } %>
- :id: blog-phoenix-moves-to-esbuild-for-assets
  :date: '2021-07-27'
  :category: blog
  :title: 'FYI: Phoenix drops webpack and npm for esbuild'
  :author: mark
  :thumbnail:
  :alt:
  :link: blog/phoenix-moves-to-esbuild-for-assets
  :path: blog/2021-07-27
  :body: |2


    The [Phoenix Framework](https://www.phoenixframework.org/) is the go-to web framework for Elixir developers. A recent [PR was merged](https://github.com/phoenixframework/phoenix/pull/4377) that replaces the use of [node](https://nodejs.org/en/), [npm](https://www.npmjs.com/), and [webpack](https://webpack.js.org/) with [esbuild](https://esbuild.github.io/).

    For those new to [esbuild](https://esbuild.github.io/), it is written in [Go](https://golang.org/) so it compiles to native code and runs really fast. It is a JavaScript bundler that performs many common asset pipeline tasks.

    To understand **why** the change happened, it's good to get a sense of the problems the Phoenix team has been dealing with.

    Of the [Phoenix project's 2,034 total issues](https://github.com/phoenixframework/phoenix/issues), npm was involved in 591, webpack played a role in 79, and brunch was a factor for 171.

    Some of the issues overlapped into multiple areas, so we can safely say that around 30% of reported Phoenix issues were related to JavaScript packaging! That's a lot of issues to support and they aren't even for the language the framework is written in!

    Chris McCord, the creator and maintainer of Phoenix had [this to say](https://twitter.com/chris_mccord/status/1417177471502503936):

    > I say this in the fairest way possible after years of support & churn – I now consider placing the burden of node/npm/webpack on new users as actively harmful. Also, long-term reproducible builds are essential for maintainable software & node has not stood this test of time. With esbuild, those that want to take advantage of the innovation in the node community need only to `npm install` and esbuild will handle it, so the opt-in path is as simple as it gets when folks are doing complex client-side development that necessarily involves node tooling.
    >
    > This change allows newcomers a seamless getting started experience, long-term moderate js/css users a rock-solid reproducible build for the lifetime of their projects, and advanced client-side SPA users a trivial opt-in path. So esbuild allows us to execute on all fronts!

    If your team does a lot of front-end javascript, you likely have your own customized setup using webpack or other tools anyway. The Phoenix change to esbuild **does not prevent** you from using whatever asset processing tools you need.

    Speaking of the esbuild change, José Valim, the creator of Elixir [said](https://twitter.com/josevalim/status/1417017859847475201):

    > This matters because it gives us full ownership of the getting started process. If someone runs the `phx.new` installer for v1.6 in 5 years from now, they should still get a fully functional project.
    >
    > In the last weeks, `phx.new` has been broken because of `node-sass` and then npm v7. Plus other reasons in the past. We always have to catch up.
    >
    > It is extremely important that your first ever Phoenix command always succeeds, and this gets us very close to that!

    Interested in trying it out right now? [José Valim explains how](https://twitter.com/josevalim/status/1419041371315482625):

    > If you are moving an existing project, then running [Phoenix] v1.6 should be totally fine.
    >
    > If you want only esbuild, you don't need to move to v1.6 ([see example](https://github.com/josevalim/phx_esbuild_demo/commit/0546034f26ac5d58d12867e5843037ce1b2d4dd1)), but remember esbuild can still leave zombie processes until a PR is merged upstream.

    The change to esbuild will be part of the upcoming Phoenix 1.6 release which is expected "soon". As the Phoenix release nears and more people are trying it out, we'll get a better sense of any snags or migration issues.

    This is an exciting change! Over the years, I've fought many times with broken JS build pipelines because some dependency updates were needed. I'm very interested in seeing if this meets the goal of being a more stable system over time.

    <%= partial "shared/posts/cta", locals: {
      title: "Fly ❤️ Elixir",
      text: "Fly is an awesome place to run your Elixir apps. Deploying, clustering, connecting Observer, and more is all supported!",
      link_url: "https://fly.io/docs/elixir/",
      link_text: "Deploy your Elixir app today!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>"
    } %>
- :id: blog-last-mile-redis
  :date: '2021-07-16'
  :category: blog
  :title: Last Mile Redis
  :author: kurt
  :thumbnail: last-mile-redis-thumbnail.jpg
  :alt:
  :link: blog/last-mile-redis
  :path: blog/2021-07-16
  :body: "\n\n<div class=\"lead\">Fly runs apps (and databases) close to users, by
    taking Docker images and transmogrifying them into Firecracker micro-vms, running
    on our hardware around the world. You should try deploying an app, right now:
    [it only takes a few minutes](https://fly.io/docs/speedrun/).</div>\n\n100 milliseconds
    is the magic number. For a backend application, a sub-100ms response time is effectively
    instantaneous, and people love using \"instant\" apps. Since we're all dirty capitalists,
    we'd add: if people love your app, you'll make more money. But you can also chase
    sub-100ms for the endorphins.\n\nWhen app developers need to shave tens of milliseconds
    off their request times, they start with their database. But optimizing databases
    is painful. When it comes to making \"things-that-involve-data\" fast, it's easier
    to add a layer of caching than to make database changes. Even basic cache logic
    can shave tens of milliseconds off database queries. So backend developers love
    caches.\n\n<div class=\"callout\">If you want to skip the end, all you need to
    know is we've built a Redis app that runs globally, includes \"instant\" purge,
    and works with whatever backend framework you build apps with. <a href=\"https://github.com/fly-apps/redis-geo-cache\">Here's
    the source</a>.</div>\n\nWe love caches too. And  we think we can do better than
    incremental 10ms improvements.\n\nWhat's better than good cache logic in centralized
    app servers? Caching data close to end users, regardless of whether they're in
    Singapore or Amsterdam. Geo-routing can shave _hundreds of milliseconds,_ plural,
    off application response times. When a request hits a server in your own city,
    and is served hot from data conveniently stored in memory, its response is perceptibly
    faster than it would have been had it instead schlepped across an entire continent
    to get to you.\n\nDoes this sound like promotional content? That's because we
    believe it. It's why we built Fly.io: we think backend apps can, uh, scream, when
    you ship them on CDN-like infrastructure.\n\nAnd, as it happens, Redis has _very_
    interesting knobs that make it work  well when you scatter instances around the
    world.\n\n### One weird CDN thing\n\nWhen you [build a CDN](https://news.ycombinator.com/item?id=22616857),
    you [learn stuff](https://fly.io/blog/the-5-hour-content-delivery-network/). Here's
    an important thing we learned about geographic caching.\n\nCache data overlaps
    a lot less than you assume it will. For the most part, people in Singapore will
    rely on a different subset of data than people in Amsterdam or São Paulo or New
    Jersey.\n\nStop and think about it and it makes sense. People in Singapore eat
    in restaurants in Singapore, send cash to their friends in Singapore, and talk
    to their Singapore friends about meeting up. People in New Jersey care about show
    schedules in New Jersey and the traffic in New Jersey. They care a lot more about
    hoagies than people in Singapore. Humans are data magnets. They tend to work in
    companies together with people who live relatively near them, and talk with their
    friends, who,  again, are relatively close to each other.\n\nWhat you find when
    you look at a CDN cache is that for most apps, data is only duplicated in one
    or two regions. It almost never shows up in _all_ the regions.\n\nThis simplifies
    things. Take an app that needs ten Redis servers to keep up with its load. The
    conventional way to build that system is to park all those servers in `us-east-1`,
    and then implement sharding logic to spread the load across the cache servers.
    That sharding logic infects the rest of the system. But we can usually exploit
    our CDN observation to avoid that: if we can deploy our app in multiple geographic
    regions, we can just have one server per region, without any explicit sharding
    logic. Cities. They're nature's shards!\n\nLet's talk a bit about how you'd do
    this.\n\n<div class=\"callout\">We have to talk our book here for a second, because
    it'll make the rest of this make sense. The whole premise of Fly.io is that [we
    make it trivial to get a Docker container running in a bunch of different geographic
    regions](https://fly.io/docs/speedrun/). There are other ways to run containers
    around the world, and if you prefer them, what we have to say here still makes
    sense. Just take it as a given that you can easily boot stuff up in Singapore,
    Newark, and Amsterdam.</div>\n\n### JBOR\n\nThe most boring way to exploit geographic
    cache locality is \"just  a bunch  of  Redii\".\n\nRun standalone Redis servers
    and app servers in each region you care about. Treat them as independent caches.
    When a user looks up the review score for [Johnny's Beef](https://www.lthforum.com/bb/viewtopic.php?f=28&t=32598&sid=9154a4871bdc01c44b5bd7667831bc8a)
    in Chicago, the Chicago app server checks the Chicago Redis cache. Everyone involved
    in the Chicago request is blissfully unaware of whatever is going on in Singapore.\n\nWe
    lean on caches because apps are read-heavy. But writes happen. If you're running
    caches all over the world, they can eventually drift from their source of truth.
    Bad cache data will really irritate people.  It can break apps entirely, which
    is why you have the keyboard shortcut for  \"hard refresh\" in muscle memory.
    So, when data changes, the global cache fabric should _also_ change, even when
    the cache fabric is JBOR.\n\nThis sounds distributed-systems-hard. But it doesn't
    have to be. We can use a [key based cache invalidation scheme](https://archive.is/CDaVX)
    to keep things fresh for an app with standalone cache servers. Key-based invalidation
    inverts the intuitive roles of keys and values: instead of a durable key pointing
    to changeable value, values never change, only the keys (for instance, by timestamping
    them). Database changes generate new keys;  stale cache values  eventually expire
    from neglect. When all is right with the world, this can be good enough.\n\nBut
    we live in a fallen world. Apps have bugs. So do people. Bad information eventually
    pollutes caches.  If we can purge bad cache data our life is easier. If we can
    purge it everywhere, instantly, we'll be as wizards. Wizards with a \"[build a
    whole CDN and take it public](https://developer.fastly.com/reference/api/purging/)\"
    level of power.\n\nLet's seize this power for ourselves.\n\n### Abusing replication
    for instant cache purge\n\nRedis has a simple replication model: we can start
    a Redis server with `replicaof primary-redis.internal 6379` and it will grab a
    copy of the existing database and keep it in sync until we shut it down. The primary
    server doesn't even need to know ahead of time. It's blissfully simple.\n\nWe
    can exploit this. Create a primary Redis in Dallas. Add replicas in Singapore,
    Amsterdam, and Sydney.  Now: write to the primary. The whole world updates. We've
    \ got   a global cache fabric that's always up to date.\n\nLike any distributed
    cache fabric, we'll inevitably cache something we shouldn't. Somehow, the cache
    key `global-restaurant-ranking-johnnys-beef` reads `105`. Not OK! But we can just
    issue a   `DEL global-restaurant-ranking-johnnys-beef`and it'll be back to `1`
    , everywhere, fast enough to seem instant.\n\nThis seems great. But there's a
    catch: that CDN observation we made earlier. If Singapore shared a lot of information
    with Chicago, this would be close to the \"right\" global Redis configuration.
    But they don't; very little data overlaps between regions.\n\nSo we're not quite
    there yet. But we have more tricks.\n\n### Eventually consistent, never consistent:
    why not both?\n\nOne kind of database cluster has \"strong\" consistency: once
    data is written, we trust that subsequent reads, anywhere in the cluster, see
    the new data. More frequently, we have some degree of \"eventual\" consistency:
    the data will populate the whole cluster… at some point, and we're not waiting.\n\nOur
    Redis replica scheme has eventual consistency. We write to a primary Redis instance
    and trust the replicas will get themselves in sync later on.\n\nMeanwhile, the
    JBOR cluster is never consistent – in the same way that two people who've never
    met each other \"aren't dating\".   But on the other hand, we like it because
    it optimizes storage by storing only what each region needs.\n\nWhat we need is
    a way to treat each region mostly independently and sync _some_ changes from a
    central source of truth.\n\nWhich gets us to the an  [interesting config option](https://redis.io/topics/replication#read-only-replica):
    `replica-read-only`. This setting defaults to `yes`, and does what you'd expect.
    But check this out:\n\n```\nreplica-read-only no\nreplicaof primary-redis.internal
    6379\n```\n\nNow we have a replica that also accepts writes. \U0001F631.  This
    is terrible for a backend database. But this isn't a database; it's a cache fabric.
    Our primary is in Dallas. But Singapore's app server can still write directly
    to Singapore's Redis replica. And it still syncs changes from the primary! It's
    still in charge!\n\nSo, when we need to make sure bad cache data goes away everywhere,
    we can perform an \"instant\" cache purge by issuing  that  `DEL`  to the the
    Dallas primary.\n\n<%= partial \"shared/posts/cta\", locals: {\n  title: \"Cluster
    all the things!\",\n  text: \"Our favorite thing about Fly.io is how easy it is
    to prototype and deploy stuff like this. From a working Docker container locally,
    you can have a globally distributed cluster up and running in single-digit minutes.\",\n
    \ link_url: \"https://fly.io/docs/speedrun/\",\n  link_text: \"Try Fly.io for
    free&nbsp;&nbsp;<span class='opacity-50'>&rarr;</span>\"\n} %>\n\n\n### Beyond
    the purge\n\nA thing that happens when you build a platform for running clusters
    of globally-distributed stuff is that you stumble onto [interestingly simple distributed
    designs](https://fly.io/blog/globally-distributed-postgres/). This seems like
    one of them. We like it!\n\nThis sort of implied, selective replication potentially
    gives us a lot more than just global cache purge. We can push _any_  global content
    to all the cache regions, simply by writing to the primary:\n\n```\nSET daily_message
    \"happy tuesday nerds\"\n```\n\nOr, we can simulate a  distributed fan-out queue
    by pushing to a list on the primary…\n\n```\nLPUSH notifications \"time for ice
    cream\"\n```\n\n… and then popping from that list in each region.\n\n```\nBRPOP
    notifications\n\"time for ice cream\"\n\nBRPOP notifications\n\"\"\n```\n\nWe
    can let regions selectively replicate by choosing when to write to their local
    cache and when to write globally, in a manner similar to the one we used to [globally
    distribute Postgres.](https://fly.io/blog/globally-distributed-postgres/) No doubt
    there's a zillion other things we haven't thought about. It's been hard to play
    with these ideas, because almost nobody runs multi-region AWS for simple applications.
    But anyone can run a multi-region Fly.io app with just a couple commands.\n\nWe're
    psyched to see what else people come up with.\n\n---\n\nWant to know more? [Join
    the discussion](https://community.fly.io/)."
- :id: blog-livebook-with-kino
  :date: '2021-07-01'
  :category: blog
  :title: 'FYI: Livebook 0.2 with Kino'
  :author: mark
  :thumbnail:
  :alt:
  :link: blog/livebook-with-kino
  :path: blog/2021-07-01
  :body: |2


    [Livebook 0.2 was released](https://twitter.com/josevalim/status/1405586165315604486). The big news here was the announcement of "Kino" (meaning "cinema"). It's a client-side focused feature that animates data changes. José Valim created a [video demonstrating](https://www.youtube.com/watch?v=MOTEgF-wIEI) some of the new Livebook features. Use [this link](https://youtu.be/MOTEgF-wIEI?t=1132) to jump to the part of the video, it's 18:52 in, that shows Kino in action.

    Another big change is the addition of inputs! [Accessing an input's data](https://github.com/elixir-nx/livebook/pull/328) in your Elixir cell looks like this: `IO.gets("input name: ")`  Input values come in as a string so it may need to parsed or processed for your particular use.

    The other big [Kino feature is called `data_table`](https://twitter.com/josevalim/status/1407084358750572548). With this, you can easily show any data as a table with pagination and sorting.

    Livebook is actually a big deal. It lowers the bar on a lot things for Elixir. It can be used to:

    - Teach the Elixir language itself
    - Document how a library works
    - Play with Nerves and embedded hardware interactively
    - Connect to a live production system!

    I'm bullish on Livebook and what I think it can do for the entire Elixir community. Livebook is definitely something to watch!

    ## Other Elixir News

    In other Elixir news, conferences are returning! Many have been only virtual by necessity. While this has been nice because I could attend virtually without the need to travel. I have missed seeing people and meeting new people the way I did at physical conferences.

    Two significant conferences were announced that are hybrid (physical and virtual) or physical for 2 days and virtual for 2 days. Some interesting experiments going on here!

    In order of their dates:

    - [ElixirConfEU](https://www.elixirconf.eu/) - A hybrid conference being held in Warsaw, Poland and virtual as well. The dates are September 9-10.
      - Sasa Juric, author of Elixir in Action, [will keynote](https://twitter.com/ElixirConfEU/status/1407316169313374216).
      - José Valim, creator of Elixir, [will be interviewing Chris McCord](https://twitter.com/ElixirConfEU/status/1406955500512612355), creator of Phoenix Framework, at the conference. That could be fun!

    - [ElixirConf US 2021](https://2021.elixirconf.com/) is being held physically in Austin, TX October 12-13 and online, October 14-15. This is an interesting experiment. A 4-day conference where the physical and virtual talks aren't the same!

    <%= partial "shared/posts/cta", locals: {
      title: "Livebook works awesome on Fly",
      text: "Livebook supports collaborative editing. When you host your own instance you can invite people to join you!",
      link_url: "https://fly.io/docs/speedrun/",
      link_text: "Boot up a Livebook!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>"
    } %>
- :id: blog-globally-distributed-postgres
  :date: '2021-06-30'
  :category: blog
  :title: Globally Distributed Postgres
  :author: kurt
  :thumbnail: cats-around-the-world-thumbnail.png
  :alt:
  :link: blog/globally-distributed-postgres
  :path: blog/2021-06-30
  :body: "\n\n<div class=\"lead\">Fly runs apps (and databases) close to users, by
    taking Docker images and transforming them into Firecracker micro-vms running
    on our hardware around the world. You should try deploying an app: [it only takes
    a few minutes](https://fly.io/docs/speedrun/).</div>\n\nThis is a story about
    a cool hack we came up with at Fly. The hack lets you do something pretty ambitious
    with full-stack applications. What makes it cool is that it’s easy to get your
    head around, and involves just a couple moving parts, assembled in a simple but
    deceptively useful way. We won’t bury the lede: we’re going to talk about how
    you can deploy a standard CRUD application with globally-replicated Postgres,
    **for both reads and writes**, using standard tools and a simple Fly feature.\n\nIf
    you’ve build globally distributed apps in the past, you’re probably familiar with
    the challenges. It’s easy to scale out a database that’s only ever read from.
    Database engines have features to stand up “read replicas”, for high-availability
    and caching, so you can respond to incoming read requests quickly. This is great:
    you’re usually more sensitive to the performance of reads, and normal apps are
    grossly read-heavy. \n\nBut these schemes break down when users do things that
    update the database. It's easy to stream updates from a single writer to a bunch
    of replicas. But once writes can land on multiple instances, mass hysteria! Distributed
    writes are hard. \n\nApplication frameworks like Rails have features that address
    this problem. Rails will let you automatically switch database connections, so
    that you can serve reads quickly from a local replica, while directing writes
    to a central writer. But they’re painful to set up, and deliberately simplified;
    the batteries aren’t included. \n\nOur read/write hack is a lot simpler, involves
    very little code, and it’s easy to understand. Let’s stick with the example of
    a standard Rails application (a difficult and common case) and dive in.\n\n###
    When Your Only Tool Is A Front-End Proxy, Every Problem Looks Like An HTTP Header\n\nYou
    can think of Fly.io as having two interesting big components. We have a system
    for transforming Docker containers into fast, secure Firecracker micro-VMs. And
    we have a global CDN built around our Rust front-end proxy. \n\nWe run all kinds
    of applications for customers here. Most interesting apps want some kind of database.
    We want people to run interesting apps here, and so we provide Fly Postgres: instances
    of Postgres, deployed automatically in read-replica cluster configurations. If
    your application runs in Dallas, Newark, Sydney and Frankfurt, it’s trivial to
    tell us to run a Postgres writer in Dallas and replicas everywhere else. \n\nFly
    has a lot of control over how requests are routed to instances, and little control
    over how instances handle those requests. We can’t reasonably pry open customer
    containers and enable database connection-switching features for users, nor would
    anyone want us to.  \n\nYou can imagine an ambitious CDN trying to figure out,
    on behalf of its customers, which requests are reads and writes. The GETs and
    HEADs are reads! Serve them in the local region! The POSTs and DELETEs are writes!
    Send them to Dallas! Have we solved the problem? Of course not: you can’t look
    at an HTTP verb and assume it isn’t going to ask for a database update. Most GETs
    are reads, but not all of them. The platform has to work for all the requests,
    not just the orthodox ones.\n\nSo, short of getting in between the app and its
    database connection and doing something really gross, we’re in a bit of a quandary.\n\n<div
    class=\"callout\">\nA bit more on \"gross\" here: you can get your database layer
    to do this kind of stuff for you directly, using something\nlike [pgpool](https://www.pgpool.net/docs/latest/en/html/runtime-config-load-balancing.html)
    so that the database layer itself knows where to route transactions. But there's
    a problem with this: your app doesn't expect this to happen, and isn't built to
    handle it. What you see when you try routing writes at the database connection
    layer is something like this:\n\n1. A read query for data, from read replica,
    perhaps for validation: **0ms**. \U0001F918\n\n2. A write to the primary, in a
    different region: **20-400ms**. \U0001F626\n\n3. A read query to the primary,
    for consistency, in a different region: **20-400ms**. \U0001F640\n\n4. More read
    queries against primary for consistency, in a different region: **20-400ms**.
    \U0001F631\n\n5. Maybe another write to the primary: **20-400ms**. \U0001F635\n\n6.
    Repeat ☠️.\n\nIt is much, much faster to ship the whole HTTP request where it
    needs to be than it is move the database away from an app instance and route database
    queries directly. Remember: replay is happening *with Fly's network*. HTTP isn't
    bouncing back and forth between the user and our edge (that would be slow); it's
    happening inside our CDN.\n</div>\n\nIt turns out, though, that with just a little
    bit of cooperation from the app, it’s easy to tell reads from writes. The answer
    is: every instance assumes it can handle every request, even if it's connected
    to a read replica. Most of the time, it’ll be right! Apps are read-heavy! \n\nWhen
    a write request comes in, just try to do the write, like a dummy. If the writer
    is in Dallas and the request lands in Frankfurt, the write fails; you can't write
    to a read replica. Good! That failure will generate a predictable exception. Just
    catch it:\n\n```ruby\nrescue_from ActiveRecord::StatementInvalid do |e|\n  if
    e.cause.is_a?(PG::ReadOnlySqlTransaction)\n    r = ENV[\"PRIMARY_REGION\"]\n    response.headers[\"fly-replay\"]
    = \"region=#{r}\"\n    Rails.logger.info \"Replaying request in #{r}\"\n    render
    plain: \"retry in region #{r}\", status: 409\n  else\n    raise e\n  end\nend\n```\n\nIn
    8 lines of code, we catch the read-only exceptions and spit out a `fly-replay`
    header, which tells our proxy to retry the same request in the writer’s region.
    \ \n\n<div class=\"callout\">\nYou could imagine taking this from 8 lines of code
    to 1, by packaging this logic up in a gem. That’s probably a good idea. The theme
    in this blog post though is that all the machinery we’re using to make this work
    is simple enough to keep in your head.\n</div>\n\nOur proxy does the rest of the
    work. The user, none the wiser, has their write request served from Dallas. Everything…
    works?\n\n## The Fly-Replay Header\n\nThis design isn’t why we built the `fly-replay`
    feature into our proxy.\n\nThe problem we were originally aiming at with the header
    was load balancing. We have clients that serve several hundred million images
    a day off Fly. And, some time ago, we had incidents where all their traffic would
    get routed to out-of-the-way places, like Tokyo, which struggled (ie: melted)
    to keep up with the load. \n\nThe immediate routing issues were easy to fix. But
    they weren’t the real problem. Even as traffic was getting sent to overloaded
    Tokyo servers, we had tons of spare capacity in other nearby regions. Obviously,
    what we want to do is spread the load to that spare capacity. \n\nBut the obvious
    solutions to that problem don’t work as well as you’d assume in practice. You
    can’t just have Tokyo notice it’s overloaded and start sending all its traffic
    to Singapore. Now Singapore is melting! What’s worse, traffic load is an eventual
    consistency problem, and the farther away an unloaded region is from Tokyo, the
    less likely it is that Tokyo can accurately and instantaneously predict its load.
    \n\nHere’s the thing, though: HTTP is cheap, especially when you’ve got persistent
    connections (the backhaul between our edges and the workers where apps actually
    run is HTTP2). \n\nAnd so, `fly-replay`. When a request hits Tokyo, we estimate
    which process has capacity based on gossiped load data. We then send the request
    to the server running that process. _Then_ we check to see if the process still
    has capacity. If it does, great, we dump the request into the user process. If
    it doesn't, we send a reply to the edge server saying \"hey, this process is full,
    try another\". The effect is something we call latency shedding: if we can try
    every instance of an app process quickly enough we'll always be able to service
    a request.\n\n<div class=\"callout\">\n<h3>You can see this in action.</h3>\nIf
    this stuff is interesting to you, check out [this simple Rails app we put together](https://fly-global-rails.fly.dev/)
    (maybe we've mentioned that it's ridiculously easy to boot apps up on Fly?). What
    you're looking at is a globally deployed stock Rails app that explicitly steers
    the database with `fly-replay`. You're landing on an instance of the application
    because our BGP Anycast routes took you there; for me, my connecting region is
    Chicago. But you can tell the app to replay your request in a bunch of other regions;
    click the tabs.\n</div>\n\n## 80% Of The Time It Works Every Time\n\nWhether you
    understand it in your bones or not, a big part of why you’re using a database
    like Postgres is that you want strong consistency. Strongly consistent databases
    are easy to work with. You issue a write to a consistent database, and then a
    read, and the read sees the result of a write. When you lose this feature, things
    get complicated fast, which is part of why people often prefer to scale up single
    database servers rather than scaling them out.\n\nObviously, once you start routing
    database (or HTTP) requests to different servers based on local conditions, you’ve
    given up some of that consistency. We can’t bend the laws of physics! When you
    read from Frankfurt and then write to Dallas, Dallas will quickly replicate the
    altered rows to Frankfurt, but not instantly. There’s a window of inconsistency.\n\nWe
    have a couple of responses to this.\n\nFirst, in a lot of cases, it might not
    matter. There are large classes of applications where short inconsistencies between
    writes and subsequent reads aren't a big problem. Data replicates fast enough
    that your users probably won't see inconsistencies. If displaying stale data doesn't
    cause real problems, maybe worry about it another time.\n\nSecond, if you want
    “read-your-own-writes-between-requests” behavior, you can implement that with
    the same header. When you set `fly-replay` to the writer, set a timestamp in the
    user’s session, during which you fly-replay all the requests. HTTP is cheap! \n\nThird,
    you could simulate synchronous replication. _Actual_ synchronous replication doesn't
    work well on cross-geo clusters, but Postgres does let you see query replica freshness.
    [We run these kinds of queries for health checks](https://github.com/fly-apps/postgres-ha/blob/main/cmd/flycheck/pg.go#L121-L151).
    You could build a little logic into your app to check the replication lag on the
    user's region and delay the HTTP response until it's ready.\n\nFinally, though,
    it’s just the case that this pattern won’t work for every application. It’s neat,
    and it makes Postgres read-replicas much easier to use, but it isn’t a cure-all.\n\n<%=
    partial \"shared/posts/cta\", locals: {\n  title: \"flyctl postgres create\",\n
    \ text: \"That's it. That's the tweet. One command gets you a multi-database-capable
    Postgres cluster, with high-availability read replicas, in whichever regions you
    want us to run it in.\",\n  link_url: \"https://fly.io/docs/speedrun/\",\n  link_text:
    \"Try Fly for free&nbsp;&nbsp;<span class='opacity-50'>&rarr;</span>\"\n} %>\n\n##
    Maybe Your App is Actually Write Heavy\n\nThere are database engines designed
    handle distributed write scenarios, some with native geographic partitioning –
    and you can use them on Fly, too.\n\nHave you seen CockroachDB? It's amazing.
    CockroachDB gives developers all the tools they need to model geographic distribution
    into their schemas. With the right schema, Cockroach moves the chunks of data
    close to the most active users and keep writes wicked fast. You’ll need the Enterprise
    edition to really take advantage of this, but it’s legit. \n\nGenerally, if you’re
    prepared to manage your own database, [you can deploy it on Fly](https://github.com/fly-apps/cockroachdb)
    by attaching volumes to your instances; volumes are arbitrarily-sized block devices
    and some rules that we enforce in the background to pin your app to worker hosts
    where those volumes live. Volumes are the storage fabric on which our Postgres
    is built, and you can just build directly on them.\n\nSo you can see the outline
    of two big approaches here: you can configure Postgres (or, rather, we can configure
    Postgres for you) to deploy in a configuration where you’re distributed *enough*
    to get 90% of the benefit of distribution, and, if that works for you, you can
    stick with Postgres, which is great because everybody loves Postgres, and because
    Postgres is what your existing apps already do.\n\nOr, you can deploy on distributed-by-design
    databases like Cockroach, where databases are split into geographically distributed
    regions and concurrency is managed by Raft consensus.\n\n## And That's It\n\nMulti-reader,
    single-writer Postgres configurations aren't a new thing. Lots of people use them.
    But they're annoying to get working, especially if you didn't start building your
    application knowing you were going to need them.\n\nIf you're scaling out Postgres
    instead of scaling it up, there's a good chance you're doing it because you want
    to scale your application geographically. That's why people use Fly.io; it's our
    whole premise. We do some lifting to make sure that running an app close to your
    users on Fly.io doesn't involve a lot of code changes. This strategy, of exploiting
    our proxy to steer database writes, is sort of in keeping with that idea.\n\nIt
    should work with other databases too! There's no reason we can see why you can't
    use the same trick to get MySQL read replicas working this way. If you play around
    with doing that on Fly.io, [please tell us about it at our community site](https://community.fly.io/).\n\nMeanwhile,
    if you're averse to science projects, we think we're on track to become the simplest
    conceivable way to hook a full-stack application up to a scalable Postgres backend.
    Give it a try.\n\n---\n\nWant to know more? [Join the discussion](https://community.fly.io/t/multi-region-database-guide/1600).\n"
- :id: blog-monitoring-your-fly-io-apps-with-prometheus
  :date: '2021-06-28'
  :category: blog
  :title: Monitoring Elixir Apps on Fly.io With Prometheus and PromEx
  :author: akoutmos
  :thumbnail:
  :alt:
  :link: blog/monitoring-your-fly-io-apps-with-prometheus
  :path: blog/2021-06-28
  :body: |2-


    <div class="lead">Fly.io is a platform that makes deploying and running your Elixir applications fun again. You can do advanced monitoring with Prometheus on Fly.io without installing anything! [Deploy your Elixir application and try it out!](https://fly.io/docs/elixir/)</div>

    Fly.io takes Docker containers and converts them into fleets of Firecracker micro-vms running in racks around the world. If you have a working Docker container, you can run it close to your users, whether they're in Singapore or Amsterdam, with just a couple of commands. Fly.io is particularly nice for Elixir applications, because Elixir's first-class support for distributed computing meshes perfectly with Fly.io's first-class support for clusters of applications.

    This post is about another cool Fly.io feature --- built-in Prometheus metrics --- and how easy it is to take advantage of them in an Elixir application. I wrote and maintain [an Elixir library, PromEx](https://github.com/akoutmos/prom_ex), that makes it a snap to export all sorts of metrics from your Elixir applications and get them on dashboards in Grafana. Let's explore some of the concepts surrounding Prometheus and see how we can leverage the Fly.io monitoring
    tools in an Elixir application to get slick looking dashboards like this one:

    ![Phoenix Dashboard](full_phoenix_dashboard.png)

    ## Why Application Monitoring is Important

    When customers are paying for your application or service, they expect it to work every time they reach for it. When things break or errors occur, your customers will not be happy. If you are lucky, your customers send you an email letting you know that things are not working as expected. Unfortunately, many of these occurrences go unreported.

    Knowing exactly when things are going wrong is key to keeping your customers happy. This is the problem that monitoring tools solve. They keep an eye on your application, and let you know exactly when things are behaving suboptimally.

    Imagine for example that you have an HTTP JSON API. You deploy a new version that changes a bunch of endpoints. Assume it's  infeasible to go through every single route of your application every time you deploy,  or to test each endpoint individually with every permutation of input data. That would take far too much time, and it doesn't scale from an organizational perspective: it would keep engineers constantly context switching between feature work and testing new deployments.

    A more scalable solution: briefly smoke test the application after a deployment (as a sanity check), and then use monitoring tooling to pick up on and report on any errors. If your monitoring solution reports that your HTTP JSON API is now responding with 400 or 500 errors, you know you have a problem and you can either rollback the application, or stop it from propagating to across the cluster. The key point is that you can proactively address issues as opposed to being blind to them, and at the same time you can avoid sinking precious engineer time into testing all the things.

    While ensuring that production users are not experiencing issues is a huge benefit of application monitoring, there are lots of other benefits. They include:

    - Quantifying stress testing results
    - Business priority planning based on real usage data
    - System performance and capacity planning

    Let's dig into how Prometheus achieves these goals at the technical level.

    ## How Does Prometheus Work?

    At its core, Prometheus is a time-series database that enables you to persist metrics in an efficient and performant manner. Once your metrics are in the Prometheus time-series database, you can create alerting rules in Grafana. Those alerts can then be triggered once certain thresholds and criteria are met, letting you know that something has gone wrong.

    "But how exactly do my application metrics end up in Prometheus?" Well, your Prometheus instance is configured to scrape all of your configured applications. At a regular interval, each of their instances is [queried for metrics data, which is stored in a database](https://fly.io/blog/measuring-fly/). Specifically, it makes a GET HTTP call to `/metrics` (or wherever your metrics are exposed) and that endpoint will contain a snapshot in time of the state of your application. Once your metrics are in Prometheus, you can query the time-series database with Grafana to plot the data over time; Grafana uses PromQL to refresh data and update its panels.

    Given that Prometheus scrapes your applications at a regular interval, the resolution of your time-series data is bound to that interval. In other words, if you get 1,000 requests in
    the span of 10 seconds, you don't know exactly at what timestamps those 1,000 requests came in, you just know that you got 1,000 requests in a 10 second time window. While this may seem limiting, it is actually a benefit in disguise. Since Prometheus doesn't need to keep track of every single timestamp, it is able to store all the time-series data very efficiently.

    Luckily with Fly.io, the administration and management of Prometheus can be taken care of for you!

    ## Turning On Prometheus On Fly

    Managing, configuring and administering your own Prometheus instance can be a bit of a tall order if you have never worked with Prometheus before. Fortunately, all you need to do to enable Prometheus metrics for your application is add a couple of lines to your `fly.toml` manifest file. All Fly.io needs to know is what port and path your metrics will be available at. For the [TODO List Elixir application](https://github.com/fly-apps/elixir_prom_ex_example/tree/master/todo_list) for example, the following configuration was all that was needed:

    ```toml
    [metrics]
    port = 4000
    path = "/metrics"
    ```

    In order to visualize your Prometheus metrics, you'll need to have an instance of Grafana running somewhere. You could deploy your own Grafana instance on Fly.io by [following this guide](https://github.com/fly-apps/grafana), but you can also use [Grafana Cloud](https://grafana.com/products/cloud/) (it has a free plan) --- Grafana Cloud works fine with Fly. Which ever route you take, all you then need to do is configure Grafana to [communicate with the Fly.io managed Prometheus instance](https://fly.io/docs/reference/metrics/#grafana) and you are good to go!

    Now that we've got Prometheus hooked up, we need to get our Elixir application to start providing metrics.

    ## Monitoring Elixir with PromEx

    Whenever I write a production-grade Elixir application that needs monitoring, I reach for [PromEx](https://github.com/akoutmos/prom_ex).

    I wrote PromEx and maintain it because I wanted something that made it easy to manage both the collection of metrics and the lifecycle of a a bunch of Grafana dashboards. That's to say: PromEx doesn't just export Prometheus metrics; it also provides you with dashboards you can import into Grafana to immediately get value out of those metrics. I think this is a pretty ambitious goal and I'm happy with how it turned out. Let's dig in.

    At a library design level, PromEx is a plugin style library, where you enable a plugin for whatever library you want to monitor. For example, PromEx has plugins to capture metrics for Phoenix, Ecto, the Erlang VM itself, Phoenix LiveView and several more. Each of these plugins also has a dashboard to present all the captured metrics for you. In addition, PromEx can communicate with Grafana using the Grafana HTTP API, so it will upload the dashboards automatically for you on application start (if you configure it that is). What this means is that you can go from zero to complete application metrics and dashboards in less that 10 minutes!

    In the [Elixir example application](https://github.com/fly-apps/elixir_prom_ex_example/tree/master/todo_list), you can see that the PromEx module definition specifies what plugins PromEx should initialize, and what dashboards should be uploaded to Grafana:

    ```elixir
    defmodule TodoList.PromEx do
      use PromEx, otp_app: :todo_list

      alias PromEx.Plugins

      @impl true
      def plugins do
        [
          # PromEx built in plugins
          Plugins.Application,
          Plugins.Beam,
          {Plugins.Phoenix, router: TodoListWeb.Router},
          Plugins.PhoenixLiveView
        ]
      end

      @impl true
      def dashboard_assigns do
        [
          datasource_id: "prometheus"
        ]
      end

      @impl true
      def dashboards do
        [
          # PromEx built in Grafana dashboards
          {:prom_ex, "application.json"},
          {:prom_ex, "beam.json"},
          {:prom_ex, "phoenix.json"},
          {:prom_ex, "phoenix_live_view.json"}
        ]
      end
    end
    ```

    With a little bit of configuration in `runtime.exs` PromEx can communicate with Grafana to take care of the graph annotations and dashboard uploads:

    ```elixir
    config :todo_list, TodoList.PromEx,
      manual_metrics_start_delay: :no_delay,
      grafana: [
        host: System.get_env("GRAFANA_HOST") || raise("GRAFANA_HOST is required"),
        auth_token: System.get_env("GRAFANA_TOKEN") || raise("GRAFANA_TOKEN is required"),
        upload_dashboards_on_start: true,
        folder_name: "Todo App Dashboards",
        annotate_app_lifecycle: true
      ]
    ```

    With the managed Prometheus instance from Fly.io, and the metrics collection from PromEx, you have an instrumented application in record time! Here are some snapshots from the auto generated dashboards for the [Todo List application](https://github.com/fly-apps/elixir_prom_ex_example/tree/master/todo_list):

    ![Erlang VM Dashboard](beam_dashboard.png)
    ![Phoenix LiveView Dashboard](live_view_dashboard.png)

    ## And That's It!

    Elixir makes it easy to run ambitious, modern applications that take advantage of distributed computing. It should be just easy easy to see what those applications are actually doing, and to have alerts go off when they misbehave. Between Fly.io's built-in Prometheus and the PromEx library, it's easy to get this kind of visibility. Your application can be instrumented with dashboards and annotations in a coffee break's worth of time.

    Be sure to check out the [Todo List application Repo](https://github.com/fly-apps/elixir_prom_ex_example/tree/master/todo_list) for more technical details and all the code necessary to do this yourself. What used to take a few days to set up and run, now only takes a few hours, so be sure to give it a test drive!

    <%= partial "shared/posts/cta", locals: {
      title: "You can launch an observable Elixir app on Fly.io in minutes",
      text: "Fly.io is one of the easiest ways to take advantage of Elixir community libraries like prom_ex, so you can run your application and watch what it's actually doing, with pretty graphs to impress people looking over your shoulder in the cafe!",
      link_url: "https://fly.io/docs/elixir/",
      link_text: "Deploy your Elixir App!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>"
    } %>
- :id: blog-fly-is-hiring-sres
  :date: '2021-06-17'
  :category: blog
  :title: Fly Is Hiring SREs
  :author: kurt
  :thumbnail: jobs-cover-02-thumbnail.png
  :alt:
  :link: blog/fly-is-hiring-sres
  :path: blog/2021-06-17
  :body: "\n\n<div class=\"lead\">Fly.io is a platform that takes container images
    and converts them into fleets of Firecracker VMs running on our own hardware around
    the world. It’s then easy to run applications near users, whether they’re in Singapore,
    Seattle, or São Paulo. We’re also just a very pleasant place to deploy to. Try
    it out; if you’ve got a working container already, it can be running here in less
    than 10 minutes.</div>\n\nAnyways, we’re hiring for our SRE team.\n\nYou can read
    that introduction and get a good idea of how intense our SRE challenge is. To
    further set the scene, two important true things about ops at Fly.io: ops is a
    very big deal here, and, because we’re a small team where everyone does ops work,
    there’s a lot of room for ideas on how to do ops better. \n\nOur platform stack
    is, of course, Linux, plus the HashiCorp stack (Nomad, Consul, and Vault), plus
    Firecracker, Amazon’s Rust micro-VM engine, plus WireGuard, Jason Donenfeld’s
    amazing lightweight VPN, which is what our network fabric is built on. Our users
    drive Fly.io through a Rails-based GraphQL API. We host a heavy-duty Prometheus-style
    metrics cluster, an ElasticSearch cluster for logging, a monitoring system using
    Sensu Go, BGP4 peering with Bird… the list goes on.\n\nImportantly: while we need
    you to be comfortable working on all of this stuff, we don’t need you to know
    all of it coming in the door. \n\nWe think Fly.io is pretty ambitious, and often
    fun to work on. We think there are people out there for whom the idea of keeping
    a system like ours running smoothly is a cool problem. Some things you might want
    to know about us:\n\n* We’re a small team, almost entirely technical, and product
    engineering and ops are tightly integrated, which isn’t something we’re looking
    to change. We don’t have a culture of developers throwing things over the wall
    for ops to keep running.\n* We’re remote, with team members in Colorado, Quebec,
    Chicago, London, Virginia and Utah.\n* We’re an unusually public team, with an
    online community (at community.fly.io) that we try to be chatty with; you’d want
    to be comfortable not working secretively in a dark room (you can work noisily
    in a dark room if that’s your thing).\n* We’re a team, not a family, but we have
    families and want to be the kind of place where work doesn’t get in the way of
    that.\n* We have an on-call rotation, because we’re a platform. We all share it,
    and will be for the foreseeable future.\n* We’re all developers, and we’re all
    doing our own ops (Steve owns ops, but is gradually being sucked into product
    development, since any big ops project we do is something we’re going to try to
    figure out how to turn into a feature, which is something else you might want
    to be aware of).\n* We’re a real company – we hope that goes without saying –
    and this is a real, according-to-Hoyle full-time job with health care for US employees,
    flexible vacation time, hardware/phone allowances, the standard stuff.\n\nFly.io
    is weird about hiring. We’re skeptical both of resumes and of interviews. We respect
    career experience but try not to be hypnotized by it, and are actively excited
    by the prospect of discovering new talent. Here are a collection of hats we need
    you to be OK with wearing:\n\n* The hat where you manage to get a fairly large
    fleet of servers running on a new kernel configuration while absolutely minimizing
    downtime for apps in any particular location.\n* The hat where you get metric
    and log alerting configured so that Kurt gets paged reliably when something goes
    wrong, while at the same time making sure that everything that pages Kurt is actionable.\n*
    The hat where you build a process for quickly shipping new features we build to
    prod with canaries or blues and greens or whatever the cool kids are doing, because
    some of what we deploy right now is scary enough to slow us down a bit.\n* The
    hat where you can keep enough of Linux networking’s idiosyncrasies in your head
    to diagnose problems, especially when we’ve managed to BPF in new dumb idiosyncrasies.
    \n* The hat where you turn up new data centers for us in random parts of the world,
    so our users can deploy applications close to penguins or castles or really good
    muffaletta sandwiches.\n\nYou want to be comfortable coding in some programming
    language (Python and Ruby are fine; if you don’t know Go already, you probably
    will soon after joining).\n\nThere are other hats, too. But if these sound like
    hats you’d be happy to wear, here’s our process:\n\n1. You’ll reach out and let
    us know a little about yourself (you could send a resume if you like). In your
    mail, tell us a story about any computer disaster you've survived --- whatever
    comes to mind! \n2. We'll invite you to a chat to pitch the company and answer
    all your questions about both the company and our hiring process.\n3. If you’re
    sold on the gig after that, we’ll get you some lightweight ops challenge problems,
    and any information you need to shine on them (we don’t do gotcha problems). You’ll
    do these on your own time, in whatever snippets of time you have available — we’re
    not looking over your shoulder — and we’re aiming for this to eat substantially
    less time than a typical tech company interview gauntlet would.\n4. If, after
    doing all this, it’s clear that this is the kind of work you want to be doing,
    we’ll do a larger paid project ($1000 flat rate) that is representative of the
    kind of work we do.\n5. We'll evaluate all the paid projects we get over a 3-4
    week period and let you know how things went.\n6. We'll set up another video chat
    and try to convince you not to join. It'll be fun!\n7. We'll get you an offer.\n\nWe’re
    biased but think this is a pretty swell role, doing visible work for an appreciative
    and enthusiastic user base. We’d be thrilled if you reached out to ask about it.
    You can’t waste our time! \n\nInterested? Email us at jobs+servers&#64;fly.io.\n\n"
- :id: blog-observing-elixir-in-production
  :date: '2021-06-14'
  :category: blog
  :title: Observing Elixir in Production
  :author: mark
  :thumbnail:
  :alt:
  :link: blog/observing-elixir-in-production
  :path: blog/2021-06-14
  :body: |2


    <div class="lead">Fly networking lets you VPN in and run Observer directly in production. [Deploy your Elixir application and try it out!](https://fly.io/docs/elixir/)</div>

    Elixir, Erlang, and really just the BEAM has a feature called "[Observer](https://elixir-lang.org/getting-started/debugging.html#observer)". It's fun showing it to people new to Elixir because it's just so cool! It's a WxWidgets graphical interface that connects in realtime to a running Erlang node and lets you "observe" what's going on. It has some limited ability to modify things as well, most notably you can kill running processes. This can help when something is misbehaving or you just want to play "chaos monkey" and kill parts of the system to see how it recovers.

    This picture shows a process tree for the application. Using this I can inspect individual processes or even kill them!

    ![Running GameServer highlighted](observer-remote-node-game-server.png?2/3&centered)

    One very cool way to run Observer is to run it on your local machine (which has the ability to display the UI) and connect to a production server (with no windowing UI available) and "observe" it from a distance. So yeah... have a problem in production? Not sure what's going on? You can tunnel in, crack the lid and poke, prod, and peek around to see what's going on.

    The Fly platform makes it easy to do this for your own applications!

    ## What We Will Do

    Fly.io natively supports [WireGuard](https://www.wireguard.com/), Jason Donenfeld’s amazing VPN protocol. If you’ve ever lost hours of your life trying to set up an IPSec VPN, you’ll be blown away by how easy WireGuard is. It’s so flexible and performant that Fly uses it as our network fabric. And it’s supported on [every major platform](https://www.wireguard.com/install/), including macOS, iOS, Windows, and Linux. What that means for you is that if your app runs on Fly, you can open a secure, private, direct connection from your dev machine to your production network, in less time than it took me to write this paragraph. Cool, right?

    This is what we're going to do.

    ![WireGuard observer connection](elixir-wireguard-observer-tunnel.png?2/3&centered)

    We will bring up a secure WireGuard tunnel that links to your servers on Fly. In this graphic, there are two `my_app` Elixir nodes clustered together running on Fly.

    From the local machine, we can open an `IEx` terminal configured to **join** that cluster of remote Elixir nodes. Our local machine supports running Observer and drawing the UI. We use our local observer to talk to the remote nodes in the cluster!

    ## Making It Happen

    To test this out, I follow [this guide](/docs/app-guides/elixir-observer-connection-to-your-app/) and apply the changes to the multi-region [Tic-Tac-Toe game created here](/blog/building-a-distributed-turn-based-game-system-in-elixir/). The github repo for the [project is here](https://github.com/fly-apps/tictac).

    Here's what we do:

    1. Configure an Elixir release to use a cookie value we provide.
    2. Setup WireGuard for Fly. This is a VPN technology that let's us directly connect to the production private network.
    3. Create a simple script to launch Observer for us.
    4. Launch Observer and explore!

    Again, follow the [guide here](/docs/app-guides/elixir-observer-connection-to-your-app/) for a step-by-step breakdown of how to do it for your project.

    ## Multi-Region Support?

    When Elixir nodes are clustered together and running in different regions, Observer can connect to any node in the cluster.

    After making the changes to the TicTac project and deploying it to multiple regions, let's see what it looks like.

    ```cmd
    fly status
    ```
    ```output
    ...

    Instances
    ID       VERSION REGION DESIRED STATUS  HEALTH CHECKS      RESTARTS CREATED
    79510f86 17      fra    run     running 1 total, 1 passing 0        23m39s ago
    df93ea35 17      lax(B) run     running 1 total, 1 passing 0        24m3s ago
    ```

    I have the game scaled out to [two regions](/docs/reference/regions/). One is running in `fra` (Frankfurt, Germany) and the other is running in `lax` (Los Angeles, California (US)).

    When I open Observer locally, I see two remote instances of `tictactoe`!

    ![Instances in different regions](observer-multi-region-nodes.png?card&centered)

    ## Exploring My App

    I start playing a game on the web and use Observer to browse around and find the game. It's highlighted in blue and linked from `GameRegistry`.

    ![Running GameServer highlighted](observer-remote-node-game-server.png?centered)

    Using Observer, I can double-click the selected game process and even view the GenServer's state. This gives a snapshot of the state at the time I double-clicked it. I highlighted in yellow some interesting parts of the game state.

    ![Game state displayed](observer-game-state.png?centered)

    If you're wondering why the state data looks strange (at least different from Elixir), it's because that's the Erlang representation of those data types.

    ## Playing Chaos Monkey

    Something fun you can do with Observer is identify processes, inspect them, and even kill them. This can help when something is misbehaving and you want to see more about what's going on. This can also help you test how your system recovers from unexpected failures.

    I've already identified the running game process. By right-clicking it I see I have the option to "kill" it. What will happen when a game server dies?

    ![Kill process menu option](observer-kill-process.png?card&centered)

    After killing the game process, I see that a new process was immediately started. How can I tell? The PID (Process ID) value is different.

    ![Process restarted](observer-restarted-process.png?centered)

    So, it looks like my system isolates the damage in that no other running games are impacted when one crashes. Yay! My system stays up and running!

    From the user's perspective playing the game, it's not so graceful.  The player **is** able to recover but it requires them to reload their page or restart the game joining processing. I see that my UX can be improved to make crash recovery better for the user.

    That was a productive experiment!

    ## Now You Try

    Deploy a Phoenix application to Fly, setup your WireGuard VPN, and start observing your app in production!

    <%= partial "shared/posts/cta", locals: {
      title: "Fly makes using Observer easy",
      text: "With Elixir you can build resilient systems! Fly makes observing them easy. What will you find digging around in the state pile?",
      link_url: "https://fly.io/docs/elixir/",
      link_text: "Deploy your Elixir App!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>"
    } %>
- :id: blog-livebook-for-app-documentation
  :date: '2021-05-26'
  :category: blog
  :title: Livebook is a secret weapon for documentation
  :author: mark
  :thumbnail:
  :alt:
  :link: blog/livebook-for-app-documentation
  :path: blog/2021-05-26
  :body: |2


    <div class="lead">Fly runs apps close to users, you can run pretty much any Docker image on Fly. [So we’ve been playing with Livebook.](https://fly.io/docs/speedrun/)</div>

    Every application has that core, most important thing that it does. It is the reason the application exists. It's that central idea that everything else is there to support.

    A major hurdle for new developers joining a project can be understanding and becoming comfortable with the code that implements that central idea. So you document the code and may even have 100% code coverage for it.

    Code documentation and tests are valuable here. Now with Livebook, there may be a **new** way to provide documentation, interactive exploration, and expose the app logic in a way we couldn't before.

    ## Livebook for App Logic?

    What is Livebook?

    [Livebook](https://github.com/elixir-nx/livebook) is Elixir's answer to something like [Jupyter notebooks](https://jupyter.org/). It's an exciting project created to help use [Nx](https://github.com/elixir-nx/nx), [Axon](https://github.com/elixir-nx/axon) and machine learning with [Elixir](https://elixir-lang.org/). Livebook is still early days, but it's rapidly progressing, it's already solid, and has some awesome features.

    While it was created for the machine learning space, it isn't limited to it. Livebook is actually really powerful even when used on a regular Phoenix web project.

    Before I go any further, let me show you what I mean. Previously, I created the game [TicTac](https://github.com/fly-apps/tictac) that let's you play distributed Tic-Tac-Toe with a friend from across the world on [Fly.io](https://fly.io/docs/).

    Using Livebook, I documented the central piece that is the GameState. It is a module and struct used to model a game's state. This is a glimpse of what it looks like.

    ![Livebook showing game state](livebook-gamestate.png?card&2/3&centered)

    Livebook accesses the project's code and let's me simultaneously document it and execute the code showing the results!

    ## Why Livebook?

    The important features are:

    * Loads an existing Elixir project
    * Code completion
    * Elixir code execution
    * Markdown text for explanations
    * Keyboard shortcuts for easier navigation and editing
    * Notebook files are stored as plain Markdown files

    This means we can do the following:

    1. take an existing Elixir application
    2. start a notebook for the project
    3. access the project's code and document critical areas

    Livebook makes it easy to experiment with our code, execute it, and demonstrate the point we're making.

    This lets developers new to a project experiment and **learn** that central idea. It also works for experimenting and **exploring changes**.

    Because the notebook files are simple markdown files, they are easily added to source control and become part of the project if desired. They diff and merge well too.

    <blockquote>Livebook is actually really powerful even when used on a regular Phoenix web project.</blockquote>

    Let's get started!

    ## Livebook Setup

    When Elixir 1.12 was released, Livebook was updated to require that version. It simplified things for the project and let them safely make certain assumptions. This means you need to have [Elixir 1.12 installed](https://elixir-lang.org/install.html).

    Once installed, installing Livebook is this easy.

    ```cmd
    mix escript.install hex livebook
    ```

    Starting Livebook is

    ```cmd
    livebook server
    ```
    ```output
    ...
    [Livebook] Application running at http://localhost:8080/?token=iphm6yeqczkyce4hzkc5ylzged6dj7kb
    ```

    It gives us a URL with a randomly generated token we use to connect to our local instance. Go ahead and open it up.

    Yay! We're connected to our Livebook running locally.

    ## Starting Tip

    Since we are playing with Livebook on an existing Elixir project, start Livebook from the directory of your project. It works fine without that, starting here just makes it easier for navigating and starting the runtime. More on that later.

    You'll want your project to be using Elixir 1.12 as well.

    ## Start a Project Notebook

    Start a new notebook and name it for your project or some significant part of your project. Don't worry though, it's just a Markdown file and you can rename it later if you want.

    Save it. When you save, select to save it to a file. Navigate to your project directory. I suggest adding a `notebook` directory to the name before adding your filename.

    It may look something like this:

    ```
    /home/youruser/my_app/notebook/overview
    ```

    In this example, `overview` would end up named as `overview.livemd`.

    ![Save Notebook modal](livebook-save-notebook.png?card&2/3&centered)

    The "notebook" directory just gives the project a home where multiple notebook files can be stored.

    ## Connect to your Project

    To connect Livebook to your project, use the "Runtime" menu item.

    ![Livebook runtime menu item](livebook-runtime-menu.png?centered&card)

    Using the "Runtime Settings", select "Mix standalone". Navigate to your Elixir project and "Connect".

    ![Livebook connect to mix project](livebook-mix-connect.png?card&2/3&centered)

    It compiles your project and starts it.

    ![Livebook mix connected](livebook-mix-disconnect.png?card&2/3&centered)

    If everything goes smoothly, when you return to your notebook you can add an Elixir card and as you type, you should have code completion available using <kbd>CTRL</kbd>+<kbd>Space</kbd> or <kbd>Cmd</kbd>+<kbd>Space</kbd>.

    With your code loaded, you have complete access to your application running in Livebook! In fact, you can execute Ecto queries, call contexts, start GenServers, anything!

    ## Document Important App Logic

    Now that you have Livebook setup with a new notebook and it's connected to your your project source code, you are ready to start documenting app logic and showing what's important!

    This is what I showed earlier with the documentation and execution of the `GameState` module. I loved using the code completion to explore it as I documented it! It was a lot of fun writing documentation that can be interactively executed and still have all the benefits of using markdown too!

    ## Run and Interact with GenServers

    Livebook also lets you run GenServers and interact with them too!

    The other main part of the TicTac application is how a GenServer runs the GameState. The running server is managed by Horde and I was able to both **describe** that and **show** it!

    ![Livebook showing running process](livebook-genserver.png?card&2/3&centered)

    **Livebook Tip:** When working with a Livebook notebook, the keyboard shortcut you want to know is `ea` to "Evaluate All". After opening a notebook, type `ea` and everything executes so you can see the commands and the results.

    ## The Case for Livebook with your Project

    Livebook is exciting because of what it does for Elixir in the Machine Learning space. However, Livebook can **also** help your regular-old Phoenix applications!

    I hope you can see how Livebook can be used to document central pieces of your application in ways we never could before. Whether you're documenting commission calculations, game logic, credit scoring, mortgage payments, or whatever. Livebook can help you do it in a new and awesome way.

    Potential benefits:

    * Document how critical pieces work in an interactive way
    * Give new team members a better on-ramp to your project
    * A place to explore changes to your central code
    * Livebook is collaborative
    * Connects to your project code
    * Notebooks are plain markdown files and can be checked-in with the project

    Give Livebook a try on your project and see how it helps your team!

    <%= partial "shared/posts/cta", locals: {
      title: "Livebook works awesome on Fly",
      text: "Livebook supports collaborative editing. When you host your own instance you can invite people to join you!",
      link_url: "https://fly.io/docs/speedrun/",
      link_text: "Boot up a Livebook!&nbsp;&nbsp;<span class='opacity:50'>&rarr;</span>"
    } %>
- :id: blog-hooking-up-fly-metrics
  :date: '2021-05-20'
  :category: blog
  :title: Hooking Up Fly Metrics
  :author: thomas
  :thumbnail:
  :alt:
  :link: blog/hooking-up-fly-metrics
  :path: blog/2021-05-20
  :body: "\n\n<div class=\"lead\">Fly apps include built in Prometheus instrumentation
    – monitor performance, create alerts, and even export your own metrics. If you
    haven't\ntaken Fly for a spin, now's a good time: if you’ve got a Docker container,
    [it can be running on Fly in single-digit minutes.](https://fly.io/docs/speedrun/)</div>\n
    \ \nWe’ve written a bit, for a general audience, about [how Fly collects and manages
    metrics.](https://fly.io/blog/measuring-fly/) If you’re just sort of generally
    interested in metrics and observability, go read that first.\n\nMeanwhile, if
    you’re a Fly user, or considering becoming such a user, here’s the deal:\n\nAll
    Fly apps now include built-in Prometheus metrics. We give you a bunch of useful
    metrics automatically, and we’ll walk through some of them here. More importantly:
    you can export your own custom metrics. And you should export your own custom
    metrics, because metrics are supremely useful. We’ll talk a bit about that, too.\n\n###
    A Quick Overview Of Metrics At Fly\n\nThis is one of those “pictures worth a thousand
    words” scenarios, so, without further ado, here’s some metrics:\n\n![A graph of
    HTTP response codes](response-codes.png?rounded)\n\nOver the last 30 minutes,
    for each HTTP response code our app generated, these are the requests per second
    we’re seeing. \n\nFly tracks a bunch of metrics like this for you, and you can
    add your own. We use a Prometheus-style system to do this, and a Prometheus endpoint
    is part of our public API.\n\nThere’s not a lot you need to know about Prometheus
    before diving in. It’s enough to know that Prometheus metrics have names and labels,
    and that there are for the most part 3 kinds:\n* *Counters*, by far the most important
    kind, are strictly increasing; they’re simply a number you bump every time an
    event occurs. The counter `fly_edge_http_responses_count` counts every HTTP response
    our proxy handles for you; it includes labels like `app` and `region`.\n* *Gauges*
    vary up and down; think of them like the speedometer on your car. Prometheus strongly
    favors counters over gauges, in part because if you miss a measurement or two,
    a counter loses resolution, but a gauge might lose data — it could, for instance,
    hide a burst of traffic that occurred during the missed measurements.\n* *Histograms*
    break data out into buckets. Histograms are generally how you make heatmaps for
    metrics. You’ll see in a minute.\n\nYou query Prometheus using [its query language,
    PromQL](https://valyala.medium.com/promql-tutorial-for-beginners-9ab455142085).
    If you’ve used the command-line tool `jq`, you have a bit of the flavor of PromQL;
    just think of a  `jq`  that specializes in being a programmable calculator. You
    can read [PromQL tutorials](https://valyala.medium.com/promql-tutorial-for-beginners-9ab455142085),
    but also like `jq`, you can get by with just a small subset of it, which is how
    we’re going to tackle this walkthrough.\n\nMost people use Prometheus with a graphical
    front-end, and by far the most popular front-end for Prometheus metrics is [Grafana](https://github.com/grafana/grafana),
    which is what we’re going to use here. You can run your own instance of Grafana
    (it works fine as a Fly app), or you can sign up for a free [Grafana Cloud](https://grafana.com/products/cloud/)
    account.\n\n### Getting Fly Metrics Into Grafana\n\nEasy! Get a working Grafana
    somewhere. Now, run `flyctl auth token` to retrieve a Fly token. In Grafana’s
    “Configuration” menu, the first option is “Data Sources”. We’re one of those.\n\n“Add
    data source”, and pick *Prometheus*; it should be the first option. The URL for
    Fly’s Prometheus is per organization. Your personal organization is `https://api.fly.io/prometheus/personal`,
    other organizations are `https://api.fly.io/prometheus/<name>`. Turn off all the
    authentication options, and then add a “Custom Header”; make it `Authorization`,
    and make the value `Bearer <token>`. \n\nSave the data source; Prometheus will
    check it and make sure it works for you. If you get the green light, you’re good
    to go.\n\n### A Basic Fly Dashboard\n\nWe set up a simple, clean dashboard with
    a bunch of graphs on it, and you can pull it directly into your Grafana instance.
    Go to “Manage Dashboards”, click “Import”, and then paste in the JSON [from this
    Github link](https://gist.github.com/tqbf/0732a08e15b97a86e2eb96100452209f).\n\n<div
    class=\"callout\">\nGrafana has a whole community site for dashboard JSONs like
    this, and when we figure out how it works, we’ll try to get ours on it. \n</div>\n\nNow,
    let’s take a tour of the dashboard and see how it works.\n\nThe first thing we
    want you to notice is the app picker at the top left of the screen. The picker
    fetches the `fly_app_concurrency` metric from our API to get a list of your apps,
    which is stored in the `$app` variable exposed to all the queries on this dashboard.
    \n\nIf you can’t tell, we’re assuming you’re not super familiar with Grafana,
    so the other thing we’ll point out is that there’s a picker on the other side
    of the screen that determines the time scale of the metrics we’re looking at.
    You’re probably looking at 15 minutes worth of data, and you should know you can
    dial that up or down. \n\nNow, a simple graph:\n\n![A graph of network traffic](traffic.png?rounded)\n\nIf
    you click next to the title on this graph, there’s a drop-down you can use to
    edit it. Do that and you’ll see the queries we’re using:\n\n```\nsum(irate(fly_instance_net_recv_bytes{app=\"$app\"}[5m])*8)\nsum(irate(fly_instance_net_sent_bytes{app=\"$app\"}[5m])*8)\n```\n\n`fly_instance_net_(recv|sent)_bytes`
    is a counter and works the way you’d think it would. `app=“$app”` narrows us down
    to just the metrics matching our current app. You could add `, region=“ord\"`
    to the expression to narrow us down to metrics from Chicago, and now you basically
    get the idea for how names and labels work.\n\nWe add `[5m]` to the query to fetch
    a window of 5 minutes worth of metrics from, and then  `irate()` to get the rates
    of increase from the raw counter values (you’ll see `irate`, `rate`, and `increase`
    \ in almost every Prometheus counter query, and they all do roughly the same thing
    in slightly different ways). Remember, Fly’s systems are scraping your metrics
    at some interval and generating vectors of counters, and PromQL’s job is generally
    to turn those raw values into intelligible time-series information.\n\nFinally,
    we `sum()` across all the available metrics we have that match this query, and
    convert to bits.\n\nOn the right hand pane in the Grafana pane editor you can
    see that we have this set up as a line graph, and you can click around and muck
    with those settings if you like. Try switching “stack” on to see TX/RX superimposed.
    Have fun. Meanwhile, we’re moving on.\n\n![A graph of TCP connections](connections.png?rounded)\n\nConnections
    aren’t much more complicated. \n\n``` html\nsum(increase(fly_app_tcp_connects_count{app=\"$app\"}[$__interval]))\nsum(increase(fly_app_tcp_disconnects_count{app=\"$app\"}[$__interval]))\n```\n\n`fly_app_tcp_(connects|disconnects)_count`
    do what they say on the tin. We use `increase` here because Jerome is fussy (\"I
    always use `increase` for bar charts\", he tells me); `$__interval` is a Grafana-ism
    for, roughly, “have Grafana figure out the right interval”.  \n\nNow, let’s get
    ambitious:\n\n![A graph of requests by region](map.png?rounded)\n\nCheck out all
    its majesty.\n\nThe query here is actually very simple:\n\n```\nsum(rate(fly_edge_http_responses_count{app=~\"^$app$\"}[$__interval]))
    by (region)\n```\n\nYou’ve seen almost all of this before. Note that instead of
    summarizing all available metrics onto one line, we instead break them out `by
    (region)` (and set the legend for each data point accordingly).\n\nWe use the
    “World Map” visualizer in Grafana to render this. The visualizer accepts a URL
    for keyed map location data, and, wouldn’t you know it, our public API has just
    such a URL endpoint, at `https://api.fly.io/meta/regions.json`. The map visualizer
    matches the `region` in our metrics to the `key` in each row of the JSON our endpoint
    returns, and that’s basically it.\n\nOne more graph we want to walk through; this
    is a good one:\n\n![A heat map of response times](heatmap.png?rounded)\n\nI don't
    know about you, but I feel smarter just having this on my screen.\n\nAnd the query:
    \n\n```\nsum(increase(fly_edge_http_response_time_seconds_bucket{app=~\"^$app$\"}[$__interval]))
    by (le)\n```\n\n`fly_edge_http_response_time_seconds_bucket` is a histogram, which
    is a bucketed stat; the buckets here are indicated by `le`. \n\nAs the Fly Proxy
    tracks this metric, it bumps one of a collection of counters; we break them out
    as 5ms, 25ms, 50ms, 100ms, 200ms, 500ms, 1s, 5s, 15s, and, god help you, 30s,
    60s, and 120s. This is fussier than the typical Prometheus metric, which is a
    simple global counter, but it supports a graph pretty much every wants: “how scary
    have my response times looked over the last N hours”. \n\nGrafana has a nice heatmap
    that you can drive with Prometheus histograms, and we’re not doing anything interesting
    with its settings.\n\n### Generate Your Own Metrics\n\nFly provides a bunch of
    built-in metrics, but you don’t have to (and shouldn’t) stick with just ours.
    If you export metrics from your application in Prometheus format, we’ll pick them
    up, index them in our TSDB, and serve them to things like Grafana over our public
    API.\n\nFor instance, let’s say we have a Go application running on Fly. We can:\n\n```go\nimport
    (\n   //...\n   // import prometheus\n\t\"github.com/prometheus/client_golang/prometheus\"\n\t\"github.com/prometheus/client_golang/prometheus/promhttp\"\n)\n\n//...\n//
    create a counter variable\ncounter := prometheus.NewCounterVec(prometheus.CounterOpts{\n\t\tSubsystem:
    \"someEvent\",\n\t\tName:      \"count\",\n\t}, []string{\"label\", \"other-label\"})\nprometheus.MustRegister(counter)\n\n//...\n//
    serve prometheus metrics\nhttp.Handle(\"/metrics\", promhttp.Handler())\n\n//...
    later\n// bump the counter when something happens\ncounter.With(prometheus.Labels{\"label\":
    \"something\"}).Inc()\n```\n\nYou can export stats on the default Go HTTP handler
    (they’ll be exposed) or on a private handler on a different port or address; either
    way, you tell us about your metrics in `fly.toml`:\n\n```\n[metrics]\n    port
    = 8080\n    path = “/metrics”\n```\n\nWhen you deploy, we’ll start scraping, and
    whatever stats you’ve defined will show up in Grafana.\n\nNot a Golang person?
    Here’s what it looks like in Python Flask:\n\n```python\nfrom flask import Flask,
    request\nfrom prometheus_flask_exporter import PrometheusMetrics\n\napp = Flask(__name__)\nmetrics
    = PrometheusMetrics(app)\n\n@app.route('/event/<event_type>')\n@metrics.counter('events_by_type',
    'Number of events by type',\n         labels={'event_type': lambda: request.view_args['event_type']})\ndef
    hello_event(event_type):\n    return 'You have been recorded.'\n\n@metrics.do_not_track()\n@app.route('/')\ndef
    hello_world():\n    return 'Hello, World!'\n```\n\nThe `prometheus_flask_exporter`
    gets you a couple useful metrics by default, without you doing anything, and,
    also by default, listens on `/metrics`.\n\nNode.js? No problem:\n\n```javascript\nconst
    express = require(‘express’)\nconst app = express()\nconst port = 3000\n\nconst
    prom = require(‘express-prometheus-middleware’);\n\napp.use(prom({\n  metricsPath:
    ‘/metrics’,\n  collectDefaultMetrics: true,\n}));\n\napp.get(‘/‘, (req, res) =>
    {\n  res.send(‘Hello World!’)\n})\n\napp.listen(port, () => {\n  console.log(`Example
    app listening at http://localhost:${port}`)\n})\n```\n\nHopefully we’re beginning
    to see a theme here. If you’re using any mainstream framework, adding metrics
    is going to be a very short project for you. \n\nRemember, part of the idea behind
    Prometheus’s counter-first mentality is that _counters are super cheap_. They’re
    cheaper both to generate and to track than log lines are; think about them the
    same way, and instrument your application so you have a decent idea of what it’s
    getting up to.\n\n### And There You Go\n\nLike everything else on our platform,
    we want reasonable metrics to be so simple that it’s boring. They should just
    work. Even if you do nothing at all, we’re generating metrics you can pull into
    Grafana, watch graphs for, and alert on.\n\nPrometheus metrics are free right
    now. Some tier of them will be free forever. You should know that if you get super
    ambitious with metrics, we’ll eventually have a pricing plan for them — the TSDB
    that backs metrics costs us operationally to manage and scale. But we’ll keep
    things simple and cost-effective, because we want all of you to instrument your
    applications.\n"
- :id: blog-measuring-fly
  :date: '2021-05-13'
  :category: blog
  :title: Fly's Prometheus Metrics
  :author: thomas
  :thumbnail:
  :alt:
  :link: blog/measuring-fly
  :path: blog/2021-05-13
  :body: "\n\n<div class=\"lead\">Fly.io transforms container images into fleets of
    micro-VMs running around the world on our hardware. It’s bananas easy to try it
    out; if you’ve got a Docker container, [it can be running on Fly in single-digit
    minutes.](https://fly.io/docs/speedrun/)</div>\n\nWe should talk a bit about metrics
    and measurement and stuff, because they’re how we all know what’s going on.\n\nThere’s
    two reasons we’ve written this post. The first is just that we think this stuff
    is interesting, and that the world can always use another detailed case study.
    The second, though, is that the work we’re going to describe is now part of our
    public API, and you can piggyback your own application metrics on top of ours.
    Our hope is, the more we describe this stuff, the more likely you are to get value
    out of it.\n\n### Counting\n\nHere’s where we’ll start: an HTTP request arrives
    for an app running on Fly. It lands on one of our “edge nodes”, at `fly-proxy`,
    our Rust request router. The proxy’s job is to deliver the request to the closest
    unloaded “worker node”, which hosts [Firecracker VMs](https://fly.io/blog/sandboxing-and-workload-isolation/)
    running user applications. \n\nSomewhere in `fly-proxy`’s memory there’s a counter,
    for the number of incoming bytes the proxy has handled. While routing the incoming
    request, the proxy bumps the counter.\n\nBumping a counter is cheap. So there’s
    counters, and things that act on them, all over the edge node. The operating system
    counts packets processed, TCP SYN segments seen, bytes processed per interface.
    CPU load. Free memory. Naturally, the proxy counts a bunch of things: a histogram
    of the times taken to complete TLS handshakes, number of in-flight connections,
    and so on.  Some counts are global, others per-user.\n\nThe proxy directs the
    request over a [WireGuard interface](https://fly.io/blog/ipv6-wireguard-peering/)
    (more counters) to the worker node, which also runs `fly-proxy` (still more).
    \ The worker’s proxy finds the [tap interface](https://en.wikipedia.org/wiki/TUN/TAP)
    associated with the application and routes the request to its address (you guessed
    it). Inside the VM, where the user’s app is running, the guest OS counts a series
    of arriving Ethernet frames, a  newly opened TCP connection, memory used, CPU
    cycles burned. Your app running in that VM might count a new HTTP connection.\n\nWe
    count everything we can think to count, because computers like counting stuff,
    so it’s cheap. You’d do the same thing even if you were building on a [potato-powered
    MSP430 microcontroller](https://www.youtube.com/watch?v=nPZISRQAQpw).\n\n### Those
    who cannot remember the Borgmon are doomed to repeat it\n\nTo do anything with
    this information we need to collect it. There was once in our industry a lively
    debate about how to accomplish this, but Google settled it. Services expose HTTP
    endpoints that dump the stats; if your service can’t do that, it arranges to have
    something else do it for them. \n\n[Google called these web pages `/varz`](https://docs.google.com/presentation/d/1NziwSTwuz91fqsFhXeOGwyhFUoT6ght1irA_0ABLPU0/htmlpresent),
    and filled them with a line-by-line human-readable format (apocryphally: intended
    for human readers) that Google’s own tools learned to parse. In the real world,
    we’ve kept the text format, and call the pages `/metrics`.  Here’s what it looks
    like:\n\n```\nfly_proxy_service_egress_http_responses_count{status=“200”,app_id=“3146”,app_name=“debug”,alloc=“f3e39e04”,servi\nce=“app-3146-tcp-8080”,org_id=“17”}
    1586\n```\n\nIf you’re an Advent of Code kind of person and you haven’t already
    written a parser for this format, you’re probably starting to feel a twitch in
    your left eyelid. Go ahead and write the parser; it’ll take you 15 minutes and
    the world can’t have too many implementations of this exposition format. There's
    a lesson about virality among programmers buried in here somewhere.\n\nTo collect
    metrics, you scrape all the `/metrics` pages you know about, on an interval, from
    a collector with some kind of indexed storage. Give that system a query interface
    and now it’s it’s a time-series database (a TSDB). We can now generate graphs
    and alerts. \n\nWhat we’re describing is, obviously, [Prometheus](https://www.kartar.net/2017/10/prometheus/),
    which is an [escaped implementation of Borgmon](https://news.ycombinator.com/item?id=15326028),
    Google’s cluster monitoring system. Google SREs have a [complicated relationship](https://news.ycombinator.com/item?id=19620675)
    with Borgmon, which was once said to have been disclosed in an “industry-disabling
    paper”. A lot of people hate it.\n\nNone of us have ever worked for Google, let
    alone as SREs. So we’re going out on a limb: this is a good design; it’s effective,
    so easy to implement that every serious programming environment supports it, and
    neatly separates concerns: things that generate metrics just need to keep counters
    and barf them out into an HTTP response, which makes it easy to generate lots
    of metrics. \n\n### Metrics Databases\n\nThe division of labor between components
    in this design is profoundly helpful on the database side as well. What makes
    general-purpose database servers hard to optimize is that they’re general-purpose:
    it’s hard to predict workloads. TSDBs don’t have this problem. Every Prometheus
    store shares a bunch of exploitable characteristics: \n\n1. Lots of writes, not
    a lot of reads.\n2. Reads return vectors of data points — columns, not rows.\n3.
    Simply formatted numeric data.\n4. Readers tend to want data from 5 minutes ago,
    not 5 days ago.\n\nKnowing this stuff ahead of time means you can optimize. You’ll
    use something like an [LSM tree](https://yetanotherdevblog.com/lsm/) to address
    the bias towards writes, jamming updates into a write-ahead log, indexing them
    in memory, gradually flushing them to immutable disk chunks. You can [compress](http://www.vldb.org/pvldb/vol8/p1816-teller.pdf);
    timestamps recorded on a relatively consistent interval are [especially amenable
    to compression](https://blog.timescale.com/blog/time-series-compression-algorithms-explained/).
    You can downsample older data. You can use a [column store](https://clickhouse.tech/docs/en/engines/table-engines/mergetree-family/mergetree/)
    instead of a row store. \n\nAnd, of course, you can [scale this stuff out](https://monitoring2.substack.com/p/big-prometheus)
    in a bunch of different ways; you can have Prometheus servers scrape and aggregate
    other Prometheus servers, or spill old chunks of data out to S3, or shard and
    hash incoming data into to a pool of servers.\n\nPoint being, keeping an indefinite
    number of time-series metrics available for queries is a tractable problem. Even
    a single server will probably hold up longer than you think. And when you you
    need to, you can scale it out to keep up.\n\nWhich is what we’ve done at Fly.
    And, since we’ve built this, there’s no sense in keeping it to ourselves; if you’re
    running an app on Fly, you can hand us your metrics too. Let’s describe how that
    works.\n\n### Our Metrics Stack\n\nA typical Fly.io POP will run some small number
    of lightweight edge nodes, a larger number of beefier worker nodes, and, in some
    cases, an infra host or two. All of these nodes — within and between POPs — are
    connected with a WireGuard mesh. \n\nOur metrics stack — which is built around
    Prometheus-style metrics — features the following cast of characters:\n\n* [Victoria
    Metrics](https://github.com/VictoriaMetrics/VictoriaMetrics) (“Vicky”, for the
    rest of this post), in a clustered configuration, is our metrics database. We
    run a cluster of fairly big Vicky hosts. \n* [Telegraf](https://github.com/influxdata/telegraf),
    which is to metrics sort of what Logstash is to logs: a swiss-army knife tool
    that adapts arbitrary inputs to arbitrary output formats. We run Telegraf agents
    on our nodes to scrape local Prometheus sources, and Vicky scrapes Telegraf. Telegraf
    simplifies the networking for our metrics; it means Vicky (and our `iptables`
    rules) only need to know about one Prometheus endpoint per node.\n* [Vector](https://github.com/timberio/vector/),
    which is like a Rust version of Telegraf — we use Vector extensively for our log
    pipeline (a whole other blog post), and also as a remote-writer for some Prometheus
    metrics. Our Vector is likely to eat our Telegraf this year.\n* Prometheus exporters,
    to which Telegraf is wired to scrape, expose metrics for our services (and, in
    some cases, push stats directly to Vicky with the Prometheus  `remote_write` API).
    \n* [Consul](https://www.consul.io/), which manages the configuration for all
    these things, so each host’s Telegraf agent can find local metrics sources, and
    Vicky can find new hosts, &c.\n\n<div class=\"callout\">\n**_Steve here_**\n\nFor
    metrics nerds, the only interesting thing about this stack is Vicky. The story
    there is probably boring. Like everyone else, we started with a simple Prometheus
    server. That worked until it didn’t. We spent some time scaling it with [Thanos](https://www.improbable.io/blog/thanos-prometheus-at-scale),
    and Thanos was a lot, as far as ops hassle goes. We’d dabbled with Vicky just
    as a long-term storage engine for vanilla Prometheus, with [promxy](https://github.com/jacksontj/promxy)
    set up to deduplicate metrics.\n\nVicky grew into a more ambitious offering, and
    added its own Prometheus scraper; we adopted it and scaled it as far as we reasonably
    could in a single-node configuration. Scaling requirements ultimately pushed us
    into a clustered deployment; we run an HA cluster (fronted by `haproxy`). Current
    Vicky has a [really straightforward multi-tenant API](https://github.com/VictoriaMetrics/VictoriaMetrics/tree/cluster#multitenancy)
    — it’s easy to namespace metrics for customers — and it chugs along for us without
    too much minding.\n</div>\n\nAssume, in the abstract, an arbitrarily-scaling central
    Vicky cluster. Here’s a simplified view of an edge node:\n\n![An Edge node](edge.png?2/3&centered)\n\nSo
    far so simple. Edge nodes take traffic from the Internet and route it to worker
    nodes. Our edge `fly-proxy` exports Prometheus stats, and so does a standard Prometheus
    `node-exporter` , to provide system stats (along with a couple other exporters
    that you can sort of mentally bucket into the `node-exporter`). \n\n<div class=\"callout\">\n**_Jerome
    here_**\n\nFly-proxy uses the excellent [metrics](https://crates.io/crates/metrics)
    Rust crate with its sibling [metrics-exporter-prometheus](https://crates.io/crates/metrics-exporter-prometheus).
    \n\nThe proxy hosts further logic for transforming internal metrics into user-relevant
    metrics. We built our own `metrics::Recorder` that sends metrics through default
    prometheus Recorder, but also tees some of them off for users, rewriting labels
    in the process.\n</div>\n\nHere’s a worker node:\n\n![A Worker node](worker.png?2/3&centered)\n\nThere’s
    more going on here! \n\nA worker node hosts user app instances. We use HashiCorp
    Nomad to orchestrate Firecrackers; one of the three biggest components of our
    system is driver we wrote for Nomad that manages Firecrackers. Naturally, Nomad
    exports a bunch of metrics to Telegraf.\n\nReal quick though, one more level of
    detail:\n\n![A Firecracker](firecracker.png?1/2&centered)\n\nIn each Firecracker
    instance, we run our [custom init](https://github.com/superfly/init-snapshot),
    which launches the user’s application. Our Nomad driver, our init, and Firecracker
    conspire to establish a `vsock` — a host Unix domain socket that presents as a
    synthetic Virtio device in the guest — that allows init to communicate with the
    host; we bundle `node-exporter`-type JSON stats over the `vsock` for Nomad to
    collect and relay to Vicky. \n\nThe Firecracker picture includes one of the more
    important details in our system. These VMs are all IP-addressable. If you like
    (and you should!), you can expose a Prometheus exporter in your app, and then
    tell us about it in your `fly.toml`; it might look like this:\n\n```\n[metrics]\n
    \   port = 9091\n    path = “/metrics”\n```\n\nWhen you deploy, our system notices
    those directives, and will arrange for your metrics to end up in our Vicky cluster.\n\nZooming
    out, our metrics architecture logically looks something like this:\n\n![The global
    view](global.png?2/3&centered)\n\n### Metrics > Checks\n\nWhen it comes to automated
    monitoring, there are two big philosophies, “checks” and “metrics”. “Checks” is
    what many of us grew up with: you write a script that probes the system and makes
    sure it’s responding the way you expect it to. Simple.\n\n“Metrics” is subtler.
    Instead of running scripts, you export metrics. Metrics are so much cheaper than
    checks that you end up tracking lots more things. You write rules over metrics
    to detect anomalies, like “more 502 responses than expected”, or “sudden drop
    in requests processed”, or “95th percentile latency just spiked”. Because you’ve
    got a historical record of metrics, you don’t even need to know in advance what
    rules you need. You just write them as you think of them.\n\nThat’s “[white box
    monitoring](https://sre.google/sre-book/monitoring-distributed-systems/)”.  Use
    metrics to open up your systems as much as you can, then build monitoring on top,
    so you can ramp up the monitoring over time without constantly redeploying new
    versions of your applications. \n\nWhat’s more, you’re not just getting alerting;
    the same data flow gives you trending and planning and pretty graphs to put on
    LCD status boards on your wall. \n\nOnce again, the emerging standard for how
    to accomplish this appears to be Prometheus. It’s easy to add Prometheus exporters
    to your own applications, and you should do so, whether or not you run your app
    on Fly. Something will eventually collect those metrics and put them to good use!
    \n\n### At Any Rate\n\nSo that’s the metrics infrastructure we built, and the
    observability ideas we shoplifted to do it. \n\nThe neat thing about all of this
    is that Prometheus metrics are part of our public API. You can take them out for
    a spin with the free-plan Grafana Cloud right now; sign up with your Github account
    and you can add our API as a Prometheus data source. The URL will be `https://api.fly.io/prometheus/$(your-org)`
    — if you don’t know your org, `personal` works — and you’ll authenticate with
    a customer `Authorization` header `Bearer $(flyctl auth token)`. \n\nAdd a dashboard
    (the plus sign on the left menu), add an empty panel, click “Metrics”, and you’ll
    get a dropdown of all the current available metrics. For instance:\n\n```\nsum(rate(fly_app_http_responses_count[$__interval]))
    by (status)\n```\n\nYou can display any metric as a table to see the available
    labels; you can generally count on “app”, “region”, “host” (an identifier for
    the worker node your app is running on) and “instance”. Poke around and see what’s
    there!\n\nRemember, again, that if you're exporting your own Prometheus metrics
    and you've told us about it in `fly.toml`, those metrics will show up in Grafana
    as well; we'll index and store and handle queries on them for you, with the networking
    and access control stuff taken care of automatically. \n"
- :id: blog-building-a-distributed-turn-based-game-system-in-elixir
  :date: '2021-04-29'
  :category: blog
  :title: Building a Distributed Turn-Based Game System in Elixir
  :author: mark
  :thumbnail: elixir-game-thumbnail.jpg
  :alt:
  :link: blog/building-a-distributed-turn-based-game-system-in-elixir
  :path: blog/2021-04-29
  :body: |2


    <div class="lead">We’re Fly.io, and we run full-stack apps on our hardware around the world. This, it turns out, makes us one of the easiest places in the world to run clustered and distributed Elixir apps, and if you’re into Elixir, you should [give us a try](/docs/elixir/); it’ll take just a couple minutes to get started.</div>

    One of the best things about building web applications in Elixir is LiveView, the [Phoenix Framework](https://www.phoenixframework.org/) feature that makes it easy to create live and responsive web pages without all the layers people normally build.

    Many great [Phoenix LiveView](https://github.com/phoenixframework/phoenix_live_view) examples exist. They often show the ease and power of LiveView but stop at multiple browsers talking to a single web server. I wanted to go further and create a fully clustered, globally distributed, privately networked, secure application. What's more, I wanted to have fun doing it.

    So I set out to see if I could create a fully distributed, clustered, privately networked, global game server system. **Spoiler Alert: I did**.

    ## What I didn't have to build

    What I find remarkable is what I **didn't** need to build.

    I **didn't** build a Javascript front end using something like React.js or Vue.js. That is the typical approach. Building a JS front-end means I need JS components, a front-end router, a way to model the state in the browser, a way to transfer player actions to the server and a way to receive state updates from the server.

    On the server, I **didn't** build an API. Typically that would be REST or GraphQL with a JSON structure for transferring data to and from the front-end.

    I **didn't** need other external systems like Amazon SQS, Kafka, or even just Redis to pass state between servers. This means the entire system requires less cross-technology knowledge or specialized skills to build and maintain it. I used `Phoenix.PubSub` which is built on technology already in Elixir's VM, called the BEAM. I used the Horde library to provide a distributed process registry for finding and interacting with GameServers.

    As for [Fly.io's WireGuard connected private network](/docs/reference/private-networking) between [geographically distant regions](/docs/reference/regions/) and data centers? I don't even know how I would have done that in AWS, which is why I've always given up on the idea.

    ## What I did build

    What I built was just a proof of concept, but I'm surprised at how it came together. I ended up with a platform that can host many different types of games, all of which:

    * Can be multi-player
    * Offer a [Jackbox](https://en.wikipedia.org/wiki/Jackbox_Games)-style 4-letter game code system
    * Have on-demand game and match creation
    * With a fast, response UI

    And, just one little extra detail: the platform supports multiple connected servers operating together in clusters. Elixir for the win!

    I created this as an open source project on Github, so you can check it out yourself.

    [https://github.com/fly-apps/tictac](https://github.com/fly-apps/tictac)

    ## Technology

    I've worked with enough companies and teams to imagine several different approaches to build a system like this. Those approaches would all require large multi-disciplinary teams like a front-end JS team, a backend team, a DevOps team, and more. In contrast, I set out to do this by myself, in my spare time, and with a whole lot of "life" happening too.

    Here's what I chose to use:

    - [Elixir programming language](https://elixir-lang.org/) – A dynamic, functional language for building scalable and maintainable applications.
    - [Phoenix Framework](https://www.phoenixframework.org/) – Elixir's primary web framework
    - [Phoenix LiveView](https://github.com/phoenixframework/phoenix_live_view) – Rich, real-time user experiences with server-rendered HTML delivered by websockets
    - [libcluster](https://github.com/bitwalker/libcluster) – Automatic cluster formation/healing for Elixir applications.
    - [Horde](https://github.com/derekkraan/horde) – Elixir library that provides a distributed and supervised process registry.
    - [Fly.io](https://fly.io/) – Hosting platform that enables private networked connections and multi-region support.

    ## Application Architecture

    There are many guides to [getting started with LiveView](https://www.google.com/search?hl=en&q=getting%20started%20with%20phoenix%20liveview), I'm not focusing on that here. However, for context, this demonstrates the application architecture when running on a local machine.

    The "ABCD" in the graphic is a running game identified by the 4-letter code "ABCD".

    ![local system architecture](tictac-single-node-game-state-and-gen-server.png)

    Let's walk it through.

    1. A player uses a web browser to view the game board. The player clicks to make a move.
    2. The browser click triggers an event in the player's LiveView. There is a bi-directional websocket connection from the browser to LiveView.
    3. The LiveView process sends a message to the game server for the player's move.
    4. The GameServer uses `Phoenix.PubSub` to publish the updated state of game ABCD.
    5. The player's LiveView is subscribed to notifications for any updates to game ABCD. The LiveView receives the new game state. This automatically triggers LiveView to re-render the game immediately pushing the UI changes out to the player's browser.
    6. All connected players see the new state of the board and game.

    ## We need a game

    I needed a simple game to play and model for this game system. I chose Tic-Tac-Toe. Why?

    - It's simple to understand and play.
    - Easy to model.
    - Doesn't bog down the project with designing a game.
    - Quick to play through and test it being "over".

    I want to emphasize that this system can be used to build **many** turn-based, multi-user games! This simple Tic-Tac-Toe game covers all of the basics we will need. Besides, [Tic-Tac-Toe was even made into a TV Show](https://www.youtube.com/watch?v=xHObMqUdBa8)!


    This is what the game looks like with 2 players.

    <%= video_tag "tictac_local_playing.mp4?card" %>

    The game system works great locally. Let's get it deployed!

    ## Hosting on Fly.io

    Following the [Fly.io Getting Started Guide for Elixir](/docs/elixir/), I created a Dockerfile to generate a release for my application. Check out the repo here:

    [https://github.com/fly-apps/tictac](https://github.com/fly-apps/tictac)

    The README file outlines both how to run it locally and deploy it globally on Fly.io.

    What is special about hosting it on Fly.io? Fly makes it easy to deploy a server geographically closer to the users I want to reach. When a user goes to my website, they are directed to **my nearest server**. This means any responsive LiveView updates and interactions will be even faster and smoother because the regular TCP and websocket connections are just that much physically closer.

    But for the game, I wanted there to be a single source of truth. That GameServer can only exist in one place. Supporting a private, networked, and fully clustered environment means my server in the EU can communicate with the GameServer that might be running in the US. But my EU players have a fast and responsive UI connection close to them. This provides a better user experience!

    ![Fly region clustering](tictac-fly-region-cluster.png)

    Here is what I find compelling about Fly.io for hosting Elixir applications:

    - Secure HTTPS automatically using Let's Encrypt. I didn't do anything to set that up!
    - Distributed nodes use [private network](/docs/reference/private-networking/) connections through [WireGuard](https://www.wireguard.com/).
    - Nodes auto-clustered using `libcluster` and the `DNSPoll` strategy. (See [here for details](https://github.com/fly-apps/tictac/blob/main/config/runtime.exs#L25))
    - Geographically distributed servers near my users are clustered together.
    - This was the easiest multi-region yet still privately networked solution I've ever seen! (I have experience with AWS, DigitalOcean, and Heroku)

    ## Conclusion

    For a proof-of-concept, I couldn't be happier! In a short time, by myself, I created a working, clustered, distributed, multi-player, globe-spanning gaming system!

    The pairing of Elixir + LiveView + Fly.io is excellent. Using Elixir and LiveView, I built a powerful, resilient, and distributed system in orders of magnitude shorter time and effort. Deploying it on Fly.io let let me easily do something I would never have even tried before, namely, deploying servers in regions around the globe while keeping the application privately networked and clustered together.

    Whenever I've thought of creating a service with a global audience, I'd usually scapegoat the idea saying, "Well I don't know how I'd get the translations, so I'll just stick with the US. It's a huge market anyway." In short, I've never even considered a globally connected application because it would be "way too hard".

    But here, with Elixir + LiveView + Fly.io, I did something by myself in my spare time that larger teams using more technologies struggle to deliver. I'm still mind blown by it!

    ## What will you build?

    Tic-Tac-Toe is a simple game and doesn't provide "hours of fun". I know **you** can think of a much cooler and more interesting multi-player, turn-based game that you could build on a system like this. What do you have in mind?

    ---

    Discuss this project (and more) in our [community forums](https://community.fly.io/t/building-a-distributed-turn-based-game-system-in-elixir/1310).
- :id: blog-docker-without-docker
  :date: '2021-04-08'
  :category: blog
  :title: Docker without Docker
  :author: thomas
  :thumbnail: starry-containers-thumbnail.jpg
  :alt:
  :link: blog/docker-without-docker
  :path: blog/2021-04-08
  :body: "\n\n<div class=\"lead\">We’re Fly.io. We take container\nimages and run
    them on our hardware around the world.  It’s pretty\nneat, and you [should check
    it out](https://fly.io/docs/speedrun/); with an already-working Docker\ncontainer,
    you can be up and running on Fly in well under 10 minutes.</div>\n\nEven though
    most of our users deliver software to us as Docker\ncontainers, we don’t use Docker
    to run them. Docker is great, but\nwe’re high-density multitenant, and despite
    strides, Docker’s\nisolation isn’t strong enough for that. So, instead, we transmogrify\ncontainer
    images into [Firecracker micro-VMs](https://fly.io/blog/sandboxing-and-workload-isolation/).\n\nLet's
    demystify that.\n\n## What’s An OCI Image?\n\nThey do their best to make it look
    a lot more complicated, but OCI\nimages — OCI is [the standardized container format
    used by Docker](https://github.com/opencontainers/image-spec) — are\npretty simple.
    \ An OCI image is just a stack of tarballs.\n\nBacking up: most people build images
    from Dockerfiles. A useful way to\nlook at a Dockerfile is as a series of shell
    commands, each generating\na tarball; we call these “layers”. To rehydrate a container
    from its\nimage, we just start the the first layer and unpack one on top of the\nnext.\n\nYou
    can write a shell script to pull a Docker container from its\nregistry, and that
    might clarify. Start with some configuration; by\ndefault, we’ll grab the base
    image for `golang`:\n\n```bash\nimage=\"${1:-golang}\"\nregistry_url='https://registry-1.docker.io'\nauth_url='https://auth.docker.io'\nsvc_url='registry.docker.io'\n```\n\nWe
    need to authenticate to pull public images from a Docker registry –\nthis is boring
    but relevant to the next section – and that’s easy:\n\n```bash\nfunction auth_token
    { \n  curl -fsSL \"${auth_url}/token?service=${svc_url}&scope=repository:library/${image}:pull\"
    | jq --raw-output .token\n}\n```\n\nThat token will allow us to grab the “manifest”
    for the container,\nwhich is a JSON index of the parts of a container.\n\n```bash\nfunction
    manifest { \n  token=\"$1\"\n  image=\"$2\"\n  digest=\"${3:-latest}\"\n\n  curl
    -fsSL \\\n    -H \"Authorization: Bearer $token\" \\\n    -H 'Accept: application/vnd.docker.distribution.manifest.list.v2+json'
    \\\n    -H 'Accept: application/vnd.docker.distribution.manifest.v1+json' \\\n
    \   -H 'Accept: application/vnd.docker.distribution.manifest.v2+json' \\\n      \"${registry_url}/v2/library/${image}/manifests/${digest}\"\n}\n```\n\nThe
    first query we make gives us the “manifest list”, which gives us\npointers to
    images for each supported architecture:\n\n```json\n  \"manifests\": [\n    {\n
    \     \"digest\": \"sha256:3fc96f3fc8a5566a07ac45759bad6381397f2f629bd9260ab0994ef0dc3b68ca\",\n
    \     \"platform\": {\n        \"architecture\": \"amd64\",\n        \"os\": \"linux\"\n
    \     },\n    },\n```\n\nPull the `digest` out of the matching architecture entry
    and perform\nthe same fetch again with it as an argument, and we get the manifest:\nJSON
    pointers to each of the layer tarballs:\n\n```json\n   \"config\": {\n      \"digest\":
    \"sha256:0debfc3e0c9eb23d3fc83219afc614d85f0bc67cf21f2b3c0f21b24641e2bb06\"\n
    \  },\n   \"layers\": [\n      {\n         \"digest\": \"sha256:004f1eed87df3f75f5e2a1a649fa7edd7f713d1300532fd0909bb39cd48437d7\"\n
    \     },\n```\n\nIt’s as easy to grab the actual data associated with these entries
    as\nyou’d hope:\n\n```bash\nfunction blob {\n  token=\"$1\"\n  image=\"$2\"\n
    \ digest=\"$3\"\n  file=\"$4\"\n\n  curl -fsSL -o \"$file\" \\\n      -H \"Authorization:
    Bearer $token\" \\\n        \"${registry_url}/v2/library/${image}/blobs/${digest}\"\n}\n```\n\nAnd
    with those pieces in place, pulling an image is simply:\n\n```bash\nfunction layers
    { \n  echo \"$1\" | jq --raw-output '.layers[].digest'\n}\n\ntoken=$(auth_token
    \"$image\")\namd64=$(linux_version $(manifest \"$token\" \"$image\"))\nmf=$(manifest
    \"$token\" \"$image\" \"$amd64\")\n\ni=0\nfor L in $(layers \"$mf\"); do\n  blob
    \"$token\" \"$image\" \"$L\" \"layer_${i}.tgz\"\n  i=$((i + 1 ))\ndone\n```\n\nUnpack
    the tarballs in order and you’ve got the filesystem layout the\ncontainer expects
    to run in. Pull the “config” JSON and you’ve got the\nentrypoint to run for the
    container; you could, I guess, pull and run\na Docker container with nothing but
    a shell script, which I’m probably\nthe [1,000th person to point out](https://github.com/p8952/bocker).
    At any rate,\n[here’s the whole thing](https://gist.github.com/tqbf/10006fae0b81d7c7c93513890ff0cf08).\n\n![Vitally
    important system diagram](gwg.png?2/3&centered)\n  \nYou’re likely of one of two
    mindsets about this: (1) that it’s\nextremely Unixy and thus excellent, or (2)
    that it’s extremely Unixy\nand thus horrifying.\n\nUnix `tar` is problematic.
    Summing up [Aleksa Sarai](https://www.cyphar.com/blog/post/20190121-ociv2-images-i-tar):
    `tar` isn’t well\nstandardized, can be unpredictable, and is bad at random access
    and\nincremental updates. Tiny changes to large files between layers\npointlessly
    duplicate those files; the poor job `tar` does managing\ncontainer storage is
    part of why people burn so much time optimizing\ncontainer image sizes.\n\n<div
    class=\"callout\">\nAnother fun detail is that OCI containers share a security
    footgun\nwith git repositories: it’s easy to accidentally build a secret into
    a\npublic container, and then inadvertently hide it with an update in a\nlater
    image.\n</div>\n\nWe’re of a third mindset regarding OCI images, which is that
    they are\nhorrifying, and that’s liberating. They work pretty well in practice!\nLook
    how far they’ve taken us! Relax and make crappier designs;\nthey’re all you probably
    need.\n\nSpeaking of which:\n\n## Multi-Tenant Repositories\n\nBack to Fly.io.
    Our users need to give us OCI containers, so that we\ncan unpack and run them.
    There’s standard Docker tooling to do that,\nand we use it: we host a [Docker
    registry](https://docs.docker.com/registry/spec/api/) our users push to.\n\nRunning
    an instance of the Docker registry is very easy. You can do it\nright now; `docker
    pull registry && docker run registry`. But our\nneeds are a little more complicated
    than the standard Docker registry:\nwe need multi-tenancy, and authorization that
    wraps around our\nAPI. This turns out not to be hard, and we can walk you through
    it.\n\nA thing to know off the bat: our users drive Fly.io with a command\nline
    utility called `flyctl`. `flyctl` is a Go program (with\n[public source](https://github.com/superfly/flyctl))
    that runs on Linux, macOS, and Windows. A nice thing about\nworking in Go in a
    container environment is that the whole ecosystem\nis built in the same language,
    and you can get a lot of stuff working\nquickly just by importing it. So, for
    instance, we can drive our\nDocker repository clientside from `flyctl` just by
    calling into\nDocker’s clientside library.\n\n<div class=\"callout\">\nIf you're
    building your own platform and you have the means, I highly\nrecommend the CLI-first
    tack we took. It is so choice. `flyctl` made\nit very easy to add new features,
    like [databases](https://fly.io/docs/reference/postgres/),\n[private networks](https://fly.io/blog/incoming-6pn-private-networks/),
    [volumes](https://fly.io/blog/persistent-storage-and-fast-remote-builds/), and
    our [bonkers SSH access system](https://fly.io/blog/ssh-and-user-mode-ip-wireguard/).\n</div>\n
    \ \nOn the serverside, we started out simple: we ran an instance of the\nstandard
    Docker registry with an authorizing proxy in front of\nit. `flyctl` manages a
    bearer token and uses the Docker APIs to\ninitiate Docker pushes that pass that
    token; the token authorizes\nrepositories serverside using calls into our API.\n\nWhat
    we do now isn’t much more complicated than that. Instead of\nrunning a vanilla
    Docker registry, we built a custom repository\nserver. As with the client, we
    get a Docker registry implementation\njust by importing Docker’s registry code
    as a Go dependency.\n\n[We’ve extracted and simplified some of the Go code we
    used to build\nthis here](https://gist.github.com/tqbf/ebd504a625813e6b8c5913fc28cc9515),
    just in case anyone wants to play around with the same\nidea. This isn’t our production
    code (in particular, all the actual\nauthentication is ripped out), but it’s not
    far from it, and as you\ncan see, there’s not much to it.\n\nOur custom server
    isn’t architecturally that different from the\nvanilla registry/proxy system we
    had before. We wrap the Docker\nregistry API handlers with authorizer middleware
    that checks tokens,\nreferences, and rewrites repository names. There are some
    very minor\ngotchas:\n\n* Docker is content-addressed, with blobs “named” for
    their SHA256 hashes, and attempts to reuse blobs shared between different repositories.
    You need to catch those cross-repository mounts and rewrite them.\n* Docker’s
    registry code generates URLs with  `_state` parameters that embed references to
    repositories; those need to get rewritten too. `_state` is HMAC-tagged; our code
    just shares the HMAC key between the registry and the authorizer.\n\nIn both cases,
    the source of truth for who has which repositories and\nwhere is the database
    that backs our API server. Your push carries a\nbearer token that we resolve to
    an organization ID, and the name of\nthe repository you’re pushing to, and, well,
    our design is what you’d\nprobably come up with to make that work. I suppose my
    point here is\nthat it’s pretty easy to slide into the Docker ecosystem.\n\n##
    Building And Running VMs\n\nThe pieces are on the board:\n\n* We can accept containers
    from users\n* We can store and manage containers for different organizations.\n*
    We've got a VMM engine, Firecracker, that [we've written about already](https://fly.io/blog/sandboxing-and-workload-isolation).\n\nWhat
    we need to do now is arrange those pieces so that we can run containers\nas Firecracker
    VMs.\n  \nAs far as we're concerned, a container image is just a stack of tarballs
    and a blob of configuration (we layer additional configuration in as well). The
    tarballs expand to a directory tree for the VM to run in, and the configuration
    tells us what\nbinary in that filesystem to run when the VM starts.\n\nMeanwhile,
    what Firecracker wants is a set of block devices that Linux will mount as it boots
    up.\n\nThere's an easy way on Linux to take a directory tree and turn it into
    a block device: create a file-backed [loop device](https://man7.org/linux/man-pages/man4/loop.4.html),
    and copy the directory tree into it. And that's how we used to do things. When
    our orchestrator asked to boot up a VM on one of our servers, we would:\n\n1.
    Pull the matching container from the registry.\n2. Create a loop device to store
    the container's filesystem on.\n3. Unpack the container (in this case, using Docker's
    Go libraries) into the mounted loop device.\n4. Create a second block device and
    inject our init, kernel, configuration, and other goop into.\n5. Track down any
    [persistent volumes](https://fly.io/blog/persistent-storage-and-fast-remote-builds/)
    attached\n   to the application, unlock them with LUKS, and collect their unlocked
    block devices.\n6. Create a [TAP device](https://en.wikipedia.org/wiki/TUN/TAP),
    configure it for our network,\n   and [attach BPF code to it](https://fly.io/blog/bpf-xdp-packet-filters-and-udp/).\n7.
    Hand all this stuff off to Firecracker and tell it to boot .\n\nThis is all a
    few thousand lines of Go.\n\nThis system worked, but wasn't especially fast. Part
    of [the point of Firecracker](https://www.usenix.org/system/files/nsdi20-paper-agache.pdf)
    is to boot so quickly that you (or AWS) can host Lambda functions in it and not
    just long-running programs. A big problem for us was caching; a server in, say,
    Dallas that's asked to run a VM for a customer is very likely to be asked to run
    more instances of that server (Fly.io apps scale trivially; if you've got 1 of
    something running and would be happier with 10 of them, you just run `flyctl scale
    count 10`). We did some caching to try to make this faster, but it was of dubious
    effectiveness.\n\nThe system we'd been running was, as far as container filesystems
    are concerned, not a whole lot more sophisticated than the\nshell script at the
    top of this post. So Jerome replaced it.\n\n<%= partial \"shared/posts/cta\",
    locals: {\n  title: \"This was all Andrew Dunham's idea.\",\n  text: \"We asked
    if he wanted credit and he hesitated and said maybe and we said we'd keep it subtle,
    so here you go. He doesn't work for us, he's just awesome.\",\n  link_url: \"https://www.youtube.com/watch?v=n2fM1vc-T94\",\n
    \ link_text: \"We ❤️ Andrew&nbsp;&nbsp;<span class='opacity-50'>&rarr;</span>\"\n}
    %>\n  \nWhat we do now is run, on each of our servers, an instance of [`containerd`](https://containerd.io/).
    `containerd` does a whole bunch of stuff, but we use it as as a cache.\n\nIf you're
    a Unix person from the 1990s like I am, and you just recently started paying attention
    to how Linux storage works again, you've probably noticed that _a lot has changed_.
    Sometime over the last 20 years, the block device layer in Linux got interesting.
    LVM2 can pool raw block devices and create synthetic block devices on top of them.
    It can treat block device sizes\nas an abstraction, chopping a 1TB block device
    into 1,000 5GB synthetic devices (so long as you don't actually use 5GB on all\nthose
    devices!). And it can create snapshots, preserving the blocks on a device in another
    synthetic device, and sharing those\nblocks among related devices with copy-on-write
    semantics.\n\n`containerd` knows how to drive all this LVM2 stuff, and while I
    guess it's out of fashion to use the `devmapper` backend\nthese days, it works
    beautifully for our purposes. So now, to get an image, we pull it from the registry
    into our server-local `containerd`, configured to run on an LVM2 thin pool. `containerd`
    manages snapshots for every instance of a VM/container\nthat we run. Its API provides
    a simple \"lease\"-based garbage collection scheme; when we boot a VM, we take
    out a lease on\na container snapshot (which synthesizes a new block device based
    on the image, which containerd unpacks for us); LVM2 COW\nmeans multiple containers
    don't step on each other. When a VM terminates, we surrender the lease, and containerd
    eventually\nGCs.\n\nThe first deployment of a VM/container on one of our servers
    does some lifting, but subsequent deployments are lightning\nfast (the VM build-and-boot
    process on a second deployment is faster than the logging that we do). \n \n##
    Some Words About Init\n\nJerome wrote our `init` in Rust, and, after being cajoled
    by Josh Triplett, [we released the code](https://github.com/superfly/init-snapshot),
    which you can go read.\n\nThe filesystem that Firecracker is mounting on the snapshot
    checkout we create is pretty raw. The first job our `init` has is to fill in the
    blanks to fully populate the root filesystem with the mounts that Linux needs
    to run normal programs. \n\nWe inject a configuration file into each VM that carries
    the user, network, and entrypoint information needed to run the image. `init`
    reads that and configures the system. We use our own DNS server for private networking,
    so `init` overrides `resolv.conf`. We run a tiny SSH server for user logins over
    WireGuard; `init` spawns and monitors that process. We spawn and monitor the entry
    point program. That’s it; that’s an init.\n\n## Putting It All Together\n  \nSo,
    that's about half the idea behind Fly.io. We run server hardware in racks around
    the world; those servers are tied\ntogether with an orchestration system that
    plugs into our API. Our CLI, `flyctl`, uses Docker's tooling to push OCI\nimages
    to us. Our orchestration system sends messages to servers to convert those OCI
    images to VMs. It's all pretty\nneato, but I hope also kind of easy to get your
    head wrapped around.\n\nThe other \"half\" of Fly is our Anycast network, which
    is a CDN built in Rust that uses BGP4 Anycast routing to direct traffic to the
    nearest instance of your application. About which: more later.\n\n<%= partial
    \"shared/posts/cta\", locals: {\n  title: \"You can play with this right now.\",\n
    \ text: \"It'll take less than 10 minutes to get almost any container you've got
    running globally on our Rust-powered anycast proxy network.\",\n  link_url: \"https://fly.io/docs/speedrun/\",\n
    \ link_text: \"Try Fly for free&nbsp;&nbsp;<span class='opacity-50'>&rarr;</span>\"\n}
    %>\n  "
- :id: blog-the-5-hour-content-delivery-network
  :date: '2021-03-16'
  :category: blog
  :title: The 5-hour CDN
  :author: kurt
  :thumbnail:
  :alt:
  :link: blog/the-5-hour-content-delivery-network
  :path: blog/2021-03-16
  :body: "\n\nThe term \"CDN\" (\"content delivery network\") conjures Google-scale
    companies managing huge racks of hardware, wrangling hundreds of gigabits per
    second. But CDNs are just web applications. That's not how we tend to think of
    them, but that's all they are. You can build a functional CDN on an 8-year-old
    laptop while you're sitting at a coffee shop. I'm going to talk about [what you
    might come up with](https://github.com/fly-apps/nginx-cluster) if you spend the
    next five hours building a CDN.\n\nIt's useful to define exactly what a CDN does.
    A CDN hoovers up files from a central repository (called an `origin`) and stores
    copies close to users. Back in the dark ages, the origin was a CDN's FTP server.
    These days, origins are just web apps and the CDN functions as a proxy server.
    So that's what we're building: a distributed caching proxy.\n\n## Caching proxies\n\nHTTP
    defines a whole infrastructure of [intricate and fussy caching features](https://portswigger.net/research/practical-web-cache-poisoning).
    It's all very intimidating and complex. So we're going to resist the urge to build
    from scratch and use the work other people have done for us.\n\nWe have choices.
    We could use [Varnish](https://varnish-cache.org/) (scripting! edge side includes!
    PHK blog posts!). We could use [Apache Traffic Server](https://trafficserver.apache.org/)
    (being the only new team this year to use ATS!). Or we could use [NGINX](https://www.nginx.com/)
    (we're already running it!). The only certainty is that you'll come to hate whichever
    one you pick. Try them all and pick the one you hate the least.\n\n(We kid! Netlify
    is built on ATS. Cloudflare uses NGINX. Fastly uses Varnish.)\n\n\nWhat we're
    talking about building is not basic. But it's not so bad. All we have to do is
    take our antique Rails setup and run it in multiple cities. If we can figure out
    how to get people in Australia to our server in Sydney and people in Chile to
    our server in Santiago, we'll have something we could reasonably call a CDN.\n\n##
    Traffic direction\nRouting people to nearby servers is a solved problem. You basically
    have three choices:\n\n1. Anycast: acquire routable address blocks, advertise
    them in multiple places with BGP4, and then pretend that you have opinions about
    \"communities\" and \"route reflectors\" on Twitter. Let the Internet do the routing
    for you. Downside: it's harder to do, and the Internet is sometimes garbage. Upside:
    you might become insufferable.\n2. DNS: Run trick DNS servers that return specific
    server addresses based on IP geolocation. Downside: the Internet is moving away
    from geolocatable DNS source addresses. Upside: you can deploy it anywhere without
    help.\n3. Be like a game server: Ping a bunch of servers and use the best. Downside:
    gotta own the client. Upside: doesn't matter, because you don't own the client.\n\nYou're
    probably going to use a little of (1) and a little of (2). DNS load balancing
    is pretty simple. You don't really even have to build it yourself; you can host
    DNS on companies like DNSimple, and then define rules for returning addresses.
    Off you go!\n\nAnycast is more difficult. We have more to say about this — but
    not here. In the meantime, you can use [us](https://fly.io), and deploy an app
    with an Anycast address in about 2 minutes. This is bias. But also: true.\n\nBoom,
    CDN. Put an NGINX in each of a bunch of cities, run DNS or Anycast for traffic
    direction, and you're 90% done. The remaining 10% will take you months.\n\n##
    The Internet is breaking\nThe briny deeps are filled with undersea cables, crying
    out constantly to nearby ships: [\"drive through me\"](https://www.theguardian.com/business/2008/feb/01/internationalpersonalfinancebusiness.internet)!
    Land isn't much better, as the old networkers shanty goes: \"backhoe, backhoe,
    digging deep — make the backbone go to sleep\". When you run a server in a single
    location, you don't so much notice this. Run two servers and you'll start to notice.
    Run servers around the world and you'll notice it to death.\n\nWhat's cool is:
    running a single NGINX in multiple cities gives you a lot of ready-to-use redundancy.
    If one of them dies for some reason, there are bunch more to send traffic to.
    When one of your servers goes offline, the rest are still there serving most of
    your users.\n\nIt's tedious but straightforward to make this work. You have health
    checks (aside: when CDN regions break, they usually break by being slow, so you'd
    hope your health checks catch that too). They tell you when your NGINX servers
    fail. You script DNS changes or withdraw BGP routes (perhaps just by stopping
    your BGP4 service on those regions) in response.\n\nThat's server failure, and
    it's easy to spot. Internet burps are harder to detect. You'll need to run external
    health checks, from multiple locations. It's easy to get basic, multi-perspective
    monitoring – we use [Datadog](https://www.datadoghq.com/product/synthetic-monitoring/)
    and [updown.io](https://updown.io/), and we're building out our own [half-built
    home grown service](https://fly.io/blog/building-clusters-with-serf/). You're
    not asking for much more than what `cURL` will tell you. Again: the thing you're
    super wary about in a CDN is a region getting slow, not falling off the Internet
    completely. \n\nQuick aside: notice that all those monitoring options work from
    someone else's data center to your data center. DC-DC traffic is a good start,
    enough for a lot of jobs. But it isn't representative. Your users aren't in data
    centers (I hope). When you're really popular, what you want is monitoring from
    the vantage point of actual clients. For this, you can find hundreds of companies
    selling RUM (real user monitoring), which usually takes the form of surreptitiously
    embedded Javascript bugs. There's one rum we like. It's sold by a company called
    Plantation and it's aged in wine casks. Drink a bunch of it, and then do your
    own instrumentation with [Honeycomb](https://www.honeycomb.io/).\n\nRidiculous
    Internet problems are the worst. But the good news about them is, everyone is
    making up the solutions as they go along, so we don't have to talk about them
    so much. Caching is more interesting. So let's talk about onions. \n\n## The Golden
    Cache Hit Ratio\n\nThe figure of merit in cache measurement is \"cache ratio\".
    Cache ratio measures how often we're able to server from our cache, versus the
    origin.\n\nA cache ratio of 80% just means \"when we get a request, we can serve
    it from cache 80% of the time, and the remaining 20% of the time we have to proxy
    the request to the origin\". If you're building something that wants a CDN, high
    cache ratios are good, and low cache ratios are bad.\n\nIf you followed the link
    earlier in the post to the Github repository, you might've noticed that our [naïve
    NGINX setup](https://github.com/fly-apps/nginx/) is an isolated single server.
    Deploying it in twenty places gives us twenty individual servers. It's dead simple.
    But the simplicity has a cost – there's no per-region redundancy. All twenty servers
    will need to make requests to the origin. This is brittle, and cache ratios will
    suffer. We can do better.\n\nThe simple way to increase redundancy is to add a
    second server in each region. But doing that might wreck cache ratios. The single
    server has the benefit of hosting a single cache for all users; with two, you've
    got twice the number of requests per origin, and twice the number of cache misses.\n\nWhat
    you want to do is teach your servers to talk to each other, and make them ask
    their friends for cache content. The simplest way to do this is to create cache
    shards – split the data up so each server is responsible for a chunk of it, and
    everyone else routes requests to the cache shard that owns the right chunk.\n\nThat
    sounds complicated, but NGINX's built in load balancer supports hash based load
    balancing. It hashes requests, and forwards the \"same request\" to same server,
    assuming that server is available. If you're playing the home version of this
    blog post, here's a [ready to go example](https://github.com/fly-apps/nginx-cluster)
    of an NGINX cluster that discovers its peers, hashes the URL, and serves requests
    through available servers.\n\n![Consistent hashing diagram](./consistent-hashing.png?2/3&centered)\n\nWhen
    requests for `a.jpg` hit our NGINX instances, they will all forward the request
    to the same server in the cluster. Same for `b.jpg`. This setup has servers serve
    as both the load balancing proxy and the storage shard. You can separate these
    layers, and you might want to if you're building more advanced features into your
    CDN.\n\n<aside class=\"callout\">\n## A small, financially motivated aside\n\nOur
    clustered NGINX example uses Fly-features we think are really cool. [Persistent
    volumes](https://fly.io/blog/persistent-storage-and-fast-remote-builds/) help
    keep cache ratios high between NGINX upgrades. [Encrypted private networking](https://fly.io/blog/incoming-6pn-private-networks/)
    makes secure NGINX to NGINX communications simple and keeps you from having to
    do complicated [mTLS gymnastics](https://twitter.com/colmmacc/status/1057018254940504064?lang=en).
    Built in DNS service discovery helps keep the clusters up to date when we add
    and remove servers. If it sounds a little too perfectly matched, it's because
    we built these features specifically for CDN-like-workloads.\n\nBut of course,
    you can do all this stuff anywhere, not just on Fly. But [it's easy on Fly](https://fly.io/docs/speedrun/).\n</aside>\n\n##
    Onions have layers\nTwo truths: a high cache ratio is good, the Internet is bad.
    If you like killing birds and conserving stones, you'll really enjoy solving for
    cache ratios and garbage Internet. The answer to both of those problems involves
    getting the Internet's grubby hands off our HTTP requests. A simple way to increase
    cache ratios: bypass the out-of-control Internet and proxy origin requests through
    networks you trust to behave themselves.\n\nCDNs typically have servers in regions
    close to their customers' origins. If you put our NGINX example in Virginia, you
    suddenly have servers close to AWS's largest region. And you definitely have customers
    on AWS. That's the advantage of existing alongside a giant powerful monopoly!\n\n![Proxy
    through region diagram](proxy-region-to-origin.png?2/3&centered)\n\nYou can, with
    a little NGINX and proxy magic, send _all_ requests through Virginia on their
    way to the origin servers. This is good. There are fewer Internet bear traps between
    your servers in Virginia and your customers' servers in `us-east-1`. And now you
    have a single, canonical set of servers to handle a specific customers' requests.\n\nGood
    news. This setup improves your cache ratio AND avoids bad Internet. For bonus
    points, it's also the foundation for extra CDN features.\n\nIf you've ever gone
    CDN shopping, you've come across things like \"Shielding\" and \"Request Coalescing\".
    \ Origin shielding typically just means sending all traffic through a known data
    center. This can minimize traffic to origin servers, and _also_, because you probably
    know the IPs your CDN regions use, you can control access with simple L4 firewall
    rules.\n\n<%= partial \"shared/posts/cta\", locals: {\n  title: \"That’s not how
    we’d do it\",\n  text: \"Really. Firewalls are the wrong way to protect origin
    servers. Like WireGuard. We support WireGuard tunnels so you can talk to your
    origin privately.\",\n  link_url: \"https://fly.io/docs/reference/private-networking/#private-network-vpn\",\n
    \ link_text: \"Get tunnelin'&nbsp;&nbsp;<span class='opacity-50'>&rarr;</span>\"\n}
    %>\n\nCoalescing requests also minimizes origin traffic, especially during big
    events when many users are trying to get at the same content. When 100,000 users
    request your [latest cleverly written blog post](#) at once, and it's not yet
    cached, that _could_ end up meaning 100k concurrent requests to your origin. That's
    a face melting level of traffic for most origins. Solving this is a matter of
    \"locking\" a specific URL to ensure that if an NGINX server is making an origin
    request, the other clients pause until the cache is file. In our clustered NGINX
    example, this is a [two line configuration](https://github.com/fly-apps/nginx-cluster/blob/main/nginx.conf#L114-L115).\n\n##
    Oh no, slow\nProxying through a single region to increase cache ratios is a little
    bit of a cheat. The entire purpose of a CDN is to speed things up for users. Sending
    requests from Singapore to Virginia will make things _barely_ faster, because
    a set of NGINX servers with cached content is almost always faster than origin
    services. But, really, it's slow and undesirable.\n\nYou can solve this with more
    onion layers:\n\n![Proxy through more regions diagram](proxy-region-to-origin-2.png?2/3&centered)\n\nRequests
    in Australia could run through Singapore on the way to Virginia. Even light is
    slow over 14,624 kilometers (Australia to Virginia), so Australia to Singapore
    (4,300 kilometers) with a cache cuts a perceptible amount of latency. It will
    be a little slower on cache misses. But we're talking about the difference between
    \"irritatingly slow\" and \"150ms worse than irritatingly slow\".\n\nIf you are
    building a general purpose CDN, this is a nice way to do it. You can create a
    handful of super-regions that aggregate cache data for part of the world.\n\nIf
    you're not building a general purpose CDN, and are instead just trying to speed
    up your application, this is a brittle solution. You are _probably_ better off
    distributing portions of your application to multiple regions.\n\n## Where are
    we now?\n\nThe basic ideas of a CDN are old, and easy to understand. But building
    out a CDN has historically been an ambitious team enterprise, not a weekend project
    for a single developer.\n\nBut the building blocks for a capable CDN have been
    in tools like NGINX for a long time. If you've been\n[playing along at home with
    the Github repo](https://github.com/fly-apps/nginx-cluster), we hope\nyou've noticed
    that even the most complicated iteration of the design we're talking about, a
    design that has per-region redundancy and that allows for rudimentary control
    of request routing between regions, is mostly just NGINX\nconfiguration --- and
    not an especially complicated configuration. The \"code\" we've added is just
    `bash` sufficient\nto plug in addresses.\n\nSo that's a CDN. It'll work just great
    for simple caching. For complicated apps, it's only missing a few things.\n\nNotably,
    we didn't address cache expiration _at all_. One ironclad rule of using a CDN
    is: you will absolutely put an embarrassing typo on a launch release, notice it
    too late, and discover that all your cache servers have a copy titled \"A Better
    Amercia\". Distributed cache invalidation is a big, hairy problem for a CDN. Someone
    could write a whole article about it.\n\nThe CDN layer is also an exceptionally
    good place to add app features. Image optimization, WAF, API rate limiting, bot
    detection, we could go on. Someone could turn these into ten more articles.\n\nOne
    last thing. Like we mentioned earlier: this whole article is bias. We're highlighting
    this CDN design because we built a platform that makes it very easy to express
    (you should play with it). Those same platform features that make it trivial to
    build a CDN on Fly also make it easy to distribute your whole application; an
    application designed for edge distribution may not need a CDN at all.\n\n---\n\nWould
    you like to know more? [Ask us anything](https://community.fly.io/t/the-5-hour-cdn/946)."
- :id: blog-ssh-and-user-mode-ip-wireguard
  :date: '2021-03-01'
  :category: blog
  :title: SSH and User-mode IP WireGuard
  :author: thomas
  :thumbnail:
  :alt:
  :link: blog/ssh-and-user-mode-ip-wireguard
  :path: blog/2021-03-01
  :body: "\n \n<div class=\"lead\">Here’s a thing you’d probably want to do with an
    application hosted on a provider like [Fly.io](https://fly.io): pop a shell on
    it.</div>\n\nBut Fly is kind of an odd duck. We run hardware in data centers around
    the world, connected to the Internet via Anycast and to each other with a WireGuard
    mesh. We take Docker-type containers from users and transmogrify them into Firecracker
    micro-VMs. And, when we first got started, we did all this stuff so that our customers
    could run “edge applications”; generally, relatively small, self-contained bits
    of code that are especially sensitive to network performance, and that need to
    be run close to users. In that environment, being able to SSH into an app is not
    that important.\n\nBut that’s not all people use Fly for now. Today, you can easily
    run your whole app on Fly. We’ve made it easy to run [ensembles of services](https://fly.io/blog/building-clusters-with-serf/),
    in cluster configurations, that can [talk to each other privately](https://fly.io/blog/incoming-6pn-private-networks/),
    [store data persistently](https://fly.io/blog/persistent-storage-and-fast-remote-builds/),
    and talk to their operators over WireGuard. And, if I keep on writing like this,
    I can probably tag every blog post we’ve written in the last couple months.\n\nAnyways,
    we didn’t have an SSH feature.\n\nNow, of course, you could just build a container
    that ran an SSH service, and then SSH into it. Fly supports raw TCP ([and UDP](https://fly.io/blog/bpf-xdp-packet-filters-and-udp/))
    networking; if you told our Anycast network about your weird SSH port in your
    `fly.toml`, we’d route SSH connections to you, and that’d work just fine.\n\nBut
    that’s not how people want to build containers, and we’re not asking them to.
    So, we built an SSH feature. It is wacky. I am here to describe it, in two parts.\n
    \   \n## Part The First: 6PN and Hallpass\n  \nI’ve written [a bunch about private
    networking at Fly](https://fly.io/blog/ipv6-wireguard-peering/). Long story short:
    it’s like a simpler, IPv6 version of GCP or AWS “Virtual Private Clouds”; we call
    it “6PN”. When an app instance (a Firecracker micro-VM) is started at Fly, we
    assign it a special IPv6 prefix; the prefix encodes the app’s ID, the ID of its
    organization, and an identifier for the Fly hardware it’s running on. We use a
    tiny bit of eBPF code to statically route those IPv6 packets along our internal
    WireGuard mesh, and to make sure that customers can’t hop into different organizations.
    \n\nFurther, you can bridge the private IPv6 networks we create with other networks,
    using WireGuard. Our API will [mint new WireGuard configurations](https://fly.io/docs/flyctl/wireguard/),
    and you can stick them on an [EC2 host to proxy RDS Postgres](https://github.com/fly-apps/rds-connector).
    Or, if you like, use the Windows, Linux, or macOS WireGuard client to connect
    your development machine to your private network.\n\nYou can probably see where
    this is going.\n\nWe wrote a [teeny, tiny, trivial SSH server in Go](https://community.fly.io/t/incoming-ssh-support-for-instances/685/19),
    called Hallpass. It’s practically the “hello world” of Go’s `x/crypto/ssh` library.
    (If I was doing it over again, I’d probably just use the [Gliderlabs SSH server
    library](https://github.com/gliderlabs/ssh), where it would literally be “hello
    world”). [Our Firecracker “init”](https://github.com/superfly/init-snapshot) starts
    Hallpass on all instances, bound to the instance’s 6PN address.\n\n<%= partial
    \"shared/posts/cta\", locals: {\n  title: \"Jerome published our init source\",\n
    \ text: \"Normally, this big balloon thingy would be an elaborate scheme to get
    you to check out our product, but here it's just pointing out some new source
    code we haven't talked about elsewhere.\",\n  link_url: \"https://github.com/superfly/init-snapshot\",\n
    \ link_text: \"Go read an init&nbsp;&nbsp;<span class='opacity-50'>&rarr;</span>\"\n}
    %>\n  \nIf you can talk to your organization’s 6PN network (say, over a WireGuard
    connection), you can log into your instances with Hallpass.\n\nThere’s only one
    interesting thing about how Hallpass works, and that’s authentication. The infrastructure
    in our production network doesn’t generally have direct access to our API or the
    databases that back it, nor, of course, do the instances themselves. This makes
    communicating configuration changes — like “what keys are allowed to log into
    instances” — a bit of a project. \n\nWe sidestepped that work by using SSH client
    certificates. Rather than propagating keys every time a user wants to log in from
    a new host, we establish a one-time root certificate for their organization; the
    public key for that root certificate is hosted in our private DNS, and Hallpass
    consults the DNS to resolve the certificate every time it gets a login attempt.
    Our API signs new certificates for users they can use to log in.\n\nYou probably
    have questions. \n\nFirst: certificates. Decades of [X.509 madness](https://medium.com/@sleevi_/path-building-vs-path-verifying-implementation-showdown-39a9272b2820)
    have probably left a bad taste in your mouth for “certificates”. I don’t blame
    you. But you should be using certificates for SSH, because they are great. SSH
    certificates aren’t X.509; they’re [OpenSSH’s own format](https://github.com/openssh/openssh-portable/blob/master/PROTOCOL.certkeys),
    and there isn’t much to them. Like all certificates, they have expiration dates,
    so you can create short-lived keys (which is [almost always what you want](https://engineering.fb.com/2016/09/12/security/scalable-and-secure-access-with-ssh/)).
    And, of course, they allow you to provision a single public key, on a whole bunch
    of servers, that can authorize an arbitrary number of private keys, without repeatedly
    updating those servers.\n\nNext: our API, and signing certificates. Welp! We’re
    pretty careful, but it’s basically as secure as your Fly access token is; it couldn’t,
    right now, be any more secure than that, because your access token allows you
    to deploy new versions of your app container. There’s a lot of ceremony involved
    with WebPKI X.509 CA signing; this isn’t that, and we’re pretty unceremonious.\n\nFinally,
    the DNS. This, I concede, seems batshit. But it is less batshit than it seems!
    Every host we run instances on runs a local version of our private DNS server
    (a small Rust program). eBPF code ensures that you can only talk to that DNS server
    (technically: you can only make queries of the private DNS API of the server;
    it’ll recurse for anybody) from the 6PN address of your server. The DNS server
    can — I know this is weird — reliably discern your organization identity from
    source IP addresses. So that’s what we do.\n\nAll this stuff is happening behind
    the scenes. What you, as a user, saw was the command `flyctl ssh issue -a`, which
    would grab a new certificate from our API and insert it into your local SSH agent,
    at which point SSH would just sort of work. It’s been neat.\n\nBut: things can
    always be neater.\n\n## Part The Second: User-Mode TCP/IP WireGuard\n  \nHere’s
    a problem: not everyone has WireGuard set up. They should; WireGuard is great,
    and it’s super useful for managing applications running on Fly. But, whatever,
    some people don’t. \n\nThey’d still like to SSH into their apps.\n\nAt first glance,
    not having WireGuard installed seems like a dealbreaker. The way WireGuard works
    is, you get a new network interface on your machine, either a kernel-mode WireGuard
    interface (on Linux) or a tunnel device with a userland WireGuard service attached
    (everywhere else). Without that network interface, you can’t talk over WireGuard.\n\nBut
    if you squint at WireGuard and tilt your head the right way, you can see that’s
    not technically true. You need operating system privileges to configure a new
    network interface. But you don’t need any privileges to send packets to `51820/udp`.
    You can run the whole WireGuard protocol as an unprivileged userland process –
    that’s how [wireguard-go](https://git.zx2c4.com/wireguard-go) works.\n\nThat gets
    you as far as handshaking WireGuard. But you’re still not talking over a WireGuard
    network, because you can’t just send random strings to the other end of a WireGuard
    connection; your peer expects TCP/IP packets. The native socket interface on your
    machine won’t help you establish a TCP connection over a random connected UDP
    socket.\n\nHow hard could it be to put together a tiny user-mode TCP, just for
    the purposes of doing pure-userland WireGuard networking, so people could SSH
    into instances on Fly without installing WireGuard? \n\nI made the mistake of
    musing about this on a Slack channel I share with Jason Donenfeld. I mused about
    it just before I went to bed. I woke up. Jason had implemented it, using gVisor,
    and made it part of the WireGuard library. \n\nThe trick here is gVisor. [We’ve
    written about it before](https://fly.io/blog/sandboxing-and-workload-isolation/).
    For those who aren’t familiar, gVisor is Linux, running in userland, reimplemented
    in Golang, as a container `runc` replacement. It is a pants-are-shirts bananas
    crazy project and, if you run it, I think you should brag about it, because wow
    is it a lot. And, buried in its guts is a complete TCP/IP implementation, written
    in Go, whose inputs and outputs are just `[]byte` buffers. \n\nSome tweets were
    twote, and, a couple hours later, I got a friendly email from [Ben Burkert](https://benburkert.com/).
    Ben had done some gVisor networking work elsewhere, was interested in what we
    were working on, and wanted to know if we wanted to collaborate. Sounded good
    to us! And, long story short, we now have an implementation of certificate-based
    SSH, running over gVisor user-mode TCP/IP, running over userland wireguard-go,
    built into `flyctl`.\n\nTo use it, you just use flyctl to ssh:\n\n```cmd\nflyctl
    ssh shell personal dogmatic-potato-342.internal\n```\n\nTo give you some perspective
    on how bananas this is: `dogmatic-potato-342.internal` is an internal DNS name,
    resolving only over private DNS on 6PN networks. It works here because, in `ssh
    shell` mode,\n`flyctl` is using gVisor's user-mode TCP/IP stack. But gVisor isn't
    providing\nthe DNS lookup code! That's just the Go standard library, which has
    been\nhornswoggled into using our bunko TCP/IP interface. \n\nFlyctl, by the way,
    [is open source](https://github.com/superfly/flyctl) (it has to be; people have
    to run it on their dev machines) so you can just go read the code. The [good code,
    under `pkg`](https://github.com/superfly/flyctl/tree/tqbf-ben-netstack/pkg), Ben
    wrote. The nightmare code, elsewhere, is me. User-mode IP WireGuard is astonishingly
    straightforward in Go. Like, if you’ve done low-level TCP/IP work before: you
    probably won’t believe how simple it is: the objects gVisor’s TCP stack code hand
    back plug directly into the standard library’s network code.\n\nHere, let's take
    a look:\n\n```go\ntunDev, gNet, err := netstack.CreateNetTUN(localIPs, []net.IP{dnsIP},
    mtu)\nif err != nil {\n    return nil, err\n}\n\n// ...\n\nwgDev := device.NewDevice(tunDev,
    device.NewLogger(cfg.LogLevel, \"(fly-ssh) \"))\n```\n\n`CreateNetTUN` is part
    of `wireguard-go`; it packages up gVisor and gives\nus (1) a sythetic tun device
    we can use to read and write raw packets to\ndrive WireGuard, and (2) [a net.Dialer
    that wraps gVisor](https://git.zx2c4.com/wireguard-go/tree/tun/netstack/tun.go#n209)
    that\nwe can drop into Go code to use that WireGuard network.\n\nAnd that's...
    it? I mean, here's us using it for DNS:\n\n```go\nresolv: &net.Resolver{\n    PreferGo:
    true,\n    Dial: func(ctx context.Context, network, address string) (net.Conn,
    error) {\n        return gNet.DialContext(ctx, network, net.JoinHostPort(dnsIP.String(),
    \"53\"))\n    },\n},\n```\n  \nIt's just normal Go networking code. Bananas.\n
    \ \n## So Obviously Everybody Should Be Doing This\n\nFor a couple hundred lines
    of code (not counting the entire user-mode Linux you’ll be pulling in from gVisor,
    HEY! Dependencies! What are you gonna do!) you can bring up a new, cryptographically
    authenticated network, any time you want to, in practically any program.\n\nGranted,
    it’s substantially slower than kernel TCP/IP. But how often does that really matter?
    And, in particular, how often does it matter for the random tasks that you’d ordinarily
    build a weird, clanking TLS tunnel for? When it starts to matter, you can just
    swap in real WireGuard. \n\nAt any rate, it solves a huge problem for us. It’s
    not just SSH; we also host Postgres databases, and it’ll be super handy to be
    able to bring up a `psql` shell anywhere, regardless of whether you can install
    macOS WireGuard right that second, with a simple command.\n  \nAlso [Ben Burkert
    is fantastic](https://benburkert.com/) and you should all bid his consulting rates
    up, because he got this project done in what seemed like a few hours.\n\n<%= partial
    \"shared/posts/cta\", locals: {\n  title: \"Also buy our cereal!\",\n  text: \"OK,
    we're still going to try to get you to check out our product, because it is really
    neat, and you can SSH into things as if they were servers from 2002.\",\n  link_url:
    \"https://fly.io/docs/speedrun/\",\n  link_text: \"Check out Fly&nbsp;&nbsp;<span
    class='opacity-50'>&rarr;</span>\"\n} %>\n  \nI would write more gracefully about
    him, and about Jason, and about gVisor, and about this whole project, except that
    Tailscale tweeted this morning about\na gVisor TCP/IP feature they're going to
    roll out, and I won't be beaten to the\npunch. Not this time, Tailscale! Not this
    time!\n\n  \n"
- :id: blog-we-are-hiring-elixir-developer-advocates
  :date: '2021-02-23'
  :category: blog
  :title: Elixir is <em>amazing</em> – coincidentally, we're hiring Elixir dev advocates
  :author: kurt
  :thumbnail: jobs-cover-01-thumbnail.png
  :alt:
  :link: blog/we-are-hiring-elixir-developer-advocates
  :path: blog/2021-02-23
  :body: "\n\n<div class=\"lead\">We're starting to think that Elixir might be the
    [Flyest](https://fly.io) ❤️ programming language.</div>\n\n[Fly](https://fly.io)
    is a hosting platform for applications. Our users give us containers; we transmute
    them into fleets of [Firecracker micro-VMs](https://fly.io/blog/sandboxing-and-workload-isolation/)
    and run them on a WireGuard-backed Anycast network that runs application code
    close to users, anywhere in the world. \n\nDon't get us wrong: at Fly, we write
    in Rust, Go, and a bit of Ruby. There's no Elixir anywhere in our stack today.
    But as we've been playing more and more with it, Elixir's pull has gotten stronger:
    it's just amazing for building clustered applications, and that's what we're building
    Fly for.\n\nLike a lot of people who arrived at Elixir by way of Ruby, we started
    with the wrong impression of Elixir. We thought \"oh, it's Erlang with Ruby syntax\",
    or worse, \"it's Ruby compiled down to the Erlang VM\". It is not either of these
    things. If you're like us, and take another look. Elixir is:\n\n* A functional
    language with immutable variables.\n* Powerful macros, so you can win at code
    golf.\n* Pattern-matched, with graceful error handling.\n\nAnd that's before you
    get to the library ecosystem, which obviously includes [Phoenix](https://www.phoenixframework.org/),
    which might be the most pleasant web application framework in the industry, without
    the sprawl and lack of rigor we deal with in other frameworks that have optimized
    for developer happiness. \n\nOne of the first things everybody points out about
    the Erlang ecosystem Elixir lives in is that everything is distributed. At  Fly,
    we've found ourselves with a flexible and easy [private networking](https://fly.io/blog/incoming-6pn-private-networks/)
    system\nthat makes it straightforward to securely express clusters of server instances.
    We think Elixir might be the best language to take advantage of that, because
    of things like:\n\n* A cluster configuration simple enough that a Phoenix app
    [is 8 lines](https://github.com/fly-apps/phoenix-liveview-cluster/blob/master/config/releases.exs#L23-L31).
    \n* Phoenix's ridicuously slick [LiveView](https://elixirschool.com/blog/phoenix-live-view/),
    which links server-rendered DOM updates up with browsers over WebSockets to make
    dynamic client applications without writing Javascript.\n* The Elixir community's
    embrace of Postgres; we do [high availability database clusters](https://fly.io/docs/reference/postgres/)
    on the same private network, spanning multiple regions.\n\nSo, anyways, we're
    hiring an Elixir developer advocate to help us figure this out. Do you enjoy experimenting
    with infrastructure so you can show devs how to get more from Elixir? We could
    be a fit for each other.\n\n---\n\n## The work\n\nOur developer outreach is content
    based, this is not a high travel job. We think the job will break down like this:\n\n*
    20% working on the [Fly.io](https://fly.io) UX for deploying and operating Elixir
    apps. This will mean working in Go and wrangling Docker – so Elixir folks don't
    have to.\n* 80% community engagement: examples, blog posts, and community outreach.
    Hopefully you like working with open source projects and showing other people
    how to get the most out of them.\n\nThat 80% covers a lot! If you are actively
    working on a relevant open source project, you could theoretically spend almost
    all that time developing your work, posting about it on our blog, and showing
    people in the community how to use it.\n\nSome of your content might be useful
    for talks. We want you to help us decide how valuable meetup and conference talks
    are. Later. When the pandemic is over.\n\nThis is our first attempt at focused
    developer relations. There is a lot to figure out. Your work will determine how
    we spend money on future marketing. If you're the type of person who wants to
    try a bunch of outreach to see what works, help build a dev relations organization
    from the bottom up, and even hire people to do the same work in other communities,
    you might _really_ like this job.\n\n---\n\n## Hiring process\n\nOur hiring process
    is project based. We want to let you try the job on, see your work, and pay you
    to do a little more work.\n\n1. Email jobs+elixir&#64;fly.io, tell us in a few
    sentences what you like about Elixir.\n2. Schedule a call with us so we can pitch
    the company to you and answer all your questions. We'll also tell you the bad
    parts.\n3. Sample project: we have a small Phoenix + LiveView demo we want you
    to improve. This should only take about 2 hours, but you can spend as much time
    on it as you want.\n\nWe rate the sample projects as objectively as possible.
    The best projects do what they're supposed to, use idiomatic Elixir, and are read-to-show.\n\nIn
    our experience, the hardest part of a sample project like this is just getting
    it done.\n\n###### Have experience? Jump ahead\nIf you have already built a public,
    open source Phoenix example that fits Fly well, let us know! We will evaluate
    that instead and save you a little time.\n\n### A larger, paid project\n\nIf we
    like your sample project work, we want to pay you to work on a larger project.
    We will offer a paid project ($1,000 flat rate) to about half the people who submit
    complete sample projects.\n\nThe goal here is to get a real, firm idea of what
    doing the job will be like. We'll get you setup with Slack access, a channel to
    work in, and future coworkers to collaborate with.\n\nWe want you to do four things
    for us:\n\n* Write \"Elixir community report\" describing where Fly fits well
    with a plan for community outreach.\n* Come up with a bunch of sample project
    ideas (like, 10). Single sentence descriptions of projects to demo Elixir on Fly.\n*
    Build a from-scratch Elixir app to demo.\n* Write a blog post about the demo app.\n\nWhen
    you're done, we'll even publish it if you're cool with that.\n\n###### Have experience?
    Skip steps\nIf you are active in the Elixir community, show us how you've worked
    with the community in the past. Then skip the \"community report\".\n\nLikewise,
    if you've built open source demos of Elixir before, we don't need a bunch of ideas.
    We'll still want you to build a demo for us with a corresponding blog post, though!\n\n---\n\n###
    Working at Fly.io\n\nWe are a remote-first company with people in Chicago, Montreal,
    Boulder, and London. We're hoping we can take field trips to visit each other
    soon, right now all our work happens over chat with periodic audio breaks.\n\nWe're
    not a family, but we do _have_ families and try to keep work projects from dominating
    our lives.\n\nBenefits are pretty typical for a company of our size – pretty-good
    healthcare for US based employees, flexible vacation time, and a hardware/phone
    allowance.\n\nBut, we're small! We all wear many hats and sometimes multiple hats
    at the same time. Come wear a hat for us.\n\n<%= partial \"shared/posts/cta\",
    locals: {\n  title: \"Level us up with your Elixir skills\",\n  text: \"To apply,
    send us an email telling us what you like about Elixir. Or just ask questions!\",\n
    \ link_url: \"mailto:jobs@fly.io\",\n  link_text: \"Get hired &nbsp;&nbsp;<span
    class='opacity-50'>&rarr;</span>\"\n} %>\n\n"
- :id: blog-persistent-storage-and-fast-remote-builds
  :date: '2021-02-15'
  :category: blog
  :title: Persistent Storage and Fast Remote Builds
  :author: jerome
  :thumbnail:
  :alt:
  :link: blog/persistent-storage-and-fast-remote-builds
  :path: blog/2021-02-15
  :body: |2


    If you’ve been keeping up with us at Fly, you may be picking up on a bit of a narrative with us.

    Fly launched, [in the long-long-ago](https://news.ycombinator.com/item?id=22616857), with a somewhat narrow use case. We took containers from our customers and transmogrified them into fleets of [Firecracker micro-VMs](https://fly.io/blog/sandboxing-and-workload-isolation/) connected to an anycast network that kept code running close to users. When we were talking to investors, we called this “edge computing”, and sold it as a way to speed up websites. And that works great.

    But it turns out though if you build a flexible platform for edge apps, you wind up with a pretty good way to run lots of [other kinds of applications](https://fly.io/blog/building-clusters-with-serf/). And so our users [have been doing that](https://community.fly.io/). And we’re not going to [complain!](https://community.fly.io/t/quick-dns-note/730/3?u=thomas) Instead, we’re working to make it easier to do that.

    ## The Storage Problem
    The biggest question mark for people thinking about hosting whole apps on Fly has always been storage.

    Until somewhat recently, micro-VMs on Fly were entirely ephemeral. A worker gets an order to run one. It builds a root filesystem for it from the container image. It runs the micro-VM, maybe for a few hours, maybe for a few weeks. The VM exits, the worker cleans it up, and that’s it.

    You can get a [surprising](https://fly.io/blog/run-apollo-graphql-close-to-your-users/) [amount](https://fly.io/blog/how-to-build-a-global-message-service-with-nats/) [done](https://fly.io/blog/maps-apps-and-tracks/) with a configuration like this and little else, but if you want to run an entire app, front-to-backend, you need more.

    Until now, the one really good answer we had was to [use external services for storage](https://fly.io/blog/using-heroku-postgres-from-a-fly-app/). You can, for instance, keep your data in something like RDS, and then use a [fast, secure WireGuard gateway](https://fly.io/blog/ipv6-wireguard-peering/) to bridge your Fly app instances [to RDS](https://github.com/fly-apps/rds-connector). Or you can set up S3 buckets in a bunch of regions. That works fine and for some apps might be optimal.

    But, obviously, developers want persistent storage. And we’re starting to roll that out.

    ## Fly Volumes
    You can, on Fly today, attach persistent storage volumes to Fly apps. It’s straightforward: you run `flyctl volumes create data --region ewr --size 25` to create a 25 gig volume named “data” in Newark. Then you tell your app about it in `fly.toml`:
    ```
    [[mounts]]
      source = "data"
      destination = "/data"
    ```

    Once connected to a volume, we’ll only run as many instances of your app as you have matching volumes. You can create lots of volumes, all named “data”, in different regions. We modeled this UX after Docker and tried to make it predictable for people who work with containers.

    ## What’s Happening With Volumes
    I’m going to share what’s happening under the hood when you use volumes, and then rattle off some of the limitations, because they are important.

    Fly runs on dedicated hardware in data centers around the world. On volume-eligible servers, that hardware includes large amounts of NVME storage. We use Linux LVM to carve out a thin pool. We track the amount of space in all those pools.

    [LVM2 thin pools](https://man7.org/linux/man-pages/man7/lvmthin.7.html) are interesting. Rather that preallocating disk space for new logical volumes, thin volumes allocate as data is written. They have in effect a current size and a cap. And the caps of all the volumes in a single thin pool can overlap; you can create more thin volume space than there is disk, like a bank creates money. This was a big deal in enterprise disk storage in the 2000s, is built into LVM now, and we… don’t use it for much. Our disks aren’t oversubscribed, but thin allocation makes administration and snapshot backups a little easier for us, and I just thought you’d like to know.

    We encrypt volumes for users, using standard Linux block device crypto ([XTS](https://sockpuppet.org/blog/2014/04/30/you-dont-want-xts/) with random keys). This doesn’t mean much to you, but we do it anyways, because somebody somewhere is going to talk to a [SOC 2 auditor](https://latacora.micro.blog/2020/03/12/the-soc-starting.html) who is going to want to know that every disk in the shop is “encrypted”, and, OK, these ones are too.

    The problem of course is that for us to manage your encrypted devices, we have to have the keys. Which means that anybody who manages to own up our devices also has those keys, and, as you’d hope, people who can’t own up our devices already don’t have access to your disk. About the only problem disk encryption solves for us is somebody forklifting our servers out of their secure data centers in some weird hardware heist.

    The problem is mitigated somewhat by our orchestration system. The control plane for Fly.io is [Hashicorp Nomad](https://www.nomadproject.io/), about which we will be writing more in the future. Nomad is in charge of knowing which piece of our hardware is running which applications. Because we have a fairly intense [TLS certificate feature](https://fly.io/blog/how-cdns-generate-certificates/), we also have a deployment of Hashicorp Vault, which is basically the the de facto standard on-prem secret storage system. Nomad knows how to talk to Vault, and we store drive secrets there; when a micro-VM is spun up on a piece of hardware, it gets a lease for the app’s associated secrets, and hardware not running that app doesn’t.

    This is all sort of built in to Nomad and Vault (the secret storage and leasing, that is; the disk management is all us) and it’s neat, and might matter a little bit in the future when we start doing volume migrations between servers, but really: serverside full disk encryption is pretty much rubber chicken security, and I’m just taking this opportunity to get that take out there.

    Anyways.

    When you create a volume with `flyctl`, you talk to our API server, which finds a compatible server with sufficient space. Once a match is found, we decrement the available space on the server and push a Consul update recording the existence of the new volume.

    Workers listen for Consul updates for volume changes and create/remove LVM thin volumes, with ext4 filesystems, as needed.

    When an instance of a volume-attached app is scheduled to deploy, our orchestrator treats the volume as a constraint when finding eligible servers. Deployments are routed to (and limited to) hosts with attachable volumes. Before booting up the micro-VM for those apps, we look up the logical volume we made, recreate its block device node in the jail Firecracker runs inside of, and set up mount points.

    And that’s pretty much all there is to it. You get a persistent filesystem to play with, which survives multiple reboots and deployments of your app. It’s performance-competitive (usually, a little faster) with EBS.

    ### There Are Implications To This
    The storage nerds in our audience are raising their hands in the air waiting to point this out: we haven’t talked about data resilience. And that’s because, right now, there isn’t much to talk about.

    Right now, for raw Fly volumes, resilience is your problem. There! I said it!

    This is not a storage model that’s appropriate to every application, and we want to be super clear about that. We think it makes sense in two major cases.

    The first is cluster storage. In the past few months, we’ve made it easy to boot up clusters of things that talk to each other, and to create ensembles of services that work together. So one way to use theoretically-unreliable storage is with a replicating database cluster, where you’re effectively backing up the data in real time. You do resilience at the app layer.

    The second, of course, is for the kinds of data where loss is an inconvenience and not a disaster; caches, metrics, and things you can back up on a slower-than-real-time cadence. This is a big class of applications! We really wanted disks for CDN workloads. Caches can go away, but minimizing cache churn and keeping “warm” caches around between deploys is handy. And big caches are great! Not having cache sizes bound by memory is great! If you’re building a CDN with Fly, go nuts with volumes.

    Attaching volumes to apps changes the way they scale. Once you attach a volume, you can only have as many instances of that app as you have volumes. Again, a lot of the time, volume storage will make sense for a cluster of storage/database servers you run alongside the rest of your app.

    Another subtle change is deployment. By default, Fly uses “canary” deployments: we spin up a new VM, make sure it passes health checks, and then tear down the old one. But volumes are provisioned 1:1 with instances; if you have 3 volumes for an app, we can’t make a 4th volume appear for the canary deploy to work. So apps with volumes do rolling deploys.

    _An aside: we don’t currently expose snapshots or volume migration in our API. But if you’re a storage nerd: we can generally do the things you’d expect to be able to do with logical volumes, like shipping LVM snapshots, and [you should totally reach out](https://community.fly.io/t/persistent-storage-and-fast-remote-builds/764) if you need something like that or want to know how it would work._

    This is a starting point for us, not the final destination. We’ve got more storage stuff coming. This is going to be a whole thing with us in 2021.

    <%= partial "shared/posts/cta", locals: {
      title: "Store data like on a real computer from 1992",
      text: "You're less than 10 minutes away from having any container you can build running globally, with attached persistent storage.",
      link_url: "https://fly.io/docs/speedrun/",
      link_text: "Try Fly for free&nbsp;&nbsp;<span class='opacity-50'>&rarr;</span>"
    } %>

    ## In Other News Remote Docker Builds Got Way Faster
    Hey, one of the things you can do now that we have volumes available is drastically improve build times.

    There’s two ways `flyctl` will get a Dockerfile deployed on Fly. The first is easy: we’ll talk to your local Docker instance, have it build the image, and then push that image to our registry. Until recently, this was the “good” way to do it, because it’s fast.

    The second way it can work is that `flyctl` can do a remote build. This works by talking to a remote Docker server instead of one running locally — which, incidentally, is how Docker works already if you’re on macOS and it’s running in a VM.

    For the last couple years, to do remote builds, we used AWS CodeBuild. CodeBuild is great and all, but if you think about what Fly is, it’s a little weird that we’re using it (our internal CI/CD system runs on our hardware). And our user experience with CodeBuild has been… not the best. Until now, remote build has been the “bad” way to do it; it’s slow enough that, on our own projects, when we saw “remote builds” kicking in, we stopped the deploy, started up our local Docker instance, and started over.

    No more! If you’re running recent `flyctl`, you may have noticed remote builds got a lot faster.

    That’s because when you run a remote build now, we first match the build to a “builder” app running in an instance in our network. The builder has attached storage to cache layers and manifests and exposes a Docker server, to which `flyctl` authenticates and requests a Docker build, which then gets pushed to our repository.

    Once you do a remote build, you’ll see them in your app list. Say hi! We’re not billing you for them.

    You can make a remote build happen if you `deploy` without a local Docker running, call `deploy --remote-only`, or are running on ARM.

    Getting this working was interesting, because it adds a new kind of instance to the mix. You don’t want a builder hanging around doing nothing when you’re not deploying. Our builder instances terminate after 10 minutes of inactivity, and are created/deployed automatically by `flyctl` (we added a GQL call to do this) as needed. Up until now, if your instance died, our orchestration’s job was to immediately restart it; builder jobs are “ephemeral” and aren’t restarted if they exit successfully.

    This is something we get a lot of requests for. People want permanent storage and short lived VMs that boot on demand and exit when their job is done. We’ve got that now and are figuring out the best way to surface it. If you’re interested, let us know.

    <hr>

    Do you have storage feature requests or questions about the new remote builders? We have a [community discussion](https://community.fly.io/t/persistent-storage-and-fast-remote-builds/764) just for you.
- :id: blog-the-tokio-1-x-upgrade
  :date: '2021-02-12'
  :category: blog
  :title: The Tokio Upgrade from 0.2 to 1.x
  :author: jerome
  :thumbnail:
  :alt:
  :link: blog/the-tokio-1-x-upgrade
  :path: blog/2021-02-12
  :body: "\n\nAt [Fly.io](https://fly.io), we run a Rust-based load-balancer which\nhandles
    almost all of our traffic. It stands on the shoulders of\n[Tokio](https://github.com/tokio-rs/tokio)
    and\n[Hyper](https://github.com/hyperium/hyper). When the Tokio team\nannounced
    0.3 and then 1.0, we figured we'd have to upgrade sooner\nthan later to access
    related crate upgrades. The Rust ecosystem moves\npretty fast and we wanted to
    be able to keep our software up to date.\n\nAfter a few weeks of patience, most
    of the ecosystem had already been\nupgraded. I believe this was a well-coordinated
    move from the Tokio team,\nmaking sure a good chunk of the ecosystem was going
    to be available\nwhen (or soon after) they released 1.0. We use [hyper](https://github.com/hyperium/hyper),\n[reqwest](https://github.com/seanmonstar/reqwest),\n[async-compression](https://github.com/Nemo157/async-compression),\n[tokio-io-timeout](https://github.com/sfackler/tokio-io-timeout),\n[tokio-tungstenite](https://github.com/snapview/tokio-tungstenite)
    and\n`sentry` in addition to `tokio`, so we waited until they were ready.\n\n##
    Differences with tokio 0.2\n\nHere are some notes about the Tokio upgrade. They're
    by no means exhaustive.\n\n### No implementations of `Stream`\n\nWith Tokio 1.0,
    the team has [decided to forego](https://docs.rs/tokio/1.0.0/tokio/stream/index.html)
    all `Stream`\nimplementations until its stabilization in the stdlib.\n\nIf you
    still want `Stream`'s, then you probably should try [`tokio-stream`](https://docs.rs/tokio-stream).
    It implements `Stream` for `TcpListener`, `Interval` and many more.\n\nIf not,
    most of the time, you can get around this change by looping:\n\n```rust\nlet listener
    = tokio::net::TcpListener::bind(\"[::]:0\").await?;\nloop {\n    let conn = listener.accept().await?;\n}\n```\n\n###
    Mutable access requirements relaxed\n\nIn the previous code snippet, you might've
    noticed I don't need a\n`mut`-able `TcpListener`. You can now use more of the
    tokio API\nwithout mutable access! That's a welcome change: it reduces much of\nthe
    locking required in our proxy.\n\n### New `AsyncRead` and `AsyncWrite` traits\n\nNotably,
    `AsyncRead::poll_read` and `AsyncWrite::poll_write` don't return the number of
    bytes read/written anymore, just `Result<()>`. A quick work around is this:\n\n```rust\nfn
    poll_read(\n    self: Pin<&mut Self>,\n    cx: &mut Context<'_>,\n    buf: &mut
    ReadBuf<'_>,\n) -> Poll<io::Result<()>> {\n  let before = buf.filled().len();\n
    \ // inner poll_read...\n  let nread = buf.filled().len() - before;\n  // ...\n}\n```\n\n###
    More pinning required\n\nFutures likes `Sleep` and `Shutdown` are now explicitly
    `!Unpin`. If you need to use them multiple times in a `tokio::select!` (like we
    do, all the time), then you'll need to either pin them on the heap with `Box::pin`
    or pin them on the stack with `tokio::pin!`. [The documentation explains it in
    more detail](https://docs.rs/tokio/1.2.0/tokio/time/struct.Sleep.html).\n\n```rust\nlet
    sleep = tokio::time::sleep(Duration::from_secs(1));\ntokio::pin!(sleep);\n\nloop
    {\n  tokio::select! {\n    _ = &mut sleep => {\n      println!(\"timed out after
    1 second!\");\n    },\n    _ = &mut fut => {\n      println!(\"future finished\");\n
    \   }\n  }\n}\n```\n\n\n\n### `runtime::Handle` doesn’t have `block_on` (yet)\n\nThere
    are discussions around adding back `Handle::block_on`, but for now, it's left
    unimplemented.\n\nWe needed it for the following pattern:\n\n```rust\nhandle.spawn_blocking(move
    || handle.block_on(future));\n```\n\nWe ended up getting around that restriction
    by using `Arc<Runtime>` since `block_on` doesn't require mutable access to `Runtime`
    anymore.\n\n```rust\nlet (tx, rx) = tokio::sync::oneshot();\nthread::spawn(||{\n
    \ let rt = Arc::new(tokio::runtime::Builder::new_current_thread().build().unwrap());\n\n
    \ // share your Runtime somehow, we send a single-threaded runtime via a oneshot.\n
    \ tx.send(rt.clone()).unwrap();\n\n  // Runtime::block_on does not require a `&mut
    self`\n  rt.block_on(async move {\n    // do your thing, asynchronously\n    //
    probably want to trigger the end of this runtime somehow (Signal, or other)\n
    \ });\n});\n\nlet rt = rx.await.unwrap();\n\n// now you can use spawn_blocking
    and block_on\nrt.spawn_blocking({\n  let rt = rt.clone();\n\tmove || {\n    rt.block_on(fut)\n
    \ }\n});\n```\n\n### `TcpListener::from_std` needs to be set to nonblocking\n\n...
    or else you'll be surprised that everything hangs when you start accepting connections.\n\nFor
    example, if you use `socket2` to fashion a `TcpListener` with a few more options,
    you'll need to use `tokio::net::TcpListener::from_std(std_listener)`. Before doing
    that, you'll want to `set_nonblocking(true)` on your std listener.\n\n```rust\nlet
    sock = socket2::Socket::new(\n    socket2::Domain::ipv4(),\n    socket2::Type::stream(),\n
    \   Some(socket2::Protocol::tcp()),\n).unwrap();\n\nsock.set_reuse_port(true).unwrap();\nsock.set_nonblocking(true).unwrap();
    // <---\nsock.bind(&sock_addr).unwrap();\nsock.listen(10240).unwrap();\n\ntokio::net::TcpListener::from_std(sock.into_tcp_listener()).unwrap();\n```\n\n<%=
    partial \"shared/posts/cta\", locals: {\n  title: \"Come for the Rust esoterica,
    stay for the hosting\",\n  text: \"It'll take less than 10 minutes to get almost
    any container you've got running globally on our Rust-powered anycast proxy network.\",\n
    \ link_url: \"https://fly.io/docs/speedrun/\",\n  link_text: \"Try Fly for free&nbsp;&nbsp;<span
    class='opacity-50'>&rarr;</span>\"\n} %>\n \n### Miscellaneous API changes\n\n-
    `tokio::time::delay_for` -> `tokio::time::sleep`\n- `tokio::sync::watch::Sender::broadcast`
    -> `tokio::sync::watch::Sender::send`\n- `Notify::notify` -> Notify::`notify_waiters`\n\n##
    Hyper WebSockets\n\nWebSockets upgrades are now on the whole request/response
    instance,\nnot just `Body` ( `body.on_upgrade()` vs\n`hyper::upgrade::on(req)`).
    [Here's an example](https://github.com/hyperium/hyper/blob/121c331/examples/upgrades.rs).\n\n[Hyper
    now relies on `OnUpgrade` being present on the `Response`'s or `Request`'s `Extensions`](https://github.com/hyperium/hyper/blob/f0ddb669328163001fd18a4a21109e95047848bf/src/upgrade.rs#L254-L288).
    We were previously replacing extensions with our own. We had to make sure we copied
    `OnUpgrade` into our new `Extensions`, if present, before overwriting.\n\n## Kudos
    to the Tokio team\n\nThis was a major version upgrade, but it wasn't hard at\nall.
    Especially with the sporadic question/answer sessions on the very\nhelpful [Tokio
    Discord](https://discord.gg/tokio) (hat tip to\n[Alice Ryhl](https://ryhl.io)
    in particular).\n\n"
- :id: blog-practical-smokescreen-sanitizing-your-outbound-web-requests
  :date: '2021-02-05'
  :category: blog
  :title: You should know about Server-Side Request Forgery
  :author: thomas
  :thumbnail:
  :alt:
  :link: blog/practical-smokescreen-sanitizing-your-outbound-web-requests
  :path: blog/2021-02-05
  :body: "\nThis is a post about the most dangerous vulnerability most web applications
    face, one step that we took at Fly to mitigate it, and how you can do the same.\n\n---\n\nServer-side
    request forgery (SSRF) is application security jargon for “attackers can get your
    app server to make HTTP requests on their behalf”. Compared to other high severity
    vulnerabilities like SQL injection, which allows attackers to take over your database,
    or filesystem access or remote code injection, SSRF doesn’t sound that scary.
    But it is, and you should be nervous about it.\n\nThe deceptive severity of SSRF
    is one of two factors that makes SSRF so insidious. The reason is simple: your
    app server is behind a security perimeter, and can usually reach things an ordinary
    Internet user can’t. Because HTTP is a relatively flexible protocol, and URLs
    are so expressive, attackers can often use SSRF to reach surprising places; in
    fact, leveraging HTTP SSRF to reach non-SSRF protocols has become a sport among
    security researchers. A meaty, complicated example of this is [Joshua Maddux’s
    TLS SSRF trick](https://portswigger.net/daily-swig/when-tls-hacks-you-security-friend-becomes-a-foe)
    from last August. Long story short: in serious applications, SSRF is usually a
    game-over vulnerability, meaning attackers can use it to gain full control over
    an application’s hosting environment.\n\nThe other factor that makes SSRF nerve-wracking
    is its prevalence. As an industry, we’ve managed to drastically reduce instances
    of vulnerabilities like SQL injection by updating our libraries and changing best
    practices; for instance, it would be weird to see a mainstream SQL library that
    didn’t use parameterized queries to keep attacker meta-characters out of query
    parsing. But applications of all shapes and sizes make server-side HTTP queries;
    in fact, if anything, that’s becoming more common as we adopt more and more web
    APIs. \n\nThere are two common patterns of SSRF vulnerabilities. The first, simplest,
    and most dangerous comprises features that allow users to provide URLs for the
    web server to call directly; for instance, your app might offer “web hooks” to
    call back to customer web servers. The second pattern involves features that incorporate
    user data into URLs. In both cases, an attacker will try to leverage whatever
    control you offer over URLs to trick your server into hitting unexpected URLs.\n\nFortunately,
    there’s a mitigation that frustrates attackers trying to exploit either pattern
    of SSRF vulnerabilities: SSRF proxies.\n\n## You should know about Smokescreen\nImagine
    if your application code didn’t have to be relentlessly vigilant about every URL
    it reached out to, and could instead assume that a basic security control existed
    to make sure that no server-side HTTP query would be able to touch internal resources?
    It’s easy if you try! What you want is to run your server-side HTTP through a
    proxy.\n\nWe've been putting [Smokescreen](https://github.com/stripe/smokescreen)
    to work at Fly, and it's so useful, we thought we should share. Smokescreen is
    an egress proxy that was built at Stripe to help manage outgoing connections in
    a sensible and safe way.\n\nSmokescreen’s job is to make sure your outgoing requests
    are sane, sensible and safe. SmokeScreen was created by Stripe to ensure that
    they knew where all their outgoing requests were going. Specifically, it makes
    sure that the IP address requested is a publicly routed IP and that means checking
    any request isn't destined for `10.0.0.0/8`, `172.16.0.0/12`, `192.168.0.0/16`,
    or `fc00::/7` (the IPv6 \"ULA\" space, which includes [Fly's 6PN private addresses](https://fly.io/blog/incoming-6pn-private-networks/).
    \n\n## Out of the box\n\nThere's more SmokeScreen can do, but before we get to
    that, let's talk about how Smokescreen determines who you are. By default, Smokescreen
    uses the client certificate from a TLS connection, extracts the common name of
    the certificate and uses that as the role. There is another mechanism documented
    for non-TLS connections using a header but doesn't seem to be actually wired up
    Smokescreen (probably because it's way too simple to present to be another system).
    So you'll have to use TLS CA certs for all the systems connecting through Smokescreen
    and that is an administrative pain.\n\n## Getting Basic\n\nWe wanted Smokescreen
    to be simpler to enable, and with Fly we have the advantage of supporting Secrets
    for all applications. Rather than repurposing TLS CAs to provide a name, we can
    store a secret with the Smokescreen proxy and with the app that sends requests
    to the outside world. That secret? For the example, we've gone with a `PROXY_PASSWORD`
    that we can distribute to all inside the Fly network.\n\nHere's the [Fly Github
    repository for the Fly Smokescreen](https://github.com/fly-examples/smokescreen).\n\n##
    I'm on the list...\n\nIn all cases, what Smokescreen does is turn the identity
    of an incoming request into a role. That role is then looked up in the `acl.yaml`
    file. Here's the Fly example ACL:\n\n```yaml\n---\nversion: v1\nservices:\n  -
    name: authed\n    project: users\n    action: report\n\n\ndefault:\n    project:
    other\n    action: enforce\n```\n\nWe've gone super simple on the roles here.
    There's one and that's `authed`. You're either `authed` or you fall through to
    default. The project field is there to make logging more meaningful by associating
    roles with projects. \n\nThe control of what happens with requests comes from
    the `action` field; this has three settings: `open` lets all traffic through,
    `report` lets all traffic through but logs the request if it's not on the list,
    and `enforce` only lets through traffic on the list. The list in this example
    isn't there, so `report` logs all requests and `enforce` blocks all requests.
    \n\nAdding `allowed-domains` and a list of domains lets you fine tune these options.
    For a general purpose block-or-log egress proxy, this example is enough. Smokescreen
    has more [ACL control options](https://github.com/stripe/smokescreen#acls), including
    global allow and deny lists if you want to maintain simple but specfic rules but
    want to block a long list of sites.\n\n## Smokescreen inside\n\nIf you are interested
    in how this modified Smokescreen works, look in the [main.go](https://github.com/fly-examples/smokescreen/blob/master/main.go)
    file. This is where the smokescreen code is loaded as a Go package. The program
    creates a new configuration for Smokescreen with a alternative `RoleFromRequest`
    function. It's this function that extracts the `Proxy-Authorization` password
    and checks it against the `PROXY_PASSWORD` environment variable. If it passes
    that test, it returns `authed` as a role. Otherwise, it returns an empty string,
    denoting no role. It's this function that you may want to customize to create
    your own mappings from username and password combinations to Smokescreen roles.\n\n##
    Deploy now\n\n### Fly\n\nThis is where we show how to deploy on Fly first:\n\n```\nfly
    init mysmokescreen --import source.fly.toml --org personal\nfly set secret PROXY_PASSWORD=\"somesecret\"\nfly
    deploy\n```\n\nAnd that's it for Fly; there'll be a mysmokescreen app set up with
    Fly's internal private networking DNS (and external DNS if we needed that, which
    we don't here), and it'll be up and running. Turn on your [Fly 6PN (Private Networking)
    VPN](https://fly.io/docs/reference/private-networking/#private-network-vpn) and
    test it with:\n\n```\ncurl -U anyname:somesecret -x mysmokescreen.internal:4750
    https://fly.io\n```\n\nAnd that will return the Fly homepage to you. Run `fly
    logs` and you'll see entries for the opening and closing of the proxy's connection
    to fly.io. What's neat with the Fly deployment is that with just two commands
    you can deploy the same application globally.\n\n### Docker - locally\n\nIf you're
    on another platform, you should be able to reuse the Dockerfile. Running locally,
    you just need to do:\n\n```\ndocker build -t smokescreen .\ndocker run -p 4750:4750
    --env PROXY_PASSWORD=somesecret smokescreen\n```\n\nAnd to test, in another session,
    do:\n\n```\ncurl -U anyname:somesecret -x localhost:4750 https://fly.io\n```\n\nYou'll
    see the log output appearing in the session where you did the `docker run`. We
    leave it as an exercise to readers to deploy the application to their own Cloud.\n\n##
    Using a Proxy from an app\n\nTo wrap up this article, we present two code examples,
    one in Go and one in Node, that take from the environment a PROXY_URL pointed
    at our smokescreen and a PROXY_PASSWORD for that smokescreen and issue a simple
    GET for an https: URL. \n\nOn Fly, the PROXY_URL can be as simple as \"http://mysmokescreen.internal:4750/\".
    Fly's 6PN network automatically maps deployed applications' names and instances
    into the the .internal TLD for DNS. On other platforms, you'll have to configure
    a hostname for your smokescreen and make sure you change it everywhere if you
    move your proxy.\n\n### Calling through an authenticated Proxy from Go\n\nThis
    example uses only the system libraries. There are no extra modules needed.\n\n```go\npackage
    main\n\nimport (\n\t\"encoding/base64\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"log\"\n\t\"net/http\"\n\t\"net/url\"\n\t\"os\"\n)\n\nfunc
    main() {\n\t// Set the proxy's URL\n\tproxy, ok := os.LookupEnv(\"PROXY_URL\")\n\tif
    !ok {\n\t\tlog.Fatal(\"Set PROXY_URL environment variable\")\n\t}\n\n\t// And
    parse it.\n\tproxyURL, err := url.Parse(proxy)\n\tif err != nil {\n\t\tlog.Fatal(\"The
    proxyURL is unparsable: \" + proxy)\n\t}\n\n\tproxyPASS, ok := os.LookupEnv(\"PROXY_PASSWORD\")\n\n\tif
    !ok {\n\t\tlog.Fatal(\"Set PROXY_PASSWORD environment variable\")\n\t}\n\n\t//
    Get you a transport that understands Proxies and Proxy authentication\n\ttransport
    := &http.Transport{Proxy: http.ProxyURL(proxyURL)}\n\n\t// Create a usename:password
    string\n\tauth := \"anyname:\" + proxyPASS\n\n\t// Base64 that string with \"Basic
    \" prepended to it\n\tbasicAuth := \"Basic \" + base64.StdEncoding.EncodeToString([]byte(auth))\n\n\t//
    Put a header into the proxy connect header\n\ttransport.ProxyConnectHeader = http.Header{}\n\n\t//
    And then add in the Proxy-Authorization header with our auth string\n\ttransport.ProxyConnectHeader.Add(\"Proxy-Authorization\",
    basicAuth)\n\n\t// Now we are ready to get pages, just create HTTP clients which
    use the\n\t// Proxy transport.\n\n\tclient := &http.Client{Transport: transport}\n\n\trawURL
    := \"https://fly.io\"\n\n\trequest, err := http.NewRequest(\"GET\", rawURL, nil)\n\tif
    err != nil {\n\t\tfmt.Println(err)\n\t\treturn\n\t}\n\n\tresponse, err := client.Do(request)\n\tif
    err != nil {\n\t\tfmt.Println(err)\n\t\treturn\n\t}\n\tfmt.Println(\"Read ok\")\n\n\tif
    response.StatusCode != 200 {\n\t\tfmt.Println(response.Status)\n\t}\n\n\tbs, err
    := ioutil.ReadAll(response.Body)\n\n\tfmt.Println(string(bs))\n\n}\n```\n\n###
    Calling through an authenticated Proxy from Node.js\n\nThis example uses the [https-proxy-agent](https://www.npmjs.com/package/https-proxy-agent)
    package.\n\n```javascript\nvar url = require('url');\nvar https = require('https');\nvar
    HttpsProxyAgent = require('https-proxy-agent');\n\n// Create a URL for our proxy
    from the env var\nvar proxyOpts = new URL(process.env.PROXY_URL);\n\n// Get a
    password from the environment too\nvar proxyPass= process.env.PROXY_PASSWORD;\n\n//
    Inject a Proxy-Authorization header into the proxy using the password\nproxyOpts.headers
    = {\n  'Proxy-Authorization': 'Basic ' + (`anyname:${proxyPass}`).toString('base64')\n};\n\n//
    Create an HTTPS Proxy Agent with our accumulated options\nvar agent = new HttpsProxyAgent(proxyOpts);\n\nvar
    options = new URL(\"https://fly.io\");\n\noptions.agent = agent;\n\nhttps.get(options,
    function (res) {\n  console.log('\"response\" event!', res.headers);\n  res.pipe(process.stdout);\n});\n```\n\n\n##
    Smokescreen summarized\n\nWe've shown you examples of setting up a custom Smokescreen
    with password authentication. You'll find all the code for setting that up at
    the [Fly Github repository for this Smokescreen](https://github.com/fly-examples/smokescreen).
    Have fun sanitizing your outgoing web requests."
- :id: blog-the-january-2021-fly-changelog
  :date: '2021-01-26'
  :category: blog
  :title: The January 2021 Fly ChangeLog
  :author: dj
  :thumbnail:
  :alt:
  :link: blog/the-january-2021-fly-changelog
  :path: blog/2021-01-26
  :body: "\n\n<div class=\"lead\">\n\nPrivate networking between Fly Apps, WireGuard
    VPNs, updates to scaling, and backend improvements. This is what's new at Fly
    in this month's ChangeLog.\n\n</div>\n\nIf you missed 2020, lucky you, to get
    you all caught up, here's our [Previously On Fly](/blog/fly-in-2020/) segment.
    Caught up? Good.\n\nAnd now here is the Fly ChangeLog, covering the end of December
    2020 to January 2021. \n\n## 6PN Networking and IPv6 WireGuard VPNs\n\nLet your
    apps talk amongst themselves with 6PN networking. [6PN private networks](/blog/incoming-6pn-private-networks/)
    build on top of Fly's WireGuard mesh networking and make it easy to look up other
    apps in your organizations, apps in regions, or globally view all instances of
    an app. \n\n* There are no new commands to learn, just a small adjustment to your
    `fly.toml` file to enable 6PN DNS resolution for your apps. Read more in the [private
    networking](/docs/reference/private-networking/) section of the docs.\n\nAnd if
    that wasn't enough, [IPv6 Wireguard Peering](/blog/ipv6-wireguard-peering/) lets
    you create VPN tunnels into your organization so you can safely work from inside
    the Fly infrastructure when developing or use it to securely bridge outside networks
    and applications into Fly. \n\n* There's a new command for `flyctl` - `wireguard`
    lets you create, list, and remove tunnels. You can learn more in the [private
    network VPN](/docs/reference/private-networking/#private-network-vpn) section
    of the docs.\n\n## Count Scaling\n\nWe've switched our default scaling model to
    a more predictable model, scaling by the count of instances, with autoscaling
    getting its own command.\n\n* New `flyctl` command - `autoscale` controls autoscaling
    and `scale` controls count-scaling. Read more about the changes in our updated
    [scaling and autoscaling](/docs/reference/scaling/) section in the docs.\n\n##
    Backend Improvements\n\nIt's not all new commands and features. We've been tuning
    up the Fly infrastructure too. We recently switched to `containerd` for pulling
    your images in, removing the entire `building roofs` stage, and letting us reuse
    more layers. The result is shorter subsequent boot times for larger images and
    a faster Fly for all.\n\nAnother backend change means that events in an app's
    lifecycle—including being configured, starting up, and shutting down—are now being
    logged in the app's own logs. This change means more information in logs about
    what's happening with an app.\n\nAnd [volumes](/docs/reference/volumes/), the
    persistent storage option on Fly, are now being lazily initialized and encrypted
    (by default) so that configuring them is faster.\n\n\n<p class=\"callout\">\n\nThis
    is the Fly ChangeLog, where we list all significant changes to the Fly platform,
    tooling, and websites. You can also use the RSS feed of just changelog posts available
    on [fly.io/changelog.xml](/changelog.xml) or consult our dedicated [ChangeLog](/changelog/)
    page with all the recent updates.\n\n</p>\n\n<!-- start -->\n\n## 22nd January\n\n**flyctl:
    Version [0.0.162 released](https://github.com/superfly/flyctl/releases/tag/v0.0.162)**\n-
    ++ Fix `darwin getCPUinfo` warning on M1 Macs\n- ~~ More feature foundation work\n\n##
    15th January\n\n**flyctl: Version [0.0.161 released](https://github.com/superfly/flyctl/releases/tag/v0.0.161)**\n-
    ~~ Error message enhancement removed\n\n## 14th January\n\n**flyctl: Version [0.0.160
    released](https://github.com/superfly/flyctl/releases/tag/v0.0.160)**\n- ~~ Feature
    foundation work\n\n**flyctl: Version [0.0.159 released](https://github.com/superfly/flyctl/releases/tag/v0.0.159)**\n-
    ++ Add \"scale memory\" command\n- ++ Updated Cobra - zsh command completion enabled\n-
    ++ Go, Ruby and Deno builtins use latest version unless overriden in settings\n-
    ~~ Fix bug in `orgs list`\n\n## 6th January\n\n**flyctl: Version [0.0.158 released](https://github.com/superfly/flyctl/releases/tag/v0.0.158)**\n-
    ~~ fix Autoscale messaging\n- ~~ Fix `GetVolumes` and encrypted setting\n- ~~
    Make `volumes delete` not report \"volume destroyed\"\n- ~~ Fix volumes help,
    correct default size (10GB, not 5GB)\n\n\n## 23rd December\n\n**flyctl: Version
    [0.0.157 released](https://github.com/superfly/flyctl/releases/tag/v0.0.157)**\n-
    ++ Wireguard config now includes PersistentKeepAlive\n- ++ `--encrypted` flag
    added for volumes create command\n- ~~ `volumes show` support for new flag added\n-
    ++ `autoscaling disable` command added\n- ++ `scale count` command added\n\n##
    17th December\n\n**flyctl: Version [0.0.156 released](https://github.com/superfly/flyctl/releases/tag/v0.0.156)**\n-
    ++ `fly platform regions` shows a checkmark against regions with gateways\n- ~~
    Fix error on Windows installer when upgrading\n- -- remove private ops from the
    status display (use `fly ips private`)\n- ~~ Restrict WireGuard peer names to
    alphanumeric\n\n## 15th December \n\n**flyctl: Version [0.0.155 released](https://github.com/superfly/flyctl/releases/tag/v0.0.155)**\n-
    ++ Unhide Wireguard commands for general use\n- ~~ Updated scale help\n- ++ On
    failed deployment, offer URL for troubleshooting guide\n\n## 10th December\n\n**flyctl:
    Version [0.0.154 released](https://github.com/superfly/flyctl/releases/tag/v0.0.154)**\n-
    ++ New VM Sizes put in place\n- ++ `fly ips private` displays private IP addresses
    of app.\n\n## 3rd December\n\n**flyctl: Version [0.0.153 released](https://github.com/superfly/flyctl/releases/tag/v0.0.153)**\n-
    ++ Enable wireguard commands \n- ++ show private IPs on status output\n- ~~ Fixes
    for init import\n- -- Remove prices from vm-sizes output (reference now the pricing
    page)\n"
- :id: blog-fly-in-2020
  :date: '2021-01-18'
  :category: blog
  :title: Fly In 2020 - A year in features (and articles)
  :author: dj
  :thumbnail:
  :alt:
  :link: blog/fly-in-2020
  :path: blog/2021-01-18
  :body: "\n\n**TL:DR;** Launch, Turboku, Custom Domains, Scaling, One-Click Apps,
    Persistent Storage, Private Networking and Wireguard VPNs.\n\nFly's had an unprecedented
    year, along with everyone else. For us, it was bringing our Application Platform
    to the world, and much more. Here's a run down of the 12 months...\n\n**January**
    - [Fly Global Application Platform launches](https://fly.io/blog/command-lines-flyctl-and-fly/)
    and we declare the command line is king, especially for driving. Search parties
    are sent out to look for a snappier name.\n\n**February** - We take Fly to Heroku
    apps and make them faster with [Turboku](https://fly.io/blog/turboku/). It gets
    Heroku apps right up to the edge of a global network, boosting TLS response times.
    Also that month, we roll out [Cloud Native Buildpack](https://fly.io/blog/simpler-fly-deployments-nodejs-rails-golang-java/)
    support.\n\n**March** - [Custom Domains](https://fly.io/blog/how-to-custom-domains-with-fly/)
    arrive, complete with a GraphQL API for managing them. We also introduce new [Scaling](https://fly.io/blog/scaling-fly-for-all/)
    controls for Fly applications and the [Datasette starts supporting Fly](https://fly.io/blog/making-datasets-fly-with-datasette-and-fly/).\n\n**April**
    - [One-Click Imaginary](https://fly.io/blog/imaginary-on-fly-just-one-click/)
    launches on Fly, letting you click and deploy an image processing service. It's
    the first of a range of \"one-click\" services on Fly. Also in April, as [Deno](https://deno.land)
    heads to version 1.0, Fly adds support for [Deno](https://fly.io/blog/deno-on-fly-using-buildpacks/).\n\n**May**
    - [Pausing and resuming](https://fly.io/blog/fly-now-with-power-pause/) Fly apps
    is now an option after we [tuned up the Scale system](https://fly.io/blog/updating-scale/).\n\n**June**
    - Thomas writes about [How CDNs generate Certificates](https://fly.io/blog/how-cdns-generate-certificates/)
    and [Flyctl starts speaking JSON](https://fly.io/blog/flyctl-meets-json/) for
    easier automation.\n\n**July** - Static Websites with small servers are now a
    breeze with [GoStatic and Fly.io](https://fly.io/blog/serve-small-with-fly-io-and-gostatic/),
    and Thomas explains how [sandboxes and workloads can be isolated](https://fly.io/blog/sandboxing-and-workload-isolation/)
    and how we do it at Fly (spoiler: Firecracker VMs).\n\n**August** - We add simple
    [\"builtin\" builders to Flyctl](https://fly.io/blog/flyctl-builtins-the-fly-changelog-for-august/)
    for Node, Ruby, Deno, Go, and static websites.\n\n**September** - Heroku databases
    are in this month, [How to Turboku with them](https://fly.io/blog/migrating-heroku-database-apps-to-fly/)
    and [How to just use them](https://fly.io/blog/using-heroku-postgres-from-a-fly-app/).
    And [Flyctl gets easier to update](https://fly.io/blog/the-september-fly-changelog-new-names-and-easier-updates/).\n\n**October**
    - It's a busy month:\n\n* We [preview persistent volumes on Fly](https://fly.io/blog/the-october-fly-changelog-preview-disks-and-dns-and-better-builtins/).\n*
    A [New Logging System](https://fly.io/blog/fly-behind-the-scenes-fresh-logging/)
    goes live on Fly and we talk about what's been done to make that happen. \n* Thomas
    gives a complete history of [BPF, XDP, Packet Filters and UDP](https://fly.io/blog/bpf-xdp-packet-filters-and-udp/)
    as a prelude to features to come.\n* We also [tracked a marathon runner](https://fly.io/blog/maps-apps-and-tracks/)
    and stuffed a [Pi-Hole](https://fly.io/blog/stuff-your-pi-hole-from-anywhere/).\n\n**November**
    - A month of examples as we show how to deploy [Redis, MinIO, Gogs, and MQTT](https://fly.io/blog/appkata-example-apps-on-fly/)
    along with [Node-Red and Redis with TLS](https://fly.io/blog/more-appkata-examples-to-try-on-fly/).
    Nearly all of the examples make use of the persistent volumes of Fly.\n\n**December**
    - It's time for a festive feature-filled end of the year:\n\n* [6PN Private Networking](https://fly.io/blog/incoming-6pn-private-networks/)
    arrives on Fly letting apps communicate between each other securely. \n* [WireGuard
    peering](https://fly.io/blog/ipv6-wireguard-peering/) is launched, letting users
    securely connect into their organization's 6PN networks.\n* We added [new graceful
    shutdown options for VMs](https://fly.io/blog/graceful-vm-exits-some-dials/).\n*
    And we [revised our range of VM sizes](https://fly.io/blog/new-vms-more-ram-extra-cpu-and-a-dollar-menu/)
    to better fit everyone.\n\nAnd that was 2020. 2021? Ssssh! Spoilers! Sign up to
    [community.fly.io](https://community.fly.io) to be among the first to know.\n\n"
- :id: blog-how-to-build-a-global-message-service-with-nats
  :date: '2021-01-14'
  :category: blog
  :title: How to build a global message service with NATS
  :author: dj
  :thumbnail:
  :alt:
  :link: blog/how-to-build-a-global-message-service-with-nats
  :path: blog/2021-01-14
  :body: |2+


    <div class="lead">

    We're looking at new example applications that show you how to make the best use of the latest Fly features with the applications you want to run.

    </div>

    There are actually two examples that make up this example. One example is a Websocket-based chat that relays its messages through that NATS cluster. The other part is that NATS cluster, configured as a global messaging cluster that you can install inside your Fly organization's network.

    * [Chat: A 6PN Example with NATS](https://fly.io/docs/app-guides/6pndemochat/) builds on a previous Fly example messaging application using WebSockets to talk to users, and using NATS powered messaging to run in any region.

    * [Global NATS Cluster](https://fly.io/docs/app-guides/nats-cluster/) shows to configure a three-node cluster of [NATS](https://nats.io/) servers that automatically discover and communicate with each other thanks to Fly's 6PN networking.

    Both examples use Fly's 6PN and its DNS features like [`.internal`](https://fly.io/docs/reference/services/#internal-addresses) addresses that make locating running instances of apps in an organization or region as simple as a DNS lookup.

    <div class="callout">

    Find more Fly examples and demonstration applications in [Flight Plans - Guides and Examples](/docs/guides/).

    </div>



- :id: blog-building-clusters-with-serf
  :date: '2021-01-14'
  :category: blog
  :title: Building clusters with serf, my new favorite thing
  :author: thomas
  :thumbnail:
  :alt:
  :link: blog/building-clusters-with-serf
  :path: blog/2021-01-14
  :body: "\n\nAssume for a second we’d like to see what happens when a web page loads
    in a browser in Singapore. Easy enough; Fly.io will take a container image you
    throw at it, transform it into a Firecracker VM, and run it in Singapore.\n\n##
    Getting Up And Running\nWe want a container that loads a web page in a browser;
    that sounds like a job for Headless Chromium. [Here’s a Dockerfile](https://gist.github.com/tqbf/de134d379d03d857d2a38031e6b9aee2);
    actually, don’t bother, it’s just one of those Dockerfiles that installs the right
    apt-get packages and then downloads the right Chromium distribution; the entrypoint
    runs Chromium with the right arguments.\n\nDeploy the app on Fly:\n\n```bash\n$
    flyctl apps create # accept the defaults\n$ flyctl regions set sin\n$ flyctl deploy\n```\n\nAnd
    this will pretty much just work. Say we named the Fly app `ichabod-chrome`. When
    `flyctl deploy` finishes, our image will be running as VM somewhere near Singapore,
    and reachable from around the world as `ichabod-chrome.fly.dev`.  You could drive
    the Chrome instance running in Singapore using the [Chrome debug protocol](https://chromedevtools.github.io/devtools-protocol/),
    which has implementations in a ton of languages; for instance, if we want to screenshot
    a page in Ruby, we could just install the [ferrum](https://github.com/rubycdp/ferrum)
    gem, and then:\n\n```ruby\nrequire 'ferrum'\nc = Ferrum::Browser.new :base_url
    => \"http://ichabod-chrome.fly.dev:9222/\"\nc.goto \"https://news.ycombinator.com\"\nc.screenshot
    :path => \"hn.png\"\n```\n\nSuper boring! Neat that it works, though! But there’s,
    like, an obvious problem here: Chrome Debug Protocol isn’t authenticated, so we’re
    just kind of hanging out on the Internet hoping nobody does something dumb with
    the browser proxy we’ve created on this public URL. \n\nLet’s fix that. We’ll
    run our Chrome as a [6PN application](https://fly.io/blog/incoming-6pn-private-networks/),
    and talk to it over WireGuard.  We crack open the `fly.toml` that `flyctl` generated
    for us, and add:\n\n```toml\n[experimental]\n   private_network = true\n```\n\nWe
    also yank out the whole `[[services]]`  section, because we’re not exposing any
    public services to health-check anymore. And we change our [entrypoint](https://github.com/fly-apps/ichabod/blob/main/entrypoint.sh)
    to bind to its private IPv6 address.\n\nA `flyctl deploy` run loads our “new”
    application, which speaks CDP only over private IPv6 addresses. But: now we can’t
    talk to it! We’re not on the private IPv6 network.\n\nThat’s easy to fix: [install
    WireGuard](https://www.wireguard.com/install/) (it runs everywhere). Then run
    `flyctl wireguard create`, which will generate a WireGuard configuration for us
    that we can load in our client. Hit the connect button, and we’re good to go again,
    this time with a cryptographically secure channel to run CDP over. On our internal
    DNS, which is included in the WireGuard configuration we generate, our app is
    now reachable at `ichabod-chrome.internal`. \n\n## Clusters and DNS\nLet’s say
    we want a bunch of Headless Chromiums, in a bunch of different locations. Maybe
    we want to screenshot the CNN front page from different countries, or run [Lighthouse](https://github.com/GoogleChrome/lighthouse)
    tests from around the world. I’m not here to judge your life decisions.\n\nGetting
    those Chromium instances up and running is mercifully boring. Let’s say we want
    roughly to run in Singapore, Sydney, Paris, and Chile:\n\n```bash\nflyctl regions
    add sin syd cdg scl\nflyctl scale count 4\n```\n\n… and that’s it; Fly will figure
    out how to satisfy those constraints and deploy appropriately (we’re asking now
    for 4 instances, and Fly will try to spread those instances around as many data
    centers as it can).\n\nNow, we want to drive these new instances, and do so selectively.
    To do that, we have to be able to find them. We can use the DNS to do that:\n\n```bash\n$
    dig txt regions.ichabod-chrome.internal +short\nsin,syd,cdg,scl\n$ dig aaaa syd.ichabod-chrome.internal
    +short\nfdaa:0:18:a7b:aa4:3a64:c0ba:2\n```\n\nAnd this pretty much works, and
    you can probably get a long ways just using DNS for instance discovery, especially
    if your cluster is simple.\n\nBut for me, for this app, this is kind of an annoying
    place to leave off. I could pick a bunch of nits, but the big one is that there
    isn’t a good way to automatically get updates when the DNS changes. I can get
    a pretty good picture of the world when an instance starts up, but I have to go
    through contortions to update that picture as time ticks on.\n\nWhen we were putting
    DNS together at Fly, we had the same thoughts. And yet we did nothing about them!
    We quickly concluded that if people wanted “interesting” service discovery, they
    could B.Y.O.  \n\nLet’s see how that plays out with this cluster. I’m going set
    up [HashiCorp Serf](https://www.serf.io/) to make all the components of this cluster
    aware of each other.\n\n## Running HashiCorp Serf\nThey do somewhat similar things,
    but Serf gets less attention than its HashiCorp sibling Consul. Which is a shame,
    because Serf is a simpler, more approachable system that does 80% of what a lot
    of people use Consul for.\n\nA reasonable mental model of Consul is that it’s
    a distributed system that solves 3 problems:\n1. Serving an up-to-date catalog
    of available services\n2. Storing configuration for those services\n3. Removing
    unhealthy services from the catalog\n\nUnlike Consul, Serf handles just one of
    these problems, #1. In fact, Consul uses Serf under the hood to solve that problem
    for itself. But Consul is much more complicated to set up. Serf runs without leaders
    (cluster members come and go, and everybody just figures things out) and no storage
    requirements.\n\nSerf is easy. In a conventional configuration — one where we
    run Serf as a program and not as a library embedded into our application — every
    node in the cluster runs a Serf agent, which, after installing Serf, is just the
    `serf agent` command. All the configuration can get passed in command line arguments:\n\n```bash\n
    \ name=\"${FLY_APP_NAME}-${FLY_REGION}-$(hostname)\"\n  paddr=$(grep fly-local-6pn
    /etc/hosts | cut -f 1)\n\n  serf agent \t\t\t\t   \t    \\\n\t-node=\"$name\"
    \t\t\t\t    \\ \n  -profile=wan\t\t\t\t    \\\n\t-bind=\"[$paddr]:7777\"\t\t\t\\\n\t-tag
    role=\"${FLY_APP_NAME}\"\t\t\\\n\t-tag region=\"${FLY_REGION}\"\t\t\n```\n\nThere’s
    not much to it. We give every node a unique name. Serf by default assumes we’re
    running on a LAN and sets timers accordingly; we switch that to WAN mode. Importantly,
    we bind Serf to our 6PN private address. Then we set some tags, for our convenience
    later when selecting members.\n\nTo help Serf find other members in the cluster
    and converge on the complete picture of its membership, can make some quick introductions:\n\n```bash\n
    \   dig aaaa \"${FLY_APP_NAME}.internal\" +short | while read raddr ; do\n      if
    [ \"$raddr\" != \"$paddr\" ]; then\n        serf join \"$raddr\"\n      fi\n    done\n```\n\nHere
    we’re just dumping the current snapshot of the cluster from DNS and using `serf
    join` to introduce those members. Now, if we have nodes Alice, Bob, and Chuck,
    and Alice introduces herself to Bob and Bob introduces herself to Chuck, Bob will
    make sure Alice knows about Chuck as well. We’ll talk about how that works in
    a second.\n\nI wrap these two actions, running the agent and making introductions,
    up in a [little shell script](https://github.com/fly-apps/ichabod/blob/main/serfctl.sh).
    Because I’m now running multiple thingies in my Docker image, I use [overmind](https://github.com/DarthSim/overmind)
    as my new entrypoint, which drives a [Procfile](https://devcenter.heroku.com/articles/procfile).
    [Here’s the whole Dockerfile](https://github.com/fly-apps/ichabod/blob/main/Dockerfile).\n\nWhat
    did this get me? Well, from now on, if I’m on the private IPv6 network for my
    organization, I can find *any* node and instantly get a map of all the other nodes:
    \n\n```bash\n$ serf members -status=alive\nichabod-yyz-3a64c0ba   [fdaa:0:18:a7b:aa4:3a64:c0ba:2]:7777
    \ alive   role=ichabod,region=yyz\nichabod-iad-bdc999ff   [fdaa:0:18:a7b:ab8:bdc9:99ff:2]:7777
    \ alive   region=iad,role=ichabod\nichabod-ord-401dbe36   [fdaa:0:18:a7b:7d:401d:be36:2]:7777
    \  alive   role=ichabod,region=ord\n```\n\nI can integrate this information with
    a shell script, but I can also just bring it into my application code directly
    (here with the relatively simple `serfx` gem:\n\n```ruby\nrequire 'ferrum'\nrequire
    'serfx'\n\nserf = Serfx.connect\nraddr = IPAddr.new_ntoh serf.members.body[\"Members\"].first[\"Addr\"]\n\nc
    = Ferrum::Browser.new :base_url => \"http://[#{raddr}]:9222/\"\n```\n\nI could
    easily filter this down by location (via the “region”) tag, role, or, as we’ll
    see in a sec, network closeness. This interface is simpler than DNS, it’s lightning
    fast, and it’s always up-to-date.\n\n## A Quick Word About Security\nSerf has
    a security feature: you can key your Serf communications statically, so rogue
    nodes without the key can’t participate or read messages. \n\nIt’s fine, I guess.
    I’d be nervous if I was deploying Serf in an environment where I was really depending
    on Serf’s encryption for security. But, frankly, it doesn’t matter to us here,
    because we’re already running on a private network, and our external connectivity
    to that network takes advantage of the vastly more sophisticated cryptography
    in WireGuard. \n\n## What Serf Is Doing\nThe first bit of distributed systems
    jargon that comes up when people describe Serf is SWIM, the “Scalable Weakly-Consistent
    Infection Membership” protocol. Distributed systems are full of protocols with
    acronymical names that are hard to get your head around, and SWIM is not one of
    those; I don’t think you even need a diagram to grok it.\n\nYou can imagine the
    simplest possible membership protocol, where you make introductions (like we did
    in the last section) and every member simply relays messages and tries to connect
    to every new host it learns about. That’s probably what you’d come up with if
    you ran into the membership problem unexpectedly in a project and just needed
    to bang something out to solve it, and it works fine to a point.\n\nSWIM is just
    a couple heuristic steps forward from that naive protocol, and those steps make
    the protocol (1) scale better, so you can handle many thousands of nodes, and
    (2) quickly detect failed nodes. \n\nFirst, instead of spamming every host we
    learn about with heartbeats on an interval, we instead select a random subset
    of them. We essentially just ping each host in that subset; if we get an ACK,
    we’ve confirmed they’re still members (and, when new nodes connect up to us, we
    can share our total picture of the world with them to quickly bring them up to
    date). If we don’t get an ACK, we know something’s hinky.\n\nNow, to keep the
    group membership picture from flapping every time a ping fails anywhere in the
    network, we add one more transaction to the protocol: we mark the node we couldn’t
    ping as SUS, we pick another random subset of nodes, and we ask *them* to ping
    the SUS node for us. If they succeed, they tell us, and the node is no longer
    SUS. If nobody can ping the node, we finally conclude that the node is the impostor,
    and eject them from the ship.\n\nSerf’s SWIM implementation has [some CS grace
    notes](https://www.serf.io/docs/internals/gossip.html), but you could bang the
    basic protocol out in an hour or two if you had to.\n\nSerf isn’t just a SWIM
    implementation, and SWIM isn’t the most interesting part of it. That honor would
    have to go to the network mapping algorithm [Vivaldi](https://sites.cs.ucsb.edu/~ravenben/classes/276/papers/vivaldi-sigcomm04.pdf).
    Vivaldi, which was authored by a collection of my MIT CSAIL heroes including Russ
    Cox, Frans Kaashoek, and (yes, that) Robert Morris, computes an all-points pairwise
    network distance map for a cluster. [Here’s a funny thread](https://twitter.com/_rsc/status/1026479058899619841)
    where Russ Cox finds out, years later, that HashiCorp implemented his paper for
    Serf.\n\nHere’s roughly how Vivaldi works:\n\nWe model the members of our cluster
    as existing in some space. To get your head around it, think of them as having
    Cartesian 3D coordinates. These coordinates are abstract; they have no relation
    to real 3D space.\n\nTo assign nodes coordinates in this space, we attach them
    to each other with springs of varying (and, to start with, indeterminate) lengths.
    Our job will be to learn those lengths, which we’ll do by sampling network latency
    measurements. \n\nTo begin with, we’ll take our collection of spring-connected
    nodes and squish them down to the origin. The nodes are, to begin with, all sitting
    on top of each other.\n\nThen, as we collect measurements from other nodes, we’ll
    measure error, comparing our distance in the model to the distance reflected by
    the measurement. We’ll push ourselves away from the nodes we’re measuring in some
    random direction (by generating a random unit vector), scaled by the error and
    a sensitivity factor. That sensitivity factor will itself change based on the
    history of our error measurements, so that we update the model more or less confidently
    based on the quality of our measurements.\n\nOur cluster converges on a set of
    *network coordinates* for all the nodes that, we hope, relatively accurately represents
    the true network distance between the nodes. \n\nThis all sounds complicated,
    and I guess it is, but it’s complicated in the same sense that TCP congestion
    control (which was originally also based on a physical model) is complicated,
    not in the sense that, say, Paxos is: the complexity is mostly not exposed to
    us and isn’t costing meaningful performance. Serf sneaks Vivaldi data into its
    member updates, so we get them practically for free.\n\nWe can now ask Serf to
    give us the RTT’s between *any two points* on the network: \n\n```bash\n> $ serf
    rtt ichabod-yyz-3a64c0ba ichabod-iad-bdc999ff                                       \nEstimated
    ichabod-yyz-3a64c0ba <-> ichabod-iad-bdc999ff rtt: 19.848 ms\n```\n\nIf you’re
    like me, you read Serf’s description of their Vivaldi implementation and have
    a record scratch moment when they say they’re using an 8-dimensional coordinate
    system. What do those coordinates possibly represent? But you can sort of intuitively
    get your head around it this way: \n\nImagine that network performance was entirely
    dictated by physical distance, so that by sampling RTTs and updating a model what
    we were effectively doing was recapitulating the physical map of where nodes where.
    Then, a 2D or 3D coordinate space might effectively model network distance. But
    we know there are many more factors besides physical distance that impact network
    distance! We don’t know what they are, but they’re embedded somehow in the measurement
    data we’re collecting. We want enough dimensionality in our coordinates so that
    by iteratively and randomly sproinging away from other nodes, we’re capturing
    all the factors that determine our RTTs, but not so much that the data that we’re
    collecting is redundant. Anyways, 8 coordinates, plus (again) some grace notes.
    \n\nArmon Dadger, the CTO of HashiCorp, has a [really excellent talk on Vivaldi](https://www.youtube.com/watch?t=1747&v=AszPoJjWK9Q&feature=youtu.be)
    that you should just watch if this stuff is interesting to you.\n\nFrankly, I’m
    writing about Vivaldi because it’s neat, not because I get a huge amount of value
    out of it. In theory, Serf’s Vivaldi implementation powers “nearness” metrics
    in Consul, which in our experience have been good but not great; I’d trust relative
    distances and orders of magnitude. But RTT’s aside, you could also theoretically
    take the 8D coordinates themselves and use them to do more interesting modeling,
    like automatically creating clusters of nearby or otherwise similar nodes. \n\nA
    last Serf thing to point out: membership is interesting, but if you have a membership
    protocol, you’re epsilon away from having a messaging system, and Serf does indeed
    have one of those. You can send *events* to a Serf cluster and tell your agent
    to react to them, and you can define *queries*, which are events that generate
    replies. So, I can set up Serf this way:\n\n```bash\n  serf agent    \t\t\t\t\t\\\n\t-node=\"$name\"
    \t\t\t\t    \\\n  -profile=wan\t\t\t\t    \\\n\t-bind=\"[$paddr]:7777\"\t\t\t\\\n\t-tag
    role=\"${FLY_APP_NAME}\"\t\t\\\n\t-tag region=\"${FLY_REGION}\"\t\t\\\n\t-event-handler=query:load=uptime\n```\n\nAnd
    now we can query the load on all our nodes:\n\n```bash\n> $ serf query load                                                                          \nQuery
    'load' dispatched\nAck from 'ichabod-iad-bdc999ff'\nResponse from 'ichabod-iad-bdc999ff':
    \ 23:51:58 up 1 day, 19:25,  0 users,  load average: 0.00, 0.00, 0.00\nAck from
    'ichabod-yyz-3a64c0ba'\nAck from 'ichabod-ord-401dbe36'\nResponse from 'ichabod-ord-401dbe36':
    \ 23:51:47 up 1 day, 19:25,  0 users,  load average: 0.00, 0.00, 0.00\nResponse
    from 'ichabod-yyz-3a64c0ba':  23:51:59 up 1 day, 19:26,  0 users,  load average:
    0.00, 0.00, 0.00\nTotal Acks: 3\nTotal Responses: 3\n```\n\nUnder the hood, Serf
    is using [logical timestamps](https://en.wikipedia.org/wiki/Lamport_timestamp)
    to distribute these messages somewhat (but imperfectly) reliably. I love logical
    timestamps so much. \n\n<%= partial \"shared/posts/cta\", locals: {\n  title:
    \"Easily run clusters on Fly\",\n  text: \"Painlessly boot up clusters on Fly
    and link them with WireGuard to your computer, AWS, or GCP.\",\n  link_url: \"https://fly.io/docs/speedrun/\",\n
    \ link_text: \"Try Fly for free&nbsp;&nbsp;<span class='opacity-50'>&rarr;</span>\"\n}
    %>\n\n\n## & Scene!\nSo anyways, my point here is, you can do a lot better than
    DNS for service discovery in a 6PN setup on Fly. Also my point is that Serf is
    really useful and much, much easier to set up and run than Consul or Zookeeper;
    you can bake it into a Dockerfile and forget about it. \n\nAlso, my point is that
    Fly is a pretty easy way to take distributed systems tools like this out for a
    spin, which you should do! You can boot a Consul cluster up on Fly, or, if you’re
    an Elixir person, you could use Partisan instead of Serf, which does roughly the
    same kind of thing. \n\n"
- :id: blog-get-fly-with-your-fly-command-line
  :date: '2021-01-05'
  :category: blog
  :title: Get fly with your Fly command line
  :author: dj
  :thumbnail:
  :alt:
  :link: blog/get-fly-with-your-fly-command-line
  :path: blog/2021-01-05
  :body: "\n\n<div class=\"lead\">\n\nThe `flyctl` command is your route to harnessing
    the immense power of the global Fly network... well, it's a pretty cool command.
    There are plenty of features in it that not everyone knows. With that in mind,
    here are some tips for some higher Flying:\n\n</div>\n\n* **The command...** It
    started off as `flyctl` but we kept being asked when would the command become
    `fly`. \"Why not both?\" we said and now, on most platforms, you can use `flyctl`
    or `fly` interchangeably. Boom! Three keystrokes saved!\n\n* **The current app...**
    You already know that Fly looks for a `fly.toml` file in the current directory
    to work out what the app you are working with is called. But for many commands
    you can also add `-a appname` to your Fly command and it'll use that appname instead.
    If there's a `fly.toml` in your current directory, it will ask if you are sure
    that's the name you want to use to be on the safe side. So now you don't have
    to change directory to, say, get a status on `mygreatapp99` - just do `fly status
    -a mygreatapp99`.\n\n* **Building from outside the directory...** The `fly deploy`
    command can work with that. Say you want to build the app in a child directory
    \"mygreatchildapp\". Then just point the deploy at it. `fly deploy ./mygreatchildapp`
    and Fly will find the `fly.toml` in there and get to deploying.\n\n* **Deploy
    different...** What should you do if you have a Fly app that you want to deploy
    a couple of different ways. If, for each way, you've built a `fly.toml` file how
    do you pull that all of them together under one directory? Rename the `fly.toml`
    files to something meaningful for your app `test.toml`, `preview.toml`, `production.toml`
    and then leverage the `-c` aka `--config` flag on `fly deploy`. That gets you
    the ability to run `fly deploy -c preview.toml` and away it goes deploying with
    that config. Oh, and you can combine that with the previous item on this list
    and then build out of your mono repo tree without changing directory.\n\n* **Easy
    opening...** Are you working out what your application's \".fly.dev\" host name
    is? Save time with the `fly open` command that works out the app's URL for you
    and opens your browser on that page.\n\n* **Get Metrics...** Some things look
    better rendered in your browser, like metrics. Now you may know the `fly dash`
    command which opens up your Fly web dashboard in your browser. Well, you can go
    one better with `fly dash metrics` which takes you straight to the dashboard metrics
    page for some graphical delight and a lot of information.\n\n* **Go Docs...**
    If you want to look something up in the docs and you are at the command line,
    try `fly docs` which opens your browser straight into the documentation's top
    page. \n\n"
- :id: blog-graceful-vm-exits-some-dials
  :date: '2020-12-29'
  :category: blog
  :title: Graceful VM exits, some dials
  :author: michael
  :thumbnail:
  :alt:
  :link: blog/graceful-vm-exits-some-dials
  :path: blog/2020-12-29
  :body: "\n\n<div class=\"lead\">\n  \nFly.io transforms containers into swarms of
    fast-booting VMs and runs them close to users. You can now delay VM shutdowns
    up to 24 hours to let the overly attached clients finish their work.\n\n</div>\n\nFly
    apps are typically fast to boot, and it's relatively easy to boot new VMs. We
    start them up, do some health checks, and then add them to our load balancer and
    DNS service discovery. But what comes up must go down. We shut VMs down for any
    number of reasons – new deploys, or scaling, or for maintenance on underlying
    hardware.\n\nBy default, we send a `SIGINT` to tell a VM it's time to go away.
    Then we wait 5 seconds and, if the VM is still running, we forcefully terminate
    it. This works fine _most_ of the time, especially for application servers, but
    some work takes longer to clean up. A live video streaming service may a have
    users (often teenagers) connected for hours at a time. Or database servers might
    have in flight transactions to commit before terminating. \n\nKeeping processes
    alive longer is a boring, simple way to solve these kinds of problems. So you
    can now tell us to keep VMs for up to 24 hours after we send a shutdown signal.
    And you can also specify what signal we send (because signal handling is wildly
    inconsistent). Just add these options to your `fly.toml` configuration file:\n\n*
    `kill_timeout`: Number of seconds to wait before killing a VM. Shared CPU VMs
    allow up to 5 minutes and dedicated CPU VMs allow up to 24 hours. The default
    is 5 seconds.\n* `kill_signal`: defaults to `SIGINT`. Also accepts `SIGTERM`,
    `SIGQUIT`, `SIGUSR1`, `SIGUSR2`, `SIGKILL`, or `SIGSTOP`.\n\n<%= partial \"shared/posts/cta\",
    locals: {\n  title: \"Run apps with long lived connections\",\n  text: \"Launch
    your Docker apps on Fly and we'll keep them alive while your users finish what
    they're doing.\",\n  link_url: \"https://fly.io/docs/speedrun/\",\n  link_text:
    \"Try Fly for free&nbsp;&nbsp;<span class='opacity-50'>&rarr;</span>\"\n} %>\n\n###
    Example: drain long lived TCP connections\n\nHAProxy is an open source project
    for load balancing TCP and HTTP connections. Most HTTP requests are fast, but
    you might also run HAProxy to handle large user uploads or load balance across
    databases.\n\nBy default, a `SIGINT` causes an HAProxy server to immediately close
    all connections and shut down, aka \"hard stop\". If you'd rather cleanly drain
    connections instead of serving errors, you can use [the \"soft stop\" mode](https://cbonte.github.io/haproxy-dconv/1.7/configuration.html#3.1-hard-stop-after)
    and specify a long kill timeout.\n\n```toml\n# stop accepting new connections
    while existing connections drain\nkill_signal = \"SIGUSR1\"\n# allow 2 minutes
    for all connections to finish before killing the server\nkill_timeout = 3600\n```\n\n###
    Example: gracefully shutdown a database server\n\nPostgres responds to the `SIGINT`
    signal (our default) by immediately aborting open transactions and closing all
    connections. This is called the \"fast shutdown\" mode and results in discarding
    data and causing application errors. Instead, you can now use [the \"smart shutdown\"
    mode](https://www.postgresql.org/docs/9.4/server-shutdown.html) by sending `SIGTERM`
    and giving it five minutes to commit transactions.\n\n```toml\n# stop accepting
    new connections while existing sessions complete\nkill_signal = \"SIGTERM\"\n#
    allow 5 minutes to cleanly shutdown\nkill_timeout = 300\n```\n<hr>\n\n## Shared
    infrastructure and long lived connections\n\nModern cloud infrastructure forces
    a lot of application compromises, especially when you're sharing infrastructure.
    Most cloud function and container hosting products sit behind layers of shared
    services, each needing frequent releases to keep them humming along. Releases
    are disruptive, especially for software that proxies user connections to arbitrary
    containers.\n\nProviders simplify their lives by limiting what customer containers
    can do – they might only serve HTTP, for example, or have to implement a custom
    event handler. If an app can only speak HTTP, has to complete every requests within
    30 seconds, it's very simple to roll out new proxy releases.\n\nThis is silly,
    but we're not immune. We run a global load balancer service in front of Fly apps.
    When you use it for HTTP or TCP connections, our releases can disrupt in flight
    connections. We do as much as we can to minimize the impact and drain connections
    over a period of minutes when necessary.\n\nSome apps need us to get the heck
    out of the way. We've built our plumbing specifically to allow this. You can opt
    out of our HTTP and TLS handlers, for example. And if you run a UDP service, our
    load balancing is entirely stateless. And we have experimental stateless TCP load
    balancing! If you have an app that needs to keep connections alive as long as
    possible, [let us know](https://community.fly.io/), we'll help you how to try
    it out.\n\n<hr>\n\nDo you want to know more? Or have an idea? We've got a [community
    forum just for you](https://community.fly.io/t/new-feature-graceful-vm-shutdown-options/504).\n\n"
- :id: blog-ipv6-wireguard-peering
  :date: '2020-12-23'
  :category: blog
  :title: IPv6 WireGuard Peering
  :author: thomas
  :thumbnail:
  :alt:
  :link: blog/ipv6-wireguard-peering
  :path: blog/2020-12-23
  :body: "\n\n<div class=\"lead\">Fly.io transforms containers into swarms of fast-booting
    VMs and runs them close to users. Now you can connect those swarms privately to
    other networks with WireGuard.</div>\n\nThey say that when you’re starting a product
    company, it’s a better plan to chase down something a bunch of people will really
    love a lot than it is to try to build something that everyone will just like a
    little bit. So when Fly.io launched, it had a [pretty simple use case](https://news.ycombinator.com/item?id=22616857):
    taking conventional web applications – applications built for platforms outside
    of Fly.io – and speeding them up, by converting them into [Firecracker MicroVMs](https://fly.io/blog/sandboxing-and-workload-isolation/)
    that we can run close to users on a network of servers around the world. \n\nThis
    works great, and you should try it; for instance, if you’ve got an application
    running on Heroku, [we have it down to just a button click](https://fly.io/blog/turboku/)
    on a web page. Or, [our Fly.io speed-run](https://fly.io/docs/speedrun/) can get
    a Dockerized application deployed everywhere from Chile to Singapore in just a
    couple commands. It’s easier than figuring out how to use `rsync`; so easy, it’s
    boring.\n\nBut, predictably, people have wanted to launch other stuff on Fly,
    besides making existing applications go fast. And we want to help them do that.
    We like stuff! We like talking to people about stuff! And we’ve gotten pretty
    good at getting stuff working on Fly. But stuff hasn’t always been as boring as
    we’d like.\n\nWe’re striking blows for the forces of boredom, by making it straightforward
    to get just about anything running on Fly.io. One way we’re doing that is by making
    it easy to peer networks with Fly, using WireGuard and IPv6.\n\nHere’s the TL;DR:\n\n*
    If you deploy several different apps to Fly.io, they can find and talk to each
    other, privately, without any configuration.\n* If you’re running things on GCP
    or AWS, you can peer them to Fly.io with a simple `flyctl`, using WireGuard.\n\n##
    IPv6 Private Networking at Fly\nApps on Fly.io belong to accounts, which are associated
    with organizations. You don’t have to grok this; you just have an “organization”,
    trust us. Every Fly.io organization has its own private IPv6 network; we call
    them “6PNs”, or, at least I do; I’m trying to make it a thing. \n\nEvery app in
    your organization is connected to the same 6PN. Every app instance has a “6PN
    address”. Bind services to that address and they’re available other apps in your
    private network; bind a service only to it, and it’s only reachable privately.\n\nYou
    don’t need to understand the rest of this section, but in case you’re interested:\n\nWe
    carve up IPv6 addresses and embed information in them. We start with the IPv6
    ULA prefix `fdaa::/16` (the ULA space in IPv6 is analogous to the 10-net space
    in IPv4, except there’s a lot more of it). \n\nThen, for every instance of every
    app we run, we collect a bit of information: a “network ID” associated with the
    app’s organization, an identifier for the hardware the instance is running on,
    and an identifier for the instance itself, and come up with this gem of an IPv6
    address:\n\n\n<div id=\"pricing-1\">\n  | fdaa     | 16 bits | ULA prefix           |\n
    \ | -------- | ------- | -------------------- |\n  | network  | 32 bits | organization
    address |\n  | host     | 32 bits | machine identifier   |\n  | instance | 32
    bits | app instance ID      |\n  |          | 16 bits | free space           |\n</div>\n\nTechnically,
    what we end up delegating to each instance is a /112, which is the IPv6 equivalent
    of an IPv4 Class B address; you can address 65,000 (and change) different things
    inside of an instance if you wanted to. I haven’t come up with any kind of use
    for this, but, why not? \n\nMeanwhile, an organization has effectively a /48,
    or “mind-bogglingly huge”, 6PN prefix.\n\nThe core design idea of this system
    is pretty simple: we control IPv6 address assignments and routing inside our network.
    To lock an instance into a 6PN network, all we really need is a trivial BPF program
    that enforces the “don’t cross the streams” rule: you can’t send packets between
    different 6PN prefixes. [We’re already BPF’ing all our interfaces to make UDP
    work](https://fly.io/blog/bpf-xdp-packet-filters-and-udp/), so this is an easy
    change.\n\n## 6PN DNS\nHaving all these IPv6 addresses doesn’t help much if your
    apps can’t find each other, so we run an internal DNS service for our 6PN networks.
    So in reality, you never think about 6PN at all. You just need the names of your
    apps.\n\nOur DNS service is a small [Tokio](https://github.com/tokio-rs/tokio)
    Rust program backed by sqlite databases that our service discovery system builds
    on all our hosts. It accepts packets only from 6PN addresses, and, because of
    its network position, can trust source addresses, which it uses to determine the
    answers to questions. \n\nAlso, it forwards external DNS queries, so it can stand
    in as the sole nameserver in `resolv.conf`. That was another 20 lines of code.\n\nI
    wish the server was interesting enough to talk about more, but it’s not; it’s
    practically the “hello world” of the [NLNet “domain” crate](https://github.com/NLnetLabs/domain).
    If you don’t use Fly.io, my message to you in this section is mostly “go forth
    and build ye a Rust DNS server, for lo, it is pretty easy to do”.  Also, deploy
    it on Fly.io, it’s great.\n\nIf you do use Fly.io, first, thanks and congratulations.
    Also you might be interested in our naming scheme. Assume your app is `fearsome-bagel-43`,
    and has a sibling app `serf-43`. Then:\n\n<div id=\"pricing-2\">\n|                                    |
    \       |                                                                   |
    \n| ---------------------------------- | ------ | -----------------------------------------------------------------
    |\n| fearsome-bagel-32.internal         | AAAA   | addresses of all fearsome-bagel-32
    instances                      |\n| serf-43.internal                   | AAAA
    \  | addresses of all serf-43 instances                                |\n| handsome-badget-92.internal
    \       | AAAA   | trick question! that app isn't in your organization, so: NXDOMAIN
    |\n| regions.fearsome-bagel-32.internal | TXT    | comma-separated list of all
    regions fearsome-bagel-32 runs in     |\n| nrt.fearsome-bagel-32.internal     |
    AAAA   | addresses of all fearsome-bagel-32 instances in Japan             |\n|
    _apps.internal                     | TXT    | comma-separated list of all apps
    in your organization             |\n| _peer.internal                     | TXT
    \   | we'll get to that in a second.                                    |\n</div>\n\nFor
    instances of apps running in Fly, DNS is available on `fdaa::3` (the one exception
    to the “no crossing the streams” 6PN access rule).\n\n## Internal WireGuard at
    Fly\nThe basic architecture of Fly.io is that we have hardware colocated in datacenters
    around the world. We direct global traffic to the edge of our network with BGP-driven
    Anycast, and we route it to nearby worker servers over [Jason Donenfeld’s WireGuard](https://www.wireguard.com/papers/wireguard.pdf).\n\nWireGuard
    is amazing. It will likely replace all other VPN protocols. But it’s so lightweight
    and performant that I think it’s going to change the role VPNs have. It’s just
    as easy to set up a WireGuard connection as it is an SSH account. And you pay
    practically no performance penalty for using it. So you end up using VPNs for
    new things. \n\nWhat makes WireGuard so interesting?\n* It’s based on Trevor Perrin’s
    [Noise protocol framework](https://noiseprotocol.org/), and inherits Noise’s modern,
    best-practices cryptography, much of which also powers Signal ––– an authenticated
    Curve25519 handshake, ChaCha20+Poly1305 for encryption, and Blake2s for hashing.\n*
    It does no negotiation; there are no cryptographic parameters to select. \n* Its
    [reference implementation](https://github.com/torvalds/linux/tree/master/drivers/net/wireguard)
    is just 5000 lines of kernel code; you can read it in an hour or so.\n* Unlike
    other VPN protocols, WireGuard was designed with implementation security in mind,
    so that, for example, it’s straightforward to implement without requiring on-demand
    dynamic memory allocation.\n* It’s extremely fast.\n\nWe run an internal WireGuard
    mesh, about which we’ll write more in the future. All you need to know here is
    that connectivity between hosts in our network happens entirely over WireGuard.\n\nI
    come from old-breed ISP stock, from a time when [Cisco AGS+’s](https://rcsri.org/collection/cisco-ags/)
    roamed the land, and what I was taught very early on is that the best routing
    protocol is static routing, if you can get away with it. With the information
    embedded in our addresses, we can route 6PN statically. \n\nBut there’s a catch.
    A central part of WireGuard’s design is the notion of “cryptokey routing”. WireGuard
    peers are identified by a Curve25519 key (a short Base64 string), and each peering
    connection is tagged with a set of “Allowed IPs”. When the OS wants to send traffic
    over a WireGuard link, it routes packets to the WireGuard device, and WireGuard
    checks its table of peers to see which “allows” that destination. For that to
    work, “Allowed IPs” can’t overlap between links.\n\nThat’s a problem for our 6PN
    design, because a 6PN prefix obviously has to run across a bunch of hosts, and
    there’s no way to wildcard a chunk out of the middle of an address. \n\nThe solution
    is straightforward, though: we just use BPF to temporarily swap the “host” and
    “network” chunks of the address before and after routing through WireGuard. Our
    WireGuard mesh sees IPv6 addresses that look like `fdaa:host:host::/48` but the
    rest of our system sees `fdaa:net:net::48`. This turns out to be an extremely
    simple transform, since swapping bytes in an IPv6 header doesn’t alter checksums.\n\n##
    WireGuard Peering\nHere’s a thing you might want to do with an app running on
    Fly: connect it to to a database managed by AWS RDS. \n\nHere’s a way to do that:
    boot up a WireGuard gateway in AWS ([here, with a few dozen lines of Terraform](https://github.com/fly-apps/rds-connector),
    but use whatever you like; if Fly.io stands for anything, it’s “not having to
    know Terraform”) that peers into your 6PN network and exposes a Postgres proxy
    like PgBouncer. It’s a pretty boring configuration, which is the kind we like.\n\nThis
    works today at Fly.io because of WireGuard Peering. We will generate WireGuard
    configurations for you that will work in APAC, North America, and Europe. To do
    that, just run `flyctl wireguard create`. We’ll spit out a config that will drop
    into Linux, macOS, or Windows WireGuard.\n\nWireGuard peers get /120 delegations
    (the equivalent of an IPv4 class C), and an organization-specific DNS endpoint
    baked into the config. When you add a WireGuard peer, we update DNS across the
    fleet, so your peer is available by its name; if we called this peer `rds-us-east-1`,
    \ our apps could reach it at `rds-us-east-1._peer.internal`. We can get a list
    of peers by looking up the TXT at `_peer.internal`. \n\nA nice thing about this
    design is that it doesn’t require you to expose any management services on the
    AWS side; your AWS WireGuard gateway connects out to us, and the default security
    rules for your VPC should keep everything hermetically sealed inside (you obviously
    want to verify this part of your configuration; we’re just saying, we’re not asking
    you to open up any ports). \n\nOf course, you can also use WireGuard 6PN peering
    to manage your app instances directly; for example, the config we generate drags-and-drops
    into [macOS WireGuard](https://apps.apple.com/us/app/wireguard/id1451685025?mt=12).
    \n\n<%= partial \"shared/posts/cta\", locals: {\n  title: \"Connect your containers
    with WireGuard\",\n  text: \"Launch your Docker apps on Fly and we'll seamlessly
    connect them to any network you'd like using WireGuard\",\n  link_url: \"https://fly.io/docs/speedrun/\",\n
    \ link_text: \"Try Fly for free&nbsp;&nbsp;<span class='opacity-50'>&rarr;</span>\"\n}
    %>\n\n## Service Discovery In Fly Private Networks\nI think you can get pretty
    far designing applications with the DNS we expose right now, but we’ve deliberately
    kept it boring, because we assume different people will want different things.\n\nBut
    nothing stops you from making service discovery exciting! I have, for instance,
    a multi-perspective DNS resolver example I’ll publish shortly that uses [Hashicorp
    Serf](https://www.serf.io/) to auto-discover new nodes. The important thing to
    know about 6PN networking is that it’s direct between nodes; we don’t proxy it
    or meddle with it in any way. Anything you want to run, including your own full
    [Consul cluster](https://www.consul.io/), should just work\n\nI’m pretty happy
    with how this design is turning out and optimistic that it achieves “boring” for
    connecting arbitrary services to Fly applications. So you can use Fly not only
    to make existing applications run faster, but also as a core component of new
    applications. I’m kind of in love with the ergonomics of our dev UX (I can say
    that because, as a late arrival to the Fly team, I had no hand in designing it),
    and anything that lets me use `flyctl` for more stuff is a win in my book.\n\n<hr>\n\nDo
    you want to know more? Or have an idea? We've got a [community forum just for
    you](https://community.fly.io/t/ipv6-wireguard-peering/480).\n"
- :id: blog-new-vms-more-ram-extra-cpu-and-a-dollar-menu
  :date: '2020-12-10'
  :category: blog
  :title: 'New VMs: more RAM, extra CPU, and a dollar&nbsp;menu'
  :author: kurt
  :thumbnail:
  :alt:
  :link: blog/new-vms-more-ram-extra-cpu-and-a-dollar-menu
  :path: blog/2020-12-10
  :body: "\n\n<div class=\"lead\">Fly.io turns your container apps into swarms of
    fast-booting VMs and runs them close to your users. Bless your users with 2GB
    of RAM for about ten bucks per month.  Or shop the dollar menu for tiny VMs <a
    href=\"#pricing\">*</a>.</div>\n\nWe first showed Fly.io VMs to developers in
    early 2020. They were most interested in running CPU intensive apps doing image
    processing, machine learning predictions, and even video transcoding (despite
    what were, until recently, [_offensive_ bandwidth prices](/blog/we-cut-bandwidth-prices-go-nuts/)).
    So when we launched, most of the available VMs were thicc like oatmeal, but weak
    on the RAM.\n\nThe requests changed when we got in front of more developers. We
    knew we'd have to solve databases \"at some point\", we just didn't expect devs
    to ask [day one](https://news.ycombinator.com/item?id=22618406). Databases – and
    apps that look like databases when you squint – need a higher RAM to CPU ratio.
    Small databases hoover up RAM, but largely leave the CPU alone.\n\nOur new VMs
    come in two flavors, `shared-cpu` with up to 2GB of RAM for lightweight apps,
    and `dedicated-cpu` with up to 64GB of RAM for not-quite-big-data-but-it-should-be-fast.
    Here's the [price breakdown](/docs/about/pricing/#compute) (and if you're a future
    reader, you might see _even more_ VM types):\n\n<div id=\"pricing\">\n| &nbsp;
    \              | CPU(s)           | RAM   | Price                     |\n| --------------------
    | -------------    | ----- | ------------------------- |\n| **shared-cpu-1x**
    \   | 1&nbsp;shared    | 256MB | $0.0000008/s ($1.94/mo)   |\n|                      |
    \                 | 1GB   | $0.0000022/s ($5.70/mo)   |\n|                      |
    \                 | 2GB   | $0.0000041/s ($10.70/mo)  |\n| **dedicated-cpu-1x**
    | 1&nbsp;dedicated | 2GB   | $0.0000120/s ($31.00/mo)  |\n|                      |
    \                 | 4GB   | $0.0000158/s ($41.01/mo)  |\n|                      |
    \                 | 8GB   | $0.0000235/s ($61.02/mo)  |\n| **dedicated-cpu-2x**
    | 2&nbsp;dedicated | 4GB   | $0.0000239/s ($62.00/mo)  |\n|                      |
    \                 | 8GB   | $0.0000355/s ($92.02/mo)  |\n|                      |
    \                 | 16GB  | $0.0000509/s ($132.04/mo) |\n| **dedicated-cpu-4x**
    | 4&nbsp;dedicated | 8GB   | $0.0000478/s ($124.00/mo) |\n|                      |
    \                 | 16GB  | $0.0000749/s ($194.04/mo) |\n|                      |
    \                 | 32GB  | $0.0001057/s ($274.08/mo) |\n| **dedicated-cpu-8x**
    | 8&nbsp;dedicated | 16GB  | $0.0000957/s ($248.00/mo) |\n|                      |
    \                 | 32GB  | $0.0001536/s ($398.08/mo) |\n|                      |
    \                 | 64GB  | $0.0002153/s ($558.16/mo) |\n</div>\n\n## Cloud VM
    pricing, an abridged guide\n\nWe like working on Fly.io, and want to continue
    hacking away. Sustaining the company means getting to 70% margins on VMs – a number
    that [comes up all the time](https://fly.io/blog/we-cut-bandwidth-prices-go-nuts/#making-it-work-for-everyone).\n\nThe
    dirty secret of Virtual Machine pricing is ... the machines are virtual. The specs
    we promise are only loosely constrained by the underlying hardware. And there's
    no reason to tell customers the host hardware specs <sup><a href=\"#host-hardware\"
    id=\"host-hardware-ref\">[1]</a></sup>. We can even sell the same RAM several
    times over. Yay margins!\n\nFortunately, we don't want to make money on VMs. We
    do, however, want to help happy customers and minimize our own operational headaches.
    Oversubscribing host hardware is a great way to piss off customers and keep us
    hoppin' while we're on call. A few unexpected OOM errors will ruin everyone's
    day.\n\n### To the slide rule!\n\nIf you do the math, you'll estimate that our
    cost to run a `dedicated-cpu-1x` VM is a little under $10 per month. That's a
    ridiculous simplification, but good for hasty math.\n\nThe actual cost is a function
    of hardware + colocation + power. And we commit to the hardware in yearly increments,
    while we bill you in seconds. For additional fun, we have to buy servers before
    we get customers. So there's dead time before we're even covering the expense,
    much less making margins.\n\nReading that back, it actually sounds pretty terrible.
    But we're lucky to have levers that make it work.\n\n1. We fund new hardware with
    revenue from large customers. Large customers are an immediate margin win, and
    we can \"borrow from margins\" to deploy more hardware for the people we really
    love (the developers with a $50 budget and a promising application).\n2. Shared
    CPU VMs. Over subscribing hardware is a terrible thing to do _unless you set the
    right expectations_.\n\nShared CPU instances let us fill in the gaps with a lower
    priced product that's decoupled from margins. We can load large hosts up with
    shared VMs, cover our costs pretty quickly, and avoid nasty surprises.\n\n###
    Shared CPU VMs\n\nOver subscribing CPUs is less fraught than over subscribing
    RAM since there's enough oomf on any given server to move things around and avoid
    contention. We've run `micro` VMs for the last 6 months to get an idea of how
    to price these, and the results have been favorable for pricing.\n\nWe can pool
    a bunch of CPUs together, and share each with an average of 12 VMs. This works
    great for bursty apps, the big pool of CPUs lets us mix and match busy and idle
    VMs to ~~produce vast quantities of heat~~ maximise CPU utilization.\n\nThis works
    100% of the time until it doesn't. CPU contention is a given. We have safeguards
    in place to ensure that VMs trying to eat a whole CPU have to give it up to your
    polite VM (`cpuset.priority` if you're a cgroups nerd). And when that doesn't
    solve the contention, we can move VMs to different hardware. Sometimes in different
    regions!\n\nThe best part? These VMs cost you $1.94 per month to run full time.
    That's cheaper than most dollar menus, these days <sup><a href=\"#dollar-menus\"
    id=\"dollar-menus-ref\">[^]</a></sup>.\n\n<%= partial \"shared/posts/cta\", locals:
    {\n  title: \"New VMs, delivered fresh\",\n  text: \"Launch your Docker apps on
    Fly and we'll run them in Firecracker VMs for even cheaper than you expected.\",
    \n  link_url: \"https://fly.io/docs/speedrun/\",\n  link_text: \"Try Fly for free&nbsp;&nbsp;<span
    class='opacity-50'>&rarr;</span>\"\n} %>\n\n## But running databases on ephemeral
    VMs is silly\n\nDatabase apps also need persistent storage. VMs that boot up,
    then go away and take all their storage with them are not a good place to run
    DBs. We don't sell persistent storage. But, you know, [we definitely should](https://community.fly.io/t/preview-persistent-disks-for-fly-apps/332).\n\n<hr>\n<ol>\n<li
    id=\"host-hardware\"><a href=\"#host-hardware-ref\">^</a>  We mostly run AMD EPYC
    CPUs, 24-32 cores, 256GB of RAM. Sometimes equivalent Intel CPUs (some people
    enjoy researching [PassMark scores](https://www.cpubenchmark.net/)), a few machines
    only have 128GB of RAM.</li>\n<li id=\"dollar-menus\"><a href=\"#dollar-menus-ref\">^</a>
    Seriously, [what the heck McDonalds](https://www.mcdonalds.com/us/en-us/full-menu/123dollarmenu.html)?</li>\n</ol>\n"
- :id: blog-incoming-6pn-private-networks
  :date: '2020-12-08'
  :category: blog
  :title: Incoming! 6PN Private Networks
  :author: thomas
  :thumbnail:
  :alt:
  :link: blog/incoming-6pn-private-networks
  :path: blog/2020-12-08
  :body: "\n\nMore often than not, modern applications are really ensembles of cooperating
    services, running independently and transacting with each other over the network.
    At Fly.io, we’d like it to be not just possible to express these kinds of applications,
    but pleasant, perhaps even boring.\n\nUp till now, that’s been a hard promise
    for us to fulfill, because services deployed on Fly.io ran as strangers to each
    other. You could arrange to have a front-end cache service talk to a backend app
    service, but they’d need to rendezvous through public IP addresses. More frustratingly,
    you’d need to secure their connection somehow, and the best answer to that is
    usually [mTLS](https://twitter.com/colmmacc/status/1057018254940504064) and certificates.
    Ack! Thbhtt!\n\nIt shouldn’t be this hard. Fly.io is fully connected through a
    WireGuard mesh joining every point in our network where services can run. We already
    promise a secure transport for your packets. You might derive satisfaction from
    running your own CA, and if that’s your thing, we’re not here to judge. But you
    shouldn’t have to. And now you don’t.\n\n##  Introducing 6PN\n6PN (for IPv[6]
    [P]rivate [N]etworking) is our answer to the basic “VPC” feature most cloud providers
    offer. It’ll soon be on by default and requires no additional configuration. A
    6PN network connects all the applications in a Fly.io organization.\n\nEvery instance
    of every application in your organization now has an additional IPv6 address —
    its “6PN address”, in `/etc/hosts` as `fly-local-6pn`. That address is reachable
    only within your organization. Bind services to it that you want to run privately.
    \n\nIt’s pretty inefficient to connect two IPv6 endpoints by randomly guessing
    IPv6 addresses, so we use the DNS to make some introductions. Each of your Fly
    apps now has an internal DNS zone. If your application is `fearsome-bagel-43`,
    its DNS zone is `fearsome-bagel-43.internal` — that DNS resolves to all the IPv6
    6PN addresses deployed for the application. You can find hosts by region: `nrt.fearsome-bagel-43.internal`
    are your instances in Japan. You can find all the regions for your application:
    the TXT record at `regions.fearsom-bagel-43.internal`. And you can find the “sibling”
    apps in your organization with the TXT record at `_apps.internal`. \n\nTo fully
    enable this feature, you need to add a snippet of config to your `fly.toml`:\n\n```output\n[experimental]\n
    \ private_network = true\n```\n\n## Some Examples\n### Caching services\n\nLet’s
    say we want to run a high-capacity sharded nginx cache. We can create an almost-vanilla
    `nginx.conf` with an `upstream` for our cache nodes:\n\n```output\nupstream shards
    { \n  hash \"${scheme}fearsome-bagel-43${request-uri}\" consistent;\n  # SHARDS
    \ \n}\nserver { \n  listen 8080 default_server;\n  location / { \n    proxy_pass
    http://shards/;\n  }\n}\n```\n\nIn the `Dockerfile` for this application, we can
    include a script that looks up the 6PN addresses for our application — in bash,
    you can just use `dig aaaa fearsome-bagel-43.internal +short` to get them. Substitute
    them into `nginx.conf` as server lines:\n\n`server [fdaa:0:1:a01:a0a:dead:beef:2]:8080
    ;`\n\n… and then reload nginx. The `consistent` hashing feature in nginx balances
    traffic across your shards, and minimizes disruption as instances join and leave.\n\n###
    Databases\n\nHere’s a simpler example: let’s run a Redis server for the apps in
    our organization. We can start from the standard Redis Dockerfile — `FROM redis`,
    and write a trivial `start.sh`:\n\n```output\n#!/bin/sh\nredis-server —bind $(grep
    fly-local-6pn /etc/hosts | awk ‘{print $1}’)\n```\n\nCreate a Fly app, like `redis-bagel-43`
    (I don’t know what it is with me today). The rest of your apps will see it once
    it’s deployed, as `redis-bagel-43.internal`. \n\nThe nice thing about this is
    that Redis is default-locked to your organization, just as a consequence of how
    6PN works. You could set up TLS certificates to authenticate clients, but if you’re
    not doing something elaborate with your organization, there’s probably no need.\n\n###
    Messaging with [NATS](https://nats.io/)\n\nOr, how about linking up all your applications
    with a global messaging fabric? You could use that to build a chat app, or to
    ship logs, or to build event-driven applications. Once again: we can use an almost-verbatim
    vendor Dockerfile:\n\n```output\nFROM nats:2.1.9-scratch\nADD nats.conf /etc/nats.conf\nCMD
    [\"-c\", \"/etc/nats.conf\"]\n```\n\nAnd the only interesting part of that configuration:\n\n```output\ncluster:
    {\n    host: \"fly-local-6pn\",\n    routes: [\n        \"nats-route://nats-bagel-43.internal:4248\"\n
    \   ],\n}\n```\n\nNATS will configure itself with available peers, and your other
    applications can get to it at `nats-bagel-43.internal`. \n\n## Behind The Scenes\nI’ll
    take a second to explain a bit about how this works. Skip ahead if you don’t care!\n\nIPv6
    addresses are big. You just won’t believe how vastly, hugely, mind-boggling big
    they are. Actually, no, you will; they’re 16 bytes wide. So, just “pretty big”.\n\nWe
    use that space to embed routing and access information; an identifier for your
    organization, an identifier for the Fly host that your app is running on, and
    an identifier for the individual instance of your app. These addresses are assigned
    directly by our orchestration system. You’ll see them in your instance, on `eth0`;
    they’re the addresses starting in `fdaa`. \n\nWe route with a sequence of small
    BPF programs; they enforce access control (you can’t talk to one 6PN network from
    another), and do some silly address rewriting footwork so that we can use WireGuard’s
    cryptokey routing to get packets from one host to another, without running a dynamic
    routing protocol. \n\nIt’s a boring detail, but in case you’re wondering: our
    service discovery system populates a database on each host that we run a Rust
    DNS server off of, to serve the “internal” domain. We inject the IP of that DNS
    server into your `resolv.conf` — the IP address of that server is always `fdaa::3`.
    \n\n## Where Our Heads Are At\nThere’s a theme to the way we build things at Fly
    (or at least, Kurt has a theme that he keeps hitting us over the head with). We
    like interesting internals — WireGuard, Firecracker, Rust, eBPF — but boring,
    simple UX. Things should just work, in the manner you’d hope they would. Managing
    Fly.io app ensembles shouldn’t even be close to anyone’s full-time job. \n\nSo
    we’ve kept 6PN as boring as we can. You can make things interesting and weird
    if you want! Run a [Serf cluster](https://www.serf.io/) between all your apps!
    Boot up [Consul](https://www.consul.io/) or [etcd](https://etcd.io/). [Set up
    a CA](https://github.com/FiloSottile/mkcert). You can make service discovery and
    security as interesting as you want. We’re going to try to stay out of the way.\n\nYou
    might be able to guess what our next steps are: we’re going to make it boring
    to connect other networks and services to your private network.  Follow us on
    [community.fly.io](https://community.fly.io/) for early announcements of new networking
    features. \n\n<%= partial \"shared/posts/cta\", locals: {\n  title: \"We got private
    networking,&nbsp;y’all\",\n  text: \"Launch your Docker apps on Fly and get baked
    in, secure private networking between instances.\", \n  link_url: \"https://fly.io/docs/speedrun/\",\n
    \ link_text: \"Try it for free&nbsp;&nbsp;<span class='opacity-50'>&rarr;</span>\"\n}
    %>\n"
- :id: blog-we-cut-bandwidth-prices-go-nuts
  :date: '2020-12-03'
  :category: blog
  :title: We cut bandwidth prices. Go nuts.
  :author: kurt
  :thumbnail:
  :alt:
  :link: blog/we-cut-bandwidth-prices-go-nuts
  :path: blog/2020-12-03
  :body: "\n\n<p class=\"lead\">Fly.io turns your Docker apps into Firecracker VMs
    and runs them all over the world. We're now charging $0.02 per GB for outbound
    data transfer from North America and Europe, and $0.04 per GB almost everywhere
    else.</p>\n\nWhen we launched Fly way back in March, we charged $0.085 per GB
    to send data out from North America and Europe, and $0.14 per GB for Asia Pacific.
    Our volume has increased _dramatically_ and pushed our unit costs down, so we
    lowered our prices as of December 1st 2020. The net result is 75% cheaper bandwidth
    for apps running on Fly.io <sup><a href=\"#india\" id=\"india-ref\">[1]</a></sup>.\n\n##
    The joy of bandwidth pricing\n\nIf you shop around, you'll find book stores-turned-hosting-providers
    who charge their captives exorbitant fees to move data around. It's high margin
    and locks people in. What more could you ask for?\n\nYou'll also find companies
    who don't charge for bandwidth ... directly. They do, but it's not a line item
    on anyone's bill.\n\nWe've gone back and forth on the right way to price bandwidth
    into our services. Pricing projects are my favorite kind of scope creep. It's
    easy to go from \"what should this cost?\" to \"how do we want people to behave?\"
    to \"what even is consciousness?\"\n\nUltimately, we want to accomplish two things:\n\n1.
    It should scale well, from small mostly text based web apps to large, intense
    apps that handle a lot of data. You should be able to build a CDN on Fly.io without
    worrying too much about your bandwidth costs. You should also be able to run a
    hobby project for close to free.\n2. Transparency and predictability are good.
    People should be able to glance at pricing, do minimal mental math, and judge
    how well it works for their app.\n\nThese are at odds with each other! So we've
    made a few trade offs.\n\n## Making it work for everyone\nNormally, scaling bandwidth
    pricing means graduated tiers or minimum commits with overage. Those are complex,
    though, and hard to wrap your head around (and, depending on who you're buying
    from, hard to even get in writing). So we decided to charge everyone the same
    price. Customers with intense, high volumes apps will pay the same bandwidth rates
    as individual devs on Fly.io.\n\nThose high volume tiers are important for attracting
    the right customers, though. People who build video communications apps, game
    servers, and photo services need to keep bandwidth prices low so they can make
    money. Normal cloud bandwidth pricing (including our previous pricing) is much
    too expensive for these kinds of apps.  \n\nWhat we've really done is make a one
    tier pricing scheme with the lowest price we can charge and still make reasonable
    margins. If you've ever wondered what reasonable margins are, the answer is 70%.
    We need to make a little more than three times our costs to keep doing what we're
    doing.\n\n## Some light math\n\nIf you do the margin math, you'll estimate that
    our cost to deliver 1GB of data in North America is about $0.006 per GB. In Singapore
    it's about $0.012 per GB. These are pretty close, but they aren't exact. We have
    roughly 11 different bandwidth rates, depending on the facility and region we're
    running servers in. We also pay for bandwidth between servers and regions. One
    GB of data from Singapore to an end-client in New York City has two different
    \"costs\". <sup><a id=\"hops-ref\" href=\"#hops\">[2]</a></sup>\n\nInflicting
    that complexity on you all would help with margin control, but ugh. What we've
    done instead is set a blended price that fits most apps running on Fly.io, and
    decided to just eat the extra cost from outliers. If you want to exploit that,
    run an app in Sydney with a whole bunch of users in India. We'll lose money on
    your app and you will win one round of capitalism.\n\nSince we tend to favor transparency
    and predictability over \"price that scales well\", we've ended up with pricing
    that will be too high for some customers. Which is fine, we're growing, but we
    have a plan for that. If these bandwidth prices don't work for your use case,
    and you can commit to a large amount of data transfer each month, we'll lower
    these prices for _everyone_.\n\n## The many meanings of the word \"free\"\n\nWe
    should also talk about free bandwidth. Free can be more complicated than transparent
    pricing. Companies that pitch free bandwidth _typically_ mean free for a narrow,
    low volume use case. When your volume goes up, either because you're building
    a YouTube competitor or because your self hosted community was featured on CNN,
    a friendly sales person will ring you up and offer an enterprise package to ensure
    continued service quality. Or they shift traffic to lower cost (and less reliable)
    networks. Either way, it's not really my favorite surprise.\n\nThat said, when
    I run side projects, or apps that don't serve much data, I don't really want to
    think about how much bandwidth is costing me. We created a free tier to cover
    this, you get 160GB to use each month before you start paying us. We hope that's
    enough for your side projects (and if it's not, [tell us what you're working on](https://community.fly.io)).\n\n\n##
    We don't really want to make money on bandwidth\n\nGo build yourself a CDN. Or
    the next YouTube. Or just make a little family photo album app in Elixir. We'll
    take it, launch it in Firecracker VMs, and charge you very little for bandwidth.\n\n<hr>\n\n<ol>\n<li
    id=\"india\"><a href=\"#india-ref\">^</a> Bandwidth is cheaper everywhere except
    India! Outbound traffic from India is $0.12 per GB. Very few apps on Fly.io send
    much traffic to India. This will hopefully change and bring the price down.</li>\n<li
    id=\"hops\"><a href=\"#hops-ref\">^</a> We pay for outbound transit from Singapore,
    and then outbound transit from New York City. The last mile is \"cheap\" but Singapore
    to NYC costs us the same as Singapore to the internet at large. In some cases,
    we pay twice in a single region.</a>\n"
- :id: blog-the-november-fly-changelog
  :date: '2020-11-30'
  :category: blog
  :title: The November Fly Changelog
  :author: dj
  :thumbnail:
  :alt:
  :link: blog/the-november-fly-changelog
  :path: blog/2020-11-30
  :body: "\n\n<p class=\"lead\">\n\nMore example applications, enhancements to the
    Fly command line and more. This is what's new on Fly in November 2020.  \n\n</p>\n\n##
    Example Applications\n\nWe've been focussing on getting more example applications
    for Fly into your hands. Each guide is complete with a walkthrough of how they
    were put together so you can see the power of Fly in action. Or you can hop straight
    to the Github repositories for each and use the files there to deploy immediately.
    There are guides for:\n\n* [Gogs - the standalone Git server](https://fly.io/docs/app-guides/git-gogs-server/)\n*
    [MQTT and Mosquitto messaging](https://fly.io/docs/app-guides/mqtt/)\n* [MinIO's
    S3 compatible object storage](https://fly.io/docs/app-guides/minio/)\n* [Node-RED's
    web-based flow programming](https://fly.io/docs/app-guides/node-red/)\n* [Redis
    with TLS support](https://fly.io/docs/app-guides/redistls/)\n* [Redis without
    TLS](https://fly.io/docs/app-guides/redis/)\n\nOne thing this set of Guides do
    is make use of the - currently in preview - persistent Volumes support for Fly
    apps, aka writable disk space. They are used here for everything from database
    persistence and data storage, to preserving state in programming environments
    and queuing up messages. And more example applications are on the way. \n\n##
    Importable secrets\n\nWe've added an `import` command to `fly secrets` to let
    you import secrets in bulk into an app. Just create a \"key=value\" file of settings
    and pipe it into `fly secrets import` and they will be transferred to the app
    in one block. As an added bonus, you can create multiline secrets by surrounding
    that value in two pairs of three double quotes:\n\n```\nkey=\"\"\"This is an\nexample
    of a \nmultiline value.\"\"\"\n```\n\n## Other changes\n\nThere's a whole bunch
    of other enhancements and bug fixes in the Fly CLI. More options in the `fly list
    apps` command to help filter and sort through your applications. More JSON output
    to assist developers building automation. The Deno builtin now supports a Version
    setting which defaults to 1.5.4; as newer Deno release appear it makes it easier
    to either pin to an older version or track the latest developments. Read the full
    changelog below for more details.\n\n\n<p class=\"callout\">\n\nThis is the Fly
    Changelog where we list all significant changes to the Fly platform, tooling,
    and websites. You can also use the RSS feed of just changelog posts available
    on [fly.io/changelog.xml](/changelog.xml) or consult our dedicated [ChangeLog](/changelog/)
    page with all the recent updates.\n\n</p>\n\n<!-- start -->\n\n## Over November\n\n**Fly
    Platform**\n\n- ++ Virtual machines: Upgraded to Firecracker v0.23.\n- ~~ Improved
    handling of the connections being shutdown during restarts.\n- ~~ Improved counting
    of connections/actions to prevent restarts causing tasks to prematurely end.\n-
    ~~ Fixes for slow downloads triggering timeouts.\n- ~~ Improved isolation of application's
    context and connections.\n\n**Fly Web**\n- ++ New guides in the [Example Applications](https://fly.io/docs/guides/#app).\n\n##
    30th November\n\n**flyctl: Version [0.0.151 released](https://github.com/superfly/flyctl/releases/tag/v0.0.151)**\n\n-
    ++ Deno builtin now supports Version as a setting, the default is now 1.5.4.\n-
    ++ Support `--json` for `list orgs` and `builtins list`.\n- ++ Crash logs now
    show the previous 30 lines of log \n- ++ `list apps` now shows humanized time
    (add -e/--exact for precise time)\n- ++ `list apps` now sorted by name (use --sort
    created to sort by creation date)\n- ~~ Removed black text from titles (User contribution:
    thanks @fornwall).\n- ~~ Fixes for internal port handling in init.\n\n## 16th
    November\n\n**flyctl: Version [0.0.150 released](https://github.com/superfly/flyctl/releases/tag/v0.0.150)**\n-
    ++ Removed logs deduplicator (no longer needed).\n\n## 12th November\n\n**flyctl:
    Version [0.0.149 released](https://github.com/superfly/flyctl/releases/tag/v0.0.149)**\n-
    ~~ Fixed fly dash [metrics] command.\n\n**flyctl: Version [0.0.148 released](https://github.com/superfly/flyctl/releases/tag/v0.0.148)**\n-
    ~~ Improved image resolution for deployment.\n\n## 11th November\n\n**flyctl:
    Version [0.0.147 released](https://github.com/superfly/flyctl/releases/tag/v0.0.147)**\n-
    ++ Added secrets import (from stdin) and multiline secrets import.\n- ~~ Fixed
    no change in secrets false error.\n\n## 4th November\n\n**flyctl: Version [0.0.146
    released](https://github.com/superfly/flyctl/releases/tag/v0.0.146)**\n- ++ Added
    progress spinner to resume.\n- ++ Show the initial region of a new app when running
    `fly init`.\n- ~~ improved error handling when API server is not available.\n\n\n\n"
- :id: blog-more-appkata-examples-to-try-on-fly
  :date: '2020-11-27'
  :category: blog
  :title: More Appkata Examples to Try on Fly
  :author: dj
  :thumbnail:
  :alt:
  :link: blog/more-appkata-examples-to-try-on-fly
  :path: blog/2020-11-27
  :body: "\n\n<p class=lead>\n\nThere are two new additions to our Appkata collection
    of example apps, one's a secured Redis and the other is Node-RED, a powerfule
    online application construction kit. Try them out today!\n\n</p>\n\n\nThe latest
    additions to our [Appkata collection of example apps](https://fly.io/blog/appkata-example-apps-on-fly/)
    have landed and they cover how to fit Fly to your selected applications and are
    some super useful apps anyway. So let's dive in.\n\n## Node-RED\n\nEver wanted
    to be able to dive into your cloud application and graphically wire up a dashboard
    or data transfer? You can with Node-RED, a GUI flow programming environment that
    hails from the world of IoT. It's built on top of Node and has a vast library
    of modules so you can create flows which bridge MQTT, Redis, Graphana, and more
    with your own logic. Or you can turn the data into a web dashboard. It's a powerful
    tool and the new Appkata guide will get it up and running quickly.\n\n* Read the
    [guide](https://fly.io/docs/app-guides/node-red/), browse the [example](https://github.com/fly-apps/appkata-noder-red/)
    and \ndiscuss it in the [Fly Community](https://community.fly.io/t/node-red-on-fly-now-an-appkata-example/413)\n\n##
    Redis with TLS\n\nOur first set of examples included Redis as a simple service,
    but we wanted to give you a more solid version. Enter [Redis with TLS](https://fly.io/docs/app-guides/redistls/).
    This uses `mkcert` to make it as quick and simple as possible to create TLS certificates
    so you can secure your Redis. It uses techniques I wrote about on the developer
    site [dev.to](https://dev.to/codepope/certificates-for-your-cloud-backend-50ga).\n\n*
    Read the [guide](https://fly.io/docs/app-guides/redistls/), browse the [example](https://github.com/fly-apps/appkata-redistls/)
    and \ndiscuss it in the [Fly Community](https://community.fly.io/t/new-redistls-appkata-example/405)\n\n<div
    class=\"callout\">\n\n## Community.fly.io\n\nIf you follow the links to [community.fly.io](https://community.fly.io),
    you will see that our new community site is a great place to ask questions, look
    for hints and tips and get your Fly apps running at their best. If you have a
    Fly account, sign in today. It's easy as we've enabled single sign-on between
    Fly and the community site making it simple to participate.\n\n</div>\n"
- :id: blog-appkata-example-apps-on-fly
  :date: '2020-11-18'
  :category: blog
  :title: Appkata - Example Apps on Fly
  :author: dj
  :thumbnail:
  :alt:
  :link: blog/appkata-example-apps-on-fly
  :path: blog/2020-11-18
  :body: "\n\n<p class=\"lead\">\n\nLearn by example with our new example apps for
    Fly. We're starting off with databases, object storage, Git services, and event
    messaging and in the process we'll learn how to make any app Fly.\n\n</p>\n\nTo
    build a modern application, you need to know how to take down your technical challenges
    in style. It's a style we call Appkata! Over the coming weeks, we'll be sharing
    with you the moves you need to deploy apps onto Fly that let you build faster.
    This week we kick off with four apps for your backend that give you a fast key/value
    database, an accessible S3 storage node, your own private Git server and a lightweight,
    powerful event-driven messaging server.\n\n\n## Redis\n\nWe already have a [Redis
    on Fly](https://fly.io/docs/reference/redis/), but this [Redis](https://redis.io)
    is all your own to run in whatever region or regions you need it in.  It gives
    you complete control of the server and makes features like pubs available. This
    first Redis example shows how you build on existing Docker images and can customize
    them for better performance of Fly. It also shows how to use the Volumes feature
    of Fly to add persistent storage to your Redis so it can be restarted and recover
    like nothing had happened.\n\n* Read the [guide](https://fly.io/docs/app-guides/redis/),
    browse the [example](https://github.com/fly-apps/redis), and discuss it in the
    [Fly Community](https://community.fly.io/t/new-redis-example/366).\n\n## MinIO\n\nTalking
    about the persistent storage that Volumes offer got us to thinking that we should
    expose that directly to the world, and what better way than turning it into an
    S3-compatible object store (yes, yes, there are better ways, but everyone knows
    how to use S3). For this we reached for [MinIO](https://min.io), a free software
    implementation of the S3 platform. In this Appkata example, we set up a single
    node for MinIO with an attached persistent volume and access it over Fly's TLS
    edge connections, through the web for admin and through the MinIO client. \n\n*
    Read the [guide](https://fly.io/docs/app-guides/minio/), browse the [example](https://github.com/fly-apps/appkata-minio),
    and discuss it in the [Fly Community](https://community.fly.io/t/appkata-minio-s3-compatible-storage-with-fly/389).\n\n##
    Gogs\n\nAnother way to expose persistent storage, if you wave your hands abstractly
    enough, is Git which is more typically thought of some sort of source code control
    system. One of the lightest ways to get a useful Git service is with the open-sourced
    [Gogs](https://gogs.io) server, designed to be as painless as possible. The Appkata
    example for Gogs shows how you can deploy it, using its SQLite3 database option
    and Fly's volumes, to have your own private Git server, with issue tracker, web
    hooks and other modern development essentials, up and running in no time.\n\n*
    Read the [guide](https://fly.io/docs/app-guides/git-gogs-server/), browse the
    [example](https://github.com/fly-apps/appkata-gogs), and discuss it in the [Fly
    Community](https://community.fly.io/t/gogs-standalone-git-service-as-a-fly-example/358).\n\n\n##
    MQTT\n\nMessaging between apps and devices makes your Appkata style flow smoothly.
    Now, you can use Redis to pass messages, but there's plenty of other messaging
    platforms about which have easily accessible pubs and quality of service features.
    One of those is [MQTT](https://mqtt.org/), a protocol developed for Internet of
    Things applications which is also at home connecting apps. We've put together
    an Appkata example which uses [Mosquitto](https://mosquitto.org), an open source
    MQTT broker, and brings it online with TLS-encrypted connections and user/password
    authentication, all backed by a Fly Volume so the broker can persist messages
    when things are busy or slow.\n\n* Read the [guide](https://fly.io/docs/app-guides/mqtt//),
    browse the [example](https://github.com/fly-apps/appkata-mqtt), and discuss it
    in the [Fly Community](https://community.fly.io/t/appkata-mqtt/390).\n\n \nSo,
    there are four example apps, each of which can be used as part of your app fighting
    style. We'll be adding new examples and building on these examples over the coming
    weeks.\n\n<div class=\"callout\">\n\n## Community.fly.io\n\nIf you follow the
    links to [community.fly.io](https://community.fly.io), you will see that our new
    community site is a great place to ask questions, look for hints and tips and
    get your Fly apps running at their best. If you have a Fly account, sign in today.
    It's easy as we've enabled single sign-on between Fly and the community site making
    it simple to participate.\n\n</div>\n"
- :id: blog-fly-answers-questions-suspend-resume-restart-and-redis
  :date: '2020-10-30'
  :category: blog
  :title: Fly Answers Questions - Suspend, Resume, Restart and Redis
  :author: dj
  :thumbnail:
  :alt:
  :link: blog/fly-answers-questions-suspend-resume-restart-and-redis
  :path: blog/2020-10-30
  :body: "\n\n<p class=\"lead\">\n\nWe get asked questions about Fly in a lot of places
    on the web which we answer. But, not everyone is everywhere on the web, so with
    Fly Answers Questions, we bring those answers to you. If you have questions about
    Fly, why not ask [@flydotio](https://twitter.com/flydotio) on Twitter, or drop
    a query in the [Fly Community](https://community.fly.io/).\n\n</p>\n\n**Q: I'd
    like to park my Fly application for a little while so it isn't consuming resources.
    How can I do this without destroying the application and redeploying it later?**\n\n**A:**
    As you'll have noticed, Fly applications stay running all the time ready to service
    your traffic. You can, as you say, destroy the application and redeploy it, but
    you'll lose configuration with that process. \n\nWhich is why the [`fly suspend`](https://fly.io/docs/flyctl/suspend/)
    command exists. Rather than destroy the app, it turns the number of instances
    — VMs running the app — down to zero. At this point the application is effectively
    not running, but all its status and settings are intact. \n\nBringing it back
    is a case of running [`fly resume`](https://fly.io/docs/flyctl/resume/) which
    initially brings back one instance  to ensure all is well and then fully redeploys
    the application using its previous configuration. \n\n**Q: Followup question:
    what's the `restart` command then?**\n\n**A:** Well spotted. The [`fly restart`](https://fly.io/docs/flyctl/restart/)
    command is nothing to do with suspended applications. It doesn't trigger any deployments
    either. What it does is go to each instance of your application and restart them
    in place. It's useful for when your app may have got into an indeterminate state
    and you'd like to start as fresh as possible but without redeploying. \n\n**Q:
    I was looking at using Fly's Redis support but got an error when I went to use
    the `subscribe` command. What's up with that?**\n\n**A:** Publish/Subscribe is
    not supported on Fly's Redis implementation. It joins a [small set of unsupported
    Redis commands](https://fly.io/docs/reference/redis/#unsupported-redis-commands).
    Most unsupported commands are related to server or client management. The PubSub
    commands, along with Redis scripting commands and the Geo-based Redis commands
    are currently not available. \n\n**Q: Can I set the minimum number of instances
    for an app to 0?**\n\n**A:** No, Fly applications will always have one or more
    instances running. If you want to go to 0, you have to suspend your application,
    see the first question for more on that.\n\n**Q: Isn't the Fly CLI called `flyctl`?
    You've been using the `fly` command in these answers.**\n\n**A:** The answer to
    that is \"Why not both?\". We're slowly migrating the flyctl command to the quicker
    - to type and say - `fly` command. When you install flyctl now, it creates a symlink
    from `flyctl` to `fly` and you can use either to run the command. Over time, we'll
    move completely to the shorter command, with an eye to back-compatibility for
    your scripts.\n\n_That's it for this **Fly Answers Questions**. Get your questions
    in on community.fly.io or on [Twitter](https://twitter.com/flydotio)._\n\n<div
    class=\"callout\">\n\n## Community.fly.io\n\nIf you follow the links to [community.fly.io](https://community.fly.io),
    you will see that our new community site is a great place to ask questions, look
    for hints and tips and get your Fly apps running at their best. If you have a
    Fly account, sign in today. It's easy as we've enabled single sign-on between
    Fly and the community site making it simple to participate.\n\n</div>\n"
- :id: blog-the-october-fly-changelog-preview-disks-and-dns-and-better-builtins
  :date: '2020-10-27'
  :category: blog
  :title: 'The October Fly Changelog: Preview Disks and DNS and Better Builtins'
  :author: dj
  :thumbnail:
  :alt:
  :link: blog/the-october-fly-changelog-preview-disks-and-dns-and-better-builtins
  :path: blog/2020-10-27
  :body: "\n\n<p class=\"lead\">\n\nPersistent storage is now available, in preview,
    for Fly applications. Builtin builders are more configurable and there's a new
    watch mode for automatically updating app status. All this and more in the Fly
    Changelog for October.\n\n</p>\n\n## Disks in Preview\n\nIf your application needs
    to have persistent storage, then the currently in preview volumes feature is for
    you. The `volumes` command allows you to create persistent disks for your application.
    These persist between restarts, deployments and even the app being suspended.
    Further details about the preview and how to use it are in [the disks preview
    thread in the fly community](https://community.fly.io/t/preview-persistent-disks-for-fly-apps/332).\n\n###
    Env Variables Support\n\nYou can now set environment variables in your `fly.toml`
    file, rather than overloading the secrets feature of Fly. It's an extra section
    in the `fly.toml` file and that's documented in the [Fly configuration reference](http://localhost:4567/docs/reference/configuration/#the-env-variables-section).\n\n##
    Other Changes\n\nThe latest release of Fly's CLI tool has a whole range of other
    improvements: \n\n### Better Builtins\n\nIf you use builtins and have found them
    not quite flexible enough for you, we've now introduced builtin settings that
    allow you to control selected features in the builtins. Examples of this include
    the ability to set permissions in the Deno builtin builder and turn on HTTPS auto-upgrading
    and logging in the static web server. \n\n### Status Watching\n\nIf you're tired
    of repeatedly typing in `fly status` to check your apps status (or hitting cursor
    up and return), the new `--watch` flag will make your life even easier. It switches
    to poll the status every 5 seconds and provides you with a regular update.\n\n<p
    class=\"callout\">\n\nThis is the Fly Changelog where we list all significant
    changes to the Fly platform, tooling, and websites. You can also use the RSS feed
    of just changelog posts available on [fly.io/changelog.xml](/changelog.xml) or
    consult our dedicated [ChangeLog](/changelog/) page with all the recent updates.\n\n</p>\n\n<!--
    start -->\n\n## _27th October_\n\n**flyctl: Version [0.0.145 released](https://github.com/superfly/flyctl/releases/tag/v0.0.145)**\n\n-
    ++ Enhanced builtins support with settings\n- ++ New Python/Procfile builtin available\n-
    ++ Updated Deno builtin with permissions\n- ++ Updated Static website builtin
    with https and logging options\n- ++ New commands for Volumes (persistent disk
    support) in [preview](https://community.fly.io/t/preview-persistent-disks-for-fly-apps/332)\n-
    ++ init will automatically use the default org if only one org\n- ++ init will
    not ask for a port when using a builtin - they already default to 8080\n- ++ Added
    --watch to status to autorefresh status\n- ++ Added --rate to --watch to control
    refresh rate\n- ++ Logging out now checks in case environment variables are set
    which contain API tokens\n- ~~ Fixed wildcard domain support in certs command\n\n\n##
    _22nd October_\n\n**Fly Platform/Web**\n\n- ++ Environment variable settings in
    app config enabled\n\n\n\n\n\n"
- :id: blog-bpf-xdp-packet-filters-and-udp
  :date: '2020-10-20'
  :category: blog
  :title: BPF, XDP, Packet Filters and UDP
  :author: thomas
  :thumbnail:
  :alt:
  :link: blog/bpf-xdp-packet-filters-and-udp
  :path: blog/2020-10-20
  :body: "\n\nImagine for a moment that you run a [content distribution network for
    Docker containers](https://fly.io). You take arbitrary applications, unmodified,
    and get them to run on servers close to their users around the world, knitting
    those servers together with [WireGuard](https://www.wireguard.com/). If you like,
    imagine that content delivery network has an easy-to-type name, perhaps like \"fly.io\",
    and, if you really want to run with this daydream, that people can sign up for
    this service in like 2 minutes, and have a Docker container deployed globally
    in less than 5. Dream big, is what I'm saying.\n\nIt's easy to get your head around
    how this would work for web applications. Your worker servers run [Firecracker
    instances](https://fly.io/blog/sandboxing-and-workload-isolation/) for your customer
    applications; your edge servers advertise anycast addresses and run a proxy server
    that routes requests to the appropriate workers. There are a lot of [details hidden
    there](https://fly.io/docs/reference/architecture/), but the design is straightforward,
    because web applications are meant to be proxied; almost every web application
    deployed at scale runs behind a proxy of some sort.\n\nBesides running over TCP,
    HTTP is proxy-friendly because its requests and responses carry arbitrary metadata.
    So, an HTTP request arrives at an edge server from an address in Santiago, Chile;
    the proxy on that edge server reads the request, slaps an `X-Forwarded-For` on
    it, makes its own HTTP request to the right worker server, and forwards the request
    over it, and this works fine; if the worker cares, it can find out where the request
    came from, and most workers don't have to care.\n\nOther protocols – really, all
    the non-HTTP protocols – aren't friendly to proxies. There's a sort of standard
    answer to this problem: [HAProxy's `PROXY` protocol](https://www.haproxy.org/download/1.8/doc/proxy-protocol.txt),
    which essentially just encapsulates messages in a header that ferries the original
    source and destination socket addresses. But remember, our job is to get as close
    to _unmodified_ Docker containers as we can, and making an application `PROXY`-protocol-aware
    is a big modification.\n\nYou can make any protocol work with a custom proxy.
    [Take DNS](https://fly.io/blog/stuff-your-pi-hole-from-anywhere/): your edge servers
    listen for UDP packets, slap `PROXY` headers on them, relay the packets to worker
    servers, unwrap them, and deliver them to containers. You can intercept all of
    UDP with `AF_PACKET` sockets, and write the last hop packet that way too to fake
    addresses out. And at first, that's how I implemented this for Fly.\n\nBut there's
    a problem with this approach. Two, really. First, to deliver this in userland,
    you're adding a service to all the edge and worker servers on your network. All
    that service does is deliver a feature you really wish the Linux kernel would
    just do for you. And services go down! You have to watch them! Next: it's slow
    — no, that's not true, [modern `AF_PACKET` is super fast](https://www.kernel.org/doc/Documentation/networking/packet_mmap.txt)
    — but it's not fun. That's the real problem.\n\n### Packet filters, more than
    you wanted to know:\n\nPacket filters have a long and super-interesting history.
    They go back much further than the \"firewall\" features the term conjures today;
    at least all the way back to the Xerox Alto. Here follows an opinionated and inaccurate
    recitation of that history.\n\nFor most of the last 20 years, the goal of packet
    filtering was observability (tcpdump and Wireshark) and access control. But that
    wasn't their motivating use case! They date back to operating systems where the
    \"kernel networking stack\" was just a glorified ethernet driver. Network protocols
    were changing quickly, nobody wanted to keep hacking up the kernel, and there
    was a hope that a single extensible networking framework could be built to support
    every protocol.\n\nSo, all the way back in the mid-1980s, [you had CSPF](https://www.hpl.hp.com/techreports/Compaq-DEC/WRL-87-2.pdf):
    a port of the Alto's \"packet filter\", based on a stack-based virtual machine
    (the Alto had a single address space and just used native code) that evaluated
    filter programs to determine which 4.3BSD userland program would receive which
    Ethernet frame. The kernel divided packet reception up into slots (\"ports\")
    represented by devices in `/dev`; a process claimed a port and loaded a filter
    with an ioctl. The idea was, that's how you'd claim a TCP port for a daemon.\n\nThe
    CSPF VM is extremely simple: you can push literals, constants, or data from the
    incoming packet onto a stack, you can compare the top two values on the stack,
    and you can AND, OR, and XOR the top two values. You get a few instructions to
    return from a filter immediately; otherwise, the filter passes a packet if the
    top value on the stack is zero when the program ends. This scaled… sort of… for
    rates of up to a million packets per day. You took a 3-6x performance hit for
    using the filter instead of native kernel IP code.\n\nFast forward 4 years, to
    McCanne, Van Jacobsen and tcpdump. Kernel VMs for filtering are a good idea, but
    CSPF is too simplistic to go fast in 1991. So, swap the stack for a pair of registers,
    scratch memory, and packet memory. Execute general-purpose instructions – loads,
    stores, conditional jumps, and ALU operations – over that memory; the filter ends
    when a RET instruction is hit, which returns the packet outcome. [You've got the
    Berkeley Packet Filter](https://www.usenix.org/legacy/publications/library/proceedings/sd93/mccanne.pdf).\n\nIf
    you're loading arbitrary programs from userland into the kernel, you've got two
    problems: keeping the program from mucking up kernel memory, and keeping the program
    from locking up the kernel in an infinite loop. BPF mitigates the first problem
    by allowing programs access only to a small amount of bounds-checked memory. The
    latter problem BPF solves by disallowing backwards jumps: you can't write a loop
    in BPF at all.\n\nThe most interesting thing about BPF isn't the virtual machine
    (which, even in the kernel, is  [like a page or two of code](https://github.com/openbsd/src/blob/7c39eed1e661644fe7eab894601cc0753a39bcc7/sys/net/bpf_filter.c#L159);
    just a for loop and a switch statement). It's tcpdump, which is a no-fooling optimizing
    compiler for a high-level language that compiles down to BPF. In the early 2000s,
    I had the pleasure of trying to extend that compiler to add demultiplexing, and
    can attest: it's good code, and it isn't simple. And you barely notice it when
    you run tcpdump (and Wireshark, which pulls in that compiler via libpcap).\n\nBPF
    and libpcap were successful (at least in the network observability domain they
    were designed for), and, for the next 20 years, this is pretty much the state
    of the art for packet filtering. Like, a year or two after BPF, you get the [invention
    of firewalls and iptables-like filters](https://en.wikipedia.org/wiki/IPFilter).
    But those filters are boring: linear search over a predefined set of parameterized
    rules that selectively drop packets. Zzz. \n\nSome stuff does happen. In '94,
    Mach tries to use BPF as its microkernel packet dispatcher, to route packets to
    userland services that each have their own TCP/IP stack. Sequentially evaluating
    hundreds of filters for each packet isn't going to work, so [Mach's \"MPF\" variant
    of BPF](https://www.usenix.org/conference/usenix-winter-1994-technical-conference/efficient-packet-demultiplexing-multiple)
    (_note: that paper is an actual tfile_) lets you encode a lookup table into the
    instruction stream, so you only decode TCP or UDP once, and then dispatch from
    a table. \n\n[McCanne's back in the late ‘90s, with BPF+](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.597.3024&rep=rep1&type=pdf).
    Out with the accumulator register, in with a serious 32-bit register file. Otherwise,
    you have to squint to see how the BPF+ VM differs from BPF. The compiler, though,
    is radically different; now it's SSA-form, like LLVM (hold that thought). BPF+
    does with SSA optimization passes what MPF does with lookup tables. Then it JITs
    down to native code. It's neat work, and it goes nowhere, at least, not under
    the name BPF+.\n\nMeanwhile, Linux things happen. To efficiently drive things
    like tcpdump, Linux has poached BPF from FreeBSD. Some packet access extensions
    get added.\n\nThen, around 2011, [the Linux kernel BPF JIT lands](https://lwn.net/Articles/437981/).
    BPF is so simple, the JIT is actually a pretty small change.\n\nThen, a couple
    years later, BPF becomes eBPF. And all hell breaks loose.\n\n### eBPF\n\nIt's
    2014. You're the Linux kernel. If virtually every BPF evaluation of a packet is
    going to happen in JIT'd 64 bit code, you [might as well work from a VM that's
    fast on 64-bit machines](https://lwn.net/Articles/593476/#internals). So:\n\n*
    Out with the accumulators and in with a serious 64-bit register file.\n* What
    the hell, let's just load and store _from arbitrary memory_. \n* While we're at
    it, let's let BPF call kernel functions, and give it lookup tables.\n\n<div class=\"callout\">\nAn
    aside about these virtual machines: I'm struck by how similar they all are — BPF,
    BPF+, [eBPF](https://github.com/iovisor/bpf-docs/blob/master/eBPF.md), throw in
    [DTrace](https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-924.pdf) while you're
    at it. General register file, load/store (maybe with some special memories and
    addressing modes, but less and less so), ALU, conditional branches, call it a
    day.\n\nA bunch of years ago, I was looking for the simplest instruction set I
    could find that GCC would \ncompile down to, and ended up banging out an emulator
    for [the \nMSP430](https://en.wikipedia.org/wiki/TI_MSP430#MSP430_CPU), which
    ended up becoming a site called \n[Microcorruption](https://microcorruption.com/login).
    Like eBPF, the whole MSP430 instruction set \nfits on a page of Wikipedia text.
    And they're not that dissimilar! If you threw compat out the \nwindow — which
    we basically did anyways — and, I guess, made it 64 bits, you could have _used_
    MSP430 as the \"enhanced\" BPF: \nweirdly, eBPF had essentially the same goal
    I did: be easy to compile down to.\n\nEmphatically: **if you're still reading
    and haven't written an emulator, do it.** It's not a hard project! I wrote one
    for eBPF, in Rust (a language I suck at) in about a day. For a simple architecture,
    an emulator is just a loop that decodes instructions (just like any file format
    parser would) and then feeds them through a switch statement that operates on
    the machine's registers (a small array) and memory (a big array). Take a whack
    at it! I'll post my terrible potato eBPF emulator as encouragement.\n</div>\n\nThe
    eBPF VM bears a family resemblance to BPF, but the execution model is radically
    different, **and terrifying**: programs written in userland can now grovel through
    kernel memory. Ordinarily, the technical term for this facility would be \"kernel
    LPE vulnerability\". \n\nWhat makes this all tenable is the new eBPF verifier.
    Where BPF had a simple \"no backsies\" rule about jumps, the kernel now does a
    graph traversal over the CFG to find loops and dead code. Where BPF had a fixed
    scratch memory, [eBPF now does constraint propagation](https://github.com/torvalds/linux/blob/9ff9b0d392ea08090cd1780fb196f36dbb586529/kernel/bpf/verifier.c#L39),
    tracking the values of registers to make sure your memory accesses are in bounds.
    \n\nAnd where BPF had the tcpdump compiler, eBPF has LLVM. You just write C. It's
    compiled down to SSA form, optimized, emitted in a simple modern register VM,
    and JIT'd to x64. In other words: it's BPF+, with the MPF idea tacked on. It's
    funny reading the 90's papers on scaling filters, with all the attention they
    paid to eliminating common subexpressions to merge filters. Turned out the answer
    all along was just to have a serious optimizing compiler do the lifting. \n\nLinux
    kernel developers quickly come to the same conclusion the DTrace people came to
    15 years ago: if you're going to have a compiler and a kernel-resident VM, you
    might as well use it for everything. So, the seccomp system call filter gets eBPF.
    Kprobes get eBPF. Kernel tracepoints gets eBPF. Userland tracing gets eBPF. If
    it's in the Linux kernel and it's going to be programmable (even if it shouldn't
    be), it's going to be programmed with eBPF soon. If you're a Unix C programmer
    like I am, you're kind of a pig in shit.\n\n### XDP\n\n> In astronomy, a revolution
    means a celestial object that comes full circle.\" *Mike Milligan*\n\nRemember
    that packet filters weren't originally designed as an observability tool; researchers
    thought they'd be what you build TCP/IP stacks out of. You couldn't make this
    work when your file transfer protocol ran at 1/6th speed under a packet filter,
    but packet filters today are optimized and JIT'd. Why not try again?\n\nIn 2015,
    [developers added eBPF to TC](https://lwn.net/Articles/638588/), the Linux traffic
    classifier system. You could now theoretically intercept a packet just after it
    hit the socket subsystem, make decisions about it, modify the packet, and pick
    an interface or bound socket to route the packet to. The kernel socket subsystem
    becomes programmable.\n\nA little less than a year later, [we got XDP](https://lwn.net/Articles/682538/),
    which is eBPF running right off the driver DMA rings. JIT'd eBPF is now practically
    the first code that touches an incoming packet, and that eBPF code can make decisions,
    modify the packet, and _bounce it to another interface_ - XDP can route packets
    without the TCP/IP stack seeing them at all.\n\nXDP developers are a little obsessed
    with the link-saturating performance you can get out of using eBPF to bypass the
    kernel, and that's neat. But for us, the issue isn't performance. It's that there's
    something we want the Linux kernel networking stack to do for us — shuttle UDP
    packets to the right firecracker VM — and a programming interface that Linux gives
    us to do that. Why bother keeping a daemon alive to bounce packets in and out
    of the kernel?\n\nFly.io users [register the ports](/docs/reference/configuration/#the-services-section)
    they want their apps to listen on in a simple configuration file. Those configurations
    are fed into distributed service discovery; our servers listen on changes and,
    when they occur, they update a routing map – a simple table of addresses to actions
    and next-hops; the Linux bpf(2) system call lets you update these maps on the
    fly. \n\nA UDP packet arrives and our XDP code checks the destination address
    in the routing table and, if it's the anycast address of an app listening for
    UDP, slaps a proxy header on the packet and shuttles it to the next-hop WireGuard
    interface for the closest worker. \n\nOn the worker side, we're lucky in one direction
    and unlucky in the other. \n\nRight now, XDP works only for ingress packets; you
    can't use XDP to intercept or alter a packet you're sending, which we need to
    do to proxy replies back to the right edge. This would be a problem, except that
    Firecracker VMs connect to their host OS with tap(4) devices – fake ethernet devices.
    Firecrackers transmitting reply packets translates to ingress events on the host
    tap device, so XDP works fine.\n\nThe unlucky bit is WireGuard. [XDP doesn't really
    work on WireGuard](https://lore.kernel.org/netdev/20200813195816.67222-1-Jason@zx2c4.com/T/);
    it only pretends to (with the \"xdpgeneric\" interface that runs in the TCP/IP
    stack, after socket buffers are allocated). Among the problems: WireGuard doesn't
    have link-layer headers, and XDP wants it to; the discrepancy jams up the socket
    code if you try to pass a packet with `XDP_OK`. We janked our way around this
    with `XDP_REDIRECT`, and Jason Donenfeld even wrote a patch, but the XDP developers
    were not enthused, just about the concept of XDP running on WireGuard at all,
    and so we ended up implementing the worker side of this in TC BPF. \n\n### Some
    programming advice\n\nIt's a little hard to articulate how weird it is writing
    eBPF code. You're in a little wrestling match with the verifier: any memory you
    touch, you need to precede with an \"if\" statement that rules out an out-of-bounds
    access; if the right conditionals are there, the verifier \"proves\" your code
    is safe. You wonder what all the Rust fuss was about. (At some point later, you
    remember loops, but as I'll talk about in a bit, you can get surprisingly far
    without them). The verifier's error messages are not great, in that they're symbolic
    assembly dumps. So my advice about writing BPF C is, you probably forgot to initialize
    a struct field. \n\n(If you're just looking to play around with this stuff, by
    the way, I can give you a Dockerfile that will get you a janky build environment,
    which is how I did my BPF development before I started using perf, which I couldn't
    get working under macOS Docker).\n\nThe huge win of kernel BPF is that you're
    very unlikely to crash the kernel with it. The big downside is, you're not going
    to get much feedback from the TCP/IP stack, because you're sidestepping it. I
    spent a lot of time fighting with iptables (my iptables debugging tip: `iptables
    -Z` resets the counters on each rule, and `iptables -n -v -L` prints those counters,
    which you can watch tick) and watching SNMP counters. \n\nI got a hugely useful
    tip from [Julia Evans' blog](https://jvns.ca/blog/2017/09/05/finding-out-where-packets-are-being-dropped/),
    which is a treasure: there's a \"dropwatch\" subsystem in the kernel, and a userland
    \"dropwatch\" program to monitor it. [I extended Dropwatch](https://gist.github.com/tqbf/c0e78e33192f49bf6d9c1aa235a013f4)
    to exclude noisy sources, lock in on specific interfaces, and select packets by
    size, which made it easy to isolate my test packets; Dropwatch diagnosed about
    half my bugs, and I recommend it. \n\nMy biggest XDP/BPF breakthrough came from
    switching from printk() debugging to using perf. Forget about the original purpose
    of perf and just think of it as a performant message passing system between the
    kernel and userland. printk is slow and janky, and perf is fast enough to feed
    raw packets through. A bunch of people have written perf-map-driven tcpdumps,
    and you don't want to use mine, but [here it is](https://gist.github.com/tqbf/341236019f27fb10aac89a3a8a3df5e3)
    (and a [taste of the XDP code that drives it](https://gist.github.com/tqbf/b65d08d68f67d2fa976319f6512893a5))
    just so you have an idea how easy this turns out to be to build with the Cilium
    libraries. In development, my XDP and TC programs have trace points that snapshot
    packets to perf, and that's all the debugging I've needed since.\n\n---\n\nTo
    sum up this up [the way Hannibal Buress would](https://www.chicagomag.com/Chicago-envelope/March-2017/Why-We-Love-Chicago/Mild-Sauce/):
    I am terrible at ending blog posts, and you can now, in a beta sort of way, deploy
    UDP applications on [Fly.io](https://fly.io/blog/stuff-your-pi-hole-from-anywhere/).
    So, maybe give that a try. Or write an emulator or play with BPF and XDP in a
    Docker container; we didn't invent that but you can give me some credit anyways.
    \n\n"
- :id: blog-fly-behind-the-scenes-fresh-logging
  :date: '2020-10-15'
  :category: blog
  :title: 'Fly Behind The Scenes: Fresh Logging'
  :author: dj
  :thumbnail:
  :alt:
  :link: blog/fly-behind-the-scenes-fresh-logging
  :path: blog/2020-10-15
  :body: "\n\n<p class=\"lead\">\n\nFly users are noticing faster, more reliable logs.
    Find out what happened behind the scenes to make that happen in this article.\n\n</p>\n\nSince
    Fly launched, we've been collecting and managing logs for all the applications
    running on the Fly platform. It's a critical but often rarely noted function of
    the platform. When you type `flyctl logs`, behind the scenes, there is  a lot
    of computing power and storage being brought to bear. Over the last weeks, and
    transparently to users, the entire logging platform has been replaced with a whole
    new logging platform and a new approach to working with logs. We talked to the
    person who drove the change, Steve Berryman.\n\n**Dj**: How did this project begin?\n\n**Steve**:
    The previous logging system was built around seven fairly large, centralized Graylog
    servers. Even with all of them running, the volume of logs we got at various times
    of the day couldn't be processed fast enough, and that meant things were dropped.
    \n\n**Dj**: What kind of volume are we talking about?\n\n**Steve**: Between 20,000
    and 30,000 logs per second.\n\nIn theory, we could have just expanded the servers
    and added more power to manage things. The way logs get to customers, though,
    was by polling the Graylog API and polling APIs isn't really the nicest way to
    work with an API. But then there wasn't any other way to get that information
    out other than through the API.\n\n**Dj**: What were the options?\n\n**Steve**:
    We considered Kafka and other message streaming services, but they would have
    been another big tool in the chain. It was pointed out to me by Jerome that there
    was a new Log processing tool called [Vector](https://vector.dev/). It's  written
    in Rust and it's very efficient. Like Logstash, it does all the capture, process,
    and transform of logs. We initially thought of using that to send things to Graylog,
    with Vector running on each server, but we found that Vector didn't support the
    various Graylog protocols. It was then we had an idea.\n\n**Dj**: Which was?\n\n**Steve**:
    Why even bother sending the logs to Graylog if we're processing the logs in Vector
    at each server. Send the logs straight to Elasticsearch and take out the Graylog
    middleman. All Elasticsearch has to do then is index the logs and retrieve them.\n\nVector
    runs on every server and logs go straight from `journald`, or other applications,
    into Vector. There it parses them and runs a number of transforms on them. For
    example, this includes taking fields out from journald logs where we aren't interested
    in them, some regex parsing, transforming the names of things into slightly nicer
    things for the new schema. It  then ships the results to Elasticsearch which happily
    takes them in.\n\n**Dj**: A new schema?\n\n**Steve**: Yes, although people can't
    see it externally, I decided to move us to using ECS, the [Elasticsearch Common
    Schema](https://www.elastic.co/guide/en/ecs/current/ecs-reference.html). It's
    a general log schema they've defined for various purposes, with a lot of common
    fields already defined - there's file fields, log fields, network source and destination
    fields, geo fields, HTTP fields, TLS fields and more.  It also lets us add our
    own fields, so we have fields for Fly app names, Fly alloc id, Fly regions and
    other Fly-related things.\n\nThe good part is that, with all the apps feeding
    logs in according to the schema, it makes searching across apps much easier. Searching
    for say a source IP address across different apps may have meant searching for
    \"src.IP\", \"source-IP\", \"IP.source\" and any other variation. With a common
    schema, we know if there is a source IP address, it'll be in the source field.
    It's nice to know what you're looking for regardless of application.\n\nObviously,
    not everything will follow this schema, but then as soon as you find something
    you want to parse or transform, you can add a bit of config into the config management
    system for Vector. The Vector configuration language is pretty simple, it's a
    bunch of [TOML](https://toml.io/en/) that defines sources, transforms, and sinks.
    Logs come in through the sources. Transforms then modify the log's structure,
    adding or removing fields, or making other changes. The result is then sent on
    to one or more sinks. The important sinks for us is Elasticsearch.\n\nDeploy those
    Vector config changes and the configuration management system will distribute
    to all the servers and from then on, all the logs will reflect that change. That's
    a bit more work than before with Graylog.\n\n**Dj**: Why's that?\n\n**Steve**:
    Graylog's rules and transformations are centralized on Graylog servers, so it
    was one place to change things. I like Graylog a lot, but it is a big, heavy,
    Java, enterprise app that does a lot and where centralization makes sense. But
    it also centralizes the work needed to be performed on logs. \n\nWith Vector,
    we've distributed that work out to all the servers where it barely registers as
    load and we get to manage that with our own configuration system. We can also
    add the hardware that was servicing Graylog to the Elasticsearch fleet to make
    Elasticsearch perform better.\n\nAs an aside, one cool feature of Vector is that
    it allows us to [unit test](https://vector.dev/docs/reference/tests/) configurations
    locally. Along with the [validate](https://vector.dev/docs/administration/validating/)
    command for checking configurations, it means we have the tools to efficiently
    check configurations before deployment, giving us a lot more confidence.\n\n**Dj**:
    How long did this take?\n\n**Steve**: In all, about two weeks alongside other
    work. The actual implementation of getting Vector onto servers and feeding Elasticsearch
    didn't take long at all. The bulk of the work was in the snagging, getting all
    the little issues handled, from configuring mapping and schemas on Elasticsearch
    and getting the encryption support working right, to fixing up various fields'
    content and making it all run smoothly. Also, the Vector Discord channel was very
    helpful too with Timber.io devs participating in the chat.\n\n**Dj**: So we've
    got a distributed log collection and processing platform with Vector. What about
    getting that data to users?\n\n**Steve**: Currently, we have the API servers picking
    up the data from Elasticsearch using the same polling mechanism as before. Now,
    though, we can optimize that and make it more searchable and flexible.\n\n<div
    class=\"callout\">\n\n<h3> How `flyctl` Displays Logs </h3>\n\nWe normally tail
    logs by polling a cursor against a REST endpoint. When a deployment fails, we
    also want to show logs for the failed allocations. So we query our GraphQL API
    for the failed allocations, and for each one of them, the GraphQL resolver gets
    the last N lines of log entries with that allocation's id. The problem was with
    the old logging system, there was a good chance the logs had not been processed
    when this query was made. With the new system, it's fast enough that the logs
    are already available.\n\n</div>\n\nOne of the cool things we'll be able to do
    with the new logging platform - we've not done it yet - is being able to point
    logs at different log service endpoints. Eventually, we hope to be able to send
    logs to Papertrail, Honeycomb, Kafka, possibly anything with a [Vector sink component](https://vector.dev/components/).
    \n\n**Dj**:  Beyond that, any specific plans?\n\n**Steve**: Probably incorporating
    internal metrics into the platform so we can see how efficiently we are handing
    logs and optimizing all our feeds into Vector, especially with Firecracker logs.
    \n\nWe are better set up for the future with this new architecture, so who knows
    what's next. \n\n\n<p class=\"callout\">\n\n_Want to learn more about Fly? Head
    over to our [Fly Docs](/docs/) for lots more, including a [Hands On](/docs/hands-on/start/)
    where you can get a free account and deploy your first app today._\n\n</p>\n"
- :id: blog-stuff-your-pi-hole-from-anywhere
  :date: '2020-10-13'
  :category: blog
  :title: Stuff Your Pi-Hole From Anywhere
  :author: dj
  :thumbnail:
  :alt:
  :link: blog/stuff-your-pi-hole-from-anywhere
  :path: blog/2020-10-13
  :body: "\n\n<p class=\"lead\">\n\nHow do you take the ad-scrubbing Pi-Hole and turn
    it into a globally available app? Quickly and simply with Fly and in this article,
    we'll show you how.\n\n</p>\n\nA Pi-Hole could be the hero of your web connection.
    It blocks advertising at the roots; the DNS roots that is. By setting up your
    own Pi-Hole, you can tip all your local network's requests for known advertising
    domains down it. \n\n## What is Pi-Hole?\n\nThink of Pi-Hole as a firewall for
    popups, overlays, banners and other mental focus-stealing and bandwidth-eating
    advertising. Unlike a firewall which filters through all your traffic, Pi-Hole
    does its work by acting as your DNS server with a difference, a database of advertising
    and tracking hosts and domains. \n\nWhen a machine queries the Pi-Hole DNS for
    something that matches one of the entries in the database, Pi-Hole politely doesn't
    resolve it and returns a dummy address. Ads that are served from ad providers
    just can't load, while embedded ads are blocked from downloading their assets.
    Even dynamically inserted ads get caught by this technique. \n\nOnce you start
    stuffing things in your Pi-Hole, you'll wish you could take it with you everywhere.\n\n##
    Pi-Hole at Home\n\nThe typical Pi-Hole is running on the eponymous single-board
    computer, tucked away somewhere on the home network. There are a lot of good reasons
    to do that. For example, a Pi-Hole can also be a DHCP server for your network.
    That means it can automatically give out its address for DNS resolution to all
    your laptops, phones, aging desktops, smart TVs, and Amazon surveillance devices,
    practically configuring all your home devices to use it.\n\nBut then that makes
    it somewhat hard to take on the road with you, even if you are the most organized
    road warrior to ride the roads. You stop at a coffee shop and then the next hour
    is spent connecting the Pi-Hole to the diner Wi-Fi and reconfiguring it for its
    new environment and, well, you aren't going to get a lot of browsing done. \n\nOf
    course, you only need to point your DNS at the Pi-Hole and if you have a server
    at home which is running Pi-Hole, you could just point your mobile devices at
    that and vanish those ads. To make that easier, the diggers of the Pi-Hole have
    packaged it up as a Docker image that you can run on a machine on the edge of
    your network. There's also a guide for [configuring it with a VPN](https://docs.pi-hole.net/guides/vpn/overview/)
    to make things easier. \n\n## Pi-Hole The Planet!\n\nRemember traveling? We do,
    and we also remember that the further you go from home, the further you are from
    your home server. And that means latency. And we hate latency at Fly. Physics
    dictates though that distance means latency and given how many DNS lookups a modern
    browser or app could be doing, that all mounts up.\n\nAh, you say, I have a cunning
    plan; I will run my Pi-Hole in a datacenter somewhere on a singular server. And
    so you might, but all you are changing then is that the latency depends on the
    distance between you and the datacenter, rather than the distance between you
    and your home.\n\n## What we need is a Pi-Hole that is everywhere.\n\nA global
    Pi-Hole to keep you free from adverts and your connection free from the bandwidth
    hogs would be wonderful but how could you build it?\n\nWell, imagine you had a
    global edge network connected to dozens of data centers around the world. You
    could deploy Pi-Hole as an edge application to that network and get the benefits
    of automatically low latency and global availability. Fly's network is a global
    edge network with data centers around the world...  so we built a global Pi-Hole
    using it.\n\nSeasoned Fly users will know that DNS uses UDP, a sister protocol
    of TCP and a protocol which has not been supported on Fly to date. Well, don't
    tell anyone, but we have UDP support in beta so building a global Pi-Hole was
    a great way to give this a workout.\n\n## From Pi-Hole to Fli-Hole\n\nGetting
    Pi-Hole ready for global filtering doesn't require any changes to Pi-Hole itself.
    We even use the official [Pi-Hole Docker image](https://hub.docker.com/r/pihole/pihole)
    for Fly deployment. Here's our [Dockerfile](https://github.com/fly-apps/pi-hole/blob/main/Dockerfile):\n\n```docker\nFROM
    pihole/pihole:latest\n\nENV INTERFACE eth0\nENV DNSMASQ_LISTENING ALL\n```\n\nIt
    simply takes that image and sets environment variables for which interface to
    listen on, and to allow DNSMasq (a component of Pi-Hole) to listen on all subnets
    for DNS traffic.\n\nThe other part of the configuration is the [`fly.toml`](https://github.com/fly-apps/pi-hole/blob/main/fly.toml)
    file which defines how our application should be deployed to the Fly platform.
    You generate the core of this file with the command `fly init` and when asked,
    let it generate a name, use the Dockerfile builder and set the internal port to
    80.\n\nThere's just one final addition needed, an extra `[[services]]` section
    that looks like this:\n\n```toml\n[[services]]\n  internal_port = 53\n  protocol
    = \"udp\"\n\n  [[services.ports]]\n    port = \"53\"\n```\n\nThis routes external
    UDP traffic on port 53 to the application's internal port 53.\n\nThat's nearly
    all the configuration done. The last step is to set the password on Pi-Hole's
    web interface. This is picked up from an environment variable `WEBPASSWORD`. Fly's
    secrets are passed to running applications as environment variables, so the secure
    way to set this is to run:\n\n```bash\nfly secrets set WEBPASSWORD=\"horse-battery-staple\"\n```\n\nObviously,
    substituting in [your own password](https://www.correcthorsebatterystaple.net/)
    as needed. \n\nThe Fli-Hole is now ready to deploy. Run:\n\n```bash\nfly deploy\n```\n\nAnd
    Fly will place an instance of Fli-Hole in a region close to where you are running
    the commands. If you want to check out the App dashboard, run:\n\n```bash\nfly
    open /admin\n```\n\nWhich will open the dashboard. From there you can log in,
    using the password you set in secrets, to look at the other settings and logs
    gathered by Pi-Hole. \n\n## Going Regional\n\nNow, you could deploy Fli-Hole to
    [every Fly region](/docs/reference/regions/) on the planet, but that's not the
    smart way of doing it. Say you were traveling, in an entirely hypothetical sense
    of course, to Europe. All you need to do is run:\n\n```bash\nfly regions add fra\n```\n\nAnd
    that would add the Frankfurt data center and associated region to the pool and
    start routing nearby traffic through there. Now you are ready to go on a European
    vacation with your Pi-Hole. If you were in the UK, for example, then the edge
    network would send your requests to Fra, via the UK's edge  nodes, for the fastest
    response. \n\nIt's worth pointing out these instances of Pi-Hole have no persistence
    or sharing of settings. Each one is an independent Pi-Hole. It's a strength and
    an easily mitigated weakness; just update the Dockerfile to change settings and
    configuration files and redeploy the application to get permanent global updates.\n\n##
    Through the Fli-Hole\n\nWhat we've shown here is how quick it can be to configure
    an application to run on the Fly platform, from taking a Docker image off the
    shelf, connecting it to the web and UDP, setting configuration and secrets, and
    then sending it out into the world. You may not need a Pi-Hole everywhere, but
    your next big app might need to be everywhere at the drop of a hat.\n"
- :id: blog-maps-apps-and-tracks
  :date: '2020-10-06'
  :category: blog
  :title: Maps, Apps, and Tracks
  :author: dj
  :thumbnail:
  :alt:
  :link: blog/maps-apps-and-tracks
  :path: blog/2020-10-06
  :body: "\n\n<p class=\"lead\">\n\nHow do you make an online map that tracks a Marathon
    runner from a mobile device? And how do you deploy that to the world using Fly?
    Well, we just did that and here's the whole story.\n\n</p>\n\n![The finishing
    line](image.png?1/3&wrap-right)\n\nMarathons, long and hard. Personally, I wouldn't
    be caught doing one, but I'm always up to help someone who is, especially when
    I can use a bit of Fly to do it. Steve, our global infrastructure whiz,  asked
    if I could think of a good way to track Mark, a friend of his, as he did his own
    personal London Marathon, the Markathon\n\nYou see, we're not always hammering
    away at the keyboards at Fly fulfilling our mission to deploy apps globally with
    the lowest latency. Sometimes we're hammering away on keyboards at Fly on side
    projects. And this was a fun one.\n\n## Map Making\n\nLet's start with the route.
    This is likely to be the fiddliest bit of getting an app like this set up as you'll
    need to draw out your exact route. There are apps like [OnTheGoMap.com](https://onthegomap.com/#/create)
    which will let you create the route. From there Steve exported Mark's route to
    GPX format. Then it was over to Google Maps, specifically the Your Places/My Maps
    view, which could load in the GPX format data. It was there that Steve added mile
    markers in and  exported the file as KML. We now had data to work with, stored
    as `Markathon.kml` - if you want to follow along, you'll find all the code and
    assets in the [Markathon](https://github.com/markathon-london/markathon) Github
    Repository.\n\n```\nmarkathon+\n         |\n         + mappages - assets - Markathon.kml\n```\n\nFor
    displaying the map, we went with [Leaflet.js](https://leafletjs.com/), the rightly
    popular interactive JavaScript map viewer. While Leaflet has a range of plugins,
    we found it easiest to work with [leaflet-kml](https://github.com/windycom/leaflet-kml),
    a branch of the official KML plugin, nicely isolated for better maintenance. All
    we needed to do was copy the `L.KML.js` file into our `mappages` directory. \n\nNext
    up, we had to pull this all into a web page, `index.html`. For this, we took our
    cues from the leaflet-kml package's [README example](https://github.com/windycom/leaflet-kml/blob/master/README.md).
    It's a simple minimal map displayer:\n\n```html\n<head>\n        <link rel=\"stylesheet\"
    href=\"https://unpkg.com/leaflet@1.4.0/dist/leaflet.css\" />\n        <script
    src=\"https://unpkg.com/leaflet@1.4.0/dist/leaflet.js\"></script>\n        <script
    src=\"/L.KML.js\"></script>\n        <script src=\"/mark.js\"></script>\n    </head>\n
    \   <body>\n        <div style=\"width: 100vw; height: 100vh\" id=\"map\"></div>\n
    \       <script type=\"text/javascript\">\n            // Make basemap\n            const
    map = new L.Map('map', { center: new L.LatLng(58.4, 43.0), zoom: 11 });\n            const
    osm = new L.TileLayer('https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png');\n\n
    \           map.addLayer(osm);\n\n            // Load kml file\n            fetch('assets/Markathon.kml')\n
    \               .then(res => res.text())\n                .then(kmltext => {\n
    \                   // Create new kml overlay\n                    const parser
    = new DOMParser();\n                    const kml = parser.parseFromString(kmltext,
    'text/xml');\n                    const track = new L.KML(kml);\n                    map.addLayer(track);\n\n
    \                   // Adjust map to show the kml\n                    const bounds
    = track.getBounds();\n                    map.fitBounds(bounds);\n                });\n
    \       </script>\n    </body>\n</html>\n```\n\nThe only change is the name of
    the asset we're using. With that in  place, we're ready to show some maps. Head
    back to the markathon directory:\n\n```\nmarkathon+\n         |\n         + mappages
    + assets - Markathon.kml\n                    |- index.html\n                    +-
    L.KML.js \n```\n\nNow, we need to serve that up locally. I recommend [`serve`](https://www.npmjs.com/package/serve)
    for that. It's a neat little node server you can install with `npm install -g
    serve`. Once installed, run `serve mappages` and head to [`localhost:5000`](http://localhost:5000)
    in your browser and you should see a map. It'll appear to be covered with broken
    image icons though because we have no images for those mile-markers. Looking in
    the KML file, we find lots of `<href>images/icon-1.png</href>` defining those
    mile-marker images, so we found a set and after renaming them appropriately, popped
    them in the images directory.\n\n## Tracking Down Mark\n\nThere's an elephant
    in the room at this point. We haven't talked about how we track Mark as he does
    his run. We really don't want to get into writing our own mobile app; ideally
    we want nothing to do with that. Well, the good news is that [OwnTracks](https://owntracks.org/)
    exists. \n\nOwnTracks is an open source app you can install on iOS or Android
    and it can be configured to send your location to an MQTT server, or as in our
    case, an HTTP endpoint. You can read about [configuring OwnTracks HTTP settings](https://owntracks.org/booklet/tech/http/)
    in its booklet. We'll come back to that though because right now, we need a server.\n\n##
    Services Noted\n\nWhile the map view is static, we need to be able to collect
    Mark's location data and display it to people viewing the map. We're using Node
    and Express here, so first, we have to set that up. Run `npm init` and hit return
    for the default for most answers except for the entry point, which you should
    set to `server.js`. Then run:\n\n```jsx\nnpm install express body-parser\n```\n\nWe
    can now create `server.js` . There's a bit of preamble in the server.js to bring
    those packages in:\n\n```jsx\nconst express = require('express')\nconst bodyParser
    = require(\"body-parser\");\nconst app = express()\nconst port = process.env.PORT
    || 3000\napp.use(bodyParser.urlencoded({ extended: false }));\napp.use(bodyParser.json());\n```\n\nOn
    to the business of the app. We need to set up an endpoint for the OwnTracks app
    to POST location data to. This is the `/log/` endpoint:\n\n```jsx\nsharedlat=0;\nsharedlon=0;\nmatchtopic='owntracks/mark/Markphone1';\n\napp.post('/log/',
    (req, res) => {\n  owt=req.body;\n  if(owt._type==\"location\") {\n    if(owt.topic==matchtopic)
    {\n      sharedlat=owt.lat;\n      sharedlon=owt.lon;\n      console.log(`Saved
    ${sharedlat},${sharedlon}`)\n    }\n  } else {\n    console.log(`Not the one ${owt._type}`)\n
    \   console.log(owt)\n  }\n  res.json([])\n})\n```\n\nIn this version, for simplicity,
    we're going to keep Mark's latitude and longitude in memory; in `sharedlat` and
    `sharedlon`. Note that when we need it, we can use a Redis cache on Fly which
    can not only save the data but also let us share it between instances of our app
    in case thousands start to view. \n\nWe're only tracking one runner in this app
    so the `matchtopic` is hard coded to Mark. When Mark sets up his OwnTracks, that
    `matchtopic` is his id to post his location. When someone (we assume Mark, but
    note we haven't turned on any authentication) posts to the `/log` endpoint, the
    posted JSON is parsed. If the type is \"location\" and the topic is the same as
    `matchtopic`, then the lat and lon are extracted from the POST body and stored
    in our `sharedlat` and `sharedlon` variables. Oh yes, and do, do respond to that
    POST with something. \n\nHow do other users find out where Mark is? Simply by
    asking where:\n\n```jsx\napp.get('/where/',(req,res) => {\n  res.json({ lat:sharedlat,lon:sharedlon})\n})\n```\n\nThere
    are two other tasks for this server, serving our `mappages` directory up, and
    listening on our selected port:\n\n```jsx\napp.use(express.static('mappages'))\n\napp.listen(port,
    () => {\n  console.log(`Service listening on ${port}`)\n})\n```\n\nOur directory
    now looks like this:\n\n```\nmarkathon+\n         | server.js\n         + node_modules\n
    \        | package.json\n         | package-lock.json\n         + mappages + assets
    - Markathon.kml\n                    |- index.html\n                    |- images
    - icon-[99].png\n                    +- L.KML.js \n```\n\n## Services Rendered\n\nNow
    we need to go back to the `mappages` directory and enable the map to get, and
    update with, the stored coordinates in the server. Most of this work is done in
    a `mark.js` file:\n\n```jsx\nvar mymarker=null\n\nasync function updateMark()
    {\n  try {\n    marklocation=fetch(\"/where/\").then(response =>\n      response.json().then(data
    => {\n        marklocation=data\n        if(mymarker==null) {\n          mymarker=L.marker([marklocation.lat,marklocation.lon]).addTo(map);\n
    \       } else {\n          var newLatLng = new L.LatLng(marklocation.lat, marklocation.lon);\n
    \         mymarker.setLatLng(newLatLng);\n        }\n    }));\n  } catch(err)
    {\n    console.log(err); // Failed to fetch\n  }\n}\n```\n\nThis is a simple function
    that queries the /where endpoint, parses the JSON response and, first time through,
    puts a marker on the map. Subsequent queries will move that marker on the map.
    We now need to hook in that function so it gets run regularly. First, add it to
    the `<script>` includes at the start of `index.html`:\n\n```html\n\t\t\t<script
    src=\"/mark.js\"></script>\n```\n\nThen, after the `fetch...then` chain further
    down add:\n\n```jsx\n                .then(any => {\n                    updateMark();\n
    \                   window.setInterval(updateMark,30*1000);\n                });\n```\n\nNow
    the browser will query every 30 seconds for a location.\n\nLet's test this. First
    run the app locally with `npm start` and it should come up on port 3000. Navigate
    to [http://localhost:3000/](http://localhost:3000/) to view the map.\n\nNow pop
    this into a `test-local.sh` file:\n\n```shell\n\n#!/bin/sh\npayload=$(jo _type=location
    \\\n   t=u \\\n   batt=11 \\\n   lat=51.419891 \\\n   lon=-0.078449 \\\n   tid=SB
    \\\n   tst=$(date +%s) \\\n   topic=\"owntracks/mark/Markphone1\")\ncurl --data
    \"${payload}\" -H \"Content-Type: application/json\" http://localhost:3000/log/\n```\n\nThis
    shell script makes a JSON object which will emulate the OwnTracks app calling
    into the server. Run `sh ./test-local.sh` and if it all works, you should see
    a marker pop up around the start marker within a minute or so. \n\n## Fly To The
    Finish Line\n\nTo make this into a Fly app, first [download and install the Fly
    CLI tool flyctl](https://fly.io/docs/getting-started/installing-flyctl/) and run
    `fly auth signup` to get a Fly account.\n\nNow we can initialize the app to run
    on Fly by running `fly init` in the `markathon` directory.\n\nHit return to get
    a generated app name, hit return to use your personal organization and then select
    `node` to use Fly's node builtin builder. Finally hit return to use the default
    port. Fly will now write out a configuration file for the app.\n\nOh, yes, and
    then run `fly deploy` to put it online into a region near you.  Then run `fly
    open` to view the app in your browser. You'll have to set up OwnTracks to send
    its location to this server's `/log` endpoint, at which point you should be able
    to see yourself on the map. \n\nIf you can't set up OwnTracks to test it, copy
    your `test-local.sh` to `test-remote.sh` and change the last line to:\n\n```shell\ncurl
    --data \"${payload}\" -H \"Content-Type: application/json\" https://$(fly info
    --host)/log/\n```\n\nThat automatically gets the hostname of your Fly app and
    posts the mock location data up.\n\nWe've set this up for London, but if you create
    a route for anywhere in the world, the map viewer should automatically focus on
    the new route.\n\n## Next...\n\nThere's plenty that could be added to this app:
    basic authentication for the HTTP post from OwnTracks, Redis backing support,
    or more ambitiously, support for multiple runners.\n\nWe've been able to concentrate
    on building our app instead of working out how to get it into the cloud.  That
    meant that Mark could run his Marathon and [raise money for his chosen charity](https://uk.virginmoneygiving.com/MarkSmith336).
    And we all got to know how to make a tracking map that runs on Fly.\n"
- :id: blog-the-september-fly-changelog-new-names-and-easier-updates
  :date: '2020-09-30'
  :category: blog
  :title: 'The September Fly Changelog: New Names and Easier Updates'
  :author: dj
  :thumbnail:
  :alt:
  :link: blog/the-september-fly-changelog-new-names-and-easier-updates
  :path: blog/2020-09-30
  :body: "\n\n<p class=\"lead\">\n\nSeptember's changes have been all about making
    the Fly CLI easier to install, update, and use on every platform including Windows
    10. And we've added some new ways to build and deploy websites. Read on for details.\n\n</p>\n\nSome
    common requests are addressed this month. One is 'can you let us type \"fly\"
    instead of \"flyctl\"?', and it is a shorter command, that is true. Another is
    can you 'make updating flyctl easier?', something we are more than happy to do.
    And in September, we did both.\n\n## Fly and Flyctl\n\nWe've modified the installer
    for the Fly CLI so that while it installs itself as `flyctl` it also sets up a
    symbolic link to the `fly` command name. Once you are up to date with Flyctl,
    you'll be able to use  `fly` as your preferred command. We're taking this carefully
    as this is the start of a migration which will end with `fly` being the actual
    name of the CLI app. \n\n## Easier Updating\n\nThe Fly CLI regularly checks in
    to see if there's a new version of itself available. We've previously enhanced
    this feature so that it could show people the command they needed to run to update.
    Well, in our September updates, we've gone one better. Now, when there is a new
    version available, all you need to do is run `fly version update` and the Fly
    CLI will take care of the rest.\n\n## Extra Builtin Builders\n\nTwo new static
    builders for creating static web sites quickly — `hugo-static` and `staticplus`
    — have been added. The former runs a Hugo build and then deploys the results as
    a static website. The latter added HTTPS redirection for sites with a custom domain,
    but we'll be retiring that soon, thanks to a combination of a change being accepted
    upstream and a big new feature coming for Fly builtins in a future release.\n\n##
    Windows 10 Improvements\n\nBetter Windows installation and updating using PowerShell
    have been implemented. We'd love [to hear what you think](https://community.fly.io/t/do-you-use-fly-on-windows-10/258)
    if you use Fly and Windows 10.\n\n## Init Extended\n\nThere are also two new ways
    to initialize an app with `fly init`. When you really want to always overwrite
    the existing configuration, `--overwrite` will do that job for you. And when you
    want to do the exact opposite — create a new app but not rewrite the configuration
    — then `--nowrite` is the option for you.\n\n## Other Changes\n\nUpdated and new
    commands for the upcoming DNS+domains feature have been incorporated in this release
    too; read the [feature preview](https://community.fly.io/t/feature-preview-dns-domains/95/6)
    on the Fly Community site for more about that.\n\n<p class=\"callout\">\n\n This
    is the Fly Changelog where we list all significant changes to the Fly platform,
    tooling, and websites. You can also use the RSS feed of just changelog posts available
    on [fly.io/changelog.xml](https://fly.io/changelog.xml) or consult our dedicated
    [ChangeLog](https://fly.io/changelog/) page with all the recent updates.\n\n</p>\n\n<!--
    start -->\n\n## _30th September_\n\n**flyctl: Version [0.0.144 released](https://github.com/superfly/flyctl/releases/tag/v0.0.144)**\n\n-
    ++ We've changed the commands for managing our upcoming domain support: `domains`
    and `dns-records` are subcommands for the functionality. Background and details
    on the preview in the [feature preview](https://community.fly.io/t/feature-preview-dns-domains/95/6)\n-
    ++ A new `init --nowrite` option has been added which allows an app to be created
    without overwriting the current fly.toml. This allows multiple deployments under
    different app names (new alternate app names are recorded in `fly.alias`)\n- ~~
    Builtins now avoid using Alpine as a base image due to reported DNS instability
    in the Alpine release\n- ~~ The Deno builder no longer requires a `deps.ts` file.\n\n##
    _17th September_\n\n**flyctl: Version [0.0.143 released](https://github.com/superfly/flyctl/releases/tag/v0.0.143)**\n\n-
    ++ `version update` command added - automatically downloads and installs the latest
    version.\n- ++ Docker authentication used to log into Docker hub if present.\n-
    ++ Add a Windows build script\n- ++ Powershell-based Windows 10 installer\n- ~~
    Listing of failing allocations in the status monitor is now more effective\n-
    ~~ Cleaner display of failed allocations when watching allocations\n- ~~ Cleaner
    table views of builtins (more content and fewer lines)\n- ~~ Version parsing made
    more reliable and permissive\n\n## _15th September_\n\n**flyctl: Version [0.0.142
    released](https://github.com/superfly/flyctl/releases/tag/v0.0.142)**\n\n- ~~
    Fixed fly//flyctl links in homebrew formula\n\n## _14th September_\n\n**flyctl:
    Version [0.0.141 released](https://github.com/superfly/flyctl/releases/tag/v0.0.141)**\n\n-
    ++ Node builtin: Now performs a `npm run build` if a build step exists.\n- ++
    New `staticplus` builtin added for custom domains.\n- ++ Added `--overwrite` flag
    to `fly init` to always overwrite `fly.toml` when creating an app.\n- ++ `flyctl`
    can also be run as `fly` now and a symlink is created in the installer to allow
    both versions to be used. The plan is to transition to `fly` in the future.\n-
    ~~ Requirement for app-name made more consistent across commands.\n- ~~ Fix flagging
    of pre-release versions in the installer.\n- ~~ Builtins now note they use internal
    port 8080 in their documentation.\n- -- 32-bit builds are no longer created for
    Fly CLI releases.\n\n## _27th August_\n\n**flyctl: Version [0.0.140 released](https://github.com/superfly/flyctl/releases/tag/v0.0.140)**\n\n-
    ++ Deployments now say, at the very end, if there was an allocation failure even
    when a rollback has taken place\n- ~~ Restart counts in allocations are now more
    accurate\n\n\n"
- :id: blog-using-heroku-postgres-from-a-fly-app
  :date: '2020-09-28'
  :category: blog
  :title: Using Heroku Postgres From A Fly App
  :author: dj
  :thumbnail:
  :alt:
  :link: blog/using-heroku-postgres-from-a-fly-app
  :path: blog/2020-09-28
  :body: "\n\n<p class=\"lead\">\n\nIn a [previous article](/blog/migrating-heroku-database-apps-to-fly/),
    we showed how simple migrating a database-using application. But what we didn't
    mention was that you don't have to migrate an application from Heroku to Fly to
    use Heroku's databases.  You can use Heroku databases with a native Fly application.
    \n\n</p>\n\nHeroku supports applications with no apps in them and only add-ons,
    like Postgres, as a way of providing those services to other applications. As
    we showed,  you can access Heroku Postgres from Fly so it makes it a useful way
    to get yourself a database for some storage.\n\nSo let's step through the process
    of what you need to do, and what you need to look out for.\n\n## Getting Heroku
    Prepared\n\nFirst, [sign up for Heroku](https://signup.heroku.com/login) and get
    your free account there. For this example we'll be using their Hobby plan so there's
    no expense involved.\n\n Now, you should arrive, once logged in to Heroku, at
    the Heroku dashboard. Select **New** on the right of the dashboard and **New App**
    in the dropdown menu. On the next screen, give your application a name and select
    whether you want the database to work out of the US or Europe. Then click **Create
    App** and arrive at the initial application setup screen. The details shown won't
    be of much relevance to you for this task. Select the **Overview** tab to get
    a higher-level view of the app.\n\nYou should see entries about the configuration
    of the app, including at the top, `Installed add-ons`. Select **Configure add-ons**
    in that section and you'll be taken to a page which doesn't obviously say it's
    for add-ons. That's underneath the top section for Dynos. The add-ons section
    has a text field where you can type add-on names. Type `postgres` into there and
    select **Heroku Postgres**. You should see a dialog inviting you to choose a plan
    - stay on Hobby Dev for a 1GB database capable of holding up to 10,000 rows to
    keep things free - then click **Provision**.\n\n## Getting Your Credentials\n\nAnd
    now, there's a Heroku Postgres database listed in your add-ons. Click on the database
    name in the add-ons list to drill down into the database's configuration.  Select
    **Settings** and then **View Credentials**. You'll want to take a note of what
    is now displayed, but the most important line in the credentials is the `URI`
    line. It's a long one, with a long hostname, database, username, and password,
    but it contains all you need to connect to the new database. This is the value
    that turns up in Heroku apps as `DATABASE_URL` but for our purposes, we're going
    to have to manually transfer it to our app.\n\nAlso, pay attention to the warning:
    `Please note that these credentials are not permanent. Heroku rotates credentials
    periodically and updates applications where this database is attached.`. You won't
    get any warning that credentials have changed, but if you do have an issue in
    the future, make checking the credentials your first port of call.\n\n## Preparing
    To Fly\n\nFor this example, we're going to use Node and [Sequelize](https://sequelize.org/)
    to create a simple Postgres-backed REST API server for storing [truefacts](https://github.com/fly-apps/truefacts).
    The part we're most interested in is setting up the connection. Here's the opening
    of the main code:\n\n```javascript\nconst express = require(\"express\");\nconst
    bodyParser = require('body-parser');\nconst { Sequelize, DataTypes } = require(\"sequelize\");\nvar
    parse = require(\"pg-connection-string\").parse;\n\nconst connectionString = process.env.DATABASE_URL;\n```\n\nWe
    have to make some modifications to the `DATABASE_URL` which we got from Heroku.
    To do those changes as reliably as possible, we use `pg-connection-string` to
    parse the URL into its component parts (in terms of Postgres).\n\n```javascript
    \  \nconnector = parse(connectionString);\n```\n    \nOnce parsed, we can create
    the Sequelize connection, and add in the changes needed to connect to Heroku:\n\n```javascript\nconst
    sequelize = new Sequelize(\n    connector.database,\n    connector.user,\n    connector.password,\n
    \   {\n        dialect: \"postgres\",\n        host: connector.host,\n        dialectOptions:
    {\n            ssl: { sslmode: \"require\", rejectUnauthorized: false },\n        },\n
    \   }\n);\n```\n\n As you can see, we pass over the database, user, password,
    and host untouched to the Sequelize constructor. What we do add is in the options
    object. That `dialectOptions` value requires SSL (because external access to Heroku
    demands SSL be turned on) and then turns off the rejection of self-signed certificates
    on the server's certificate chain, an issue which Heroku's Postgres has.\n\nThe
    rest of the example application is a simple REST API, where you add true facts
    about something by POSTing a JSON object to one endpoint and retrieving those
    facts as JSON objects. You'll find the code in the [truefacts](https://github.com/fly-apps/truefacts)
    repository in fly-examples.\n\nThat's pretty much all you need to know to get
    a connection to Fly, apart from how you pass your database credentials to the
    application. For this, you need to set a secret in the form:\n\n```\nfly secrets
    set DATABASE_URL=\"yoururlfromherokupostgres\"\n```\n\nNow you are ready to `fly
    deploy` your application. It'll connect up to the backend Postgres on Heroku.
    Remember that's limited to 20 connections unless you upgrade to a paid plan. \n\n##
    Wrapping up\n\nWe've shown how to connect to Heroku's Postgres without a Heroku
    application being migrated. Using Heroku's services is an option for users who
    need a database option right now, and with Heroku's data centers in the US and
    Europe, the database won't be too far away, while your Fly application can be
    as close as possible to the user.\n\n<p class=\"callout\">\n\n_Want to learn more
    about Fly? Head over to our [Fly Docs](/docs/) for lots more, including a [Hands
    On](/docs/hands-on/start/) where you can get a free account and deploy your first
    app today._\n\n</p>\n\n\n\n"
- :id: blog-migrating-heroku-database-apps-to-fly
  :date: '2020-09-25'
  :category: blog
  :title: Migrating Heroku Database Apps To Fly
  :author: dj
  :thumbnail:
  :alt:
  :link: blog/migrating-heroku-database-apps-to-fly
  :path: blog/2020-09-25
  :body: "\n\n<p class=\"lead\">\n\nHow do you handle databases when you migrate an
    application from Heroku to Fly? Migrating Heroku apps to Fly gives your app a
    [real boost in performance](/blog/turboku/) using Turboku, but how do you move
    the database connection of an app over?\n\n</p>\n\nI was recently asked how the
    database migration in the original Turboku demo worked. For that original demo,
    the migration was practically automatic. We already knew that you could [access
    a Heroku database from outside Heroku](https://devcenter.heroku.com/articles/connecting-to-heroku-postgres-databases-from-outside-of-heroku).
    Since then, Changes in how Heroku and the Postgres driver handle things have meant
    you need to be a little more proactive with your database connecting code. So,
    if we're setting out to make an app that migrates without changes to Fly, what
    do we need to do?\n\n## Start With The DATABASE_URL\n\nWhen you attach a Postgres
    addon to your Heroku app, it also creates a URL which contains everything you
    need to connect to the database. You can get the value of `DATABASE_URL` from
    the environment and that has all the credentials you need to connect to the Postgres
    database. It also contains the host, port and database name to connect to. For
    most situations, it can be used without modification. The demo uses [Massivejs](https://massivejs.org/)
    here as a layer on top of the node-postures (aka pg) package and connecting looks
    something like this:\n\n```javascript\nconst massive = require('massive');\n\nvar
    connectionString = process.env.DATABASE_URL;\n\n\n(async () => {\n    try {\n
    \       const db = await massive(connectionString);\n     } catch (e) {\n        console.log(\"No
    DB\",e)\n    }\n})();\n```\n\nNow, if we wanted to connect to Heroku Postgres
    from outside Heroku, the number one requirement is that you switch to SSL. Now,
    you could do that by appending a string with the required change to the end of
    the connection string. But experience tells me that connection strings can change
    and change in sometimes unpredictable ways. So...\n\n## Turning on SSL\n\nThe
    first thing we need to do is to parse the connection string. To handle that, we'll
    use the [pg-connection-string](https://www.npmjs.com/package/pg-connection-string)
    package and we'll extract the parse function:\n\n```javascript\nvar parse = require('pg-connection-string').parse;\n```\n\nThis
    is the same package the pg library uses to parse connection strings so you shouldn't
    have any compatibility issues with it. We can now take the connection string and
    parse it:\n\n```javascript\nvar connectionString = process.env.DATABASE_URL\n\nconnector=parse(connectionString);\n```\n\nAnd
    now we can set the `ssl` field:\n\n```javascript\nconnector.ssl={ \"sslmode\":
    \"require\", \"rejectUnauthorized\":false };\n```\n\nAh! You may say, all you
    need to do is set `ssl` to true. And in a perfect world, that would be correct.
    Firstly, any value in the `ssl` setting equates to `ssl` being set to true. Secondly,
    we need to tweak the SSL configuration because Heroku Postgres has servers with
    a self-signed certificate in their chain of server certificates. If we **just**
    had SSL turned on, the client driver would try and verify the server's identity
    and throw an error when it saw that self-signed certificate. You can read more
    about this in this [github issue](https://github.com/brianc/node-postgres/issues/2009).\n\nThe
    fix, now, is to set the `sslmode` to `require` which requires SSL to be enabled
    and then `rejectUnauthorized` to `false` to stop that certificate verification
    check. The best part is we don't need to do anything else to connect; we can use
    our parsed connector rather than the connection string on most connection calls
    to Postgres:\n\n```javascript\nconst db = await massive(connector);\n```\n\n##
    Putting It Together\n\nPutting all this together, we get this code:\n\n```javascript\nconst
    massive = require('massive');\n\nvar connectionString = process.env.DATABASE_URL\n\nif
    (connectionString==undefined) {\n    // Fallback to a local Postgres\n    connector=parse(\"postgres://postgres@localhost/postgres\");\n
    \   connector.ssl=false;\n} else {\n    connector=parse(connectionString);\n    connector.ssl={
    \"sslmode\": \"require\", \"rejectUnauthorized\":false };\n}\n\n(async () => {\n
    \   try {\n        const db = await massive(connector);\n        app.set('db',
    db);\n        tables=db.listTables()\n    } catch (e) {\n        console.log(\"No
    DB\",e)\n    }\n})();\n```\n\nThere's a little extra code to help out when testing
    locally against a local Postgres - don't define `DATABASE_URL` and it'll fall
    back to a non-SSL localhost connection. In our example code, [flydictionary](https://github.com/fly-apps/flydictionary/),
    the rest of the code a simple dictionary with the ability to search or add words
    and save them in the database.\n\n## Pushing to Heroku\n\nIf you want to install
    this app, grab the [flydictionary code](https://github.com/fly-apps/flydictionary/),
    log in to Heroku and deploy the app with `heroku create` then push the app up
    to Heroku with `git push heroku master`. \n\nThe app will start running but stop
    immediately as there's no database for it to take to - Use:\n\n```\nheroku addons:create
    heroku-postgresql\n```\n\nThis will create a Postgres database and attach it to
    your app. Now you can run `heroku open` to open a browser onto your newly created
    app. Add some words to the dictionary.\n\n## Coming to Fly\n\nBringing Heroku
    web apps to Fly is an uncomplicated process thanks to the specialized Heroku migration
    support.\n\nAll you need to do is go to the [https://fly.io/heroku](https://fly.io/heroku).
    Log in to Heroku there. That will show the available apps on the page and you
    can select the app you just created on Heroku. \n\nThen you press the `Turboku!`
    button and the app will be automatically migrated over to Fly. That includes migrating
    the Heroku `DATABASE_URL` environment variable which will be turned into a Fly
    secret.\n\nThings to remember with a Heroku database connection: Heroku only has
    two regions: the US and Europe. Database performance will be a function of how
    close your selected Fly regions are to your Heroku database region. Consider caching
    within Fly regions if performance is mission-critical. \n\n## Fly Closer\n\nThanks
    to the connection changes we made to the application, it'll just start running
    on Fly and we can treat it like other Fly applications, adding and removing regions,
    scaling up, scaling down and more, while getting the benefit of being closer to
    the user. And remember that with Turboku, apps migrated this way are automatically
    updated when changes are pushed to Heroku.\n\n<p class=\"callout\">\n\n_Want to
    learn more about Fly? Head over to our [Fly Docs](/docs/) for lots more, including
    a [Hands On](/docs/hands-on/start/) where you can get a free account and deploy
    your first app today._\n\n</p>\n\n\n\n\n\n\n\n\n\n"
- :id: blog-always-be-connecting-with-https
  :date: '2020-09-11'
  :category: blog
  :title: Always Be Connecting (with HTTPS)
  :author: dj
  :thumbnail:
  :alt:
  :link: blog/always-be-connecting-with-https
  :path: blog/2020-09-11
  :body: "\n\n![“Always Be Connecting” written on a blackboard](always-be-connecting.jpg)\n\n<p
    class=\"lead\">\n\nMaking sure your users always connect to the HTTPS secured
    version of your site is a big concern these days. With search engines marking
    down non-HTTPS sites and users adopting tools to push their web browsing to the
    more secure option, it is something that developers and site owners need to take
    seriously.\n\n</p>\n\nAt Fly, you'll have already discovered that when you create
    an app, it appears as `http://appname.fly.dev` and if you connect there, your
    browser switches automatically, by redirection,  to `https://appname.fly.dev`.
    That's how we do it for the fly.dev domain.\n\nThings are different when you bring
    your own certificate along and attach it to your app. There's no automatically
    activated HTTPS upgrading because we understand people like to have the freedom
    to choose how their connections behave.\n\nSo how do you make a connection upgrade
    to HTTPS with your own custom domain...\n\n## A Fly Connection's Life\n\nWhen
    an application connects to a Fly app, it looks up the domain, gets an IP address
    back and connects to the IP address. With Fly, those IP addresses are AnyCast
    IP addresses and the connection will be routed to the nearest edge of the Fly
    network.\n\nThese Fly network edges are made up of proxies which handle the incoming
    connection. When you define handlers for an app (in fly.toml), it's at this edge
    where the handlers step in to manage the \"http\" or \"tls\" components of the
    connect.\n\nOnce the edge proxy has located the nearest server and datacenter
    where your app has an instance running, the proxied connection is routed to that
    server. This stage of the connections journey take place over encrypted networking
    so the connection is made unencrypted.\n\nBoth https and http connections are
    sent to the internal port of the app. The app then responds and the proxy handles
    the response appropriately. If you wanted to upgrade all your HTTP connections
    to HTTPS, the appropriate response would be to redirect the client to the HTTPS
    version of the URL.\n\n## Detecting HTTP Connections\n\nYou may wonder \"But if
    all connections are flattened out to unencrypted HTTP, how do we spot actual HTTP
    connections?\". That's where being routed through a proxy comes into it. Proxies
    can add information to the connection about where the original connection came
    from.\n\nThese usually appear in the HTTP header and make up part of what we call
    the [Fly runtime environment](https://fly.io/docs/reference/runtime-environment/).
    The one header we are interested in is `X-Forwarded-Proto`. It contains the protocol
    the incoming connection used - 'http' or 'https'.\n\nSo, at its simplest, if our
    incoming connection to a has `X-Forwarded-Proto: http` then we Redirect (HTTP
    Status 301) to the same URL but with `https` in its header. All connections are
    then automatically upgraded.\n\n## Practical Upgrading - Go\n\nHere's a recent
    example of doing this in practice. We've been using an excellent lightweight Go
    server ([PierreZ's goStatic](https://github.com/PierreZ/goStatic/)). It deals
    entirely in http connections which works great with the Fly architecture but we
    wanted to get it to upgrade non-\"fly.dev\" connections. This is a shorter version
    of the Go code we added\n\n```go\nfunc handleReq(h http.Handler) http.Handler
    {\n\treturn http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n\t\tif
    r.Header.Get(\"X-Forwarded-Proto\") == \"http\" {\n\t\t\thttp.Redirect(w, r, \"https://\"+r.Host+r.RequestURI,
    http.StatusMovedPermanently)\n\t\t\treturn\n\t\t}\n\t\th.ServeHTTP(w, r)\n\t})\n}\n\n```\n\n##
    Practical Upgrading - Node (and Express)\n\nFor Node (and Express), the same effect
    can be achieved with a little bit of [Express middleware](http://expressjs.com/en/guide/using-middleware.html)
    added to the stack:\n\n```jsx\napp.use (function (req, res, next) {\n    if(req.get(\"X-Forwarded-Proto\")==\"http\")
    {\n            // request was via http, so redirect to https\n            res.redirect('https://'
    + req.headers.host + req.url);\n    } else {\n        next();\n    }\n});\n\n```\n\n##
    Practical Upgrading - With Other Apps\n\n In-App Upgrading like this is a great
    way to always ensure that where-ever your app is deployed. You can allow other
    applications to do the redirection work for you, like Nginx. Mix in something
    like:\n\n```lua\nif ($http_x_forwarded_proto = \"http\") {\n    return 301 https://$server_name$request_uri;\n}\n```\n\ninto
    your `nginx.conf` and you'll be upgrading your connections in the same way. This
    would, for example, but useful in a custom domain proxy, ensuring that all the
    customer's users were getting a secure connection.\n\n## Beyond manual Upgrading\n\nEven
    if you are not on Fly, you can make use of this technique, though rather than
    checking the request's headers, you'll need to check the request itself and the
    URL's schema to identify if it's an HTTP connection.\n\nBack on Fly, we are looking
    at ways that we can also automate this process, without forcing a default behavior
    on all our users. We'll get back to you on that when it's ready; until then we
    think these redirection tips will help keep your apps securely connected.\n\n<p
    class=\"callout\">\n\n  _Want to learn more about Fly? Head over to our [Fly Docs](/docs/)
    for lots more, including a [Hands On](/docs/hands-on/start/) where you can get
    a free account and deploy your first app today._\n\n</p>\n"
- :id: blog-more-fly-answers-to-questions
  :date: '2020-09-02'
  :category: blog
  :title: More Fly Answers To Questions
  :author: dj
  :thumbnail:
  :alt:
  :link: blog/more-fly-answers-to-questions
  :path: blog/2020-09-02
  :body: "\n\n<p class=\"lead\">\n\nWe get asked questions about Fly in a lot of places
    on the web which we answer. But, not everyone is everywhere on the web, so with
    Fly Answers Questions, we bring those answers to you. If you have questions about
    Fly, why not ask [@flydotio](https://twitter.com/flydotio) on Twitter, or drop
    a query in the [Fly Community](https://community.fly.io/).\n\n</p>\n\n**Q: Is
    it ok to run non-HTTP apps on Fly? From Pier [via community.fly.io](https://community.fly.io/t/is-it-ok-to-run-non-http-apps-in-fly/157)**\n\n**A:**
    It is, with one caveat: your applications need to have a network service of some
    kind. Why is that? Well, when your app deploys, we run health checks to ensure
    you can connect to it. They are, by default, TCP connection checks but we do have
    an option for full HTTP checks. Anyway, unless there is a network service that
    can be connected to, the health checks will fail and Fly will kill off the app
    for being apparently faulty.\n\nWe do this ourselves for some of our internal
    services. When we run our Buildkite agents, we use the fact that they export Prometheus
    metrics on port 8080 and use that port for the health checks to look at.\n\nAnd
    no, this isn't how things are going to stay. We have support for Worker processes
    on the way.\n\n**Q: What IP Ranges is Fly on? From David [via community.fly.io](https://community.fly.io/t/request-fly-ip-ranges/127)**\n\n**A:**
    This is quite a regular question and the question itself comes from the strategy
    of protecting your database or other servers from unauthorized access by clear-listing
    the IP addresses of servers that will be legitimately connecting. This also blocklists
    the rest of the internet.\n\n The first answer is \"it's complicated\". We do
    have a set of registered ranges in [AS40509](https://ipinfo.io/AS40509) but they
    may not be the addresses that are seen by servers being connected to. Instead,
    it's likely they see the server's IP address which isn't assigned within that
    AS.\n\nWe have considered publishing our IP addresses, but we are literally adding
    new servers every day. With that rate of change, it is likely there would be a
    gap between an updated list being published and various other systems syncing
    with the up-to-date list. \n\nAs that's the case, our second answer is we'd suggest
    that you regard Fly IP addresses as subject to change in the context of security-related
    features. That said, you can always use improved authentication between clients
    and servers with the exchange of client certificates during a TLS connection to
    help protect the connection.\n\nThere's a third answer for the future though.
    Everything above is related to IPV4. With IPV6, we are able to generate addresses
    per application through our API, so in the future, it would be possible to use
    those addresses to control an IPV6-capable firewall's clear-list. This, and other
    solutions, are all being worked on at Fly.\n\n**Q: How can I rename an App on
    Fly? From Dan [via community.fly.io](https://community.fly.io/t/request-rename-app/149)**\n\n**A:**
    We prefer people to use the generated app names on Fly. Apart from anything else,
    we make it easy to attach a custom domain to your application, making the issue
    of having a particular name mostly moot. We do understand, though, that people
    may want a specific name. We don't have a way to do that yet but there are plans
    in the works to provide a solution.\n\nUntil then, we do have a workaround, but
    you will lose your app's history, secrets, certificates, hostnames, and deployment
    configuration, so it's entirely at your own risk:\n\n- Make an empty directory\n-
    Run `flyctl init` in there\n- Enter your desired name for your app\n- Set the
    builder to `None`\n- Select the same organization as the original app\n- If it
    completes successfully, you have claimed your desired name\n- Go back to your
    original app\n- Make a note of its original name as set in `fly.toml` in the `app=\"\"`
    line\n- Replace that name with your now claimed app name\n- Run `flyctl deploy`
    to deploy under your new name\n\nDon't forget to run `flyctl destroy original-app-name`
    to tidy up. Then configure the new app's secrets, certificates, hostnames, and
    deployment configuration including the scaling and regions it was mapped to.\n\n##
    Community.fly.io\n\nIf you follow the links to [community.fly.io](https://community.fly.io),
    you will see that our new community site is a great place to ask questions, look
    for hints and tips and get your Fly apps running at their best. If you have a
    Fly account, sign in today as we've enabled single sign-on between Fly and the
    community site making it easier than ever to participate.\n"
- :id: blog-flyctl-builtins-the-fly-changelog-for-august
  :date: '2020-08-26'
  :category: blog
  :title: Flyctl Builtins - The Fly Changelog for August
  :author: dj
  :thumbnail:
  :alt:
  :link: blog/flyctl-builtins-the-fly-changelog-for-august
  :path: blog/2020-08-26
  :body: "\n\n<p class=\"lead\">\n\nSuper-simple builtin builders and smart certificates
    creation - it's all in the latest flyctl (v0.0.139) available now for your command
    line. Find out more about it in the Changelog.\n\n</p>\n\nFor the latest version
    of flyctl, we've focussed on making your life fast and simple. From getting your
    first deployment up and running to setting up a host's certificate.\n\n## Builtins\n\nFirst
    up, we've got the new “builtin” builders which you can select when you init your
    App. We have builtin builders for Node, Ruby, Deno and Go and there's also a static
    web server available. No need for a Dockerfile - just init and deploy. \n\nYou
    can find out more at the command line with `flyctl builtins list` which will give
    you information on all of the builtins, and `flyctl builtins show <builtin-name>`
    which will give you all the details of the named builtin, including the virtual
    Dockerfile that the builtins use.\n\nWith builtins all you need to do is:\n\n*
    Write your code\n* Run `flyctl init`\n* Select a builtin for the code's language,
    press return for defaults on the other prompts\n* Run `flyctl deploy`\n* Finally,
    run `flyctl open` to view your app in a browser\n\nYou can see full examples for
    [Go](/docs/getting-started/golang/), [Node](/docs/getting-started/node/), [Ruby](/docs/getting-started/ruby/),
    [Deno](/docs/getting-started/deno/) and [a static web server](/docs/getting-started/static/)
    in the documentation.\n\n\n## Images\n\nIn previous versions of `flyctl`, we have
    been able to deploy an image from the command line. In this version, you can now
    use the image builder (or `--image` flag on `flyctl init`) to select a public
    Docker image to be published to Fly. This persists the selected image in a `fly.toml`
    file. \n\nBy using the `fly.toml` file, it also lets you configure the ports and
    health checks for the image. Best of all, it lets you save that information into
    your git repository so you can perform repeatable builds with images.\n\n## Backup
    Regions\n\nWhen, for whatever reason, an App instance is unable to deploy in a
    particular region, Fly will look to deploying it in a backup region. This is a
    longstanding behavior of Fly, but flyctl has been not been good in communicating
    that that is what is happening, leading to confusion over why an App is deploying
    in a region but not in the region list. \n\nNot anymore! `flyctl regions list`
    will now show the regions and the associated backup regions that an app instance
    may appear in. Also, `flyctl status` will display `(b)` next to any region which
    is not in the region pool and is therefore in a backup region. This should give
    everyone a better view of where their app is running.\n\n## Guided Certs\n\nIn
    the past, when creating a certificate for a hostname, you had to refer to the
    documentation or UI for the various steps you needed to take. In this version
    of `flyctl`, we've embedded some smart-documentation into the process. When you
    add a certificate now, `flyctl` will give you detailed instructions on what you
    need to set with your DNS provider to direct traffic to your App through your
    domain name and verify your ownership (allowing a certificate to be generated).
    It's all part of making `flyctl` your preferred way to work with Fly.\n\n## Orgs
    and DNS\n\nAlso implemented in 0.0.138, are the `orgs` and `dns` commands. Orgs
    allows you to add and remove organizations - which you can also do from the Web
    UI. What you currently can't do is invite and remove users from the organization;
    you'll have to use the Web UI for that. This, and the dns command, are works in
    progress and could change in a later version. We thought it was better to let
    you see what was in the longer pipeline. \n\n## Other changes\n\n* `flyctl init`
    now has an `--import` option which will use the settings of the imported `fly.toml`
    file while creating a new app name and slot.\n* JSON status output now has timestamps.\n*
    `flyctl deploy` now has a --local-only flag to force builds to happen locally
    or not at all. \n\n\n<p class=\"callout\">\n This is the Fly Changelog where we
    list all significant changes to the Fly platform, tooling and web sites. You can
    also use the RSS feed of just changelog posts available on [fly.io/changelog.xml](https://fly.io/changelog.xml)
    or consult our dedicated [ChangeLog](https://fly.io/changelog/) page with all
    the recent updates.\n</p>\n\n<!-- start -->\n\n## _25th August_\n\n**flyctl**:
    Version [0.0.139 released](https://github.com/superfly/flyctl/releases/tag/v0.0.139).\n\n-
    ~~ fix issue with builtin init.\n\n**Fly Platform/Web**\n\n- ~~ Updated documentation
    in sync with new flyctl.\n\n## _24th August_\n\n**flyctl**: Version [0.0.138 released](https://github.com/superfly/flyctl/releases/tag/v0.0.138)\n\n-
    ++ Added flyctl builtins and `flyctl builtins list`.\n- ++ Added `flyctl builtins
    show`.\n- ++ Added images as an option to builder selection.\n- ++ Added --image
    as flag to `flyctl init`.\n- ++ Added prompting to `certs` process to give directions
    to user on next steps after adding or checking a certificate.\n- ++ Changed certs
    create/delete to add/remove (aliased for back compatibility).\n- ++ Added prompting
    for internal port to `flyctl init`.\n- ++ Added display of backup regions.\n-
    ++ Added `--import` to `flyctl`.\n- ++ Added `--local-only` option to `flyctl
    deploy`.\n- ++ Added `orgs` and `dns` commands.\n- ++ Displays app URL before
    deployment.\n- ++ JSON status output now has timestamp.\n- ~~ Caught errors in
    suspend (thanks @alrs).\n- ~~ Clarified monitoring error messages.\n- ~~ Only
    display auth URL when an error has occurred.\n- ~~ Setting secret to same value
    no longer causes a \"no deployment available\" error.\n- ~~ Improved internal
    API use.\n- ~~ Immediate duplicates in logs supressed in client.\n\n"
- :id: blog-sandboxing-and-workload-isolation
  :date: '2020-07-29'
  :category: blog
  :title: Sandboxing and Workload Isolation
  :author: thomas
  :thumbnail:
  :alt:
  :link: blog/sandboxing-and-workload-isolation
  :path: blog/2020-07-29
  :body: "\n\nWorkload isolation makes it harder for a vulnerability in one service
    to compromise every other part of the platform. It has a long history going back
    to 1990s qmail, and we generally agree that it’s a good, useful thing. \n\nDespite
    a plethora of isolation options, in the time I spent consulting for technology
    companies I learned that the most common isolation mechanism is “nothing”. And
    that makes some sense! Most services are the single tenant of their deployment
    environment, or at least so central to the logical architecture that there’s nothing
    to meaningfully isolate them from. Since isolation can be expensive, and security
    is under-resourced generally, elaborate containment schemes are often not high
    up on the list of priorities.\n\nThat logic goes out the window when you’re hosting
    other people’s stuff. Fly.io is a content delivery network for Docker containers.
    We make applications fast by parking them close to their users; we do that by
    running bare metal servers in a bunch of data centers around the world, and knitting
    them together with a global WireGuard mesh. Fly.io is extremely easy to play with
    — single-digit minutes to get your head around, and rather than talk about it,
    I’ll just suggest you grab a free account and try it. \n\nMeanwhile, I’m going
    to rattle off a bunch of different isolation techniques. I’ll spoil the list for
    you now: we use Firecracker, the virtualization engine behind Amazon’s Lambda
    and Fargate services. But the solution space we chose Firecracker from is interesting,
    and so you’re going to hear about it.\n\n### chroot\n\nPeople like to say “chroot
    isn’t a security boundary”, but of course that isn’t really true, it’s just not
    very strong by itself. Chroot is the original sandboxing technique.\n\nThe funniest
    problem with chroot is how it’s implemented: in the kernel process table, every
    struct proc (I was raised on BSD) has a pointer to its current working directory
    and to its root directory. The root directory is “enforced” when you try to cd
    to “..”; if your current working directory is already the root, the kernel won’t
    let “..” go below it. But when you call chroot(2), you don’t necessarily change
    directories; if you’re “above” your new root, the kernel will never see that new
    root in a path traversal. \n\nThe real problem, of course, is the kernel attack
    surface. We don’t need to get cute yet; by itself, considering no other countermeasures,
    chroot gives you ptrace, procfs, device nodes, and, of course, the network. \n\nYou
    shake a lot of these problems off by not running anything as “root”, but not everything.
    \ A quick-but-important aside: in real-world attacks, the most important capability
    you can concede to an attacker is access to your internal network. It’s for the
    same reason that SSRF vulnerabilities (“unexpected HTTP proxies”) are almost always
    game-over, even though at first blush they might not seem much scarier than an
    unchecked redirect: there will be something you can aim an internal HTTP request
    to that will give an attacker code execution. Network access in a chroot jail
    is like that, but far more flexible.\n\nThis problem will loom over almost everything
    I write about here; just keep it in mind.\n\nchroot is a popular component in
    modern sandboxes, but none of them really rely on it exclusively.\n\n### Privilege
    Separation\n\nIt’s 1998 and the only serious language you have available to build
    in is C. You want to receive mail for a group of users, or authenticate and kick
    off a new SSH session. But those are complicated, multi-step operations, and nobody
    knows how to write secure C code; it’ll be 30 years before anyone figures that
    out. You assume you’re going to screw up a parse somewhere and cough up RCE. But
    you need privileges to get your job done. \n\nOne solution: break the service
    up into smaller services. Give the services different user IDs. Connect services
    with group IDs. Mush the code around so that the gnarliest stuff winds up in the
    low-privileged services with the fewest connections to other services. Keep the
    stuff that needs to be privileged, like mailbox delivery or setting the login
    user, as tiny as you can.\n\nCall this approach [“privsep”][privsep].\n\nDespite
    [what its author said][qmail] about his design, this approach works well. It’s
    not foolproof, but it has in fact a pretty good track record. The major downside
    is that it takes a lot of effort to implement; your application needs to be aware
    that you’re doing it. \n\n#### Aside: Pledge\n\nIf you can change your applications
    to fit the sandbox, you can take privsep pretty far. OpenBSD got this right with
    [“pledge”][pledge] and [“unveil”,][unveil] which allow programs to gradually ratchet
    down the access they get the kernel. It’s a better, more flexible idiom than seccomp,
    about which more later. But you’re not running OpenBSD, so, moving on. \n\n###
    Prelapsarian Containers\n\nPeople like to say “Docker isn’t a security boundary”,
    but that’s not so true anymore, though it once was. \n\nThe core idea behind containers
    is kernel namespacing, which is chroot extended to other kernel identifiers —
    process IDs, user IDs, network interfaces. Configured carefully, these features
    give the appearance of a program running on its own machine, even as it shares
    a running kernel with other programs outside its container.\n\nBut even with its
    own PID space, its own users and groups, and its own network interfaces, we still
    can’t have processes writing handler paths to `/sys`, rebooting the system, loading
    kernel modules, and making new device nodes, and while many of these concerns
    can be avoided simply by not running as root, not all of them can.\n\nSystems
    security people spent almost a decade dunking on Docker because of all the gaps
    in this simplified container model. But nobody really runs containers like this
    anymore.\n\n### Incarceration\n\nEnter mandatory access control,  system call
    filtering, and capabilities.\n\nMandatory access control frameworks (AppArmor
    is the one you’ll see) offer system- (or container-) wide access control lists.
    You can read a [version of Docker’s default AppArmor template][AppArmor] to see
    what problems this fixes; it’s a nice concise description of the weaknesses of
    namespaces on their own.\n\nSystem call filters let us turn off kernel features;
    in 2020, if you’re filtering system calls, you’re probably doing it with seccomp-bpf.
    \n\nCapabilities split “root” into a whole mess of sub-privileges, ensuring that
    there’s rarely a need to give any program superuser access.\n\nThere are lots
    of implementations of this idea. \n\n[Modern Docker][tob], for instance, takes
    advantage of all these features. Though imperfect, the solution Docker security
    people arrived at is, I think, a success story. Developers don’t harden their
    application environments consciously, and yet, for the most part, they also don’t
    run containers privileged, or give them [extra capabilities][caps], or disable
    the MAC policies and [system call filters][dockerbpf] Docker enforces by default.
    \n\nIt may be even easier to jail a process outside of Docker; Googlers built
    [minijail][minijail] and [nsjail][nsjail], Cloudflare has [“sandbox”][cfsandbox],
    there’s [“firejail”][firejail], which is somewhat tuned for things like browsers,
    and [systemd will do some of this work for you][systemd]. Which tool is a matter
    of taste; nsjail has [nice BPF UX][kafel]; firejail interoperates with AppArmor.
    Some of them can be [preloaded][preload] into uncooperative processes. \n\nWith
    namespaced jails, we’ve arrived at the most popular current endpoint for workload
    isolation. You can do better, but the attacks you’ll be dealing with start to
    get subtle.\n\n### Language Runtimes\n\nA limitation of jailed application environments
    is that they tend to be applied container- or at least process-wide. At high-volumes,
    allocating a process for every job might be expensive. \n\nIf you relax the requirement
    to run ordinary Unix programs, you can get some of the benefits of jails without
    fine-grained per-process security models. Just compile everything to Javascript
    and run them in v8 isolates. The v8 language runtime makes promises, which you
    might or might not trust, about what cotenant jobs can access. Or you could use
    [Fastly’s Lucet][lucet] [serverside WASM][wasi] framework. \n\nFrom a security
    perspective, assuming you trust the language runtimes (I guess I do) these approaches
    are attractive when you can expose a limited system interface, which is what everyone
    does with them, and less attractive as a general design if you need all of POSIX.
    \n\n### Emulation\n\nHere’s a problem we haven’t addressed yet: you can design
    an intricate, minimal whitelist of system calls, drop all privileges, and cut
    most of the filesystem off. But then a Linux kernel developer restructures the
    memory access checks the kernel uses when deref’ing pointers passed to system
    calls, and someone forgets to tell the person who maintains waitid(2), and now
    [userland programs can pass kernel addresses to waitid][smepsmap] and whack random
    kernel memory. waitid(2) is innocuous, you weren’t going to filter it out, and
    yet there you were, boned. \n\nOr, how about this: every time a process faults
    an address, the kernel has to look up the backing storage to resolve the address.
    Since this is relatively slow, the kernel caches. But it has to keep those caches
    synchronized between all the threads in a process, so the per-thread caches get
    counters tied to the containing process. Except: the counters are 32 bits wide,
    and [the invalidation logic is screwed up][jannhbug], so that if you roll the
    counter, then immediately spawn a thread, then have that thread roll the counter
    again, you can desynchronize a thread’s cache and get the kernel to follow stale
    pointers.\n\nBugs like this happen. They’re called kernel LPEs. A lot of them,
    you can mitigate by tightening system call and device filters, and compiling a
    minimal kernel (you weren’t really using IPv6 DCCP anways). But some of them,
    like Jann Horn’s cache invalidation bug, you can’t fix that way. How concerned
    you are about them depends on your workloads. If you’re just running your own
    applications, you might not care much: the attacker exploiting this flaw already
    has RCE on your systems and thus some access to your internal network. If you’re
    running someone else’s applications, you should probably care a lot, because this
    is your primary security barrier. \n\nIf namespaces and filters constitute a “jail”,
    gVisor is The Village from The Prisoner. Instead of just filtering system calls,
    what if we just reimplement most of Linux? We run ordinary Unix programs, but
    intercept all the system calls, and, for the most part, instead of passing them
    to the kernel, we satisfy them ourselves. The Linux kernel has almost 400 system
    calls. How many of them do we need to efficiently emulate the rest? gVisor needs
    less than 20.\n\nWith those, gVisor implements basically all of Linux in userland.
    Processes. Devices. Tasks. Address spaces and page tables. Filesystems. TCP/IP;
    the entire IP network stack, all reimplemented, in Go, backended by native Linux
    userland.\n\nThe pitch here is straightforward: you’re unlikely to have routine
    exploitable memory corruption flaws in Go code. You are sort of likely to have
    them in the C-language Linux kernel. Go is fast enough to credibly emulate Linux
    in userland. Why expose C code if you don’t have to?\n\nAs batshit as this plan
    is, it works surprisingly well; you can build gVisor and `runsc`, its container
    runtime, relatively easily. Once you have `runsc` installed, it will run Docker
    containers for you. After reading the code, I sort of couldn’t believe it was
    working as well as it did, or, if it was, that it was actually using the code
    I had read. But I scattered a bunch of panic calls across the codebase and, yup,
    that all that stuff is actually happening. It’s pretty amazing.\n\nYou are probably
    strictly better off with gVisor than you are with a tuned Docker configuration,
    and I like it a lot. The big downside is performance; you’ll be looking at a low-double-digits
    percentage hit, degrading with I/O load. Google runs this stuff at scale in GCE;
    you can probably get away with it too. If you’re running gVisor, you should brag
    about it, because, again, gVisor is pretty bananas. \n\n### Lightweight Virtualization\n\nIf
    you’re worried about kernel attack surface but don’t want to reimplement the entire
    kernel in userland, there’s an easier approach: just virtualize. Let Linux be
    Linux, and boot it in a virtual machine.\n\nYou almost certainly already trust
    virtualization; if hypervisors are comprehensively broken, so is all of AWS, GCE,
    and Azure. And Linux [makes hypervising pretty simple][kvm]! \n\nThe challenge
    here is primarily about performance. A big part of the point of containers is
    that they’re lightweight. In a sense, the grail of serverside isolation is virtualization
    that’s light enough to run container workloads. \n\nIt turns out, this is a reasonable
    ask. A major part of what makes virtual machines so expensive is hardware emulation,
    with enough fidelity to run multiple operating systems. But we don’t care about
    diverse operating systems; it’s usually fine to constrain our workloads to Linux.
    How lightweight can we a virtual machine if it’s only going to boot a simple Linux
    kernel, with simple devices?\n\nTurns out: pretty lightweight! So we’ve got Kata
    Containers, which is the big-company supported serverside lightweight virtualization
    project that came out of Intel’s Clear Containers (mission statement: “come up
    with a container scheme that is locked in to VT-x”). Using QEMU-Lite, Kata gets
    rid of BIOS boot overhead, replaces real devices with their virtio equivalents,
    and aggressively caches, and manages to get boot time down by like 75%. [kvmtool][kvmtool],
    an alternative KVM runtime, gets even lighter.\n\nThere’s two catches.\n\nThe
    first, and really the big problem for the whole virtualization approach, is that
    you need bare metal servers to efficiently do lightweight virtualization; you
    want KVM but without nested virtualization. You’re probably not going to shell
    out for EC2 metal instances just to get some extra isolation.\n\nThe second, more
    philosophical problem is that QEMU and kvmtool are [relatively complicated C codebases][vhostbug],
    and we’d like to minimize our dependence on these. You could reasonably take the
    argument either way between gVisor, which emulates Linux in a memory-safe language,
    or Kata/kvmtool, which runs virtualized Linux with a small memory-unsafe hypervisor.
    They’re both probably better than locked-down `runc` Docker, though.\n\n### Firecracker\n\nLightweight
    virtualization is how AWS runs Lambda, its function-as-a-service platform, and
    Fargate, its serverless container platform. But rather than trusting (and painstaking
    tuning) QEMU, AWS reimplemented it, in Rust. The result is Firecracker.\n\n[Firecracker][firecrackerpaper]
    is a VMM optimized for security. It’s really kind of difficult to oversell how
    clean Firecracker is; the Firecracker paper boasts that they’ve implemented their
    block device in around 1400 lines of Rust, but it looks to me like they’re counting
    a lot of test code; you only need to get your head around a couple hundred lines
    of Rust code to grok it. The network driver, which adapts a Linux tap device to
    a virtio device a guest Linux kernel can talk to, is about 700 lines before you
    hit tests — and that’s rust, so something like 1/3 of those lines are use-statements!
    It’s really great.\n\nThe reason Firecracker (and, if you overlook the C code,
    kvmtool) can be this simple is that they’re pushing the system complexity down
    a layer. It’s still there; you’re booting an actual, make-menuconfig’d kernel,
    in all of it’s memory-unsafe glory. But you’re doing it inside a hypervisor where,
    in the Firecracker case, really you’re only worried about the integrity of the
    kvm subsystem itself. \n\nWe aren’t yet significant contributors to Firecracker,
    but it still feels weird talking the project up because it’s such a core part
    of our offering. That said: the team at AWS really did this thing the Western
    District Way:\n\n* The Firecracker VMM is tiny, easily readable, and deliberately
    implements the minimal number of concepts required to run a Linux server workload.
    \n* The VMM is written in Rust.\n* The VMM seccomp-bpf’s itself down to something
    like [40 system calls][firecrackerbpf]], several, including basic things like
    `fcntl`, `socket`, and `ioctl`, with tight argument filters.\n* Runs itself under
    an [external jailer][jailer] that chroots, namespaces, and drops privileges. \n\n###
    General Thoughts\n\nKeep in mind, I think, that no matter how intricate your Linux
    system isolation is, the most important attack surface you need to reduce is exposure
    to your network. If you can spend time segmenting an unsegmented single-VPC network
    or further tightening the default Docker seccomp-bpf policy, your time is probably
    better spent on the network. \n\nRemember also that when security tools designers
    think about isolation and attack surface reduction, they’re generally assuming
    that you need ordinary tools to run, and ordinary tools want Internet access;
    your isolation tools aren’t going to do the network isolation out of the box,
    the way they might, for instance, shield you from Video4Linux bugs.\n\nIt seems
    to me like, for new designs, the basic menu of mainstream options today is:\n\n*
    Jailing otherwise-unmanaged Unix programs with `nsjail` or something like it.\n*
    Running unprivileged Docker containers, perhaps with a tighter seccomp profile
    than the default.\n* Going full gVisor. \n* Running Firecracker, either directly
    or, in a K8s environment, with something like Kata. \n\nThese are all valid options!
    I’ll say this: for ROI purposes, if time and effort is a factor, and if I wasn’t
    hosting hostile code, I would probably tune an `nsjail` configuration before I
    bought into a containerization strategy. \n\n[vhostbug]: https://www.openwall.com/lists/oss-security/2019/09/17/1\n[kvm]:
    https://lwn.net/Articles/658511/\n[kvmtool]: https://elinux.org/images/4/44/Przywara.pdf\n[wasi]:
    https://github.com/WebAssembly/WASI/blob/master/phases/snapshot/docs.md\n[unveil]:
    https://www.openbsd.org/papers/BeckPledgeUnveilBSDCan2018.pdf\n[pledge]: http://www.openbsd.org/papers/hackfest2015-pledge/mgp00001.html\n[minijail]:
    https://github.com/google/minijail\n[systemd]: https://www.freedesktop.org/software/systemd/man/systemd.exec.html#SystemCallFilter=\n[cfsandbox]:
    https://github.com/cloudflare/sandbox\n[privsep]: https://security.stackexchange.com/questions/115896/can-someone-explain-how-sshd-does-privilege-separation\n[tob]:
    https://blog.trailofbits.com/2019/07/19/understanding-docker-container-escapes/\n[nsjail]:
    https://github.com/google/nsjail\n[caps]: https://docs.docker.com/engine/reference/run/#runtime-privilege-and-linux-capabilities\n[firejail]:
    https://firejail.wordpress.com/\n[jailer]: https://github.com/firecracker-microvm/firecracker/blob/master/docs/jailer.md\n[kafel]:
    https://github.com/google/kafel/\n[dockerbpf]: https://github.com/moby/moby/blob/master/profiles/seccomp/default.json\n[dockerbpfdoc]:
    https://docs.docker.com/engine/security/seccomp/#significant-syscalls-blocked-by-the-default-profile\n[AppArmor]:
    https://github.com/moby/moby/blob/master/profiles/apparmor/template.go\n[jannhbug]:
    https://googleprojectzero.blogspot.com/2018/09/a-cache-invalidation-bug-in-linux.html\n[smepsmap]:
    https://salls.github.io/Linux-Kernel-CVE-2017-5123/\n[preload]: https://github.com/cloudflare/sandbox/issues/1\n[qmail]:
    https://cr.yp.to/qmail/qmailsec-20071101.pdf\n[firecrackerbpf]: https://github.com/firecracker-microvm/firecracker/blob/master/src/vmm/src/default_syscalls/filters.rs\n[firecrackerpaper]:
    https://www.usenix.org/system/files/nsdi20-paper-agache.pdf\n[lucet]: https://github.com/bytecodealliance/lucet\n\n"
- :id: blog-serve-small-with-fly-io-and-gostatic
  :date: '2020-07-27'
  :category: blog
  :title: Serve small with Fly.io and GoStatic
  :author: dj
  :thumbnail:
  :alt:
  :link: blog/serve-small-with-fly-io-and-gostatic
  :path: blog/2020-07-27
  :body: "\n\nStatic websites are great for carrying unchanging content, be it assets,
    images, fonts or even, as in this case, an entire site. Well, I say entire site,
    but if you saw my [last article](/blog/hugo-s-there-flying-with-hugo-and-caddy/),
    you'll know I recently rebranded a Maker organization and needed to deploy a \"signpost\"
    page pointing people to the new site. I want this signpost to have a tiny footprint
    so it will never cost anything to deploy.\n\nNow, the thing with Docker images
    is that you don't really notice the layers of OS and applications that pile up
    in the background. Something as notionally simple as say running Apache HTTPD
    will still need an OS layer under it, no matter how minimal, you put the two parts
    together and the image size soon builds up. And you still have to add the content.\n\nThis
    is where something like [GoStatic](https://github.com/PierreZ/goStatic) comes
    in. It's a small, self-contained web page server which can run in a bare Docker
    image - no OS, just the binary. As the author points out, the official Golang
    images can weigh in with as much as half a gigabyte of image. For GoStatic, the
    image is an unchunky 6MB. \n\n## Go GoStatic\n\nSo, how do you make use of GoStatic
    on Fly? Let's step though it now.\n\nOne thing you need to know is that by default
    GoStatic uses port 8043. So add `-p 8043` to your `fly init` command when you
    create your project. That'll route traffic to port 80 and 443 to port 8043 on
    the application.\n\n```\n❯ mkdir examplegostatic\n❯ cd examplegostatic\n❯ fly
    init examplegostatic -p 8043\nSelected App Name: examplegostatic\n\n? Select organization:
    Dj (dj)\n\n? Select builder: Dockerfile\n(Create an example Dockerfile)\n\nNew
    app created\nName = examplegostatic\nOwner = dj\nVersion = 0\nStatus =\nHostname
    = <empty>\n\nWrote config file fly.toml\n```\n\nWe already have an index.html
    we want to serve, so our next stop is the Dockerfile. Delete the example contents
    and replace it with just two lines.\n\n```docker\nFROM pierrezemb/gostatic\n\nCOPY
    index.html /srv/http/index.html\n```\n\nAnd we are ready to deploy! Just run `flyctl
    deploy`:\n\n```\nDeploying examplegostatic\n==> Validating App Configuration\n-->
    Validating App Configuration done\nServices\nTCP 80/443 ⇢ 8043\n\nDeploy source
    directory '/Users/dj/examplegostatic'\nDocker daemon available, performing local
    build...\n==> Building with Dockerfile\nUsing Dockerfile: /Users/dj/examplegostatic/Dockerfile\nStep
    1/2 : FROM pierrezemb/gostatic\n ---> 4569615e9ed0\nStep 2/2 : COPY index.html
    /srv/http/index.html\n ---> b0b723d0cb24\nSuccessfully built b0b723d0cb24\nSuccessfully
    tagged registry.fly.io/examplegostatic:deployment-1595848012\n--> Building with
    Dockerfile done\nImage: registry.fly.io/examplegostatic:deployment-1595848012\nImage
    size: 7.7 MB\n==> Pushing Image\nThe push refers to repository [registry.fly.io/examplegostatic]\n77bf40a52322:
    Pushed\n3530b7ebed24: Pushed\ndeployment-1595848012: digest: sha256:d60567799841a4480e410acef113d33c1156eb960b94ae591931801089f61b1a
    size: 735\n--> Done Pushing Image\n==> Optimizing Image\n--> Done Optimizing Image\n==>
    Creating Release\nRelease v0 created\nDeploying to : examplegostatic.fly.dev\n\nMonitoring
    Deployment\nYou can detach the terminal anytime without stopping the deployment\n\n1
    desired, 1 placed, 1 healthy, 0 unhealthy [health checks: 1 total, 1 passing]\n-->
    v0 deployed successfully\n\n```\n\nOnce deployed, all you need to do then is `flyctl
    open` and a browser will open and navigate to the site. \n\n![Our served page](./screenshot.png)\n\nYou'll
    also notice that this has been upgraded to an https connection. All that is left
    is to attach a custom domain to it and we're done.\n\n## Behind the scenes\n\nSo,
    what magic is going on here? Well, the [Dockerfile](https://github.com/PierreZ/goStatic/blob/master/Dockerfile)
    in GoStatic explains a lot of it. \n\nIt uses a Docker multistage build to build
    our server binary in the first stage. Then it starts a new stage from scratch,
    literally using the command `FROM SCRATCH`. This says that there is no base image,
    just start building on top of nothing, an empty image. The rest of the GoStatic
    Dockerfile creates a passwd file so there are some usernames to work with and
    copies over the GoStatic binary. \n\nAnd then it all hands over to our own Dockerfile.
    GoStatic serves files out of `/srv/http` so we copy over our `index.html` to that
    directory. And that's it. Everything else is managed by Fly, the build, the deployment
    and the upgrading to an https connection. There used to be a version of GoStatic
    which would handle HTTPS connections and certificates, but that functionality
    has been retired now servers like Caddy exist. On Fly, the lack of HTTPS support
    means it's simple to just let Fly take on the HTTPS work for you.\n\n## A Small
    Squeeze\n\nOne last tip. Fly deploys new applications with 512MB of RAM and about
    a quarter of a virtual CPU. It's called a `micro-2x` firecracker VM. But, we're
    doing so little here, we could scale the VM size down. Let's look at the scale
    settings:\n\n```\n~/examplegostatic\n❯ flyctl scale show\n     Scale Mode: Standard\n
    \     Min Count: 0\n      Max Count: 10\n        VM Size: micro-2x\n```\n\nAnd
    now we can set the vm size:\n\n```\n~/examplegostatic\n❯ flyctl scale vm micro-1x\nScaled
    VM size to micro-1x\n      CPU Cores: 0.12\n         Memory: 128 MB\n  Price (Month):
    $2.670000\n Price (Second): $0.000001\n```\n\nIf you want to find out about the
    other VM sizes, run `flyctl platform vm-sizes`.\n\n## Wrapping up\n\nWe've got
    ourselves a tiny static web server and deployed it to [Fly.io](http://fly.io),
    and as a bonus, shrunk the VM's footprint to match it and save running costs.
    Enjoy!\n"
- :id: blog-hugo-s-there-flying-with-hugo-and-caddy
  :date: '2020-07-21'
  :category: blog
  :title: Hugo's There - Flying with Hugo and Caddy
  :author: dj
  :thumbnail:
  :alt:
  :link: blog/hugo-s-there-flying-with-hugo-and-caddy
  :path: blog/2020-07-21
  :body: "\n\nThere I was wondering what to do about a website for a new community
    venture I was running where I thought, yes, let's generate the site with [Hugo](https://gohugo.com),
    serve it with [Caddy](https://caddyserver.com/) and run it all on Fly. Why Hugo
    and Caddy? Well, they both have good reputations as Go-based tooling thats compact
    and powerful, so let's go make and host a site...\n\n## Fly First\n\nI'll need
    to be able to reference what the app name, and consequently site name, will be.
    Because of that, I'm going to start by initializing my Fly deployment:\n\n```\n$
    flyctl init makeronicc --dockerfile -p 80\n\nSelected App Name: makeronicc\n\n?
    Select organization: Dj (dj)\n\nNew app created\n  Name     = makeronicc\n  Owner
    \   = dj\n  Version  = 0\n  Status   =\n  Hostname = <empty>\n\nWrote config file
    fly.toml\n\n$\n```\n\nSelect your own app name or let the system generate one
    for you; it won't matter as we're going to front this set-up to a custom domain.
    We're going to be using a Dockerfile (hence `--dockerfile`) and our server will
    operate on port 80 (`-p 80`). \n\n## The Hugo Configuration\n\nThe site doesn't
    have much content and a stroll through the [Hugo Quickstart](https://gohugo.io/getting-started/quick-start/)
    will get us a front page and a blog article built in no time at all. It boils
    down to installing hugo, then installing a hugo theme and finally creating a `config.toml`
    file. Here's ours:\n\n```toml\nbaseURL = \"https://makeronicc.fly.dev\"\nlanguageCode
    = \"en-us\"\ntitle = \"Makeroni\"\ntheme = \"ananke\"\n[params]\nsite_logo = \"/images/makeroni-logo-small.png\"\n```\n\nThat's
    enough for our basic site to build running `hugo` which generates all the static
    files. Running `hugo server -D` lets us browse it locally on localhost:1313. The
    one thing to note? We'll be serving this site up on the [makeronicc.fly.dev](http://makeronicc.fly.dev)
    domain for now.\n\n## Docker And Hugo\n\nNext we want to get Docker to do the
    build work. Time to make a Dockerfile. \n\nThere are various Hugo docker images
    out there but the one I like is [`klakegg/hugo`](https://hub.docker.com/r/klakegg/hugo/)
    on Docker Hub. As well as having Docker images that can be used for running Hugo
    as a Docker container, there's an ONBUILD image. This is designed to be a stage
    in a multi-stage Docker build. It's run as part of the pipeline, but then results
    can be copied from its image to a new, cleaner image, less all the build tools.
    \n\nSo our Dockerfile starts like this:\n\n```docker\nFROM klakegg/hugo:0.74.0-onbuild
    AS hugo\n```\n\nThat, when built with Docker, will load up the image and the current
    working directory contents, run `hugo` over it and deposit the results in `/target`
    in the hugo image. Hugo build, done.  Now, let's talk about serving it up.\n\n##
    Caddy Hack\n\nNow we come to Caddy, which is a great web server \"built for now\"
    - It has integrated handling of obtaining and managing Let's Encrypt certificates
    so running an HTTPS site becomes super-simple. There's only one issue - Fly already
    does all that certificate management for us, so although we want Caddy because
    it's compact and easy to work with, we're going to want to turn off Caddy's own
    certificate system.\n\nCaddy is configured in a number of ways, JSON, API or the
    Caddyfile. I use the Caddyfile for this as its more human-readable. But now a
    public service announcement:\n\n<div class=\"callout\">\n\nWhen you search for
    Caddy and, well, anything at all, when you get to a result, scroll to the top
    of the page to make sure you aren't on the Caddy 1 documentation. Caddy 2 is the
    current version but the google-juice for Caddy 1 documentation is still super
    high and the two are so similar yet different, it can be terribly frustrating
    to keep landing on the wrong docs. \n\n</div>\n\nRight, back to creating our Caddyfile.
    Most of what I just talked about can be summed up in one opening block.\n\n```caddy\n{\n\tauto_https
    off\n}\n```\n\nThat turns off all the certificate management. Now we can tell
    Caddy to serve files for our [makeroni.cc](http://makeroni.cc) domain. \n\n```caddy\nhttp://makeronicc.fly.dev
    {\n\troot * /usr/share/caddy\n\tfile_server\n}\n```\n\nNotice that this is just
    for the http protocol connections. That's because, once the TLS connection has
    passed through the Fly edge, it travels on the encrypted Fly network as a normal
    HTTP request.\n\nThat's the Caddyfile created. Now to pull the two parts together
    in the Dockerfile.\n\n## Adding Caddy\n\nAt the moment, our Dockerfile simply
    brings in and runs the Hugo static generator at build time. We need to take the
    results of that and put it into a Caddy docker image. There are [official Caddy
    images](https://registry.hub.docker.com/_/caddy), so I'll use one of them:\n\n```docker\nFROM
    klakegg/hugo:0.74.0-onbuild AS hugo\n\nFROM caddy:2.1.1\n```\n\nDocker will now
    start with this Caddy image. Our Caddyfile says it will serve files out of `/usr/share/caddy`
    so we'll want to copy the files from our Hugo build over to there by adding: \n\n```docker\nCOPY
    --from=hugo /target/ /usr/share/caddy/\n```\n\nThe `--from` points to the named
    image we created at the start with `AS hugo`. Now all we need is to put the Caddyfile
    in place. \n\n```docker\nCOPY ./Caddyfile /etc/caddy/Caddyfile\n```\n\n## Ready
    To Fly\n\nWe're ready to publish the site. Run `flyctl deploy` and watch as the
    Hugo site is built, copied into a Caddy image, that image is then flattened and
    despatched to a Fly firecracker node. You don't have to worry about that though,
    just run `flyctl open` and your browser will open on your application. \n\nOne
    thing worth noticing is that, although we only configured http://makeroni.fly.dev,
    it's being automatically upgraded to https: to secure the connection. \n\nBut
    we aren't done yet. Remember we wanted the site to be provisioned on makeroni.cc.\n\n##
    Fly Domain\n\nThe first step is to get the DNS system to point [makeroni.cc](http://makeroni.cc)
    to the IP address of [makeroni.fly.dev](http://makeroni.fly.dev). I can get the
    IP Address by running `flyctl ips list`. \n\n```plain\n$ flyctl ips list\n\nTYPE
    ADDRESS                              CREATED AT\nv4   77.83.141.28                         2020-07-07T21:26:36Z\nv6
    \  2a09:8280:1:9f04:7aa6:a706:a3d7:ccba 2020-07-07T21:26:36Z\n\n```\n\nWe want
    the V4 address. Now, I need to go to the registrar of the DNS entry, in my case
    NameCheap, and get to the DNS management pages, specifically the Advanced Management
    page of that. \n\nIt's there I can add an A record. `A @ 77.83.141.28` (The @
    goes in the host column on Namecheap).\n\nSave that and let it propagate and then
    go to [http://makeroni.cc](http://makeroni.cc) and you should see your site. If
    you follow any links though, you'll notice you are back on https://makeronicc.fly.dev.
    That's because Hugo generated all the links with that address. \n\nThat's easy
    enough to fix (we'll get back to it in a moment), but there's another more important
    thing to look at. \n\nIf I try to go to [https://makeroni.cc](https://makeroni.cc)
    and I get an error saying the connection is not secure. I haven't created a TLS
    certificate for the domain. \n\nThe quickest way to do this is to add an AAAA
    record in the same way I added an A record. The AAAA record in a DNS record should
    point to the IP V6 address for the host; it's up in the `flyctl ips list` output
    too. So I add `AAAA @ 2a09:8280:1:9f04:7aa6:a706:a3d7:ccba` to the DNS. With that
    in place and propagated I can now go and request a certificate.\n\n```plain\n$
    flyctl certs create makeroni.cc\nHostname = makeroni.cc\nConfigured = true\nIssued
    =\nCertificate Authority = lets_encrypt\nDNS Provider = enom\nDNS Validation Instructions
    = CNAME _acme-challenge.makeroni.cc => makeroni.cc.2yz0.flydns.net.\nDNS Validation
    Hostname = _acme-challenge.makeroni.cc\nDNS Validation Target = makeroni.cc.2yz0.flydns.net\nSource
    = fly\nCreated At = just now\nStatus = Ready\n```\n\nAnd the traffic could flow
    securely.... Except there's one last change we need to make to the Caddyfile.\n\n##
    Caddy Changes\n\nRemember I set up the Caddyfile with\n\n```caddy\nhttp://makeronicc.fly.dev
    {\n\troot * /usr/share/caddy\n\tfile_server\n}\n```\n\nWell, I'm not serving the
    files on that URL now so I'll need to change that to match our custom domain:\n\n```\nhttp://makeroni.cc/
    {\n\troot * /usr/share/caddy\n\tfile_server\n}\n```\n\n Now it'll respond to requests
    for files from the [makeroni.cc](http://makeroni.cc) domain... but what if I want
    to make sure that people who accidentally access the old [makeronicc.fly.dev](http://makeronicc.fly.dev)
    site end up in the right place. For that, another rule in the Caddyfile is needed:\n\n```\nhttp://makeronicc.fly.dev
    {\n\tredir https://makeroni.cc/\n}\n```\n\nNow those old requests will head to
    our new server. Redeploy the image to Fly and everything is ready to roll.\n\n##
    Wrapping Up\n\nWe've gone through configuring a multistage image build which generated
    a static Hugo site, then loaded it into an image with Caddy. We've configured
    Caddy for development deployments on Fly and then we've got ourselves a custom
    domain set up, and made that work with TLS certificates from Let's Encrypt and
    a small modification to Caddy's setup.\n"
- :id: blog-flyctl-evolved-fly-changelog
  :date: '2020-07-10'
  :category: blog
  :title: Flyctl Evolved - Fly Changelog
  :author: dj
  :thumbnail:
  :alt:
  :link: blog/flyctl-evolved-fly-changelog
  :path: blog/2020-07-10
  :body: "\n\n<p class=\"lead\">\n\nThere's a new flyctl (v0.0.137) available for
    your command line, with cleaner commands and extra helpers. Find out more about
    it in the Changelog.\n\n</p>\n\nThis flyctl release brings in some big changes
    in the command structure as we move to an app-centric command style. What does
    that mean? Well, the `apps` subcommand is being deprecated; we've kept it in place
    for this release but now all its commands have top level commands of their own:\n\n|
    Was | Now |\n|---|---|\n|apps create|init|\n|apps destroy|destroy|\n|apps list|list
    apps|\n|apps move|move|\n|apps restart|restart|\n|apps resume|resume|\n|apps suspend|suspend|\n\nThe
    `move`, `restart`, `resume`, `suspend` commands also now take an appname as their
    last argument. The status command has also followed suit in this change, so you
    can now type `flyctl status appname` rather than `flyctl status -a appname` -
    The `-a` option will remain supported. \n\nThe list command at the top level has
    been around for a while and has advantages over the older list command: you can
    match appnames with fragments of text and filter on status or organization. Talking
    about organizations, `flyctl list orgs` will list the organizations your account
    has access to.\n\nWe've also made some small usability changes in how you initialize
    an application. The `init` command offers you a selection of builders or the chance
    to use a Dockerfile. If you don't have one, `init` will create one for you with
    a simple hello world deployment to get you going. If you don't want the example
    generated, use `--dockerfile` when running `init`. And, yes, you can still specify
    a builder with `--builder`, that's not going away. \n\n## Other changes\n\n- The
    `move` command will now tell you what organization your app is in so you know
    where you are moving it from.\n- Setting and unsetting `secrets` on suspended
    deployments is now blocked.\n- Setting and unsetting `secrets` will start a deployment
    monitor. Add the `--detach` flag to return before starting the deployment monitor.
    \n- If you are setting a secret on an undeployed app, then we don't start the
    deployment monitor, so no need to --detach.\n- The `info` command now supports
    `--host` which will display just the host name, along with (`-n` / `--name` )
    which just displays the appname. These flags are designed to make it easier to
    script with flyctl.\n- The `version` command now outputs just the bare version
    number - add `--full` to get the detailed version/commit/date information if you
    need it.\n\n## Platform changes\n\nIt's not all been flyctl changes. There's a
    fix for a problem with cookie headers and HTTP/2 which is now in place. Also,
    if your application parses headers, you'll find that a [Via header](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Via)
    has been added to enable applications to trace their route through the Fly edge.\n\n\n<p
    class=\"callout\">\n This is the Fly Changelog where we list all significant changes
    to the Fly platform, tooling and web sites. You can also use the RSS feed of just
    changelog posts available on [fly.io/changelog.xml](https://fly.io/changelog.xml)
    or consult our dedicated [ChangeLog](https://fly.io/changelog/) page with all
    the recent updates.\n</p>\n\n<!-- start -->\n\n## _9th July_\n\n**flyctl**: Version
    [0.0.137 released](https://github.com/superfly/flyctl/releases/tag/v0.0.137)\n\n-
    ++ New top level commands\n- ++ Apps subcommand deprecated\n- ++ `--host` added
    to info command\n- ++ `version` displays bare version number, `--full` displays
    full details\n- ++ `secrets` setting and unsetting will follow deployment where
    appropriate\n- ++ `secrets` now supports `--detach`\n- ++ `move` command now prompts
    with current organization\n- ++ `init` command now prompts with list of builders
    or option to create/use Dockerfile\n- ++ `init` command supports `--dockerfile`
    flag to completely skip builder query\n\n**Fly Platform/Web**\n\n- ~~ Fixed a
    bug related to incoming HTTP/2 request cookies sent as multiple headers. Incoming
    HTTP/2 connections can present multiple cookie headers. The headers were sent
    on as is when the Fly edge downcast the connection to HTTP/1  which is used within
    the Fly network. Some servers could not handle the multiple headers though. Now,
    the downcasting process concatenates the multiple cookie headers into a single
    cookie header. \n- ++ Added the [Via header](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Via)
    to both http requests and responses\n- ++ Boosted the performance of the `Optimizing
    Image` phase of deployment by making better use of existing identical images.\n"
- :id: blog-run-apollo-graphql-close-to-your-users
  :date: '2020-07-01'
  :category: blog
  :title: Run Apollo Server Close to Your Users
  :author: kurt
  :thumbnail:
  :alt:
  :link: blog/run-apollo-graphql-close-to-your-users
  :path: blog/2020-07-01
  :body: |2


    <p class="lead">Fly.io can run API servers close to users. It's kind of like a CDN for your GraphQL server. Here's a guide to building an edge [GraphQL Server with Apollo](https://fly.io/docs/app-guides/graphql-edge-caching-apollo/) and Redis.</p>

    I'm a newly minted GraphQL convert. We built Fly on top of GraphQL and the experience turned me into a shameless cheerleader. An API format with static typing? That's my jam.

    (If you don't care for JAMStack puns you can just go read our guide on [building an Edge GraphQL service with Apollo](https://fly.io/docs/app-guides/graphql-edge-caching-apollo/))

    Speaking of jam, you've probably used application stacks that push content close to users. Hosting JavaScript and markup on a CDN can help make an app snappy.

    You can _also_ apply CDN like infrastructure to a GraphQL API to get a nice speed boost. All you need is a way to run API servers and an application cache close to users.

    Which is why we built a platform to run API servers close to users and paired it with a global Redis cache service. It's a great place to run Apollo Server, for example, with its [cache capabilities](https://www.apollographql.com/docs/apollo-server/performance/caching/) and first class Redis support.

    We built a demo GraphQL API based on the Open Library REST API. Read [the guide](/docs/app-guides/graphql-edge-caching-apollo/) or check out [the source code](https://github.com/fly-apps/edge-apollo-cache/).
- :id: blog-how-cdns-generate-certificates
  :date: '2020-06-25'
  :category: blog
  :title: How CDNs Generate Certificates
  :author: thomas
  :thumbnail:
  :alt:
  :link: blog/how-cdns-generate-certificates
  :path: blog/2020-06-25
  :body: "\n\nIt’s been a hectic first couple of weeks at Fly, and I’m writing things
    up as I go along, because if I have to learn, so do you. This is going to be a
    bit of a meander; you’ll have to deal.\n\nLet’s start with “[what’s Fly?][whatsfly]”
    Briefly: Fly is a content delivery network for Docker containers. Applications
    hosted on Fly are fast because they’re running on machines close to users. To
    do that, we run bare metal servers in a bunch of cities and host containers on
    them in [Firecracker VMs][firecracker]. We proxy traffic from edge servers to
    containers through a global [WireGuard][wireguard] mesh. It’s much easier to play
    with than ECS or K8s is, so [signing up for a free account][signup] is probably
    the best way to get a feel for it, and a pleasant way to burn 5-10 minutes.\n\nObviously,
    to do stuff like this, you need to generate certificates. The reasonable way to
    do that in 2020 is with LetsEncrypt. We do that for our users automatically, but
    “it just works” makes for a pretty boring writeup, so let’s see how complicated
    and meandering I can make this.\n\nIt's time to talk about certificate infrastructure.\n\n\n###
    ACME\n\nRather than verifying information from [“Qualified Independent Information
    Sources”][qiis], LetsEncrypt does domain-validated certificates, based simply
    on proof of ownership of a domain, and is driven by a protocol called ACME. ACME
    is really simple. It’s been [implemented in almost pure Bourne shell][acmesh].
    The most complicated thing about it is JWS signatures, which are awful, but at
    least standardized. The ACME protocol is itself done over normal HTTP requests;
    the flow is roughly:\n\n1. You make an account and associate an ECDSA/EdDSA key
    with it, which subsequently authenticates all your requests.\n2. You then post
    an “order” for a certificate, specifying the DNS names you need it for. You’re
    given “authorization” and “finalization” URLs in return.\n3. You retrieve the
    authorizations and select challenges to prove you own the domains.\n4. You set
    up the challenges on your own side, and then poll a status URL to verify that
    the challenges have completed.\n5. You post the CSR for your certificate to the
    “finalization” URL.\n\nACME challenges are intended to verify your ownership of
    a domain. There are three of them (four, if you count preauthorization, which
    LetsEncrypt doesn’t do); originally, they were:\n\n1. `tls-http-01`, in which
    you’re given a token to put on your server, under /.well-known/acme-challenge,
    and serve to LetsEncrypt’s client on 80/tcp. This is simple to describe and implement,
    but requires you to respond to HTTP requests on 80/tcp, which lots of people (sensibly)
    don’t want to do.\n\n1. `tls-dns-01`, in which you’re given a token to put in
    a TXT record in your DNS zone. This directly proves control over a domain, but
    it can be hard for operators to do. In particular, especially in larger organizations,
    the people who need certificates are not necessarily given access to DNS configuration.\n\n1.
    `tns-sni-01`, in which you’re given a token to embed in the SAN of a certificate
    you serve to TLS clients who request it through TLS SNI, which is TLS’s equivalent
    of the HTTP “Host” header. This is more complicated to implement, but is the most
    seamless of the challenges: all you need to do it is to run the TLS server you
    were going to run anyways.\n\n### The Story Of tls-sni-01\n\nBut `tls-sni-01`
    no longer exists, because [it’s insecure][tlssni]. The problem with SNI challenges
    is shared hosting.\n\nBecause IP addresses are scarce, many hosting providers
    arrange for customers to share IP addresses. As requests arrive for customers,
    they’re routed based on SNI.\n\nIn the same way that you can configure a local
    nginx to respond to any Host header without breaking the Internet, hosting providers
    routinely allow people to “claim” arbitrary hostnames on their platforms. This
    ostensibly doesn’t matter, because without control of the DNS, you can’t get people
    to talk to your claimed hostname.\n\nSimilarly, hosting providers will often let
    you provide your own TLS certificates.\n\nYou may see where this is going already.
    Here’s what LetsEncrypt did to verify domain ownership using SNI:\n\n1. It generated
    a token for you to put in a self-signed certificate, in the form of an “.acme.invalid”
    hostname.\n2. It resolved the hostname you were generating a certificate for in
    the DNS and connected to it.\n3. It asked for the token via TLS SNI specifying
    the “.acme.invalid” name.\n4. It read the certificate generated and checked to
    make sure the token was present.\n\nIf a hosting provider let you claim names
    in the “.invalid” TLD, and upload your own certificate for them, you could get
    a certificate issued for all the customers hosted on your IP. Heroku let you do
    this, as did AWS Cloudfront, and who knows who else.\n\nLetsEncrypt quickly [took
    the SNI challenges][lesni] down while hosting providers deployed fixes. Ultimately,
    SNI was so widely used this way that CAs concluded SNI was fundamentally unsafe
    to use as a challenge, and the ACME SNI challenge was deprecated, and finally
    removed last year.\n\n### A Note About A Related Problem\n\nThis attack is an
    instance of a broader attack class called “subdomain takeover”, which is a mainstay
    among bug bounty hunters. [HackerOne will tell you all about it][h1takeover],
    if you want to make $50 or so in an evening.\n\nSo, any time you’re hosting content
    for customer domains, you have the problem of what happens when the customer stops
    using your service. As you might expect, lots of times you’ll forget to stop forwarding
    DNS to old expired services. But your account on those services has lapsed, and
    that usually means that other people can claim the same names you were using.
    Since you’re still directing traffic to the service, the new claimant has now
    hijacked one of your subdomains.\n\nWhich is bad for all kinds of reasons; it
    allows you to steal cookies, violate CORS, bypass CSP; it even impacts OAuth2.\n\nFly
    mitigates this problem for ALPN challenges by not reusing IP addresses. Every
    application gets a unique, routable IPv6 address, and we won’t attempt Lets Encrypt
    validation unless the target hostname resolves via CNAME to that IPv6 address.
    (We do something similar for DNS challenges).\n\n### ALPN\n\nRecall the virtue
    of the `tls-sni-01` challenge: it doesn’t require you to have access to your DNS
    configuration, nor do you need to open 80/tcp. You want a challenge that works
    this way. And there is one: the new third ACME challenge, `tls-alpn-01`.\n\nTo
    grok `tls-alpn-01`, you’ll of course need to know what ALPN is. It’s an easy concept:
    Imagine TLS was a transport protocol in its own right, alongside TCP and UDP;
    ALPN would be its port number. I mean, they’re strings, not numbers, but same
    idea.\n\nWhy does TLS need such a thing? Most things that use TLS have their own
    TCP ports already. The answer is, of course, [HTTP/2][h2]. HTTP/2 isn’t wire-compatible
    with HTTP/1 (it’s a binary protocol optimized for pipelining). But it can’t have
    its own TCP port, because if it did, nobody would be able to speak it: huge chunks
    of the Internet are locked down to ports 80 and 443.\n\n(We’re not, at Fly, by
    the way; you can run any TCP service you want here. But I digress from my digressions).\n\nTo
    solve this problem, when Google was designing SPDY (HTTP/2’s predecessor), they
    came up with NPN, [“Next Protocol Negotiation”.][npn] The way NPN worked was:\n\n1.
    A TLS client added an NPN extension to their ClientHello, the message TLS clients
    send to open up a connection.\n2. A supportive TLS server would respond with a
    ServerHello that had an NPN extension populated with the protocols it supported.\n3.
    A key exchange having been completed, both sides of the connection would switch
    on encryption.\n4. The TLS client would send an encrypted NextProtocol message
    that chose a next protocol (which technically may or may not have been one listed
    by the server, if both sides were trying to be sneaky about things).\n\nBy doing
    this, Chrome could opt into SPDY when talking to Google servers without burning
    a round trip for the negotiation.\n\nWhen SPDY turned into HTTP/2, something like
    NPN needed to get standardized, too. But the IETF tls-wg \n<a href=\"javascript:alert('ultimately,
    I concluded this mailing list post was too boring even for me')\">wasn’t a fan
    of NPN</a>; \nin particular, it reversed the normal order of TLS negotiation,
    where the client proposes and the server chooses. So the IETF came up with ALPN,
    [Application Layer Protocol Negotiation][alpn]. ALPN works like this:\n\n1. A
    TLS client adds an ALPN extension to its ClientHello indicating all the protocols
    it supports.\n2. A TLS server indicates which protocol it selected in the ALPN
    extension in its ServerHello.\n3. That’s pretty much it.\n\nThere’s a clear privacy
    implication here, right? Because the ALPN protocol you might be asking for is
    “tor”. The IETF ruins everything. And that’s true, but it’s complicated.\n\nFirst,
    the security offered by the encrypted NextProtocol frame was a little sketchy.
    Here’s an [outline of an attack:][npnattack]\n1. Alice connects to Bob, and Mallory
    really wants to know what protocol Alice is going to ask for.\n2. Mallory MITMs
    the connection and downgrades its security. Remember that NPN is running /inside/
    the handshake, not /after/ it, when the “Finished” message has cryptographically
    authenticated the handshake.\n3. Alice sends her NextProtocol frame to Mallory
    on the downgraded connection.\n4. Mallory drops the connection and reads the NextProtocol.\n5.
    Alice, meanwhile, re-connects, because that’s what you do, and repeats the exact
    same process with Bob directly, sending the same NextProtocol.\n\nIn practice,
    with Firefox, you could at one point do this [simply by sending a bogus certificate][ffcert];
    Firefox would complete the handshake, NPN included, even if the certificate didn’t
    validate.\n\n(For what it’s worth, some of the privacy issues here got mooted
    in [TLS 1.3][t13]). \n\n### The JPEG Cat Extension\n\nAdditionally, while privacy
    was doubtlessly on Adam Langley’s mind when he wrote the NPN spec, the more important
    problem was probably middlebox compatibility. \n\nThe way middleboxes work is,
    enterprises buy them. They’re quite expensive, and enterprises buy big ghastly
    bunches of them in one go, so vendors work really hard to win those deals. And
    one straightforward way to win a bakeoff is to come to it with more features than
    your competitors. Here’s a feature: “filter connections based on what application
    protocol the client selects”. The Chrome team, presumably seeing that dumb feature
    a mile away, took it off the table by encrypting NPN selections.\n\n(This sounds
    paranoid, but only if you’ve never worked on real-world TLS. In the NPN vs. ALPN
    tls-wg thread, AGL cited an ISP they found in the UK that took it upon themselves
    to block all the ECDHE ciphersuites. Why? Who knows? People do stuff like this.)\n\nUltimately,
    ALPN beat out NPN in the tls-wg. But, just as they were wrapping up the standard,
    Brian Smith at Mozilla (and author of Rust’s ring crypto library) [threw a wrench
    in the works][bsmith].\n\nIt had been Mozilla’s experience that, in some cases,
    middleboxes would hang when they got a ClientHello that was more than 255 bytes
    long. Hanging is very bad, because Mozilla needed timeout logic to detect it and
    try a simpler handshake, but that logic would also fire for people on crappy Internet
    connections, and had the effect of preventing those people from using modern TLS
    at all.\n\nMiraculously, a day later, Xiaoyong Wu at F5 [jumped onto the thread][f5]
    to explain that older F5 software confused 256 byte ClientHello frames with TLSv2.
    TLS frame lengths are 2 bytes wide; once the ClientHello ticks past 255 bytes,
    the high length byte becomes 01h. That byte occupies the same point in the frame
    as the message type in SSLv2. To the F5, the frame could be a long-ish ClientHello…
    or a very long SSLV2_MT_CLIENTHELLO, which was also 01h. The F5 chose SSLv2.\n\nThe
    fix? Send /more/ bytes! At 512 bytes, the high length byte is no longer 01h. And
    thus was born the “jpeg-of-a-cat” extension, which AGL took the fun out of by
    renaming it [“the TLS ClientHello Padding Extension”.][pad]\n\n### Back To ACME\n\nThis
    is a little anti-climactic, but we’ve come all this way, so you might as well
    understand how Fly (and other CDNs, and things like Caddy) generates certificates
    with ACME:\n\n1. We request a `tls-alpn-01` challenge from LetsEncrypt for your
    hostname, using our ACME account.\n2. LetsEncrypt gives us a token, for which
    we generate a self-signed certificate with the token embedded, that we load into
    our distributed certificate storage.\n3. We say (in ACME) “go ahead”, and LetsEncrypt
    looks up the hostname we’re serving, connects to it, and sends a ClientHello with
    “acme-tls/1” set as the ALPN protocol.\n4. Our Rust proxy catches the ACME ALPN
    case, retrieves the challenge certificate, and feeds it to LetsEncrypt.\n5. LetsEncrypt
    drops the connection, sets the challenge to completed, and allows us to complete
    the certificate generation.\n\nThe ALPN challenge is more explicit than the SNI
    challenge; we had to specifically set up a subservice to complete ALPN challenges
    for customers, rather than doing it sort of implicitly based on our native SNI
    handling. (We wouldn’t have had the problem anyways based on how our certificate
    handling works, but this is the logic behind why ALPN is OK and SNI isn’t).\n\nThis
    process is pretty much seamless; all you have to do is say “yeah, I want a TLS
    certificate for my app’s custom domain”. It only works with individual hostnames,
    though, which may be fine, but if it isn’t, you can do a DNS challenge with us
    to generate a wildcard certificate.\n\n[whatsfly]: https://news.ycombinator.com/item?id=22616857\n[firecracker]:
    https://firecracker-microvm.github.io/\n[wireguard]: https://www.wireguard.com/\n[signup]:
    https://fly.io/\n[qiis]: https://www.dnb.com/\n[acmesh]: https://github.com/acmesh-official/acme.sh/blob/master/acme.sh\n[tlssni]:
    https://labs.detectify.com/2018/01/12/how-i-exploited-acme-tls-sni-01-issuing-lets-encrypt-ssl-certs-for-any-domain-using-shared-hosting/\n[lesni]:
    https://community.letsencrypt.org/t/important-what-you-need-to-know-about-tls-sni-validation-issues/50811\n[h1takeover]:
    https://www.hackerone.com/blog/Guide-Subdomain-Takeovers\n[h2]: https://daniel.haxx.se/http2/\n[npn]:
    https://tools.ietf.org/id/draft-agl-tls-nextprotoneg-03.html\n[alpn]: https://tools.ietf.org/html/rfc7301\n[npnattack]:
    \ https://mailarchive.ietf.org/arch/msg/tls/ZptP2xSKLPOgJElh5TEhNbolV8c/\n[ffcert]:
    https://bugzilla.mozilla.org/show_bug.cgi?id=925791\n[bsmith]: https://mailarchive.ietf.org/arch/msg/tls/lAqVI-bqWp_KltFHK2OoSflRvL8/\n[f5]:
    https://mailarchive.ietf.org/arch/msg/tls/8wXwhM1d5WSmROHFSgrTyFmWN2o/\n[t13]:
    https://crypto.iacr.org/2018/affevents/cwtls/medias/Eric_Rescorla.pdf\n[pad]:
    https://tools.ietf.org/html/rfc7685\n"
- :id: blog-flyctl-meets-json
  :date: '2020-06-23'
  :category: blog
  :title: Flyctl meets JSON
  :author: dj
  :thumbnail:
  :alt:
  :link: blog/flyctl-meets-json
  :path: blog/2020-06-23
  :body: "\n\n<p class=\"lead\">\n\nWe've just shipped a new version of Flyctl and
    although there's only one new flag added, it's a flag that can change the way
    you use flyctl forever. \n\n</p>\n\nSay hello to `--json` or `-j` for short. This
    new flag attempts to present all output from flyctl as JSON. If a command queries
    the Fly GraphQL API, you'll get the JSON data from that call in your output. If
    a command shows you logs, you'll get the logs as structured JSON. When you are
    deploying, _nearly_ all (we'll talk about that _nearly_ later) the output of the
    command comes in the form of JSON formatted messages.\n\nYou may wonder what this
    can do for you. Well, we hope it'll let you create your own automation solutions
    for your workflow with Fly. \n\n## Getting the JSON out\n\nLet's start with an
    example:\n\n```go\n$ flyctl list apps --json\n\n[\n    {\n        \"ID\": \"Z0zYDgDjg2V3mul\",\n
    \       \"Name\": \"apienterprise\",\n        \"Status\": \"running\",\n        \"Deployed\":
    true,\n        \"Hostname\": \"apienterprise.fly.dev\",\n        \"Organization\":
    \"dj\"\n    },\n    {\n        \"ID\": \"GqXY09mQ7Rwyauq4\",\n        \"Name\":
    \"red-paper-8243\",\n        \"Status\": \"running\",\n        \"Deployed\": true,\n
    \       \"Hostname\": \"red-paper-8243.fly.dev\",\n        \"Organization\": \"dj\"\n
    \   },\n...\n```\n\nWith JSON output enabled, the list apps command returns a
    JSON array of objects with the id, name, status, deployment, hostname and organization
    of each Fly application you have access to. \n\nAny application which accepts
    JSON can work with this data. If you're scripting, you'll most likely want to
    process, filter and reorganize the JSON data. For that we recommend [`jq`](https://stedolan.github.io/jq/).
    With jq you get a tool that can slice and dice JSON files into the data you want.
    \n\nFor example, say you just wanted a list of hostnames and status  to feed into
    your management platform, and your management platform only accepted CSV files.\n\n```go\n$
    flyctl list apps --json | jq -r '.[] | [ .Hostname,.Status ] | @csv'\n\"apienterprise.fly.dev\",\"running\"\n\"red-paper-8243.fly.dev\",\"running\"\n...\n```\n\n##
    Get in the jq\n\nWe aren't going to go into jq in depth today. It's a tool that
    is rich with features. Working through the [jq tutorial](https://stedolan.github.io/jq/tutorial/)
    and browsing the [jq manual](https://stedolan.github.io/jq/manual/v1.6/) will
    get you started using jq to filter JSON content. Most useful is the [jq playground](https://jqplay.org/)
    where you can experiment with jq queries. \n\nAs an aside, the question with web
    tools like jq playground is how do you get command line data into it. Well, on
    the mac, you can pipe output to `pbcopy` which takes output and puts it on the
    clipboard. On Linux, you can use `xsel --clipboard --input` to do the same. Then
    it's just a matter of pasting your data into the appropriate web form. Therefore:\n\n```go\n$
    flyctl list apps --json | pbcopy\n```\n\nWill load up your clipboard with a list
    of all your apps that you can paste into the JSON text box in jq playground. Enter
    something like `.[] | select(.Status==\"running\")` and watch only the apps with
    running status appear in the Result box. You are now in a great place to start
    experimenting.\n\n## Making it happen\n\nFinally, let's have an example which
    runs commands. In this example we want to find all the apps which may not have
    deployed for whatever reason. For each one, we want all the application's info,
    as per the `flyctl info` command. In fact thats what we actually run - `flyctl
    info` and then we'll turn all that data into a JSON array for some unspecified
    app to process:\n\n```bash\nflyctl list apps --json | jq -c \".[] | select(.Deployed==false)
    | .Name\" | xargs -n 1 fly info --json -a | jq -s \".\"\n```\n\nLet's work this
    through, command by command:\n\n```bash\nflyctl list apps --json\n```\n\nThis
    lists out, as we already know, all the apps available in JSON format. That JSON
    gets fed to:\n\n```bash\njq -c \".[] | select(.Deployed==false) | .Name\"\n```\n\nTake
    the array and work through it element by element (`.[]`) copying it all through
    the pipe. Then for everything that matches the select criteria (`.Deployed==false`)
    copy it through the pipe to the final stage. `.Name` simply says to jq, write
    out the Name property. So this makes a list of names we can run `flyctl info`
    on. For that we turn to the buzz saw of Unix, xargs:\n\n```bash\nxargs -n 1 flyctl
    info --json -a \n```\n\nThis command takes each line from the input (in this case,
    app names) and for each one runs `flyctl info --json -a` with the input argument
    appended. This will generate a stream of JSON info objects but, for this example,
    we want them gathered up in an array. We can turn to jq again:\n\n```bash\njq
    -s \".\"\n```\n\nThis uses the jq \"slurp\" option - yes, it really is called
    that - to slurp up all the input which is passed through untouched with the \".\"
    query, but wrapped in an array. That's ready to feed into another application
    (in this case, a database, but thats another story).\n\n## Deploy watching\n\nDeployment,
    unlike other commands, doesn't produce a single JSON object. As an ongoing process,
    it'll produce a stream of JSON \"Source/Status/Message\" objects reflecting the
    progress of the deployment. Currently, there's also a stream on non-JSON messages
    mixed with the JSON stream, so although you can, we don't recommend following
    the JSON output to monitor deployments. Instead, run the `flyctl deploy` command
    with `--detach` and then poll `flyctl status` for the application's deployment
    status or run `flyctl monitor` to track deployments on an application.\n\n## Wrapping
    Up\n\nThe  new Flyctl `--json` support should make automating Fly a breeze and
    we can't wait to hear what you are building with it.\n\n\n<p class=\"callout\">\n
    \ _Want to learn more about Fly? Head over to our [Fly Docs](/docs/) for lots
    more, including a [Hands On](/docs/hands-on/start/) where you can get a free account
    and deploy your first app today._\n</p>\n"
- :id: blog-fly-changelog-9th-june
  :date: '2020-06-09'
  :category: blog
  :title: Fly Changelog for 9th June
  :author: dj
  :thumbnail:
  :alt:
  :link: blog/fly-changelog-9th-june
  :path: blog/2020-06-09
  :body: "\n\n<p class=\"lead\">\n\nThis is the Fly Changelog where we list all significant
    changes to the Fly platform, tooling and web sites. This week, new commands -
    restart, list and monitor - enhanced commands, an improved Deno buildpack and
    a new way to install flyctl.\n\n</p>\n\nIn this edition: a new way to install
    flyctl (and native Windows installers), new commands in flyctl that let you restart,
    list and monitor applications, an improved open command and an important update
    to the Deno Buildpack. \n\n\n### Flyctl\n\n* New instructions for installing flyctl
    have been rolled out with these releases. \n* `curl https://fly.io/install.sh
    | sh` works for macOS and Linux. For macOS, Homebrew installation continues to
    be available.\n* Native Windows installation is now available - `iwr https://fly.io/install.ps1
    | iex` works with Windows 10.\n* New installation location. Flyctl will now install
    in $HOME/.fly/bin (or `$FLYCTL_INSTALL/bin` if `$FLYCTL_INSTALL` is set).\n\n####
    Restart, List, Monitor\n\n* `flyctl` can now restart your application (as if you
    paused and resumed it rapidly) with the `flyctl apps restart` command.\n* Users
    can list all the applications they have access to with `flyctl list apps`. \n*
    `flyctl list apps word` will only return apps with `word` in the application's
    name. Using `-o orgname` will only return apps in that organization and `-s status`
    will only return apps with that particular status.\n* Users can list all the organizations
    they are a member of with `flyctl list orgs`.\n* If you've detached from monitoring
    a deployment, you can now restart your monitoring with `flyctl monitor`. Note
    that this does not exit on success but stays running to monitor any future deployments
    too.\n\n### Other changes:\n\n* `flyctl open` can now take a path as a paramater
    which is appended to the application's hostname e.g. `flyctl open /testing/path`
    would open `https://apphostname/testing/path`.\n* New command aliases - `apps`
    (alias `app`), `list` (alias `ls`)\n* When updating secrets using `flyctl secrets`,
    the deployment monitor will automatically be started to track the subsequent redeployment
    with the secret.\n* Version upgrade messages go to stderr now to ensure no interruption
    to CI tooling.\n\n### Deno Buildpack:\n\n* The Deno Buildpack has been modified
    so all configuration variables are taken from a `.config` file. The article [Deno
    on Fly using Buildpacks](blog/deno-on-fly-using-buildpacks/) has been updated
    to cover this. Not having a .config file will produce an error message detailing
    how to create the file. An empty `.config` file is valid.\n\n<hr/>\n\n<p class=\"callout\">\n\nYou
    can get the Changelog in the blog or through an RSS feed of just changelog updates
    available on [fly.io/changelog.xml](https://fly.io/changelog.xml). There's also
    a dedicated [ChangeLog](https://fly.io/changelog/) page with all the recent updates.\n\n</p>\n\n<!--
    start -->\n\n## _8th June 2020_\n\n**flyctl**: Version [0.0.129 released](https://github.com/superfly/flyctl/releases/tag/v0.0.129)\n\n-
    ++ New flyctl istallation process using script. Installs flyctl into `$HOME/.fly/bin`
    (or `$FLYCTL_INSTALL/bin` if $FLYCTL_INSTALL is set)\n- ++ New `flyctl list` command
    - supports listing `apps` or `orgs` of user. `list apps` can match text in name
    and filter by organization (`-o`) or status (`-s`).\n- ++ Enhanced `flyctl open`
    command - now takes URL path as a parameter\n\n## _2nd June 2020_\n\n**flyctl**:
    Version [0.0.128 released](https://github.com/superfly/flyctl/releases/tag/v0.0.128)\n\n-
    ++ New `flyctl apps restart` command will restart an application, as if it were
    paused and resumed.\n- ++ New `flyctl monitor` command - will follow deployment
    activity listing the progress for all deployments.\n- ++ New Updating secrets
    now automatically monitors subsequent redeployment.\n- ~~ Fix Upgrade messages
    now go to stderr (see [#137](https://github.com/superfly/flyctl/issues/137))\n\n**Fly
    Platform/Web**\n\n- ~~ Updated - Deno Buildpack (See updated [Deno on Fly using
    Buildpacks](blog/deno-on-fly-using-buildpacks/)) configuration now handled in
    a single `.config` file. Version of Deno is now selectable through this file.
    Set `deno_version=v1.0.2` to work around 1.0.3 to 1.0.5 dependency issues.\n"
- :id: blog-fly-changelog-may-29th-2020
  :date: '2020-05-29'
  :category: blog
  :title: Fly Changelog for 29th May
  :author: dj
  :thumbnail:
  :alt:
  :link: blog/fly-changelog-may-29th-2020
  :path: blog/2020-05-29
  :body: "\n\n<p class=\"lead\">\n\nThis is the Fly Changelog where we list all significant
    changes to the Fly platform, tooling and web sites. This week, pause/resume arrives
    for applications and a quicker way to view your Fly apps.\n\n</p>\n\n<p class=\"callout\">\n\nYou
    can get the Changelog in the blog or through an RSS feed of just changelog updates
    available on [fly.io/changelog.xml](https://fly.io/changelog.xml). There's also
    a dedicated [ChangeLog](https://fly.io/changelog/) page with all the recent updates.\n\n</p>\n\nMost
    of this week's changelog items are covered in an recent article [Fly - Now with
    Power Pause](/blog/fly-now-with-power-pause/). There's also updates to the Deno
    Buildpack.\n\n<!-- start -->\n\n## _28th May 2020_\n\n**flyctl**: Version [0.0.124
    released](https://github.com/superfly/flyctl/releases/tag/v0.0.124)\n\n\n- ++
    New `flyctl apps pause` and `flyctl apps resume` commands - Instructs the Fly
    platform to pause (remove all running instances) and resume (start up one running
    instance) of an application.\n- ++ New `flyctl regions set` - allows for the complete
    replacement of the regions in the region pool list. \n- ~~ New option `-v` or
    `--verbose`. Where supported, will produce more detailed output from commands.
    For example, `regions` commands will now only output a single line list of region
    codes for the region pool. Adding `-v` to the command will list the regions as
    a table with human readable location names.\n- ~~ Enhancement `flyctl apps list`
    now displays each applications status\n\n**Fly Platform/Web**\n\n- ~~ Fix - Deno
    Buildpack (See [Deno on Fly using Buildpacks](blog/deno-on-fly-using-buildpacks/))
    Now supports unstable Deno builds. Add `.unstable` to your application directory
    to enable it.\n\n## _18th May 2020_\n\n**flyctl**: Version [0.0.123 released](https://github.com/superfly/flyctl/releases/tag/v0.0.123)\n\n-
    ++ New `flyctl open` command - looks up the currently deployed application's hostname
    instructs a web browser to open that URL.\n"
- :id: blog-fly-now-with-power-pause
  :date: '2020-05-28'
  :category: blog
  :title: Fly - Now With Power Pause
  :author: dj
  :thumbnail:
  :alt:
  :link: blog/fly-now-with-power-pause
  :path: blog/2020-05-28
  :body: "\n\n<p class=\"lead\">\n\nThe latest feature for Fly is now available and
    it's.... a pause for your applications. And a matching resume, of course. \n\n</p>\n\n##
    What's a pause for?\n\nWhen you start building your applications, there may come
    a point where you don't want to keep your application running - mothballing while
    another project happens, schedule delays, or just plain wanting to manage your
    costs. \n\nThat's where `flyctl suspend` comes in. It'll take your application,
    save its state, and then reduce its running instances to zero. What doesn't go
    away are the networking configuration, IP addresses and certificates, which are
    maintained in the background. Think of it as a deep freeze for your application.
    That means everything is ready to resume with the least amount of fuss.\n\n##
    Ready to resume\n\nWhen you are ready to come back online, `flyctl resume` will
    bring your application back to life. It does it carefully though, bringing just
    one instance back, even if the application was originally deployed across multiple
    regions.\n\nIt does this by starting with a scale minimum count of 1. You can
    use `flyctl scale set min=n` to scale it back up, where n is, typically the number
    of regions in your region pool. And you can see that by running `flyctl regions
    list`.\n\n## Regional Setting\n\n The `regions` command has also had an enhancement
    with the addition of a `set` command. If you found adding and deleting regions
    to get to your desired region pool was a chore,  this new command will be a timesaver.
    Just enter your desired region pool and `set` will take care of it.\n\n## Status
    of Play\n\n`flyctl` has long been able to list the applications under your account.
    Now that you can pause applications, you also need to be able to quickly see what
    state all of your applications are in. So we've fixed that by adding the application's
    current status to the results. \n\n## Open for Applications\n\nFinally, we noticed
    that when users deploy a Fly application, the first thing they do is look up the
    application's host name, open up their browser and browse to that hostname. Well,
    that's something we could add a shortcut for.\n\nSay hello to the new `flyctl
    open` command which takes the current application and, assuming it's been deployed,
    opens your web browser and navigates to the application's hostname.\n\n<p class=\"callout\">\n
    \ _Want to learn more about Fly? Head over to our [Fly Docs](/docs/) for lots
    more, including a [Hands On](/docs/hands-on/start/) where you can get a free account
    and deploy your first app today._\n</p>\n\n<small>Updated Oct 29 2020 to use suspend/resume
    commands</small>\n"
- :id: blog-websockets-and-fly
  :date: '2020-05-20'
  :category: blog
  :title: WebSockets and Fly
  :author: dj
  :thumbnail:
  :alt:
  :link: blog/websockets-and-fly
  :path: blog/2020-05-20
  :body: "\n\n<p class=\"lead\">\n\nOne of the regular questions we get at Fly is
    \"Do you support WebSocket connections?\". The answer is \"Yes\", but before you
    head off let us tell you a bit more and show you an example.\n\n</p>\n\nWebSockets
    are powerful things for creating interactive applications. Example Zero for WebSocket
    examples is the chat application. This leverages WebSockets' ability to keep a
    connection alive over a long period of time while bidirectionally passing messages
    over it that ideally should be something conversational.\n\nIf you haven't got
    an example app like that, we've got one here for you - [flychat-ws in fly-examples](https://github.com/fly-apps/flychat-ws).
    It's been put together to use only raw WebSockets (and express for serving up
    pages) and no other libraries. (A shoutout to other libraries that build on WebSockets
    like [socket.io](http://socket.io/)).\n\nLet's get this application up on Fly
    first. Clone the repository and run:\n\n```\nnpm install\n```\n\n to fill out
    the node_modules directory. \n\nAssuming you have installed flyctl and signed
    up with Fly (head to the [hands-on](/docs/hands-on/) if you haven't), the next
    step is to create an application:\n\n```\nflyctl init\n```\n\nHit return to autogenerate
    a name and accept all the defaults. Now run:\n\n```\nflyctl deploy\n```\n\nAnd
    watch your new chat app deploy onto the Fly platform. When it's done run:\n\n```\nflyctl
    open\n```\n\nAnd your browser will open with your new chat window. You'll notice
    we did no special configuration or changes to the application to make it deploy.
    So let's dive in and see what's in there.\n\n## Down the WebSocket\n\nThe source
    for this application isn't extraordinary. The only thing that should stand out
    is the `fly.toml` file which was created when we ran `fly apps create`:\n\n```\nDockerfile
    \   fly.toml             public\nLICENSE       node_modules         server.js\nREADME.md
    \    package-lock.json    package.json\n```\n\nThe `fly.toml` file contains all
    the configuration information about how this app should be deployed; it looks
    like this:\n\n```\napp = \"flychatting-ws\"\n\n[[services]]\n  internal_port =
    8080\n  protocol = \"tcp\"\n\n  [services.concurrency]\n    hard_limit = 25\n
    \   soft_limit = 20\n\n  [[services.ports]]\n    handlers = [\"http\"]\n    port
    = \"80\"\n\n  [[services.ports]]\n    handlers = [\"tls\", \"http\"]\n    port
    = \"443\"\n\n  [[services.tcp_checks]]\n    interval = 10000\n    timeout = 2000\n```\n\nAnd
    to paraphrase the contents, it says \n\n- The application will take tcp connections
    on port 8080.\n- An http service will be available to the outside would on port
    80 (which will be sent to port 8080).\n- An https service will also be available
    to the world on port 443 (and again traffic will be sent to port 8080).\n- There's
    a 25 connection hard limit before a new instance of the application is spun up.\n-
    The 8080 internal port will be checked every 10 seconds (with a 2 second time
    out) for connectivity - if the app doesn't respond it'll be restarted.\n\nThis
    is all out-of-the-box Fly configuration.\n\n## Into the Server\n\nThe `server.js`
    file is a whole 17 lines long but it does plenty in 17 lines:\n\n```jsx\nconst
    { createServer }  = require('http');\nconst express = require('express');\nconst
    WebSocket=require('ws');\n```\n\nFirst it pulls in the packages needed, `express`
    and `ws` the WebSockets library.\n\nThen it configures express to serve static
    files from the `public` directory:\n\n```jsx\nconst app=express();\napp.use(express.json({
    extended: false}));\napp.use(express.static('public'));\n```\n\nThe public directory
    contains the web page and JavaScript for the chat application. Now we move on
    to starting up the servers. There are two to start up: the WebSocket Server and
    the Express server. But they need to know where to listen, so we'll grab the port
    from the environment - or default to port 3000: \n\n```jsx\nvar port = process.env.PORT
    || 3000;\n```\n\n**Now** we can start the servers:\n\n```jsx\nconst server=new
    WebSocket.Server({ server:app.listen(port) });\n```\n\nReading from the inside
    out, it starts the Express server with it listening on our selected port and then
    hands that server over to create a new `WebSocket.Server`.\n\nNow all we have
    to do is tell the code what to do with incoming connections:\n\n```jsx\nserver.on('connection',
    (socket) => {\n  socket.on('message', (msg) => {\n    server.clients.forEach(
    client => {\n      client.send(msg);\n    })\n  });\n});\n```\n\nWhen a client
    connects, it'll generate a connection event on the server. We grab the socket
    that connection came in and add an event handler for incoming messages to it.
    This handler takes any incoming message and sends it out to any connected client.
    We don't even have to track which clients are connected in our simple chat. The
    WebSocket server maintains a list of connected clients so we can walk through
    that list.\n\nAnd that's the end of the server. Yes, there isn't a lot there but
    it all works. It would be remiss of us at this point not to mention that we use
    a Dockerfile to assemble the image that's run on Fly; here it is:\n\n```docker\nFROM
    node:current-alpine\n\nWORKDIR /app\n\nCOPY package.json .\nCOPY package-lock.json
    .\n\nRUN npm install --production\n\nCOPY . .\n\nENV PORT=8080\n\nCMD [ \"npm\",\"start\"
    ]\n```\n\nI say remiss because this is where we set the port number in the environment
    to match up with the port in the `fly.toml` file from earlier. Oh, and we use
    `npm start` as the command to start the server up because in `package.json` we've
    made sure we remembered to set a script up:\n\n```json\n\"scripts\": {\n    \"start\":
    \"node server.js\"\n  }\n```\n\nSo, you can run `npm start` to run the server
    locally (by default on port 3000), or you can build and run it locally using Docker:\n\n```cmd\ndocker
    build -t test-chat .\n```\n```cmd\ndocker run -p 8080:8080 test-chat\n```\n\nOf
    course, in this case you've already deployed it to Fly with a single command.
    Let's move on to the user-facing side of things.\n\n## Now for the Client\n\nThe
    client code is all in the `public` directory. One HTML file lays out a simple
    form and calls some JavaScript on loading. That JavaScript is all in `client.js`
    and that's what we are going to look at now:\n\n```jsx\n'use strict'\nvar socket
    = null;\n\nfunction connect() {\n  var serverUrl;\n  var scheme = 'ws';\n  var
    location = document.location;\n\n  if (location.protocol === 'https:') {\n    scheme
    += 's';\n  }\n\n  serverUrl = `${scheme}://${location.hostname}:${location.port}`;\n\n```\n\nThere's
    a global socket because we only need one to connect to the server. This gets initialised
    in our connect call, and it's here that the code touches on the fact it'll be
    running on Fly. It looks up the URL it has been served from, and the port, and
    if served from an https: URL uses secure WebSockets (`wss:`). If not, it'll use
    ordinary WebSockets (`ws:`). \n\nFly's default configuration is to serve up internal
    port 8080 on external port 80 unsecured and 443 with TLS. That TLS traffic is
    terminated at the network edge so from the application's point of view, it's all
    traffic on one port and no need to do anything special to handle TLS. Pow, less
    code to write and manage. All you have to do is make sure you don't try to do
    anything special for these connections.\n\nOnce we have the URL, we open the WebSocket
    saying we want to work with a \"json\" protocol. With the socket opened, let's
    wire it up to receive messages:\n\n```jsx\n  socket.onmessage = event => {\n    const
    msg = JSON.parse(event.data)\n    $('#messages').append($('<li>').text(msg.name
    + ':' + msg.message))\n    window.scrollTo(0, document.body.scrollHeight);\n  }\n```\n\nThis
    simply decodes the JSON into a message and pops it into our chat display. Most
    of the code is about doing the page manipulation. The last part of the connect
    process wires up the submit on a form where you type messages:\n\n```jsx\n   $('form').submit(sendMessage);\n}\n```\n\nThe
    last part of this is that `sendMessage` function:\n\n```jsx\nfunction sendMessage()
    {\n  name = $('#n').val();\n  if (name == '') {\n    return;\n  }\n  $('#n').prop('disabled',
    true);\n  $('#n').css('background', 'grey');\n  $('#n').css('color', 'white');\n
    \ const msg = { type: 'message', name: name, message: $('#m').val() };\n  socket.send(JSON.stringify(msg));\n
    \ $('#m').val('');\n  return false;\n}\n```\n\nWhich is mostly CSS manipulation
    and reading the name and message fields, and who wants to spend time on that?
    The important part for the sockets side of things is these two lines:\n\n```jsx\n
    \ const msg = { type: 'message', name: name, message: $('#m').val() }\n  socket.send(JSON.stringify(msg))\n```\n\nWhere
    a message is composed as a JSON object and then that JSON object is turned into
    a string and sent. The message will return soon enough as our server broadcasts
    to every client, including the one that originated the message. That means there's
    really no need to update our messages view when we send. Score one for lazy coding.\n\n##
    Ready to Fly\n\nSo what's this walk through the code shown us? Obviously that
    it's incredibly easy to deploy an app to Fly, but also that Fly takes care of
    TLS connections so there's less code for you to make. That it's simple to make
    an Express app that also services sockets. That you can quickly test locally both
    as a native app and as a docker image. And to go remote it takes just one command
    to move it all onto Fly's global infrastructure. That WebSockets simply work on
    Fly is just part of what Fly brings to the developers' table.\n\n\n<p class=\"callout\">\n
    \ _Want to learn more about Fly? Head over to our [Fly Docs](/docs/) for lots
    more, including a [Hands On](/docs/hands-on/start/) where you can get a free account
    and deploy your first app today._\n</p>\n\n"
- :id: blog-mux-fly-wocket-and-rtmp
  :date: '2020-05-19'
  :category: blog
  :title: Mux, Fly, Wocket and RTMP
  :author: dylan
  :thumbnail:
  :alt:
  :link: blog/mux-fly-wocket-and-rtmp
  :path: blog/2020-05-19
  :body: |2


    <p class="lead">

    In this Guest Post by Dylan Jhaveri at Mux, he talkes about Wocket, a proof of concept application that streams live video from your brower to an RTMP server. And yes, they use Fly to do it.

    </p>

    ![Wocket](./mux-wocket.svg)

    ## Wocket (WebSocket to RTMP)

    This project is a proof-of-concept to demonstrate how you can stream live from your browser to an RTMP server. Streaming via RTMP is how you stream to Twitch, Youtube Live, Facebook Live, and other live streaming platforms. Typically, this requires running a local encoder software (for example: [OBS](https://obsproject.com/) or [Ecamm Live](https://www.ecamm.com/mac/ecammlive/)). Those are great products and if you are streaming seriously you probably still want to use them. But we threw this project together to show how you might be able to pull off the same thing from a browser. In this example, instead of streaming to something like Twitch, Youtube Live, etc, we will be using the live streaming API provided by Mux, which gives you an on-demand RTMP server that you can stream to.


    This project uses [Next.js](https://nextjs.org) and a custom server with WebSockets. It should be noted that this project is a fun proof-of-concept. If you want to learn more about the challenges of going live from the browser take a look at this Mux blog post [The state of going live from a browser](https://mux.com/blog/the-state-of-going-live-from-a-browser/).

    This is what this project looks like. This will access the browser's webcam and render it onto a canvas element. When you enter a stream key and click "Start Streaming" it will stream your webcam to a [Mux live stream](https://docs.mux.com/docs/live-streaming).

    ![Wocket Screenshot](./wocket-live-browser-1.jpg?card)

    ### _Clone the repo_

    ```
    git clone https://github.com/MuxLabs/wocket
    cd wocket
    ```

    ### Setup

    ### Prerequisites to run locally

      * To run the server locally you will need to install [ffmpeg](https://www.ffmpeg.org/) and have the command `ffmpeg` in your $PATH. To see if it is installed correctly open up a terminal and type `ffmpeg`, if you see something that is not "command not found" then you're good!

    For development you'll probably want to use `dev`, which will do little things like hot reloading automatically.

    ```javascript
    $ npm install
    $ npm run dev
    ```

    The last line you should see is something along the lines of:

    ```
    $ > ready on port 3000
    ```

    Visit that page in the browser and you should see Wocket!

    ### Getting a Mux stream key

    To get a stream key and actually start streaming to an RTMP ingest URL you will need a [free Mux account](https://dashboard.mux.com/signup?type=video). After you sign up create a live stream either [with the API](https://docs.mux.com/docs/live-streaming) or by navigating to 'Live Streams' in the dashboard and clicking 'Create New Live Stream' see below:

    <%= video_tag "mux-live-stream-dashboard.mp4?raw=true&card" %>

    Without entering a credit card your live streams are in 'test' mode which means they are limited to 5 minutes, watermarked with the Mux logo and deleted after 24 hours. If you enter a credit card you get $20 of free credit which unlocks the full feature set and removes all limits. The $20 of credit should be plenty to cover the costs of experimenting with the API and if you need some more for experimentation please drop us a line and let us know!

    ### Running the application in production

    Again, this should just be considered a proof of concept. I didn't write this to go to production. I beg you, don't rely on this as is for something important.

    ```
    $ npm run build
    $ npm start
    ```

    ## Deploying to fly.io

    We will deploy the server with `flyctl`. Fly.io will use the Dockerfile to host the server.

    1. Create a new fly.io app `flyctl init`
    1. When asked about an app name, hit enter to get a generated name
    1. When asked to overwrite the `fly.toml` file, say "yes"
    1. Run `flyctl deploy` - this will deploy your app to fly.io


    ### Putting it all together

    The intended way of using this would be to use the [`MediaRecorder` API](https://developer.mozilla.org/en-US/docs/Web/API/MediaStream_Recording_API) and send video whenever the MediaRecorder instance fires the `dataavailable` event. The [demo front-end](https://github.com/MuxLabs/wocket/blob/master/pages/index.js) is an example of how you could wire everything together using the `getMediaRecorder` and the `MediaRecorder` API.

    ## Other projects

    Some other projects I found when trying to figure out this whole canvas -> RTMP thing that were hugely helpful:

    * [fbsamples/Canvas-Streaming-Example](https://github.com/fbsamples/Canvas-Streaming-Example)
    * [chenxiaoqino/getusermedia-to-rtmp](https://github.com/chenxiaoqino/getusermedia-to-rtmp)

    Other ways of solving this problem:

    * [Pion](https://pion.ly/) - WebRTC implementation written in Go
    * [Chromium Broadcasting](https://github.com/muxinc/chromium_broadcast_demo)
- :id: blog-fly-changelog-may-15th-2020
  :date: '2020-05-15'
  :category: blog
  :title: Fly Changelog May 15th 2020
  :author: dj
  :thumbnail:
  :alt:
  :link: blog/fly-changelog-may-15th-2020
  :path: blog/2020-05-15
  :body: |2+


    <p class="lead">

    This is the Fly Changelog where we list all significant changes to the Fly platform, tooling and web sites. You can also use the RSS feed of just changelog posts available on [fly.io/changelog.xml](https://fly.io/changelog.xml) or consult our dedicated [ChangeLog](https://fly.io/changelog/) page with all the recent updates.

    </p>

    Since the last ChangeLog, we implemented a whole new scaling system with updates to the platform and flyctl. You can read all the details in the [Updating Scale](/blog/updating-scale/) article. We've also been improving the performance of our backhaul, which moves traffice between the Fly edge and datacenters.

    <!-- start -->

    ## _13th May 2020_

    **flyctl**: Version [0.0.122 released](https://github.com/superfly/flyctl/releases/tag/v0.0.122)

    - ++ New scaling mechanism and scaling commands implemented. See [Updating Scale](/blog/updating-scale/) article.
    - -- The autoscale command from v0.0.121 removed.

    **Fly Platform/Web**

    - -- Removed references to 32 bit versions of flyctl for Windows and Linux as part of deprecation of 32 bit flyctl support.

    ## _8th May 2020_

    **flyctl**: Version [0.0.121 released](https://github.com/superfly/flyctl/releases/tag/v0.0.121)

    - ++ New autoscale and regions commands added to flyctl.

    **Fly Platform/Web**

    - ++ New scaling system implemented in the platform, based on region pools. See [Updating Scale](/blog/updating-scale/) article.
    - ++ Rolled out new HTTP2 only backhaul. (The backhaul moves traffic around the Fly network between the edge and the datacenters). This change improves performance and stability for Fly applications that use the http handler.

    ## _7th May 2020_

    **flyctl**: Version [0.0.120 released](https://github.com/superfly/flyctl/releases/tag/v0.0.120)

    - ~~ Naming of flyctl binary artifacts made consistent with tgz artifacts. Standalone binaries now use the same platform names as the archived version. (This is typically not visible to the user through the use of getfly.fly.dev and the installer script).

    ## _4th May 2020_

    **flyctl**: Version [0.0.118 released](https://github.com/superfly/flyctl/releases/tag/v0.0.118)

    - ~~ Custom Dockerfiles are now correctly mapped (Fixes [#119](https://github.com/superfly/flyctl/issues/119)).


- :id: blog-deno-on-fly
  :date: '2020-05-14'
  :category: blog
  :title: Deno on Fly
  :author: dj
  :thumbnail:
  :alt:
  :link: blog/deno-on-fly
  :path: blog/2020-05-14
  :body: "\n\n<p class=\"lead\">![The Deno Logo](deno-logo.svg?wrap-right&1/4)[Deno
    has reached version 1.0](https://deno.land/v1), and congratulations to all involved.
    Deno is a better Nodejs, with TypeScript baked in and intergrated package management.
    We're going to show you how to deploy Deno applications onto Fly and let your
    applications run closer to your users.</p>\n\nWe've been working with Deno for
    Fly for a while and its great blend of TypeScript, V8, Rust and simplicity makes
    for a great app experience. And great apps deserve a great deployment. We enjoy
    Deno so much that when we brought out our [first cloud native buildpack](/blog/deno-on-fly-using-buildpacks/)
    we made it specifically for Deno. \n\nThat's not the only way to deploy Deno on
    Fly. Fly is flexible and can run with buildpacks or you can just use a Dockerfile.
    In this article, we'll take you through building and deploying Deno with a Dockerfile.\n\n##
    Why use a Dockerfile?\n\nUsing a Dockerfile gives you complete control of the
    packaging of your applications, paring the process down to the bare minimum steps.
    With a buildpack, to make things as simple as possible, various assumptions are
    made for you (what underlying OS, how the files are organized, which ports are
    open by default) and this can lead to larger images than necessary.\n\nSo, you
    may get going with a buildpack, but if you are comfortable with the Dockerfile
    syntax and how it builds images, you can get quite the build time performance
    boost and a smaller image file to boot.\n\n## Building with a Dockerfile\n\nWe've
    put a modified version of our tutorial example, `hellodeno`, up on the fly-examples
    repo. Clone or download [`hellodeno-dockerfile`](https://github.com/fly-apps/hellodeno-dockerfile)
    and we can begin. Let's look at what files there are first:\n\n```\n\tDockerfile\t\n\tREADME.md\t\n\tdeps.ts\t\t\n\tserver.ts\n```\n\nThe
    new files here, over the original hellodeno, are the `Dockerfile` and the `deps.ts`
    file. The Dockerfile is based on the readme example in [deno-docker](https://github.com/hayd/deno-docker),
    a repository of Docker images for Ubuntu, Centos, Debian and Alpine Linux.\n\nLet's
    step through the Dockerfile and see what it does:\n\n```dockerfile\n\tFROM hayd/alpine-deno:1.0.0\n```\n\nThe
    first line brings in the Alpine Linux and Deno base image, already loaded with
    the Deno 1.0.0 toolchain.\n\n```dockerfile\n\tEXPOSE 8080 \n```\n\nOur application
    opens up port 8080 to do its work, so we expose that in the Dockerfile configuration.\n\n```dockerfile\n\tWORKDIR
    /app\n```\nThe application will be set up in the `/app` directory of the image
    (avoiding the sometimes problematic mistake of loading code into the root of the
    filesystem).\n\n```dockerfile\n\tUSER deno\n```\n\nAnd best practices say that
    even running in a container, you shouldn't run as root, so the Dockerfile switches
    over to use a `deno` user.\n\n```dockerfile\n\tCOPY deps.ts .\n\tRUN deno cache
    deps.ts\n```\n\nAs noted, there is also a `deps.ts` file in our directory. This
    is a Deno convention of a single file which exports the dependencies that Deno
    automatically imports. Using this file, it's possible to pin Deno imports to a
    particular version. Here, the Dockerfile copies that file across and then asks
    Deno to cache all the packages referenced in it. These will all go into their
    own layer in the Docker image.\n\n```dockerfile\n\tCOPY . .\n\tRUN deno cache
    server.ts\n```\n\nWe now copy all the other files over to the Docker image, and
    do a similar caching step with our code, `server.ts`.\n\n```dockerfile\n\tCMD
    [\"run\",\"--allow-net\", \"server.ts\"]\n```\n\nFinally, we tell Docker the command
    to run this image's contents. The `ENTRYPOINT` is set to \"deno\" in the alpine-deno
    image we're building with so the `CMD` settings are combined with the entry point
    to create a startup command `deno run --allow-net server.ts`.\n\nNow, there's
    a file in the repo, the `.dockerignore` file, which lists files not to be copied
    over to that image. *Pro-tip*: remember to include your .git directory in there
    so that your Docker image doesn't include the complete history of your application.\n\n##
    Testing locally\n\nYou may want to test your image before you attempt to deploy
    it. Make sure you have Docker installed and run:\n\n```\n\tdocker build -t deno-test
    .\n\tdocker run -p 8080:8080 deno-test\n```\n\nAnd browse to `http://localhost:8080`
    to see a greeting from the example app. \n\n## Deploying to Fly\n\nWe'll assume
    you are all signed up, logged in and have the essential `flyctl` installed. If
    not, catch up by following our quick step-by-step [Hands-On](https://fly.io/docs/hands-on/start/).\n\nThe
    first thing you need to do to deploy an app on Fly is to create a slot for the
    app on the platform and a `fly.toml` file for the deployment settings. That's
    all done with one command `flyctl init`:\n\n```cmd\nflyctl init\n```\n```output\nSelected
    App Name: hellodeno-dockerfile\n? Select organization: Dj (dj)\n? Select builder:
    Dockerfile\n    (Use the existing Dockerfile)\nNew app created\n  Name     = hellodeno-dockerfile
    \ \n  Owner    = dj                    \n  Version  = 0                     \n
    \ Status   =                       \n  Hostname = <empty>               \n\nWrote
    config file fly.toml\n```\n\nThe `flyctl init` command will prompt you for an
    application name. We recommend you go for an auto-generated name, you can enter
    one but it may be rejected because it matches an already existing app. Then you'll
    be asked what organization you want the app created under. Organizations are a
    way of sharing apps between Fly users, so for now, as we aren't sharing this,
    select your personal organization (the one with your name). Once you've done that
    the `fly.toml` file will be created.\n\nNow it's time to deploy. Run `flyctl deploy`
    and the image will be created and pushed onto the Fly platform. Once it's there
    it will be deployed to a datacenter and a given a host name.\n\nTo find that hostname,
    run `flyctl info`:\n\n```cmd\nflyctl info\n```\n```output\nApp\n  Name     = hellodeno-dockerfile
    \         \n  Owner    = dj                            \n  Version  = 0                             \n
    \ Status   = running                       \n  Hostname = hellodeno-dockerfile.fly.dev
    \ \n\nServices\n  PROTOCOL   PORTS                    \n  TCP        80 => 8080
    [HTTP]        \n             443 => 8080 [TLS, HTTP]  \n\nIP Addresses\n  TYPE
    \  ADDRESS                                CREATED AT  \n  v4     77.83.140.153
    \                         1m37s ago   \n  v6     2a09:8280:1:2d25:fb3e:66b3:de90:905b
    \  1m37s ago   \n```\n\nAnd there's our hostname, `hellodeno-dockerfile.fly.dev`.
    Now connect to `http://hellodeno-dockerfile.fly.dev` (substituting in your app's
    hostname as appropriate) and you should see a greeting from the application. You'll
    also notice that you've been redirected to the https version of the site and had
    your application automatically secured.\n\n## Next with Deno\n\nThat is far from
    the end of what you can do with a Deno application on Fly. You can scale a Deno
    application around the world and add multiple domain names with automatically
    generated certificates to it, just like every other Fly application. So spread
    your Deno wings on Fly and make your great app today - don't forget to tell us
    about it too. As for us? We'll be building more examples, as well as production
    applications, with Deno and you'll read about any insights we gain from the Deno
    and Fly combination here. \n\n \n"
- :id: blog-updating-scale
  :date: '2020-05-13'
  :category: blog
  :title: Updating Scale
  :author: dj
  :thumbnail:
  :alt:
  :link: blog/updating-scale
  :path: blog/2020-05-13
  :body: "\n\n<p class=\"lead\">\n\nOne thing you will find with Fly is we never miss
    an opportunity to refine the user experience for you. As part of that process,
    today, we're unveiling a new scaling model and commands.\n\n</p>\n\nWe have been
    recently looking at how scaling commands work on Fly and we've come up with a
    new system with a simpler model and more compact command set to work with. We'd
    like to introduce it to you today.\n\n## The Scaling System\n\nPreviously, at
    least in the user interface, we offered regions as a fixed list with a number
    of instances per region set as part of a scaling plan. This worked but introduced
    some operations that were not intuitive, such as having to reset that list or
    elements in the list to remove regions. It was also fairly rigid and we wanted
    to automate the process more and let scaling manage more of the global load.\n\nThe
    new system is based on a pool of regions where the application can be run. Using
    a selected model, the system will then create at least the minimum number of application
    instances across those regions. The model will then be able create instances up
    to the maximum count. The min and max are global parameters for the scaling. There
    are two scaling models, **Standard** and **Balanced**.\n\n- **Standard**: Instances
    of the application, up to the minimum count, are evenly distributed among the
    regions in the pool. They are not relocated in response to traffic. New instances
    are added where there is demand, up to the maximum count.\n\n- **Balanced**: Instances
    of the application are, at first, evenly distributed among the regions in the
    pool up to the minimum count.  Where traffic is high in a particular region, new
    instances will be created there and then, when the maximum count of instances
    has been used, instances will be moved from other regions to that region. This
    movement of instances is designed to balance supply of compute power with demand
    for it.\n\nIt's worth noting that the scaling model, in conjunction with the platform,
    may not deploy in a predictable way. For example, if an allowed region is unable
    to allocate new instances, the platform will fallback to creating one in a nearby
    region. As this fallback operation is a feature of the system, it's not something
    to worry about when an region that is not in the pool shows up in the currently
    running instances (as displayed with `flyctl status`).\n\n## flyctl regions\n\nThis
    is a new flyctl command, taking over from the previous `flyctl scale regions`
    command. It has its own subcommands:\n\n- `list` : lists out the regions that
    are currently in the application's pool of regions:\n\n```cmd\nflyctl regions
    list\n```\n```out\nAllowed Regions:\n  ams  Amsterdam, Netherlands\n  ewr  Parsippany,
    NJ (US)\n  iad  Ashburn, Virginia (US)\n  mrs  Marseille, France\n  yyz  Toronto,
    Canada\n```\n\nThis list shows that a pool with five regions in it.\n\n\n- `add`
    : takes a space-separated list of [regions](/docs/regions/), adds those regions
    to the pool of regions and then applies that new pool to the application.\n- `remove`
    : takes a space-separated list of [regions](/docs/regions/), removes those regions
    from the pool of regions and then applies that new pool to the application.\n\nThis
    pool reflects the possible locations where the application may be run. A region
    being in the pool doesn't mean there will be an instance in that location. That
    all depends on the scaling model and min/max count settings which are selectable
    with `flyctl scale`.\n\n## flyctl scale\n\nThis command has been expanded to give
    you more control over your scaling.\n\nThe `show` subcommand will display the
    current configuration of an application:\n\n```cmd\nflyctl scale show\n```\n```output\nScale
    Mode: Standard\nMin Count: 5\nMax Count: 10\nVM Size: micro-2x\n```\n\nHere, the
    standard model is being used, with a min of 5 instances and a max of 10. All the
    instances are created with a micro-2x VM.\n\nThe `balanced` and `standard` subcommands
    select the model and can optionally set the `min` and `max` values as `key=value`
    settings. If we wanted to just switch to a `balanced` model, we would enter:\n\n```cmd\nflyctl
    scale balanced\n```\n```output\nScale Mode: Balanced\nMin Count: 5\nMax Count:
    10\n```\n\nOr, if we wanted to switch to a `standard` model with a `max` of 20
    we could enter:\n\n```cmd\nflyctl scale standard max=20\n```\n```output\nScale
    Mode: Standard\nMin Count: 5\nMax Count: 20\n```\n\nThere is also a `set` subcommand
    which lets you vary the `min` and `max` without changing the model.\n\n```cmd\nflyctl
    scale set min=6 max=10\n```\n```output\nScale Mode: Standard\nMin Count: 6\nMax
    Count: 10\n```\n\nYou can, in just in one line, set the model and all the parameters
    too.\n\n```cmd\nflyctl scale balanced min=5 max=10\n```\n```output\nScale Mode:
    Balanced\nMin Count: 5\nMax Count: 10\n```\n\nThe `vm` command remains as it was,
    showing and setting the size of the virtual machine - in terms of cpu and memory
    - that each instance of the application will be run with. `flyctl scale vm` will
    display detailed information about the currently selected vm size. Specifying
    a vm size on the command line `flyctl scale vm cpu1mem1` will switch the application
    to using that size of vm globally.\n\n## Going Forward\n\nScaling will always
    be a constantly evolving feature on Fly as we match more models with use cases
    and find new ways to fine tune models and commands. \n\nDo let us know what you
    think of the changes by dropping a line to [support](mailto:support@fly.io) or
    leaving a message on our [Spectrum Community](https://spectrum.chat/flyio?tab=posts).
    \n\n<p class=\"callout\">\n  _Want to learn more about Fly? Head over to our [Fly
    Docs](/docs/) for lots more, including a [Hands On](/docs/hands-on/start/) where
    you can get a free account and deploy your first app today._\n</p>\n\n\n"
- :id: blog-fly-changelog-1st-may-2020
  :date: '2020-05-01'
  :category: blog
  :title: Fly Changelog 1st May 2020
  :author: dj
  :thumbnail:
  :alt:
  :link: blog/fly-changelog-1st-may-2020
  :path: blog/2020-05-01
  :body: "\n<p class=\"lead\">\n\nThis is the Fly Changelog where we list all significant
    changes to the Fly platform, tooling and web sites. You can also use the RSS feed
    of just changelog posts available on [fly.io/changelog.xml](https://fly.io/changelog.xml)
    or consult our dedicated [ChangeLog](https://fly.io/changelog/) page with all
    the recent updates.\n\n</p>\n\nSince the last ChangeLog, we've introduced some
    enhancements to `flyctl` which make it easier to deploy tagged local and remote
    images directly to Fly. There is also a new load-balancing algorithm in operation
    which should be more effective with widely deployed global applications.\n\n<!--
    start -->\n\n## _30th April 2020_\n\n**flyctl**: Version [0.0.117 released](https://github.com/superfly/flyctl/releases/tag/v0.0.117)\n\n-
    ++ New `flyctl auth docker` command. This allows users to deploy images directly
    to fly by adding `registry.fly.io` to the docker daemon's authenticated registries.
    (Resolves [#104](https://github.com/superfly/flyctl/issues/104))\n- ++ Added `--image-label`
    to `flyctl deploy` enabling a specific image label to be requested at deployment
    time. (Resolves [#110](https://github.com/superfly/flyctl/issues/110)) \n- ~~
    Timeout increased at end of deployment monitoring to stop erroneous `No Deployment`
    messages. (Fixes [#113](https://github.com/superfly/flyctl/issues/113))\n- ~~
    Modified `flyctl deploy` version negotiation to resolve API compatibility errors.
    (Fixes [#112](https://github.com/superfly/flyctl/issues/112))\n\n**Fly Platform/Web**\n\n-
    ++ Added a new [Open Source](https://fly.io/open-source/) page which covers details
    of how Fly works, and will be working, with open source projects.\n- ~~  A new
    load balancing allocation algorithm is in place. Designed for global applications
    with large numbers of instances, an incoming connection will be directed to an
    instance from the closest least loaded service in a set of three services. Those
    three services will have been randomly selected from healthy services in the three
    nearest regions for the app.\n"
- :id: blog-deno-on-fly-using-buildpacks
  :date: '2020-04-23'
  :category: blog
  :title: Deno on Fly using Buildpacks
  :author: dj
  :thumbnail:
  :alt:
  :link: blog/deno-on-fly-using-buildpacks
  :path: blog/2020-04-23
  :body: "\n\n<p class=\"lead\">\n\nWe really like Deno at Fly. It really is a better
    Node and we’d love people to build more with it. Our plan? “Let's make building
    and deploying Deno apps on Fly as simple as other languages”. Now, you can configure,
    build and deploy Deno code in just two commands, and we’d like to show you how
    we did it.\n\n </p>\n\n\n\n## What’s Deno?\n\nThe [deno.land](https://deno.land/)
    website bills Deno as a secure runtime for JavaScript and TypeScript. In practice,
    it’s like working with a streamlined Node where TypeScript is the norm, with a
    solid Rust foundation. It also incorporates many of the lessons learned over Node’s
    development and growth. \n\nAs we write this, Deno is heading rapidly towards
    its first 1.0 release candidate and we are really looking forward to a post-1.0
    Deno. We believe as more people discover Deno’s elegance as a platform it’ll become
    much more popular. So how do you build a Deno app for Fly now?\n\n\n## Straight
    to the chase\n- Name your app’s entry point  `server.ts`\n- Create your Fly app
    with `flyctl init --builder flyio/builder`\n- `touch .config` to create an empty
    \".config\" file to go with your app\n- `flyctl deploy` and watch it all build
    and deploy\n\nThat’s it. \n\n## Behind the builder\n\nThe “chase” above was notably
    short. How? Let's look at what’s powering this build process. [Fly’s builder support](https://fly.io/blog/powerbuilding-with-fly/)
    landed in early April and is based on the Cloud Native [Buildpacks.io](https://buildpacks.io/)
    (CNB) specifications and implementations for building cloud native container images.
    \n\nA Cloud Native build is made up of stacks, builders and buildpacks which come
    together to make a repeatable build process, for different languages and frameworks.
    Used together, they deliver container images that are ready to run. For the Deno
    builder, we created our own stack, builder and buildpack to build Deno.\n\n**Stacks**\n\nEverything
    in CNB is built on a “stack”, the base operating system in which the builders
    operate. For the Fly stack, we selected Ubuntu bionic, added in curl and unzip
    utilities (to support the Deno installer) and built the three images - base, build
    and run - which will be used in the subsequent steps.\n\n**Builders**\n\nA builder
    in CNB is a container loaded with all the OS level tooling needed to construct
    an image. It’s fundamentally an OS image of some form. It’s not tied to any specific
    container platform such as Docker, it’s designed to run wherever you can run a
    container. \n\nThat said, the Buildpacks.io developers do have the `pack` commmand
    that uses Docker to run these containers - especially useful on Windows and macOS
    where there is no native Linux container support. \n\nThe builder brings up a
    container running the build stack and then steps through the buildpacks associated
    with the builder to work out which build script to run in that container.\n\n**Buildpacks**\n\nThe
    buildpacks are smaller bundles of scripts (or Go programs) which have two jobs,
    detect and build. \n\n***detect***\n\nThe detect script tries to work out if the
    directory contents it has been given are appropriate to build with its build script.
    For example, a detect script for Ruby would look for a `Gemfile` and go “aha!
    this is the song of my people! run my build script”. \n\nNow, Deno doesn't have
    an obvious packaging file like Ruby or well, anything else, due to it being so
    self-contained. So, we made a rule. If there’s a `server.ts` file in the directory,
    we’ll assume it's a Deno application and signal to run our build script.\n\n***build***\n\nThe
    build script runs in the Builder’s container and does the work of assembling tools
    and building the layers to create our image. In the case of our Deno buildpack,
    that includes downloading Deno into the image and running the Deno `deno cache`
    command so all the dependencies are ready to run. Finally, it writes out a set
    of launch commands to start up the application.\n\n***configuration***\n\nThere's
    a number of settings that can be passed to the Deno buildpack to control how the
    application is run. They are kept in the `.config` file as a set of key values.
    You can run with an empty `.config` file to get defaults, but you do currently
    need to have a `.config` file present. \n\nFirst up is the `permissions` setting.
    Deno restricts access to resources by default. You have to specify which resources
    are available to any program with `--allow-*` command-line flags. The permissions
    setting contains the command-line arguments you’d expect to be added to the Deno
    command-line. You are most probably always going to have to set the permissions
    setting. In the example app, this file contains:\n\n```\npermissions=\"--allow-net\"\n```\n\nWhich
    allows network access. If there’s no `permissions` setting, currently we default
    to no permission flags, in step with the default Deno experience. You can incrementally
    add appropriate permissions to your app as you iterate your code. \n\nIf you want
    to allow all access, you can put `-A` into permissions setting, allow all access
    and sort out permissions later (well, that’s what you’ll say but you know it’ll
    slip and you’ll do a production deployment with it still in place). It's not recommended
    but is usefule to know.\n\nThe arguments are added to the `deno` command when
    the container and its launch file are built.\n\nNext is the `unstable` setting.
    There are APIs that are labelled unstable in Deno which are not available by default.
    By adding `--unstable` to the command line, applications can access these APIS.
    With the configuration file, setting `unstable=true` achieves the same effect
    for the buildpack\n\nFinally, the `deno_version` setting can force the buildpack
    to use a particular Deno version. For example, if you needed to use Deno v1.0.2
    for an application, then adding:\n\n```\ndeno_version=v1.0.2\n```\n\nTo the `.config`
    file would make the buildpack use v1.0.2. Don't forget to use the `v` in the version
    number.\n\n## Step by Step\n\n**Creating an app**\n\nSo, let’s build a Deno app.
    We’re going to use [dinatra](https://github.com/syumai/dinatra), a Deno module
    which gives Sinatra-like capabilities to Deno apps. You’ll of course want to install
    Deno and create a directory for your app. Then make a `server.ts` file and put
    this in it:\n\n```\nimport {\n  app,\n  get,\n  post,\n  redirect,\n  contentType,\n}
    from \"https://denopkg.com/syumai/dinatra/mod.ts\";\n\nconst greeting = \"<h1>Hello
    From Deno on Fly!</h1>\";\n\napp(\n  get(\"/\", () => greeting),\n  get(\"/:id\",
    ({ params }) => greeting + `</br>and hello to ${params.id}`),\n);\n```\n\nAnd
    that’s an entire simple web server application. Don’t forget permissions though:
    create a `.config` file with `permissions=\"--allow-net\"` in it; all this application
    wants is network access.\n\n**Extra steps**\n\nYou may want to test your app before
    deploying it. You have two options. \n\nThe first is to just run it before packaging
    it into a container image. \n\nRunning the deno command with the permissions:\n\n```\ndeno
    run --allow-net server.ts\n```\n\nwill be all you need in that case\n\nThe second
    is to package up the container image and run that image. You’ll need [Docker](https://www.docker.com/products/docker-desktop)
    and Buildpacks.io's [pack](https://buildpacks.io/docs/install-pack/) installed
    locally to do this. Use pack to create the image:\n\n```\npack build test-server-app
    --builder flyio/builder\n```\n\nThen use Docker to run the `test-server-app` image:\n\n\n```\ndocker
    run -p 8080:8080 test-server-app\n```\n\n**Deploying to Fly**\n\nNow we can create
    a Fly app for this code by running\n\n\n```\nflyctl init --builder flyio/builder\n```\n\nAnd
    we can deploy it with `flyctl deploy`\n\n\n## What Next\n\n**For Fly Buildpacks**\n\nWe’ve
    made all the Buildpack code available in a [GitHub repository](https://github.com/superfly/fly-builders)
    for anyone who wants to improve the process. We’ll be looking to add new buildpacks
    to it too, and enhance existing buildpacks. It’s worth noting that you can build
    your own local buildpack and use it with the fly-builder stack for local/test
    builds - you’ll have to publish it with public access if you want to do Fly deployments
    with it though. \n\n**For Deno on Fly**\n\nWe’re ready for your next Deno app
    on Fly, even if it’s your first. With a simple build and deploy process, it's
    easier than ever.\n\n<p class=\"callout\">Update: 8/6/2020: Both the Deno buildpack
    and this article have been updated with a streamlined configuration mechanism.\n</p>\n"
- :id: blog-imaginary-on-fly-just-one-click
  :date: '2020-04-20'
  :category: blog
  :title: Imaginary on Fly, just One-click
  :author: dj
  :thumbnail:
  :alt:
  :link: blog/imaginary-on-fly-just-one-click
  :path: blog/2020-04-20
  :body: |2+


    <p class="lead">

    One of our most popular uses for Fly is providing global image services using Imaginary; we even have a guide for it. But at Fly we know we can make it simpler, which is why we've now got a one-click launcher for Imaginary available today.

    </p>

    <%= partial "/docs/partials/obsolete_doc" %>

    The folks behind Imaginary have worked with Fly and they also added the one-click launcher to their [README](https://github.com/h2non/imaginary/blob/master/README.md) on GitHub so that potential Imaginary users can be up, converting, resizing and more faster than ever. For Imaginary's creators, it’s also a chance to turn that interest into project funding through revenue sharing. This is something that's going to be big.

    [![Launch](https://fly.io/static/images/external/launch-on-flyio-button.svg)](/docs/app-guides/run-a-global-image-service/)

    One-click launchers make it simple for any project to create running, production-ready versions of their applications. There's no infrastructure needed, just the ability to paste in the button code where needed. And with scalable SVG buttons, even the button just works on your page.

    ## Is your project next?

    Imaginary are the first, we hope of many, open source project to work with us to make their application launchable with One-click onto Fly. Our plan is to make it a valuable way for these projects to fund themselves.

    We're looking for open source projects to work with us to create a process that works for everyone. If your project is interested in engaging with Fly around One-click launching, drop a mail to [Christina, Fly's Community Manager](mailto:christina@fly.io) to find out more.

- :id: blog-fly-changelog-17th-april-2020
  :date: '2020-04-17'
  :category: blog
  :title: Fly Changelog 17th April 2020
  :author: dj
  :thumbnail:
  :alt:
  :link: blog/fly-changelog-17th-april-2020
  :path: blog/2020-04-17
  :body: "\n\n<p class=\"lead\">\n\nThis is the Fly Changelog where we list all significant
    changes to the Fly platform, tooling and web sites. As well as the RSS feed of
    just changelog posts available on [fly.io/changelog.xml](https://fly.io/changelog.xml),
    there's now a dedicated [ChangeLog](https://fly.io/changelog/) page with all the
    recent updates.\n\n</p>\n\nThis past week has been one focused on the Fly infrastructure
    with earlier certificate renewals now in place and a dedicated Changlog page.
    \n\nWe also made time to bring in some user contributed changes to the flyctl
    which make debugging `fly.toml` files easier and clearer upgrade instructions.
    In other updates, there's also better help for scaling your VM sizes and flyctl
    is packaged for Arch Linux now.\n\n<!-- start -->\n\n## _16th April 2020_\n\n**flyctl**:
    Version [0.0.116 released](https://github.com/superfly/flyctl/releases/tag/v0.0.116)\n\n-
    ++ Enhanced error descriptions for bad `fly.toml` file. `flyctl` now lists the
    invalid sections and settings to make it easier to fix. Thanks to [Lars Lehtonen](https://github.com/alrs)
    for that.\n- ++ Readme for `flyctl` now includes the upgrade instructions for
    HomeBrew updates. Thanks to [Simon Willison](https://github.com/simonw) for that**.**\n-
    ++ Improved help for `flyctl scale vm` - now gives basic names for sizes.\n- ++
    Arch Linux users who prefer to install from the AUR package system can now use
    [flyctl-bin](https://aur.archlinux.org/packages/flyctl-bin/) to get flyctl on
    their systems.\n\n**Fly Platform/Web**\n\n- ~~ New applications now get their
    app.fly.dev certificate \"instantly\"\n- ~~ The Fly Proxy will now serve new certificates
    2 weeks before the current one expires if it can. \n\n\n## _13th April 2020_\n\n**Fly
    Platform/Web**\n\n- ++ Added a new [ChangeLog](https://fly.io/changelog/) page
    which brings together all the most recent changelogs into one readable page.\n\n"
- :id: blog-fly-changelog-9th-april-2020
  :date: '2020-04-09'
  :category: blog
  :title: Fly Changelog 9th April 2020
  :author: dj
  :thumbnail:
  :alt:
  :link: blog/fly-changelog-9th-april-2020
  :path: blog/2020-04-09
  :body: |2


    <p class="lead">

    This is the Fly Changelog where we list all significant changes to the Fly platform, tooling and web sites. There is also an RSS feed of just changelog posts available on [fly.io/changelog.xml](https://fly.io/changelog.xml).

    </p>

    <!-- start -->

    ## _7th April 2020_

    **FlyCtl** – [v0.0.115 released](https://github.com/superfly/flyctl/releases/tag/v0.0.115)

    - ++ Buildkit support. Setting environment variable `DOCKER_BUILDKIT` to `1` will enable use of the v2 buildkit backend which performs async parallel builds. For compatibility, `DOCKER_BUILDKIT` defaults to `0`.
    - -- The `--squash` option for deploy. Obsoleted.

    **Fly Platform/Web**

    - ++ Code samples in documentation now have a copy icon for easier cutting and pasting.

    ## _3rd April 2020_

    **FlyCtl** – [v0.0.114 released](https://github.com/superfly/flyctl/releases/tag/v0.0.114)

    - ++ `--build-args` support added to deploy. Passes through build arguments to the build process. More details in the blog post [Powerbuilding with Fly](/blog/powerbuilding-with-fly/).

    ## _2nd April 2020_

    **Fly Platform/Web**

    - ++ Changelog capturing begins.
    - ++ Support for `USER <uid>` and `USER <uid>:<gid>` in docker files, specifically where `<uid>` and `<gid>` are numeric values.
    - ~~ Improved TLS handshake performance
    - ~~ Integrated reaping of zombie processes
- :id: blog-powerbuilding-with-fly
  :date: '2020-04-08'
  :category: blog
  :title: Powerbuilding with Fly
  :author: dj
  :thumbnail:
  :alt:
  :link: blog/powerbuilding-with-fly
  :path: blog/2020-04-08
  :body: "\n\n<p class=\"lead\">\n\nWhen you deploy to Fly, you are deploying an image
    to the Fly infrastructure. Fly currently uses Docker images for this purpose,
    for widest compatibility. We're going to take a look at some features available
    which can power-up your image building.\n\n</p>\n\n## Identifying your build\n\nWhen
    you deploy, the [`flyctl`](https://fly.io/docs/flyctl/) application looks for
    instructions to build the image in a Dockerfile or creates a builder VM to do
    the image build. Let’s start with the flags that control where `flyctl` looks
    for things.\n\n- **-c, --config filename :** It all starts with the config file,
    which is assumed to be `fly.toml` in the current directory. Of course you may
    be building multiple different applications out of the same directory, or just
    be very organized and have a directory for your different configurations. Whatever
    the case, the `--config` option will let you point at a different configuration
    file.\n- **--dockerfile filename :** If you are building an image without a builder,
    `flyctl` looks for a file called `Dockerfile` to get its build instructions. Again,
    if you are building and deploying multiple different applications out of the same
    directory this can be a problem, which is why this option exists. It tells `flyctl`
    to use the filename as the docker file to do the build with.\n\n## Controlling
    the build\n\nSo, now we can tell `flyctl` what config and docker file we want
    to use. The next part of taking control of the build applies to any invocation
    of `flyctl` where a docker file is involved and that’s `--build-args`. \n\nBut
    let’s first rewind back into some docker commands. `ARG` and `ENV` both deal with
    variables that can be set in the build process. The `ARG` command allows variables
    to be specified that are taken from the build command and used during the build
    process. The `ENV` command allows variables to be created that will become environment
    variables set within the image when it starts running. \n\nUsing a combination
    of these, it’s possible to take a command-line argument and turn it into a runtime
    environment variable. Consider, for example, that we want to set the port that
    an NGINX server runs on. We may have had something like:\n\n```dockerfile\n  ENV
    NGINX_PORT=8080\n``` \n\nin the docker file. To make that controllable from the
    command line we can replace that with:\n\n```dockerfile\n  ARG NGINX_PORT=8080\n
    \ ENV NGINX_PORT=${NGINX_PORT}\n```\n  \nThe `ARG` command makes a variable called
    `NGINX_PORT` and sets it to a default of 8080. The `ENV` command creates an environment
    variable also called `NGINX_PORT`. This variable will live on into the running
    version of the image and can be used by scripts within it to control applications
    running in the image. It takes its value from the `ARG` setting of `NGINX_PORT`
    through the expansion of `${NGINX_PORT}` which refers specifically to the `ARG`
    variable and thus will expand to either the default or the passed-in value. \n\nWhich
    leaves the question of how to set that `ARG` value with `flyctl`. That’s where
    `--build-args`  comes in. You can use this option to pass a number of name/value
    pairs over to the build process. So for our example above we could do:\n\n```\n
    \ flyctl deploy --build-args NGINX_PORT=4000\n```\n  \nThis will set the build
    argument which will override the default of the environment variable.\n  \n\n##
    When to use --build-args\n\nIt may look like this is a good way to pass credentials
    and other sensitive data to your Fly applications, but it isn’t. This is built
    for non-sensitive data as the information is baked into the image and could be
    retrievable. If it’s sensitive information you want to pass to the application,
    check out [Fly Secrets](https://fly.io/docs/secrets/) which are securely stored
    and injected, as environment variables, into the application when the image starts
    running on the Fly platform.\n\n\n"
- :id: blog-fly-answers-more-questions
  :date: '2020-04-06'
  :category: blog
  :title: Fly Answers More Questions
  :author: dj
  :thumbnail:
  :alt:
  :link: blog/fly-answers-more-questions
  :path: blog/2020-04-06
  :body: "\n\n<p class=\"lead\">\n\nWe get asked questions about Fly in a lot of places
    on the web which we answer. But, not everyone is everywhere on the web, so with
    Fly Answers Questions, we bring those answers to you. If you have questions about
    Fly, why not ask [@flydotio](https://twitter.com/flydotio) on Twitter, or drop
    a query in our [Fly Spectrum.chat forum](https://spectrum.chat/flyio/).\n\n</p>\n\n**Q:
    What if I get a large number of malicious TCP connections from around the world
    that are malicious and cause a large number of apps to be launched in the various
    datacenters. Are we on the hook for the bill? And is there anything in place to
    stop that happening?** - ([via Spectrum.chat/flyio](https://spectrum.chat/flyio/general/ddos-protection~19d8ab37-36db-44ee-a733-e3309694f069))\n\n**A:**
    Let's start with what we have to stop that sort of problem. Fly apps have a number
    of configurable constraints to prevent runaway scaling. There’s a per-region max
    count and a global max count to prevent too many instances from being started.
    \n\nGoing beyond those maximums means that incoming requests will automatically
    be dropped. It's also worth mentioning that we have a number of protections against
    malicious connections already baked into the Fly networking infrastructure.\n\nComing
    soon, and currently in preview, is a configurable monthly maximum spend which
    will cap your spending. Also, where there was obviously a DDoS attack or similar,
    we’d look to waive usage for any crazy spikes that happened.\n\n**Q: I'm working
    on a small team project using Fly, and we'd like to have a dedicated deploy key
    for our GitHub Actions CD workflow. Is there a way to do this yet?** - ([via Spectrum.chat/flyio](https://spectrum.chat/flyio/general/deploy-keys-and-flyctl-auth-token-permissions~66bc54f2-1918-468d-950c-3e06ea2e1bb8))\n\n**A:**
    The easiest way to get a dedicated deploy key is to create a dedicated user for
    your CD workflow. Once you have a dedicated user you can get a personal access
    token for that user by going to [fly.io/user/personal\\_access\\_tokens](https://fly.io/user/personal_access_tokens)
    and selecting **Create Access Token.** \n\nThat user doesn’t have access to your
    Fly projects yet though. For that, you’ll need to create a Fly organization. Projects
    created in an Organization can be worked on by all the members of the Organization.
    Create an Organization by going to [fly.io/organizations](https://fly.io/organizations)
    and invite your new dedicated CD user to the organization. To complete the process,
    you’ll need to log in as the CD user and accept that invite. You can now configure
    your CD processes using that user to deploy to Fly. \n\n**Q: We run a monorepo
    which means we have a lot of Fly configuration files and Dockerfiles associated
    with projects. We’ve found the `--config` flag which lets us select different
    fly configuration files but flyctl always seems to build with the file called
    `Dockerfile`. What can we do?** - (via various support requests)\n\n**A:** You
    can update your `flyctl` and make use of one of a range of new options we’ve been
    building for people with more sophisticated build configurations. We developed
    `flyctl` for the path of least resistance and that meant assuming that `Dockerfile`
    would be the build file for most users. You folks are great at pushing us to make
    things better, so we now have a new option `--dockerfile` for the `deploy` command.
    That lets you say which Dockerfile should be used in the build process. We’ll
    have a new article on `flyctl deploy` options soon which will give you the power
    to bend Fly deployment to your will.\n\n\n\n"
- :id: blog-making-datasets-fly-with-datasette-and-fly
  :date: '2020-03-26'
  :category: blog
  :title: Making Datasets Fly with Datasette and Fly
  :author: dj
  :thumbnail:
  :alt:
  :link: blog/making-datasets-fly-with-datasette-and-fly
  :path: blog/2020-03-26
  :body: "\n\n<p class=\"lead\">\n\n*The creator of Datasette, the tool for Dataset
    sharing and exploration, has added Fly to the platforms you can use to publish
    and share data. We take a look at Datasette and show how well it works with Fly.*\n\n</p>\n\nI've
    always liked finding a good dataset. With a background in databases and writing,
    I know a good dataset can bring a demo to life, be it a [census of Squirrels in
    Central Park](https://data.cityofnewyork.us/Environment/2018-Central-Park-Squirrel-Census-Squirrel-Data/vfnx-vebw)
    or a [survey of grocery purchases in London](https://figshare.com/articles/Area-level_grocery_purchases/7796666).
    Datasets can also provide valuable foundations for citizen journalism and, under
    analysis, provide insights. \n\nThe key to making these datasets work for people
    is making them accessible and available, which is where [Datasette](https://github.com/simonw/datasette)
    comes in. It's a project by Simon Willison designed to make sharing datasets easy.
    It all hinges on SQLite - essentially, you load up an SQLite database with data
    and then hand it to Datasette which presents it through a web site, to the world.\n\n##
    First, the data!\n\nI'm going to use the [New York Central Park Squirrel Census
    Data](https://data.cityofnewyork.us/Environment/2018-Central-Park-Squirrel-Census-Squirrel-Data/vfnx-vebw)
    for my data source because squirrels rock. Go there and click the **View Data**
    button to see the data presented in the very fine NYC OpenData viewer. \n\nI want
    the raw data though so I'll click on **Export** and select **CSV** which will
    kick off an immediate download. We now have a `2018_Central_Park_Squirrel_Census_-_Squirrel_Data.csv`
    file to work with.\n\n## Making an Sqlite database\n\nEnsure you have SQLite3
    installed on your system; it'll already be there on a macOS system as it is heavily
    used throughout the OS. If you run `sqlite3 filename.db` the data will be persisted
    to that file rather than just held in memory, so let's begin.\n\n```\n$ sqlite3
    squirrels.db\nSQLite version 3.28.0 2019-04-15 14:49:49\nEnter \".help\" for usage
    hints.\nsqlite>\n```\n\nAnd the first thing we need to do is set the mode to CSV.
    If you look at the documentation, you'll see the `.mode` command listed as setting
    the output mode. That's not quite completely true, it's also used as a hint to
    the import command.\n\n```\nsqlite> .mode csv\n```\n\nNow we are ready to import
    with `.import`. This command takes the CSV filename and a table name to import
    into. If the table isn't there, it'll use the first row in the CSV file to create
    the table columns and then import. (ProTip: If the table already exists it just
    imports everything, including the header row so always drop the table first).\n\n```\nsqlite>
    .import 2018_Central_Park_Squirrel_Census_-_Squirrel_Data.csv squirrels\n```\n\nAnd
    we can do a quick check on what actually got imported with  the `.schema` command.\n\n```\nsqlite>
    .schema squirrels\nCREATE TABLE squirrels(\n  \"X\" TEXT,\n  \"Y\" TEXT,\n  \"Unique
    Squirrel ID\" TEXT,\n  \"Hectare\" TEXT,\n  \"Shift\" TEXT,\n  \"Date\" TEXT,\n
    \ \"Hectare Squirrel Number\" TEXT,\n ...\n  \"Lat/Long\" TEXT,\n  \"Zip Codes\"
    TEXT,\n  \"Community Districts\" TEXT,\n  \"Borough Boundaries\" TEXT,\n  \"City
    Council Districts\" TEXT,\n  \"Police Precincts\" TEXT\n);\n```\n\nThere are a
    lot of columns about squirrels. Now I can exit sqlite3 and get ready to apply
    Datasette to the database.\n\n## Installing Datasette\n\nThere are a [couple of
    ways to install and run Datasette](https://datasette.readthedocs.io/en/stable/installation.html).
    I'm going to go with the one that is simplest for most developers:\n\n* make sure
    Python3 is installed (`brew install python3` on macOS using [HomeBrew](https://brew.sh))\n*
    `pip3 install datasette` - You may get an error on macOS as it stops the file
    being copied to a system directory. Don't worry, just repeat the command with
    `--user` on the end and add the directory it suggests into your path.\n\nAnd we're
    ready to test running `datasette squirrels.db`:\n\n```\n❯ datasette squirrels.db\nServe!
    files=('squirrels.db',) (immutables=()) on port 8001\nINFO:     Started server
    process [2812]\nINFO:     Waiting for application startup.\nINFO:     Application
    startup complete.\nINFO:     Uvicorn running on http://127.0.0.1:8001 (Press CTRL+C
    to quit)\n```\n\nNavigating to http://127.0.0.1:8001 and clicking on the Squirrel
    table, we should see the squirrels data available:\n\nAnd clicking through you
    can start using Datasette's UI to compose views of particular [facets](https://datasette.readthedocs.io/en/stable/facets.html)
    of the data, write your own SQL queries, export data as JSON or CSV or access
    the data through a REST/JSON API. \n\n![Local Datasette Index](squirrels-index.png?1/2&card&center)\n\nThe
    next stop is making it available to the world.\n\n## Publishing to Fly\n\nWhile
    other platforms are already built into Datasette, the Fly publishing element of
    Datasette is a new plugin, created by Datasette's developer. That means it has
    to be installed separately with `pip3 install datasette-publish-fly`\n\nWith that
    installed, you can run\n\n```\ndatasette publish fly squirrels.db --app squirrels\n```\n\nThe
    `--app` flag lets you set the app name and it will be rejected if it clashes with
    an existing app. You may, if you are in multiple organizations, be asked to pick
    one of those too. Once you've done that, the publish command takes over, builds
    an image and deploys it onto the Fly platform. If you want to know what IP address
    and hostname the app is on, run `flyctl info -a <appname>` like so:\n\n```cmd\n
    flyctl info -a squirrels\n```\n```output\nApp\n  Name     = squirrels\n  Owner
    \   = dj\n  Version  = 0\n  Status   = running\n  Hostname = squirrels.fly.dev\n\nServices\n
    \ PROTOCOL   PORTS\n  TCP        80 => 8080 [HTTP]\n             443 => 8080 [TLS,
    HTTP]\n\nIP Addresses\n  TYPE   ADDRESS                                CREATED
    AT\n  v4     77.83.142.59                           23m21s ago\n  v6     2a09:8280:1:7bc8:bf19:7779:aef7:8f18
    \  23m21s ago\n```\n\nAnd that also tells us where we need to browse: `squirrels.fly.dev`.
    We're online and we can dig down into a table view where we can compose queries.\n\n![Squirrel
    Table](squirrel-table.png?card)\n\n## Deploying with Plugins\n\nDatasette Plugins
    aren't just for publishing; there are a [whole range of additional capabilities](https://datasette.readthedocs.io/en/stable/ecosystem.html#datasette-plugins)
    waiting to be slotted in. Take [datasette-cluster-map](https://github.com/simonw/datasette-cluster-map),
    for example. It looks for latitude and longitude columns in the data and turns
    the data into an interactive map using them. Let's see how we use this with Fly.\n\n###
    Tuning the tables\n\nThe Squirrel data has X and Y coordinates which match Longitude
    and Latitude; we'll need to rename those columns first. Now, for a long time,
    sqlite lacked the ability to rename columns, so you'll find many workarounds on
    the web if you search. The good news is, though, that since 2018 you have been
    able to rename columns with the [`alter table rename column`](https://sqlite.org/lang_altertable.html#altertabmvcol)
    command. So I'll just load up the sqlite3 database and alter those columns:\n\n```\n❯
    sqlite3 squirrels.db\nSQLite version 3.28.0 2019-04-15 14:49:49\nEnter \".help\"
    for usage hints.\nsqlite> alter table squirrels rename column X to Longitude;\nsqlite>
    alter table squirrels rename column Y to Latitude;\nsqlite> .schema squirrels\nCREATE
    TABLE squirrels(\n  \"Longitude\" TEXT,\n  \"Latitude\" TEXT,\n...\nsqlite> .exit\n❯\n```\n\n###
    Run Locally Again\n\nI now need to install that plugin with `pip3`:\n\n```\n❯
    pip3 install datasette-cluster-map\n```\n\nAnd run datasette locally:\n\n```\n❯
    datasette squirrels.db\nServe! files=('squirrels.db',) (immutables=()) on port
    8001\nINFO:     Started server process [39385]\nINFO:     Waiting for application
    startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on
    http://127.0.0.1:8001 (Press CTRL+C to quit)\n```\n\nAnd if we browse to that
    http://localhost:8001/ we'll see the index page as we did before. Now to deploy
    to Fly.\n\n### Deploying and Plugins\n\nThe difference here from when we previously
    published to Fly is that we have to list the plugins we need installed with our
    Datasette. The `--install` flag takes care of that, so now I can publish to Fly
    with:\n\n```\ndatasette publish fly squirrels.db --app squirrels-mapped --install
    datasette-cluster-map\n```\n\nAnd that will include the cluster-map plugin. If
    I now browse to \"https://squirrels-mapped.fly.dev\", and click in on the squirrels
    table:\n\n![Squirrels Mapped](squirrels-mapped-map.jpg?card)\n\nOur view of the
    Squirrels data now includes a cluster map over Central Park that we can click
    in on and get a closer view. When sightings resolve to a single squirrel, you
    can hover over it to get all the details.\n\n## Datasette and Fly\n\nSo what does
    Fly add to Datasette? Well, as well as being super simple to deploy, you may have
    noticed that all the connections we're making are HTTPS secured, with Let's Encrypt
    certificates being generated automatically. If you want more, it's simple to use
    your own custom domain or hostname with your Fly/Datasette deployment. You can
    also deploy all around the world so your dataset is available where people need
    it to be. And there's also the edge networking/SSL termination which makes interaction
    that bit snappier. There are a whole lot more to explore in Datasette - [check
    out the documentation](https://datasette.readthedocs.io/en/stable/) - and it's
    a great way to discover how you can make your applications Fly.\n\n*Thanks to
    Simon Willison, not only for Datasette and the Fly plugin, but also for his feedback
    on this article. And to the Squirrels of New York's Central Park for taking part
    in the [census](https://thesquirrelcensus.com/).*\n\n<p class=\"callout\">\n  _Want
    to learn more about Fly? Head over to our [Fly Docs](/docs/) for lots more, including
    a [Hands On](/docs/hands-on/start/) where you can get a free account and deploy
    your first app today._\n</p>\n"
- :id: blog-continuous-deployment-with-gitlab
  :date: '2020-03-23'
  :category: blog
  :title: Continuous Deployment with GitLab
  :author: dj
  :thumbnail:
  :alt:
  :link: blog/continuous-deployment-with-gitlab
  :path: blog/2020-03-23
  :body: "\n\n<p class=\"lead\">\n\nWe recently added a guide to [Continuous Deployment
    with Fly and GitHub Actions](/docs/app-guides/continuous-deployment-with-github-actions/)
    to the Fly documentation and almost immediately, I was asked if we could do the
    same for GitLab. Of course! Better still, let's make this an example for other
    platforms.\n\n</p>\n\n## Starting with Continuous Deployment to Fly\n\nTo deploy
    to Fly you essentially need four things\n\n1. The application you want to deploy.\n2.
    A runnable copy of flyctl.\n3. A fly.toml file.\n4. Your Fly API Token.\n\nFor
    this example, we'll deploy [hellofly](https://github.com/fly-apps/hellofly), our
    small Go application which says hello - GitLab can import the repository for you.
    \n\nOn GitLab, the CI/CD system takes care of the first requirement for you. The
    CI/CD process is all driven by a `.gitlab-ci.yml` file. Lets walk through the
    `.gitlab-ci.yml` I used to deploy from GitLab to Fly:\n\n```yml\ndefault:\n  image:
    golang:1.13\n```\n\nThe default section does the essential preparation. It selects
    an image to use for the virtual machine - or \"Runner\" to use GitLab terminology
    - that will be used run the CI/CD process. A simple option is to use the same
    image as you use in the Fly app - our example is a Go app so we use the `golang:1.13`
    image, but you could use any image which has sufficient toolchain components to
    build it. \n\nWith Fly, the deployment process runs in its own container, locally
    or remotely. That means only a minimal set of the components from a typical toolchain
    is needed to build the application.\n\nOnce you have selected your image and the
    deploy process begins, GitLab's CI/CD engine will automatically copy the contents
    of the repository over to the Runner image. And that's requirement one met.\n\nNext
    up is getting a runnable copy of `flyctl` into our Runner. For GitLab, we want
    to do this before it runs our deployment scripts, so we'll add it to the default
    section as the `before_script`.\n\n```yml\n  before_script:\n    - apt-get update
    -qq && apt-get install -y curl\n    - curl -L https://fly.io/install.sh | sh\n```\n\nThis
    does two things. It installs `curl` from the package repository (after making
    sure its indexes are up to date). Then it uses `curl` to download the `flyctl`
    install script and runs that. Using the script should ensure that the right version
    of `flyctl` is installed. This covers requirement two.\n\nNow, we are going to
    quickly skip to the end, as the rest of the `.gitlab-ci.yml` is just this:\n\n```yml\ndeploy:\n
    \ script:\n    - flyctl deploy\n```\n\nThat will run the `flyctl deploy` command.
    That will then build and deploy the fly application. If we committed `.gitlab-ci.yml`
    at this point though, it would fail as it lacks two things.\n\n## Configuring
    for Fly\n\nThe things needed are a `fly.toml` file and an auth token. The `fly.toml`
    file will need to be created using `fly init` so you'll likely do this locally
    and then add the file to the repository so it will be there for `flyctl` to find
    when it's run in the deployment container. Add that to the repository (do remove
    the filename from the `.gitignore` file first) and that's requirement three handled.\n\nFor
    the API token, you'll want to use flyctl again, this time to reuse the token you
    were issued when you logged in. Run `flyctl auth token` and it will display the
    API token your session is currently using. Now you'll need to turn this token
    value into the value of the `FLY_API_TOKEN` environment variable inside the deployment
    container. For GitLab, you'll need to go to the repositories settings, select
    **CI/CD**, then expand the **Variables** section. Create a new variable called
    `FLY_API_TOKEN` and copy the auth token value into the **Value** field. Then turn
    on the **Protected** and **Masked** switches so that it is not leaked through
    the logs. Save the new variable. It should look like this:\n\n![GitLab Variables](gitlabvariables.png)\n\n##
    Ready to Deploy\n\nNow we are ready to commit the `.gitlab-ci.yml` to the repository
    and install our CD pipeline. Here's the whole file:\n\n```yml\ndefault:\n  image:
    golang:1.13\n  before_script:\n    - apt-get update -qq && apt-get install -y
    curl\n    - curl -L https://fly.io/install.sh | sh\n\ndeploy:\n  script:\n    -
    flyctl deploy\n```\n\nCommit and push that up to GitLab and the deployment process
    will begin almost immediately. On the GitLab web UI, head to **CI/CD** and then
    **Jobs** and you should see the deployment job running. Click on the _running_
    badge to watch its progress.\n\n## Job's Done\n\nAnd that's pretty much it for
    deploying with GitLab's CI/CD system. There's a lot more functionality in there
    allowing you to structure the pipeline as you want and trigger different jobs
    at different times. We've just touched on the simple case of wanting to deploy
    when an update is pushed. The principles here are applicable to most CI/CD platforms
    and should let you incorporate Fly into your workflows today. \n\n<p class=\"callout\">\n
    \ _Want to learn more about Fly? Head over to our [Fly Docs](/docs/) for lots
    more, including a [Hands On](/docs/hands-on/start/) where you can get a free account
    and deploy your first app today._\n</p>\n\n\n\n"
- :id: blog-fly-answers-questions
  :date: '2020-03-18'
  :category: blog
  :title: Fly Answers Questions
  :author: dj
  :thumbnail:
  :alt:
  :link: blog/fly-answers-questions
  :path: blog/2020-03-18
  :body: "\n\n<p class=\"lead\">\n\nWe get asked questions about Fly in a lot of places
    on the web which we answer. But, not everyone is everywhere on the web, so with
    Fly Answers Questions, we bring those answers to you. If you have questions about
    Fly, why not ask [@flydotio](https://twitter.com/flydotio) on Twitter, or drop
    a query in our [Fly Spectrum.chat forum](https://spectrum.chat/flyio/).\n\n</p>\n\n###
    Q: Can I set up a custom hostname or domain with Fly?\n\n**A:** You can. In fact
    you can set up as many of them on an application as you like. When you create
    a custom hostname on Fly and validate it, you also get Let's Encrypt certificates
    automatically. You can read more about this in [Custom Domains with Fly](https://fly.io/docs/app-guides/custom-domains-with-fly/)
    which takes you through the process and how to automate it.\n\n### Q: If I delete
    the fly.toml file, can I regenerate it easily? \n\n<small>The fly.toml file is
    the file which contains a Fly application's configuration. This question was via
    [HackerNews](https://news.ycombinator.com/item?id=22512931).</small>\n\n**A:**
    Sure. `flyctl config save -a <appname>` will retrieve the last used configuration
    and save it locally as fly.toml. If you want to save it with a different name,
    use `-c` for the config file name, `flyctl config save -a <appname> -c <config.toml>`.\n\n###
    Q: When using the Turboku adapter do you actually run my Heroku slug as a container
    on each edge? \n\n<small>This question (and the two following) came via [Twitter](https://twitter.com/sudhirj/status/1236647023018790912).</small>\n\n**A:**
    Yes, we do run your Heroku slug as a container in one of our datacenters. We spin
    up instances as near as possible to where a request enters our edge network, which
    may be in the same region or geographically very close. We don't put the actual
    slug at the edge though, to give us more flexibility in responding to load. \n\n###
    Q: Isn't the DB connection latency insane, though,  for pages with lots of queries?\n\n**A:**
    You can choose the regions your app runs in to minimize database latency. We do
    this by default for Heroku/Turboku apps. You can do this for other apps too -
    for example if your database runs in datacenters in Virginia then - `flyctl scale
    regions iad=1` will bring up the applications in the Virginia region or in regions
    close to it. You'll still get benefits of SSL termination at the edge with restricted
    regions for the app.\n\n_Note: 13/05/20: The scaling commands have been [updated](/blog/updating-scale/)
    since this article was published._\n\n### Q: Can I do only SSL termination? \n\n**A:**
    Yes you can. Setting `handlers=['tls']` in the `fly.toml` configuration file will
    let you TLS terminate at the edge; currently it supports up to HTTP 1.1 (so no
    HTTP2 support).\n\n### Q: What is Fly doing about global databases in general?\n\n**A:**
    We've got Redis for local caching already and have a replication system to distribute
    changes for that in place. We have tested CockroachDB as a distributed drop-in
    replacement for PostgreSQL, but have found it not as easy to migrate from PostgreSQL
    as our users would have liked. That said, we've heard that CockroachDB is filling
    that compatibility gap and we'll be looping round to it as soon as possible.\n\n###
    Q: Our application has a repository on GitHub. Is there a recommended way to continuously
    deploy our application to Fly?\n\n**A:** You can use GitHub Actions to deploy
    straight to Fly. We've got a guide to it - [Continuous Deployment with Fly and
    GitHub Actions](https://expansionteam.fly.dev/docs/app-guides/continuous-deployment-with-github-actions/)
    - which takes you through the steps with an example application.\n\n### Q: Could
    you clarify if Fly automatically scales down to zero for completely idle apps?
    \n\n<small>This question via [Twitter](https://twitter.com/sudhirj/status/1238019203602329600).</small>\n\n**A:**
    Sure can. We don't scale to zero for completely idle apps, we keep one instance
    ticking over in a region on our default region model (close to the edge region
    where the app was created) so that its always there to respond to traffic quickly.
    \n\nScaling to zero on idle means an app can be very slow, cold-starting regularly,
    so we've gone for a single instance in a region model as our lowest scale point.
    The good thing is that the Anycast IP address and edge TLS termination still make
    this a faster option than other alternatives. \n\nIf you want to know more about
    Fly scaling, check out this [recent Scaling blog post](https://fly.io/blog/scaling-fly-for-all/)
    or delve into [Fly's scaling documentation](https://fly.io/docs/scaling/) for
    even more detail.\n\n\n\n\n\n\n"
- :id: blog-scaling-fly-for-all
  :date: '2020-03-17'
  :category: blog
  :title: Scaling Fly for All
  :author: dj
  :thumbnail:
  :alt:
  :link: blog/scaling-fly-for-all
  :path: blog/2020-03-17
  :body: "\n\n<p class=\"lead\">\n\nScaling controls are now available to all Fly
    users. You have direct control of how many instances of your Fly applications
    are running in the Fly regions around the world. And you can take control of the
    size of your application's virtual machines.\n\n</p>\n\n\n<p class=\"callout\">\n\n**Update
    15/May/2020**: _The contents of this article have been superceded by a new scaling
    system on Fly. We've refined the scaling models and commands to make things simpler
    to use. To read about these changes see the article on the [updated scale](/blog/updating-scale/)
    system or read the revised [scale documentation](/docs/scaling/)._\n\n</p>\n\nWant
    to know a Fly secret? We've got a lot of things in our platform that we're steadily
    rolling out to users. One of those things is global scaling - we've been using
    it internally and with selected customers, but now, it's available for all.\n\n\nWhen
    you create an application on Fly, it uses our default region plan. This places
    the application in the best available region for access from the Fly edge network.
    When an application is created, it is put in the default region, which for most
    applications will be the closest region to where the application was created.\n\nWhen
    an instance hits its connections limit, by default between 20-25, a new instance
    added and Fly's autopilot scaling will decide the optimal location for that new
    instance. It's a very effective default plan.\n\n## Scaling over regions\n\nWe
    are moving on though and the first thing to do is to empower you. Specifically
    with power to control where your data center instances are and the minimum nuber
    of instances that are available there. Say hello to the `flyctl scale` commands.\n\nWith
    `scale regions` you can see what your current scaling is set to or you can set
    which regions your app is running in and minimum number of app instances you want
    running in each one. Let's start by seeing what the default region looks like:\n\n```cmd\nflyctl
    scale regions\n```\n```output\nAutoscaling\n  Enabled         = true\n  Balance
    Regions = true\n```\n\nAnd that's it. There are no regions set and no instance
    counts displayed so this application runs in the default region. \n\nLet's use
    the example of scaling from our front page:\n\n```cmd\nflyctl scale regions ams=1
    hkg=1 sjc=1\n```\n```output\nUpdating autoscaling config...\nAutoscaling\n  Enabled
    \        = true\n  Balance Regions = true\n\nRegions\n  REGION   MIN COUNT   WEIGHT\n
    \ ams      1           100\n  hkg      1           100\n  sjc      1           100\n```\n\nThe
    application will now run at least one instance in ams (Amsterdam), hkg (Hong Kong)
    and sjc (Sunnyvale, California). We'll discuss weight at another time - it controls
    the Fly's preference for where new instances are created. When you do use a `scale
    regions` command, the Fly platform generates a new version of the deployed app
    and redeploys to match the new scaling. \n\n## Regions and Fly\n\nIf you aren't
    sure about what regions there are, you can [look regions up in the documentation](/docs/regions/)
    or use the `flyctl platform regions` command:\n\n```cmd\n flyctl platform regions\n
    ```\n ```output\n  CODE   NAME\n  ams    Amsterdam, Netherlands\n  atl    Atlanta,
    Georgia (US)\n  dfw    Dallas 2, Texas (US)\n  ewr    Parsippany, NJ (US)\n  fra
    \   Frankfurt, Germany\n  hkg    Hong Kong\n  iad    Ashburn, Virginia (US)\n
    \ lax    Los Angeles, California (US)\n  mrs    Marseille, France\n  nrt    Tokyo,
    Japan\n  ord    Chicago, Illinois (US)\n  sea    Seattle, Washington (US)\n  sin
    \   Singapore\n  sjc    Sunnyvale, California (US)\n  syd    Sydney, Australia\n
    \ yyz    Toronto, Canada\n```\n\n## Scaling VMs\n\nScaling out over regions is
    one of the scaling options. Another option is to scale the instances themselves.
    By default, applications are allocated a micro-VM (micro-2x) with 512MB of memory
    and a quarter share of a vCPU cores. It's small but pretty mighty. \n\n|Name|vCPUs|Memory|\n|----|---------|------|\n|micro-1x|0.12
    (shared)|128 MB|\n|micro-2x|0.25 (shared)|512 MB|\n|cpu1mem1|1 (dedicated)|1 GB|\n|cpu2mem2|2
    (dedicated)|2 GB|\n|cpu4mem4|4 (dedicated)|4 GB|\n|cpu8mem8|8 (dedicated)|8 GB|\n\nRunning
    `flyctl platform vm-sizes` will display this table with the current per second
    and per month pricing for each VM size. That information is also available on
    the [pricing page](/docs/pricing/).\n\n## Setting a VM Size\n\nThe VM size for
    an application applies to all instances currently deployed and deployed in the
    future. Just run `flyctl scale vm <sizename>` to set the size and, like the region
    scaling, a new version of the app will be created and all the instances will be
    redeployed.\n\nSo, you now know enough commands to get scaling today and configure
    your Fly app to the size that suits you. \n\nFor more on Scaling, check out our
    [Scaling documentation](/docs/scaling/) where you can learn about the other scale
    commands, what the \"Autoscaling\" in the output is about and more.\n\n<p class=\"callout\">\n
    \ _Want to learn more about Fly? Head over to our [Fly Docs](/docs/) for lots
    more, including a [Hands On](/docs/hands-on/start/) where you can get a free account
    and deploy your first app today._\n</p>\n\n"
- :id: blog-how-to-custom-domains-with-fly
  :date: '2020-03-09'
  :category: blog
  :title: How to do Custom Domains with Fly
  :author: dj
  :thumbnail:
  :alt:
  :link: blog/how-to-custom-domains-with-fly
  :path: blog/2020-03-09
  :body: "\n\n<p class=\"lead\">\n\nWe've just published [Custom Domains with Fly](https://fly.io/docs/app-guides/custom-domains-with-fly/),
    the latest of our application guides which shows how you can use Fly to proxy
    traffic or serve applications for multiple custom domains. \n\n</p>\n\nIn [Custom
    Domains with Fly](https://fly.io/docs/app-guides/custom-domains-with-fly/), you'll
    learn how to configure an NGINX server to proxy traffic to external sites and
    how to attach host and domain names to your Fly applications using Fly's command
    line `flyctl`. \n\nFor those of you who practice devops and automate everything,
    the article also has details on the Fly GraphQL API for adding, removing, checking
    and deleting hosts and associated certificates. There's also a repository of ready-rolled
    Node applications which clearly demonstrate how the API works and offer a jumping
    off point for any developer looking to customize their Fly experience.\n\n###
    Speedrun!\n\nFor a taste of how straightforward the process can be, here's a speed
    run for adding a host/domain to an application. Let's say we have _example.com_
    and a Fly app called _custom-quartz_.\n\n* Run `flyctl ips list -a custom-quartz`
    to get the IPv4 and IPv6 addresses.\n* Head over to your DNS provider and add
    A and AAAA records for _example.com_ with the IPv4 and IPv6 values.\n* Run `flyctl
    certs create -a custom-quartz example.com`\n* Run `flyctl certs show -a custom-quartz
    example.com` to watch your certificates being issued.\n* Connect to `https://example.com`
    and use your application with auto-renewing Let's Encrypt TLS certificates, edge
    TLS, HTTP/2 support and more.\n\n### For Proxies, For All\n\nThere's no limit
    to the number of certificates you can attach to an Fly application. That means
    that if you're in the business of producing branded applications with custom domains,
    Fly is a great place to unify all your proxying needs into one globally fast application
    which scales on demand. \n\nFly your own flag on your Fly applications today.\n\n<p
    class=\"callout\">\n  _Want to learn more about Fly? Head over to our [Fly Docs](https://fly.io/docs/)
    for lots more, including a [Hands On](https://fly.io/docs/hands-on/start/) where
    you can get a free account and deploy your first app today._\n</p>\n"
- :id: blog-fly-friday-feb28-news
  :date: '2020-02-28'
  :category: blog
  :title: Fly Friday - News and Tips
  :author: dj
  :thumbnail:
  :alt:
  :link: blog/fly-friday-feb28-news
  :path: blog/2020-02-28
  :body: |2


    ## _This week with Fly:_

    * We show you how to use [Fly to build your own GraphQL APIs](https://fly.io/docs/app-guides/graphql-on-fly-with-hasura/).
    * **Firefox** is switching **DNS over HTTPS** (DoH) on by default in the USA. We already have a great guide to using [Fly as your private DoH service](https://fly.io/docs/app-guides/run-a-private-dns-over-https-service/).
    * Fly's new community portal for Fly users is now open on [Spectrum.chat/flyio](Spectrum.chat/flyio) - Sign in with your Github or social media login and chat with the Fly team.
    * Spreading the Fly word! We launched [Turboku on Product Hunt](https://www.producthunt.com/posts/turboku).

    <figure class="embed"><iframe src="https://player.vimeo.com/video/393963664" width="640" height="360" frameborder="0" allow="autoplay; fullscreen" allowfullscreen></iframe></figure>

    Get yourself to [fly.io/heroku](https://fly.io/heroku) to try out Turboku, the art of faster Heroku apps.

    ## _Fly Notes - We Complete You_

    Being able to hit the [TAB] key and getting the possible next part of your command makes discovering command lines so much easier. The good news is that, from version 0.0.96, Fly's command line generates the files you need so you can activate tab completion.

    Flyctl's `version` command has a `-c`/`--completion` option which takes a shell name as a parameter, `bash` or `zsh`. Run `flyctl version -c zsh` and it will output the file you need to activate completions for flyctl. Typically running

    ```
    flyctl version -c zsh > $fpath[1]/_flyctl
    ```

    should place the generated file in the right directory to be picked up when you log in next.

    Bash users will have to decide where to store their generated file and `source` it in their `.bashrc` - there's no default directory for completions on bash.

    Don't forget to update your flyctl for this feature!

    <p class="callout">
      _Want to learn more about Fly? Head over to our [Fly Docs](https://fly.io/docs/) for lots more, including a [Hands On](https://fly.io/docs/hands-on/start/) where you can get a free account and deploy your first app today._
    </p>
- :id: blog-graphql-hasura-and-fly
  :date: '2020-02-25'
  :category: blog
  :title: GraphQL, Hasura and Fly
  :author: dj
  :thumbnail:
  :alt:
  :link: blog/graphql-hasura-and-fly
  :path: blog/2020-02-25
  :body: "\n\n<p class=\"lead\">\n\nIf you're wondering how to access your data with
    GraphQL, then it's worth looking at the [Hasura](https://hasura.io/) GraphQL engine.
    It's gives you a GraphQL backend service. Oh, and we've just [published a guide
    on how to deploy Hasura on Fly](/docs/app-guides/graphql-on-fly-with-hasura/).\n\n</p>\n\nHasura
    gives you an open source GraphQL backend which can make it simple to query PostgreSQL
    by helping you map all your schemas and roles. It also rolls in the ability to
    query other GraphQL services so you can mix in services. With an interactive console
    for analyzing and exploring, it's remarkably useful.\n\nAmong the various ways
    you can deploy Hasura, there's the option to deploy it with a Docker image. That's
    where Fly can come in and turn Hasura into a global GraphQL backend. There's some
    configuration to be done - such as where to find your database, setting secrets
    and turning the interactive console on and off - and we take you through each
    of the steps.\n\nDive in and get yourself an Hasura deployment on Fly and do GraphQL
    everywhere. It's far simpler on Fly.\n\n<p class=\"callout\">\n\n_Want to learn
    more about Fly? Head over to our [Fly Docs](/docs/) for lots more, including a
    [Hands On](/docs/hands-on/start/) where you can get a free account and deploy
    your first app today._\n  \n</p>\n\n"
- :id: blog-fly-friday-customizing-with-dockerfiles
  :date: '2020-02-21'
  :category: blog
  :title: Fly Friday - Customizing with Dockerfiles
  :author: dj
  :thumbnail:
  :alt:
  :link: blog/fly-friday-customizing-with-dockerfiles
  :path: blog/2020-02-21
  :body: "\n\n<p class=\"lead\">\n\nLast week's Fly Friday video showed how you can
    simply deploy a Docker image to Fly. Today, we'll show how straightforward it
    is to customize that deployed image with Fly.\n\n</p>\n\nWe deployed Docker's
    official httpd image in [our first Fly Friday video](/blog/fly-friday-flyctl-and-ports/).
    That image, when run, serves up files from its `/usr/local/apache2/html`. By default
    the image contains the words \"It works\". If we want it to say something else,
    we need to copy our content into that directory. \n\nTo do that we use a Dockerfile
    that is just two lines long. \n\n```docker\nFROM httpd:2.4\nCOPY ./public_html/
    \ /usr/local/apache2/htdocs/\n```\n\nThe Dockerfile contains all the instructions
    needed to create our new image. The `FROM` says take the Docker httpd image and
    use it as a base for our new image. The `COPY` says copy the contents of a local
    directory into `/usr/local/apache2/html/` within the new image. Fly will then
    deploy that new image.\n\nWe'll also have to create the `public_html` directory
    and put some HTML content in there.\n\nWant to see all this in action? Watch this
    Fly Friday video:\n\n<figure class=\"embed\">\n  <iframe src=\"https://player.vimeo.com/video/392945265\"
    width=\"640\" height=\"360\" frameborder=\"0\" allow=\"autoplay; fullscreen\"
    allowfullscreen></iframe>\n</figure>\n\n<p class=\"callout\">\n  _Want to learn
    more about Fly? Head over to our [Fly Docs](/docs/) for lots more, including a
    [Hands On](/docs/hands-on/start/) where you can get a free account and deploy
    your first app today._\n</p>\n"
- :id: blog-fly-friday-flyctl-and-ports
  :date: '2020-02-14'
  :category: blog
  :title: Fly Friday - Flyctl and Ports
  :author: dj
  :thumbnail:
  :alt:
  :link: blog/fly-friday-flyctl-and-ports
  :path: blog/2020-02-14
  :body: "\n\n<p class=\"lead\">\n\nWe're always improving the tools at Fly. There
    are the big features like [Fly buildpack support](/blog/simpler-fly-deployments-nodejs-rails-golang-java/)
    of course and then there are the smaller features which just make life that little
    bit simpler, like the new `-p` option for Fly app creation.\n\n</p>\n\nTL;DR:
    The `-p` flag is used with `flyctl apps create` when you create a Fly app for
    the first time. Setting `-p` with a numeric value automatically sets the internal
    port of the generated fly.toml configuration file. \n\nThe internal port is the
    port your application uses to communicate with the world. We defaulted it to 8080.
    \n\nWith more and more people making use of prebuilt Docker images on Fly though,
    they don't have control of which port their image uses. So they have to change
    the internal port setting.\n\nWe noticed that their workflow had become \"create
    app, edit fly.toml file to change the internal port\". That's one step too many
    for us.\n\nAnd so that's why we added the `-p` flag which sets the internal port
    for you, no edits required. It's available from release 0.94 of Flyctl. \n\nYou
    can see it in action in this, the first of our Fly Friday videos:\n\n<figure class=\"embed\">\n
    \ <iframe src=\"https://player.vimeo.com/video/391463149\" width=\"640\" height=\"360\"
    frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen></iframe>\n</figure>\n\n<p
    class=\"callout\">\n\n_Want to learn more about Fly? Head over to our [Fly Docs](/docs/)
    for lots more, including a [Hands On](/docs/hands-on/start/) where you can get
    a free account and deploy your first app today._\n\n</p>\n"
- :id: blog-simpler-fly-deployments-nodejs-rails-golang-java
  :date: '2020-02-10'
  :category: blog
  :title: Simpler Fly deployments for NodeJS, Rails, Go, and Java
  :author: dj
  :thumbnail:
  :alt:
  :link: blog/simpler-fly-deployments-nodejs-rails-golang-java
  :path: blog/2020-02-10
  :body: "\n\n<p class=\"lead\">\n\nWe've just added a new feature to flyctl which
    lets you build and deploy NodeJS, Rails, Go, and Java applications to the Fly
    platform with just two commands. That new feature is support for Cloud Native
    Buildpacks, but you don't need to know what Buildpacks are to use them with Fly.
    \n\n</p>\n\n## A Speedrun for everyone\n\nLet's get straight down to it. Make
    sure you have the latest version of flyctl and then get a Node application like
    this version of our hello world example - \"[hellonode-builder](https://github.com/fly-apps/hellonode-builtin)\".
    \n\n```\ngit clone https://github.com/fly-apps/hellonode-builtin.git\n```\n\nNow
    let's create a fly application for it with one line:\n\n```\ncd hellonode-build\nflyctl
    init --builder cloudfoundry/cnb:bionic\n```\n\nHit return for an autogenerated
    name and flyctl will set up a fly app for you. Make a note of that name. Then
    all you need to do is:\n\n```\nflyctl deploy\n```\n\nAnd the application will
    be turned into a docker image and deployed onto the Fly Global Application Platform.
    Connect to your newly deployed app by running `flyctl open` and you're already
    Flying.\n\n## Flyctl and Buildpacks\n\nWhat we have added is the ability for `flyctl`
    to use Cloud Native Buildpacks from the [buildpacks.io](https://buildpacks.io)
    project. This new Buildpack system is an evolution of existing Buildpack systems
    from Heroku and Cloud Foundry, designed to be used by different platforms. \n\nThe
    new Cloud Native Buildpack system isn't compatible with the older Heroku and Cloud
    Foundry buildpacks, but new Cloud Native Buildpacks are appearing every day to
    supercede them. As the availability expands, even more languages and frameworks
    will be able to be automatically built and deployed by flyctl.\n\nThe new Buildpack
    support complements our existing support of Dockerfiles for building your Fly
    deployment images. You can use that when there's no appropriate buildpack available
    or you want to do something different from existing buildpacks. We're making sure
    you have all the flexibility and power you need to hand.\n\n## Available Cloud
    Native Builders\n\nThere are already Cloud Native Buildpacks that support building
    Rails, NodeJS, Go and Java applications with Ubuntu, Alpine or distroless stacks.
    Here are some of the stacks we've already tested:\n\n<table class=\"table:stripe\"><thead><tr><td><b>Builder
    Name<b></td><td><b>Image</br>Size</b><td><b>Languages/Frameworks</b></td></thead>\n<tbody>\n
    \ <tr>\n    <td>cloudfoundry/cnb:bionic</td>\n    <td>Small</td>\n    <td>Java,Node.js</td>\n
    \ </tr>\n  <tr>\n    <td>cloudfoundry/cnb:cflinuxfs3</td>\n    <td>Large</td>\n
    \   <td>Java, Node.js, Python</td>\n  </tr>\n  <tr>\n    <td>cloudfoundry/cnb:tiny</td>\n
    \   <td>Tiny</td>\n    <td>Go</td>\n  </tr>\n  <tr>\n    <td>heroku/buildpacks:18</td>\n
    \   <td>Large</td>\n    <td>Ruby, Java, Node.js, Python, Golang, PHP</td>\n  </tr>\n</tbody>\n</table>\n\nIf
    you are unsure which to use, we recommend the \"heroku/buildpacks:18\" option
    for its wide coverage.\n  \n\n<p class=\"callout\">\n\n_Want to learn more about
    Fly? Head over to our [Fly Docs](/docs/) for lots more, including a [Hands On](/docs/hands-on/start/)
    where you can get a free account and deploy your first app today._\n\n</p>\n"
- :id: blog-turboku
  :date: '2020-02-05'
  :category: blog
  :title: Turboku - The Art Of Faster Heroku&nbsp;Apps
  :author: dj
  :thumbnail:
  :alt:
  :link: blog/turboku
  :path: blog/2020-02-05
  :body: "\n\n<p class=\"lead\">\n\nWhen creating a new platform for globally faster
    apps, one question often comes up: <mark>“But I already have an application on
    another platform, why should I use yours?”</mark> Easy enough. Fly runs your application
    around the globe—in datacenters that are close to your users—through an edge network
    which is optimized for faster TLS handling.\n\n</p>\n\nThe next question is how
    can they move their application the Fly. For that we've created \"Turboku\", a
    simple way to bring your Heroku apps to Fly. What we do is take your Heroku web
    dynos and turn them into Fly applications automatically. \n\n<figure class=\"embed\">\n<iframe
    src=\"https://player.vimeo.com/video/393963664\" width=\"640\" height=\"360\"
    frameborder=\"0\" allow=\"autoplay; fullscreen\" allowfullscreen></iframe>\n</figure>\n\nThat
    way they can be deployed to our global edge network of fast TLS nodes. Your users'
    requests will automatically land at the closest datacenter in the Fly network.
    You'll only have to use one IP address which works globally and handles all the
    routing for you - well, two if you are doing IPv6. \n\nThe TLS negotiation times
    will be less too in part because the DNS lookup will be quicker (with only one
    IP address to resolve), and in part because the TLS terminates closer to the user.
    \n\nFly even makes getting a modern ECDSA TLS certificate easy - it's automatic
    for your Fly domain and just needs a DNS record change to be enabled for any custom
    domain. \n\n## The Tale of the Tape\n\nTo see how this works in practice, we benchmarked
    Heroku and Fly performance around the world, averaging results and comparing the
    time taken for each stage of the connection using an existing application. We
    also recorded the total time for the network connection,  the time for the full
    request to be processed and, as Fly supports Redis caching, the time taken when
    a cache is in use.\n\n<figure>\n<table class=\"table:stripe table:stretch table:pad
    table:outline text:md text:no-wrap m:0\">\n<thead>\n<tr>\n<th>\n</th>\n<th style=\"text-align:right\">Heroku
    (ms)</th>\n<th style=\"text-align:right\">Fly (ms)</th>\n<th style=\"text-align:right\">Change
    (ms)</th>\n<th style=\"text-align:right\">Change %</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td
    style=\"text-align:right\">DNS</td>\n<td style=\"text-align:right\">25.20</td>\n<td
    style=\"text-align:right\">0.00</td>\n<td style=\"text-align:right\">-25.2</td>\n<td
    style=\"text-align:right\">-100.00%</td>\n</tr>\n<tr>\n<td style=\"text-align:right\">TCP
    Connection</td>\n<td style=\"text-align:right\">103.20</td>\n<td style=\"text-align:right\">24.11</td>\n<td
    style=\"text-align:right\">-78.09</td>\n<td style=\"text-align:right\">-75.66%</td>\n</tr>\n\n<tr>\n<td
    style=\"text-align:right\">TLS</td>\n<td style=\"text-align:right\">225.28</td>\n<td
    style=\"text-align:right\">33.41</td>\n<td style=\"text-align:right\">-191.86</td>\n<td
    style=\"text-align:right\">-85.17%</td>\n</tr>\n<tr>\n<td style=\"text-align:right\">TTFB</td>\n<td
    style=\"text-align:right\">464.07</td>\n<td style=\"text-align:right\">469.81</td>\n<td
    style=\"text-align:right\">5.74</td>\n<td style=\"text-align:right\">1.24%</td>\n</tr>\n<tr>\n<td
    style=\"text-align:right\">Network</td>\n<td style=\"text-align:right\">353.68</td>\n<td
    style=\"text-align:right\">58.53</td>\n<td style=\"text-align:right\">-295.15</td>\n<td
    style=\"text-align:right\">-83.45%</td>\n</tr>\n<tr>\n<td style=\"text-align:right\">Full
    Request</td>\n<td style=\"text-align:right\">817.75</td>\n<td style=\"text-align:right\">528.34</td>\n<td
    style=\"text-align:right\">-289.41</td>\n<td style=\"text-align:right\">-35.39%</td>\n</tr>\n<tr>\n<td
    style=\"text-align:right\">With Cache</td>\n<td style=\"text-align:right\">817.75</td>\n<td
    style=\"text-align:right\">180.16</td>\n<td style=\"text-align:right\">-637.14</td>\n<td
    style=\"text-align:right\">-77.91%</td>\n</tr>\n</table>\n</figure>\n\n## Try
    Turboku \n\nEvery component of the Fly platform can make your Heroku applications
    faster for the most important people of all. Your users. So learn to Fly today,
    it's free.\n\n<p class=\"callout\">\n\nIf you have an Heroku app you want to make
    fast, you can start it up on Fly by going to [fly.io/heroku](https://fly.io/heroku)
    or you can read our new application guide [Speed up a Heroku App](/docs/app-guides/speed-up-a-heroku-app/)
    for a complete guide to the process - including how to connect all the good stuff
    up to your own custom domains.\n\n</p>\n\n_Want to learn more about Fly? Head
    over to our [Fly Docs](/docs/) for lots more, including a [Hands On](/docs/hands-on/start/)
    where you can get a free account and deploy your first app today._\n\n<small>Article
    updated 28/Feb/2020 with benchmark data and video.</small>\n"
- :id: blog-command-lines-flyctl-and-fly
  :date: '2020-01-29'
  :category: blog
  :title: Command Lines, Flyctl and Fly
  :author: dj
  :thumbnail:
  :alt:
  :link: blog/command-lines-flyctl-and-fly
  :path: blog/2020-01-29
  :body: "\n\n<p class=\"lead\">\n\nWe've just launched the [Fly Global Application
    Platform](https://fly.io/), designed to make all your applications fast and global.
    You can [dive into Fly](https://fly.io/docs/) and sign up right now for free simply
    by installing our new command-line tool, flyctl. With the new Fly Global Application
    Platform, we're doing everything command line first. \n\n</p>\n\n## Why Command
    Lines?\n\nAs we've developed Fly, we've seen numerous benefits that come from
    driving the service through the command line. Our newest addition to our command
    armory is the [`flyctl`](/docs/flyctl/) command which now, quite literally, covers
    the entire lifecycle of a Fly application, and beyond. The web interface will
    continue to play its part, but the command line is king.\n\n## What Makes a Command-Line-Controlled
    Service Compelling?\n\nIntegration. A command-line-driven service can be integrated
    into all modern workflows and toolchains. That's something that cannot be said
    of web-driven applications which will always need an API layer to even start down
    the integration path. Command lines are simple, expressive and direct ways to
    perform a task.\n\n## Delivering an Effective Command Line Tool\n\nPrevious command-line
    technology at Fly was built on Node. Node is great for many things but there's
    a lot of runtime to carry around to get it doing the job you want it to do. So
    we moved to Go for `flyctl` and it's all rather splendid because now we can make
    a self-contained binary that you just need to download and run. One binary for
    macOS, 32- and 64-bit binaries for Linux and 32- and 64-bit executables for Windows.
    \n\nCreating `flyctl` in Go also allowed us to adopt the same command-line engine,
    [Cobra](https://github.com/spf13/cobra), that popular cloud tooling like Kubernetes,
    Moby/Docker and Hugo use. That helps make it an already familiar environment for
    developers to work in with the command/subcommand style of requests and contextual
    help at all levels of that hierarchy.\n\n## Maintaining Focus\n\nCommand-line
    tools let developers work where they are, in the terminal and in their current
    directory. There's nothing more distracting than having to change from terminal
    to browser and back. It breaks the train of thought between different working
    modes, reaching for the mouse to click, then back to typing. \n\nWeb interfaces
    do have an essential role to play in the platform. Our thinking is that web-browser-based
    elements of the user experience should be about displaying rich, informative dashboards
    and other metric displays. \n\nWeb-based dashboards and metrics are well suited
    to run alongside a richly interactive command-line experience.\n\n## Command Line
    Reproducability\n\nIf you've ever talked someone through repeating an interaction
    with a GUI, you'll know how hard it is—no matter how precise the direction or
    explicit the instruction. On the other hand, instructions for the command line
    are self-contained and self-explanatory. And those instructions can be scripted
    and repeated as many times as needed. For support and operations, these attributes
    are invaluable.\n\n## Developers First and Always Open\n\nWe've put `flyctl` and
    the Fly experience together with a developer-first ethos. That means picking the
    right tools for each part of the task, making things reproducible and documentable,
    and most of all, making it all enjoyable.\n\nAnd if there's something you want
    to improve or have feedback about, the [Flyctl Github repository](https://github.com/superfly/flyctl)
    is open for your input.\n\n_Want to learn more about Fly? Head over to our [Fly
    Docs](https://fly.io/docs/) for lots more, including a [Hands On](https://fly.io/docs/hands-on/start/)
    so you can get a free account and deploy your first app today._\n\n\n\n\n\n\n\n\n"
- :id: blog-welcome-to-fly
  :date: '2020-01-22'
  :category: blog
  :title: Welcome To Fly In 2020
  :author: dj
  :thumbnail:
  :alt:
  :link: blog/welcome-to-fly
  :path: blog/2020-01-22
  :body: |2+


    <p class="lead">

    Welcome to Fly, the home of the Fly Global Application Platform. And welcome to the Fly Blog, where you can learn about the power of Fly to bring your applications closer to your users, wherever they are in the world.

    </p>

    Check out our [all new documentation](https://fly.io/docs/) for the Fly platform which covers everything from creating Apps you can deploy to Fly to how to take existing Docker images and setting them free across the globe.

    <p class="callout">

    If you have any questions, drop a line to us on [support@fly.io](mailto:support@fly.io).

    </p>

...
